import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

from gensim import corpora
from gensim.models import Phrases
from gensim.models import Word2Vec
from gensim.models import TfidfModel
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from gensim.summarization import keywords

from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer

import csv
from six import iteritems

#Set up tokenizer and stop words
tokenizer = RegexpTokenizer('\w+|\$[\d\.]+|\S+')
stops = [word for word in stopwords.words('english')]
stops += ["=", "->", ".", ","]

def tokenize(text):
    return [token for token in simple_preprocess(text) if token not in STOPWORDS]

bigram = Phrases([tokenize(line) for line in open('./data/corpus-abstracts.csv')])
model = Word2Vec([bigram[tokenize(line)] for line in open('./data/corpus-abstracts.csv')], min_count=10,sorted_vocab=1,sample=0.1)  # default value is 5

words = "In this work a method for statistical analysis of time series is proposed, which is used to obtain solutions to some classical problems of mathematical statistics under the only assumption that the process generating the data is stationary ergodic. Namely, three problems are considered: goodness-of-fit (or identity) testing, process classification, and the change point problem. For each of the problems a test is constructed that is asymptotically accurate for the case when the data is generated by stationary ergodic processes. The tests are based on empirical estimates of distributional distance."

keys = keywords(words).split('\n')

print model.most_similar(positive=[word for word in keys if word in model.vocab], topn=20, restrict_vocab=10000)