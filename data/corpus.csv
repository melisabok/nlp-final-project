"In this paper, we present a new linear programming (LP) formulation of the Traveling Salesman Problem (TSP). The proposed model has O(n^8) variables and O(n^7) constraints, where n is the number of cities. Our numerical experimentation shows that computational times for the proposed linear program are several orders of magnitude smaller than those for the existing model [3]."
"In this paper, we present a new, graph-based modeling approach and a polynomial-sized linear programming (LP) formulation of the Boolean satisfiability problem (SAT). The approach is illustrated with a numerical example."
"Pseudorandomness has played a central role in modern cryptography, finding theoretical and practical applications to various fields of computer science. A function that generates pseudorandom strings from shorter but truly random seeds is known as a pseudorandom generator. Our generators are designed to fool languages (or equivalently, Boolean-valued functions). In particular, our generator fools advised context-free languages, namely, context-free languages assisted by external information known as advice, and moreover our generator is made almost one-to-one, stretching $n$-bit seeds to $n+1$ bits. We explicitly construct such a pseudorandom generator, which is computed by a deterministic Turing machine using logarithmic space and also belongs to CFLMV(2)/n---a functional extension of the 2-conjunctive closure of CFL with the help of appropriate deterministic advice. In contrast, we show that there is no almost one-to-one pseudorandom generator against context-free languages if we demand that it should be computed by a nondeterministic pushdown automaton equipped with a write-only output tape. Our generator naturally extends known pseudorandom generators against advised regular languages. Our proof of the CFL/n-pseudorandomness of the generator is quite elementary, and in particular, one part of the proof utilizes a special feature of the behaviors of nondeterministic pushdown automata, called a swapping property, which is interesting in its own right, generalizing the swapping lemma for context-free languages."
"The edit distance problem is a classical fundamental problem in computer science in general, and in combinatorial pattern matching in particular. The standard dynamic programming solution for this problem computes the edit-distance between a pair of strings of total length O(N) in O(N^2) time. To this date, this quadratic upper-bound has never been substantially improved for general strings. However, there are known techniques for breaking this bound in case the strings are known to compress well under a particular compression scheme. The basic idea is to first compress the strings, and then to compute the edit distance between the compressed strings. As it turns out, practically all known o(N^2) edit-distance algorithms work, in some sense, under the same paradigm described above. It is therefore natural to ask whether there is a single edit-distance algorithm that works for strings which are compressed under any compression scheme. A rephrasing of this question is to ask whether a single algorithm can exploit the compressibility properties of strings under any compression method, even if each string is compressed using a different compression. In this paper we set out to answer this question by using straight line programs. These provide a generic platform for representing many popular compression schemes including the LZ-family, Run-Length Encoding, Byte-Pair Encoding, and dictionary methods. For two strings of total length N having straight-line program representations of total size n, we present an algorithm running in O(nN log(N/n)) time for computing the edit-distance of these two strings under any rational scoring function, and an O(n^{2/3}N^{4/3}) time algorithm for arbitrary scoring functions. Our new result, while providing a signi cant speed up for highly compressible strings, does not surpass the quadratic time bound even in the worst case scenario."
"As one of the newest members in the field of arti?cial immune systems (AIS), the Dendritic Cell Algorithm (DCA) is based on behavioural models of natural dendritic cells (DCs). Unlike other AIS, the DCA does not rely on training data, instead domain or expert knowledge is required to predetermine the mapping between input signals from a particular instance to the three categories used by the DCA. This data preprocessing phase has received the criticism of having manually over-?tted the data to the algorithm, which is undesirable. Therefore, in this paper we have attempted to ascertain if it is possible to use principal component analysis (PCA) techniques to automatically categorise input data while still generating useful and accurate classication results. The integrated system is tested with a biometrics dataset for the stress recognition of automobile drivers. The experimental results have shown the application of PCA to the DCA for the purpose of automated data preprocessing is successful."
"Knowledge-based programs provide an abstract level of description of protocols in which agent actions are related to their states of knowledge. The paper describes how epistemic model checking technology may be applied to discover and verify concrete implementations based on this abstract level of description. The details of the implementations depend on the specific context of use of the protocol. The knowledge-based approach enables the implementations to be optimized relative to these conditions of use. The approach is illustrated using extensions of the Dining Cryptographers protocol, a security protocol for anonymous broadcast."
"Recently, a chaos-based image encryption algorithm called MCKBA (Modified Chaotic-Key Based Algorithm) was proposed. This paper analyzes the security of MCKBA and finds that it can be broken with a differential attack, which requires only four chosen plain-images. Performance of the attack is verified by experimental results. In addition, some defects of MCKBA, including insensitivity with respect to changes of plain-image/secret key, are reported."
"Rapid advances in image acquisition and storage technology underline the need for algorithms that are capable of solving large scale image processing and computer-vision problems. The minimum cut problem plays an important role in processing many of these imaging problems such as, image and video segmentation, stereo vision, multi-view reconstruction and surface fitting. While several min-cut/max-flow algorithms can be found in the literature, their performance in practice has been studied primarily outside the scope of computer vision. We present here the results of a comprehensive computational study, in terms of execution times and memory utilization, of four recently published algorithms, which optimally solve the {\em s-t} cut and maximum flow problems: (i) Goldberg's and Tarjan's {\em Push-Relabel}; (ii) Hochbaum's {\em pseudoflow}; (iii) Boykov's and Kolmogorov's {\em augmenting paths}; and (iv) Goldberg's {\em partial augment-relabel}. Our results demonstrate that the {\em Hochbaum's pseudoflow} algorithm, is faster and utilizes less memory than the other algorithms on all problem instances investigated."
"Several challenging problem in clustering, partitioning and imaging have traditionally been solved using the ""spectral technique"". These problems include the normalized cut problem, the graph expander ratio problem, the Cheeger constant problem and the conductance problem. These problems share several common features: all seek a bipartition of a set of elements; the problems are formulated as a form of ratio cut; the formulation as discrete optimization is shown here to be equivalent to a quadratic ratio, sometimes referred to as the Raleigh ratio, on discrete variables and a single sum constraint which we call the balance or orthogonality constraint; when the discrete nature of the variables is disregarded, the continuous relaxation is solved by the spectral method. Indeed the spectral relaxation technique is a dominant method providing an approximate solution to these problems.   We propose an algorithm for these problems which involves a relaxation of the orthogonality constraint only. This relaxation is shown here to be solved optimally, and in strongly polynomial time, in O(mn log((n^2) / m) for a graph on $n$ nodes and $m$ edges. The algorithm, using HPF (Hochbaum's Pseudo-Flow) as subroutine, is efficient enough to be used to solve these bi-partitioning problems on millions of elements and more than 300 million edges within less than 10 minutes. It is also demonstrated, via a preliminary experimental study, that the results of the combinatorial algorithm proposed often improve dramatically on the quality of the results of the spectral method."
"By using of analytical multi-logic expresses in conjunction with non-deterministic Turing machine the proposition was proved that algorithm of deterministic Turing counter machine of polynomial time complexity can be decreased to the algorithm of linear time complexity in non-deterministic Turing counter machine. Furthermore, it was shown that existence of reduction of polynomial time complexity to the linear time complexity by switching from deterministic to non-deterministic Turing machine for string recognition imply P equals to NP. Analytical generation functions of higher order logic were used for finding of k value in Fagin's R. Theorem 24."
"Approximation algorithms for classical constraint satisfaction problems are one of the main research areas in theoretical computer science. Here we define a natural approximation version of the QMA-complete local Hamiltonian problem and initiate its study. We present two main results. The first shows that a non-trivial approximation ratio can be obtained in the class NP using product states. The second result (which builds on the first one), gives a polynomial time (classical) algorithm providing a similar approximation ratio for dense instances of the problem. The latter result is based on an adaptation of the ""exhaustive sampling method"" by Arora et al. [J. Comp. Sys. Sci. 58, p.193 (1999)] to the quantum setting, and might be of independent interest."
"We investigate an optimal scheduling problem in a discrete-time system of L parallel queues that are served by K identical, randomly connected servers. Each queue may be connected to a subset of the K servers during any given time slot. This model has been widely used in studies of emerging 3G/4G wireless systems. We introduce the class of Most Balancing (MB) policies and provide their mathematical characterization. We prove that MB policies are optimal; we define optimality as minimization, in stochastic ordering sense, of a range of cost functions of the queue lengths, including the process of total number of packets in the system. We use stochastic coupling arguments for our proof. We introduce the Least Connected Server First/Longest Connected Queue (LCSF/LCQ) policy as an easy-to-implement approximation of MB policies. We conduct a simulation study to compare the performance of several policies. The simulation results show that: (a) in all cases, LCSF/LCQ approximations to the MB policies outperform the other policies, (b) randomized policies perform fairly close to the optimal one, and, (c) the performance advantage of the optimal policy over the other simulated policies increases as the channel connectivity probability decreases and as the number of servers in the system increases."
"The quality of image encryption is commonly measured by the Shannon entropy over the ciphertext image. However, this measurement does not consider to the randomness of local image blocks and is inappropriate for scrambling based image encryption methods. In this paper, a new information entropy-based randomness measurement for image encryption is introduced which, for the first time, answers the question of whether a given ciphertext image is sufficiently random-like. It measures the randomness over the ciphertext in a fairer way by calculating the averaged entropy of a series of small image blocks within the entire test image. In order to fulfill both quantitative and qualitative measurement, the expectation and the variance of this averaged block entropy for a true-random image are strictly derived and corresponding numerical reference tables are also provided. Moreover, a hypothesis test at significance?-level is given to help accept or reject the hypothesis that the test image is ideally encrypted/random-like. Simulation results show that the proposed test is able to give both effectively quantitative and qualitative results for image encryption. The same idea can also be applied to measure other digital data, like audio and video."
"One of the most prominent challenges in clustering is ""the user's dilemma,"" which is the problem of selecting an appropriate clustering algorithm for a specific task. A formal approach for addressing this problem relies on the identification of succinct, user-friendly properties that formally capture when certain clustering methods are preferred over others.   Until now these properties focused on advantages of classical Linkage-Based algorithms, failing to identify when other clustering paradigms, such as popular center-based methods, are preferable. We present surprisingly simple new properties that delineate the differences between common clustering paradigms, which clearly and formally demonstrates advantages of center-based approaches for some applications. These properties address how sensitive algorithms are to changes in element frequencies, which we capture in a generalized setting where every element is associated with a real-valued weight."
"Hilbert's two-dimensional space-filling curve is appreciated for its good locality properties for many applications. However, it is not clear what is the best way to generalize this curve to filling higher-dimensional spaces. We argue that the properties that make Hilbert's curve unique in two dimensions, are shared by 10694807 structurally different space-filling curves in three dimensions. These include several curves that have, in some sense, better locality properties than any generalized Hilbert curve that has been considered in the literature before."
"Finite physical systems have only a finite amount of distinct state. This finiteness is fundamental in statistical mechanics, where the maximum number of distinct states compatible with macroscopic constraints defines entropy. Here we show that finiteness of distinct state is similarly fundamental in ordinary mechanics: energy and momentum are defined by the maximum number of distinct states possible in a given time or distance. More generally, any moment of energy or momentum bounds distinct states in time or space. These results generalise both the Nyquist bandwidth-bound on distinct values in classical signals, and quantum uncertainty bounds. The new certainty bounds are achieved by finite-bandwidth evolutions in which time and space are effectively discrete, including quantum evolutions that are effectively classical. Since energy and momentum count distinct states, they are defined in classical finite-state dynamics, and they relate classical relativity to finite-state evolution."
"Gaussian and quadratic approximations of message passing algorithms on graphs have attracted considerable recent attention due to their computational simplicity, analytic tractability, and wide applicability in optimization and statistical inference problems. This paper presents a systematic framework for incorporating such approximate message passing (AMP) methods in general graphical models. The key concept is a partition of dependencies of a general graphical model into strong and weak edges, with the weak edges representing interactions through aggregates of small, linearizable couplings of variables. AMP approximations based on the Central Limit Theorem can be readily applied to the weak edges and integrated with standard message passing updates on the strong edges. The resulting algorithm, which we call hybrid generalized approximate message passing (HyGAMP), can yield significantly simpler implementations of sum-product and max-sum loopy belief propagation. By varying the partition of strong and weak edges, a performance--complexity trade-off can be achieved. Group sparsity and multinomial logistic regression problems are studied as examples of the proposed methodology."
"We study the converse and achievability for the degrees of freedom of the multicellular multiple-input multiple-output (MIMO) multiple access channel (MAC) with constant channel coefficients. We assume L>1 homogeneous cells with K>0 users per cell where the users have M antennas and the base stations are equipped with N antennas. The degrees of freedom outer bound for this L-cell and K-user MIMO MAC is formulated. The characterized outer bound uses insight from a limit on the total degrees of freedom for the L-cell heterogeneous MIMO network. We also show through an example that a scheme selecting a transmitter and performing partial message sharing outperforms a multiple distributed transmission strategy in terms of the total degrees of freedom. Simple linear schemes attaining the outer bound (i.e., those achieving the optimal degrees of freedom) are explores for a few cases. The conditions for the required spatial dimensions attaining the optimal degrees of freedom are characterized in terms of K, L, and the number of transmit streams. The optimal degrees of freedom for the two-cell MIMO MAC are examined by using transmit zero forcing and null space interference alignment and subsequently, simple receive zero forcing is shown to provide the optimal degrees of freedom for L>1. By the uplink and downlink duality, the degrees of freedom results in this paper are also applicable to the downlink. In the downlink scenario, we study the degrees of freedom of L-cell MIMO interference channel exploring multiuser diversity. Strong convergence modes of the instantaneous degrees of freedom as the number of users increases are characterized."
"The paper serves as the first contribution towards the development of the theory of efficiency: a unifying framework for the currently disjoint theories of information, complexity, communication and computation. Realizing the defining nature of the brute force approach in the fundamental concepts in all of the above mentioned fields, the paper suggests using efficiency or improvement over the brute force algorithm as a common unifying factor necessary for the creation of a unified theory of information manipulation. By defining such diverse terms as randomness, knowledge, intelligence and computability in terms of a common denominator we are able to bring together contributions from Shannon, Levin, Kolmogorov, Solomonoff, Chaitin, Yao and many others under a common umbrella of the efficiency theory."
"A weak odd dominated (WOD) set in a graph is a subset B of vertices for which there exists a distinct set of vertices C such that every vertex in B has an odd number of neighbors in C. We point out the connections of weak odd domination with odd domination, [sigma,rho]-domination, and perfect codes. We introduce bounds on \kappa(G), the maximum size of WOD sets of a graph G, and on \kappa'(G), the minimum size of non WOD sets of G. Moreover, we prove that the corresponding decision problems are NP-complete. The study of weak odd domination is mainly motivated by the design of graph-based quantum secret sharing protocols: a graph G of order n corresponds to a secret sharing protocol which threshold is \kappa_Q(G) = max(\kappa(G), n-\kappa'(G)). These graph-based protocols are very promising in terms of physical implementation, however all such graph-based protocols studied in the literature have quasi-unanimity thresholds (i.e. \kappa_Q(G)=n-o(n) where n is the order of the graph G underlying the protocol). In this paper, we show using probabilistic methods, the existence of graphs with smaller \kappa_Q (i.e. \kappa_Q(G)< 0.811n where n is the order of G). We also prove that deciding for a given graph G whether \kappa_Q(G)< k is NP-complete, which means that one cannot efficiently double check that a graph randomly generated has actually a \kappa_Q smaller than 0.811n."
"Consider a dynamical system $u \mapsto x, \dot{x} = f_{nl}(x,u)$ where $f_{nl}$ is a nonlinear (convex or nonconvex) function, or a combination of nonlinear functions that can eventually switch. We present, in this preliminary work, a generalization of the standard model-free control, that can either control the dynamical system, given an output reference trajectory, or optimize the dynamical system as a derivative-free optimization based ""extremum-seeking"" procedure. Multiple applications are presented and the robustness of the proposed method is studied in simulation."
"Puiseux series are power series in which the exponents can be fractional and/or negative rational numbers. Several computer algebra systems have one or more built-in or loadable functions for computing truncated Puiseux series. Some are generalized to allow coefficients containing functions of the series variable that are dominated by any power of that variable, such as logarithms and nested logarithms of the series variable. Some computer algebra systems also have built-in or loadable functions that compute infinite Puiseux series. Unfortunately, there are some little-known pitfalls in computing Puiseux series. The most serious of these is expansions within branch cuts or at branch points that are incorrect for some directions in the complex plane. For example with each series implementation accessible to you:   Compare the value of (z^2 + z^3)^(3/2) with that of its truncated series expansion about z = 0, approximated at z = -0.01. Does the series converge to a value that is the negative of the correct value?   Compare the value of ln(z^2 + z^3) with its truncated series expansion about z = 0, approximated at z = -0.01 + 0.1i. Does the series converge to a value that is incorrect by 2pi i?   Compare arctanh(-2 + ln(z)z) with its truncated series expansion about z = 0, approximated at z = -0.01. Does the series converge to a value that is incorrect by about pi i?   At the time of this writing, most implementations that accommodate such series exhibit such errors. This article describes how to avoid these errors both for manual derivation of series and when implementing series packages."
"The local minimum degree of a graph is the minimum degree reached by means of a series of local complementations. In this paper, we investigate on this quantity which plays an important role in quantum computation and quantum error correcting codes. First, we show that the local minimum degree of the Paley graph of order p is greater than sqrt{p} - 3/2, which is, up to our knowledge, the highest known bound on an explicit family of graphs. Probabilistic methods allows us to derive the existence of an infinite number of graphs whose local minimum degree is linear in their order with constant 0.189 for graphs in general and 0.110 for bipartite graphs. As regards the computational complexity of the decision problem associated with the local minimum degree, we show that it is NP-complete and that there exists no k-approximation algorithm for this problem for any constant k unless P = NP."
"Scientific communities have adopted different conventions for ordering authors on publications. Are these choices inconsequential, or do they have significant influence on individual authors, the quality of the projects completed, and research communities at large? What are the trade-offs of using one convention over another? In order to investigate these questions, we formulate a basic two-player game theoretic model, which already illustrates interesting phenomena that can occur in more realistic settings.   We find that alphabetical ordering can improve research quality, while contribution-based ordering leads to a denser collaboration network and a greater number of publications. Contrary to the assumption that free riding is a weakness of the alphabetical ordering scheme, this phenomenon can occur under any contribution scheme, and the worst case occurs under contribution-based ordering. Finally, we show how authors working on multiple projects can cooperate to attain optimal research quality and eliminate free riding given either contribution scheme."
"The polynomial hierarchy plays a central role in classical complexity theory. Here, we define a quantum generalization of the polynomial hierarchy, and initiate its study. We show that not only are there natural complete problems for the second level of this quantum hierarchy, but that these problems are in fact hard to approximate. Using these techniques, we also obtain hardness of approximation for the class QCMA. Our approach is based on the use of dispersers, and is inspired by the classical results of Umans regarding hardness of approximation for the second level of the classical polynomial hierarchy [Umans, FOCS 1999]. The problems for which we prove hardness of approximation for include, among others, a quantum version of the Succinct Set Cover problem, and a variant of the local Hamiltonian problem with hybrid classical-quantum ground states."
"We show how to underapproximate the procedure summaries of recursive programs over the integers using off-the-shelf analyzers for non-recursive programs. The novelty of our approach is that the non-recursive program we compute may capture unboundedly many behaviors of the original recursive program for which stack usage cannot be bounded. Moreover, we identify a class of recursive programs on which our method terminates and returns the precise summary relations without underapproximation. Doing so, we generalize a similar result for non-recursive programs to the recursive case. Finally, we present experimental results of an implementation of our method applied on a number of examples."
"We propose a type system for a calculus of contracting processes. Processes can establish sessions by stipulating contracts, and then can interact either by keeping the promises made, or not. Type safety guarantees that a typeable process is honest - that is, it abides by the contracts it has stipulated in all possible contexts, even in presence of dishonest adversaries. Type inference is decidable, and it allows to safely approximate the honesty of processes using either synchronous or asynchronous communication."
"Chaotic behavior arises from very simple non-linear dynamical equation of logistic map which makes it was used often in designing chaotic image encryption schemes. However, some properties of chaotic maps can also facilitate cryptanalysis especially when they are implemented in digital domain. Utilizing stable distribution of the chaotic states generated by iterating the logistic map, this paper presents a typical example to show insecurity of an image encryption scheme using chaotic logistic map. This work will push encryption and chaos be combined in a more effective way."
"Matrix completion has been well studied under the uniform sampling model and the trace-norm regularized methods perform well both theoretically and numerically in such a setting. However, the uniform sampling model is unrealistic for a range of applications and the standard trace-norm relaxation can behave very poorly when the sampling distribution is non-uniform.   In this paper we propose and analyze a max-norm constrained empirical risk minimization method for noisy matrix completion under a general sampling model. The optimal rate of convergence is established under the Frobenius norm loss in the context of approximately low-rank matrix reconstruction. It is shown that the max-norm constrained method is minimax rate-optimal and it yields a unified and robust approximate recovery guarantee, with respect to the sampling distributions. The computational effectiveness of this method is also studied, based on a first-order algorithm for solving convex programs involving a max-norm constraint."
"For a physical layer message authentication procedure based on the comparison of channel estimates obtained from the received messages, we focus on an outer bound on the type I/II error probability region. Channel estimates are modelled as multivariate Gaussian vectors, and we assume that the attacker has only some side information on the channel estimate, which he does not know directly. We derive the attacking strategy that provides the tightest bound on the error region, given the statistics of the side information. This turns out to be a zero mean, circularly symmetric Gaussian density whose correlation matrices may be obtained by solving a constrained optimization problem. We propose an iterative algorithm for its solution: Starting from the closed form solution of a relaxed problem, we obtain, by projection, an initial feasible solution; then, by an iterative procedure, we look for the fixed point solution of the problem. Numerical results show that for cases of interest the iterative approach converges, and perturbation analysis shows that the found solution is a local minimum."
"Unions of subspaces provide a powerful generalization to linear subspace models for collections of high-dimensional data. To learn a union of subspaces from a collection of data, sets of signals in the collection that belong to the same subspace must be identified in order to obtain accurate estimates of the subspace structures present in the data. Recently, sparse recovery methods have been shown to provide a provable and robust strategy for exact feature selection (EFS)--recovering subsets of points from the ensemble that live in the same subspace. In parallel with recent studies of EFS with L1-minimization, in this paper, we develop sufficient conditions for EFS with a greedy method for sparse signal recovery known as orthogonal matching pursuit (OMP). Following our analysis, we provide an empirical study of feature selection strategies for signals living on unions of subspaces and characterize the gap between sparse recovery methods and nearest neighbor (NN)-based approaches. In particular, we demonstrate that sparse recovery methods provide significant advantages over NN methods and the gap between the two approaches is particularly pronounced when the sampling of subspaces in the dataset is sparse. Our results suggest that OMP may be employed to reliably recover exact feature sets in a number of regimes where NN approaches fail to reveal the subspace membership of points in the ensemble."
"In the paper, the model of the society represented by a social network and the model of a multi-agent system built on the basis of this, is presented. The particular aim of the system is to predict the evolution of a society and an analysis of the communities that appear, their characteristic features and reasons for coming into being. As an example of application, an analysis was made of a social portal which makes it possible to offer and reserve places in rooms for travelling tourists"
"We give a semantics for the lambda-calculus based on a topological duality theorem in nominal sets. A novel interpretation of lambda is given in terms of adjoints, and lambda-terms are interpreted absolutely as sets (no valuation is necessary)."
"In this paper we present a framework for designing algorithms in shared memory of GPUs without incurring memory bank conflicts. Using our framework we develop the first comparison-based shared memory sorting algorithm that incurs no bank conflicts. It can be used as a subroutine for GPU sorting algorithms to replace current use of sorting networks in shared memory. Using our bank conflict free shared memory sorting subroutine as a black box, we design BCFMergesort, an algorithm for merging sorted streams of data that are larger than shared memory. Our algorithm performs all accesses to global memory in coalesced manner and incurs no bank conflicts during the merge."
"The perception of consonance/dissonance of musical harmonies is strongly correlated with their periodicity. This is shown in this article by consistently applying recent results from psychophysics and neuroacoustics, namely that the just noticeable difference between pitches for humans is about 1% for the musically important low frequency range and that periodicities of complex chords can be detected in the human brain. Based thereon, the concepts of relative and logarithmic periodicity with smoothing are introduced as powerful measures of harmoniousness. The presented results correlate significantly with empirical investigations on the perception of chords. Even for scales, plausible results are obtained. For example, all classical church modes appear in the front ranks of all theoretically possible seven-tone scales."
"It is well-known that deciding equivalence of logic circuits is a coNP-complete problem. As a corollary, the problem of deciding weak equivalence of reversible circuits, i.e. ignoring the ancilla bits, is also coNP-complete. The complexity of deciding strong equivalence, including the ancilla bits, is less obvious and may depend on gate set. Here we use Barrington's theorem to show that deciding strong equivalence of reversible circuits built from the Fredkin gate is coNP-complete. This implies coNP-completeness of deciding strong equivalence for other commonly used universal reversible gate sets, including any gate set that includes the Toffoli or Fredkin gate."
"We introduce a new model of competition on growing networks. This extends the preferential attachment model, with the key property that node choices evolve simultaneously with the network. When a new node joins the network, it chooses neighbours by preferential attachment, and selects its type based on the number of initial neighbours of each type. The model is analysed in detail, and in particular, we determine the possible proportions of the various types in the limit of large networks. An important qualitative feature we find is that, in contrast to many current theoretical models, often several competitors will coexist. This matches empirical observations in many real-world networks."
"Let $X$ and $Y$ be finite simplicial sets (e.g. finite simplicial complexes), both equipped with a free simplicial action of a finite group $G$. Assuming that $Y$ is $d$-connected and $\dim X\le 2d$, for some $d\geq 1$, we provide an algorithm that computes the set of all equivariant homotopy classes of equivariant continuous maps $|X|\to|Y|$; the existence of such a map can be decided even for $\dim X\leq 2d+1$. For fixed $G$ and $d$, the algorithm runs in polynomial time. This yields the first algorithm for deciding topological embeddability of a $k$-dimensional finite simplicial complex into $\mathbb{R}^n$ under the conditions $k\leq\frac 23 n-1$.   More generally, we present an algorithm that, given a lifting-extension problem satisfying an appropriate stability assumption, computes the set of all homotopy classes of solutions. This result is new even in the non-equivariant situation."
"A roadmap for a semi-algebraic set $S$ is a curve which has a non-empty and connected intersection with all connected components of $S$. Hence, this kind of object, introduced by Canny, can be used to answer connectivity queries (with applications, for instance, to motion planning) but has also become of central importance in effective real algebraic geometry, since it is used in higher-level algorithms. In this paper, we provide a probabilistic algorithm which computes roadmaps for smooth and bounded real algebraic sets. Its output size and running time are polynomial in $(nD)^{n\log(d)}$, where $D$ is the maximum of the degrees of the input polynomials, $d$ is the dimension of the set under consideration and $n$ is the number of variables. More precisely, the running time of the algorithm is essentially subquadratic in the output size. Even under our assumptions, it is the first roadmap algorithm with output size and running time polynomial in $(nD)^{n\log(d)}$."
"We introduce a new approach for designing computationally efficient learning algorithms that are tolerant to noise, and demonstrate its effectiveness by designing algorithms with improved noise tolerance guarantees for learning linear separators.   We consider both the malicious noise model and the adversarial label noise model. For malicious noise, where the adversary can corrupt both the label and the features, we provide a polynomial-time algorithm for learning linear separators in $\Re^d$ under isotropic log-concave distributions that can tolerate a nearly information-theoretically optimal noise rate of $\eta = \Omega(\epsilon)$. For the adversarial label noise model, where the distribution over the feature vectors is unchanged, and the overall probability of a noisy label is constrained to be at most $\eta$, we also give a polynomial-time algorithm for learning linear separators in $\Re^d$ under isotropic log-concave distributions that can handle a noise rate of $\eta = \Omega\left(\epsilon\right)$.   We show that, in the active learning model, our algorithms achieve a label complexity whose dependence on the error parameter $\epsilon$ is polylogarithmic. This provides the first polynomial-time active learning algorithm for learning linear separators in the presence of malicious noise or adversarial label noise."
"Online and stochastic learning has emerged as powerful tool in large scale optimization. In this work, we generalize the Douglas-Rachford splitting (DRs) method for minimizing composite functions to online and stochastic settings (to our best knowledge this is the first time DRs been generalized to sequential version). We first establish an $O(1/\sqrt{T})$ regret bound for batch DRs method. Then we proved that the online DRs splitting method enjoy an $O(1)$ regret bound and stochastic DRs splitting has a convergence rate of $O(1/\sqrt{T})$. The proof is simple and intuitive, and the results and technique can be served as a initiate for the research on the large scale machine learning employ the DRs method. Numerical experiments of the proposed method demonstrate the effectiveness of the online and stochastic update rule, and further confirm our regret and convergence analysis."
"This paper develops an interpolatory framework for weighted-$\mathcal{H}_2$ model reduction of MIMO dynamical systems. A new representation of the weighted-$\mathcal{H}_2$ inner products in MIMO settings is introduced and used to derive associated first-order necessary conditions satisfied by optimal weighted-$\mathcal{H}_2$ reduced-order models. Equivalence of these new interpolatory conditions with earlier Riccati-based conditions given by Halevi is also shown. An examination of realizations for equivalent weighted-$\mathcal{H}_2$ systems leads then to an algorithm that remains tractable for large state-space dimension. Several numerical examples illustrate the effectiveness of this approach and its competitiveness with Frequency Weighted Balanced Truncation and an earlier interpolatory approach, the Weighted Iterative Rational Krylov Algorithm."
"In this paper, we introduce the notion of augmentation for polytopes and use it to show the error in two presumptions that have been key in arriving at over-reaching/over-scoped claims of ""impossibility"" in recent extended formulations (EF) developments. One of these presumptions is that: ""If Polytopes P and Q are described in the spaces of variables x and y respectively, and there exists a linear map x=Ay between the feasible sets of P and Q, then Q is an EF of P"". The other is: ""(An augmentation of Polytope A projects to Polytope B) ==> (The external descriptions of A and B are related)"". We provide counter-examples to these presumptions, and show that in general: (1) If polytopes can always be arbitrarily augmented for the purpose of establishing EF relations, then the notion of EF becomes degenerate/meaningless in some cases, and that: (2) The statement: ""(Polytope B is the projection of an augmentation of Polytope A) ==> (Polytope B is the projection of Polytope A)"" is not true in general (although, as we show, the converse statement, ""(B is the projection of A) ==> (B is the projection of every augmentation of A)"", is true in general). We illustrate some of the ideas using the minimum spanning tree problem, as well as the ""lower bounds"" developments in Fiorini et al. (2011; 2012), in particular."
"A $k$-uniform hypergraph $H = (V, E)$ is called $\ell$-orientable, if there is an assignment of each edge $e\in E$ to one of its vertices $v\in e$ such that no vertex is assigned more than $\ell$ edges. Let $H_{n,m,k}$ be a hypergraph, drawn uniformly at random from the set of all $k$-uniform hypergraphs with $n$ vertices and $m$ edges. In this paper we establish the threshold for the $\ell$-orientability of $H_{n,m,k}$ for all $k\ge 3$ and $\ell \ge 2$, i.e., we determine a critical quantity $c_{k, \ell}^*$ such that with probability $1-o(1)$ the graph $H_{n,cn,k}$ has an $\ell$-orientation if $c < c_{k, \ell}^*$, but fails doing so if $c > c_{k, \ell}^*$.   Our result has various applications including sharp load thresholds for cuckoo hashing, load balancing with guaranteed maximum load, and massive parallel access to hard disk arrays."
"Semantic measures are widely used today to estimate the strength of the semantic relationship between elements of various types: units of language (e.g., words, sentences, documents), concepts or even instances semantically characterized (e.g., diseases, genes, geographical locations). Semantic measures play an important role to compare such elements according to semantic proxies: texts and knowledge representations, which support their meaning or describe their nature. Semantic measures are therefore essential for designing intelligent agents which will for example take advantage of semantic analysis to mimic human ability to compare abstract or concrete objects. This paper proposes a comprehensive survey of the broad notion of semantic measure for the comparison of units of language, concepts or instances based on semantic proxy analyses. Semantic measures generalize the well-known notions of semantic similarity, semantic relatedness and semantic distance, which have been extensively studied by various communities over the last decades (e.g., Cognitive Sciences, Linguistics, and Artificial Intelligence to mention a few)."
We define a semantics for first-order logic with generalized quantifiers based on double teams. We also define and investigate a notion of a generalized atom. Such atoms can be used in order to define extensions of first-order logic with a team-based semantics. We also define a game semantics and compare it with the double team semantics.
"Finding a good compromise between the exploitation of known resources and the exploration of unknown, but potentially more profitable choices, is a general problem, which arises in many different scientific disciplines. We propose a stylized model for these exploration-exploitation situations, including population or economic growth, portfolio optimisation, evolutionary dynamics, or the problem of optimal pinning of vortices or dislocations in disordered materials. We find the exact growth rate of this model for tree-like geometries and prove the existence of an optimal migration rate in this case. Numerical simulations in the one-dimensional case confirm the generic existence of an optimum."
The Cornell Semantic Parsing Framework (SPF) is a learning and inference framework for mapping natural language to formal representation of its meaning.
"We define a simple, explicit map sending a morphism $f:M \rightarrow N$ of pointwise finite dimensional persistence modules to a matching between the barcodes of $M$ and $N$. Our main result is that, in a precise sense, the quality of this matching is tightly controlled by the lengths of the longest intervals in the barcodes of $\ker f$ and $\mathop{\mathrm{coker}} f$. As an immediate corollary, we obtain a new proof of the algebraic stability of persistence, a fundamental result in the theory of persistent homology. In contrast to previous proofs, ours shows explicitly how a $\delta$-interleaving morphism between two persistence modules induces a $\delta$-matching between the barcodes of the two modules. Our main result also specializes to a structure theorem for submodules and quotients of persistence modules, and yields a novel ""single-morphism"" characterization of the interleaving relation on persistence modules."
"We study the minimum number of constraints needed to formulate random instances of the maximum stable set problem via linear programs (LPs), in two distinct models. In the uniform model, the constraints of the LP are not allowed to depend on the input graph, which should be encoded solely in the objective function. There we prove a $2^{\Omega(n/ \log n)}$ lower bound with probability at least $1 - 2^{-2^n}$ for every LP that is exact for a randomly selected set of instances; each graph on at most n vertices being selected independently with probability $p \geq 2^{-\binom{n/4}{2}+n}$. In the non-uniform model, the constraints of the LP may depend on the input graph, but we allow weights on the vertices. The input graph is sampled according to the G(n, p) model. There we obtain upper and lower bounds holding with high probability for various ranges of p. We obtain a super-polynomial lower bound all the way from $p = \Omega(\log^{6+\varepsilon} / n)$ to $p = o (1 / \log n)$. Our upper bound is close to this as there is only an essentially quadratic gap in the exponent, which currently also exists in the worst-case model. Finally, we state a conjecture that would close this gap, both in the average-case and worst-case models."
"In this paper we study how certain families of aperiodic infinite words can be used to produce aperiodic pseudorandom number generators (PRNGs) with good statistical behavior. We introduce the \emph{well distributed occurrences} (WELLDOC) combinatorial property for infinite words, which guarantees absence of the lattice structure defect in related pseudorandom number generators. An infinite word $u$ on a $d$-ary alphabet has the WELLDOC property if, for each factor $w$ of $u$, positive integer $m$, and vector $\mathbf v\in\mathbb Z_{m}^{d}$, there is an occurrence of $w$ such that the Parikh vector of the prefix of $u$ preceding such occurrence is congruent to $\mathbf v$ modulo $m$. (The Parikh vector of a finite word $v$ over an alphabet $\mathcal A$ has its $i$-th component equal to the number of occurrences of the $i$-th letter of $\mathcal A$ in $v$.) We prove that Sturmian words, and more generally Arnoux-Rauzy words and some morphic images of them, have the WELLDOC property. Using the TestU01 and PractRand statistical tests, we moreover show that not only the lattice structure is absent, but also other important properties of PRNGs are improved when linear congruential generators are combined using infinite words having the WELLDOC property."
"In a network, a node is said to incur a delay if its encoding of each transmitted symbol involves only its received symbols obtained before the time slot in which the transmitted symbol is sent (hence the transmitted symbol sent in a time slot cannot depend on the received symbol obtained in the same time slot). A node is said to incur no delay if its received symbol obtained in a time slot is available for encoding its transmitted symbol sent in the same time slot. Under the classical model, every node in a discrete memoryless network (DMN) incurs a unit delay, and the capacity region of the DMN satisfies the well-known cut-set outer bound. In this paper, we propose a generalized model for the DMN where some nodes may incur no delay. Under our generalized model, we obtain a new cut-set outer bound, which is proved to be tight for some two-node DMN and is shown to subsume an existing cut-set bound for the causal relay network. In addition, we establish under the generalized model another cut-set outer bound on the positive-delay region -- the set of achievable rate tuples under the constraint that every node incurs a delay. We use the cut-set bound on the positive-delay region to show that for some two-node DMN under the generalized model, the positive-delay region is strictly smaller than the capacity region."
"In this paper, we focus on model reduction of large-scale bilinear systems. The main contributions are threefold. First, we introduce a new framework for interpolatory model reduction of bilinear systems. In contrast to the existing methods where interpolation is forced on some of the leading subsystem transfer functions, the new framework shows how to enforce multipoint interpolation of the underlying Volterra series. Then, we show that the first-order conditions for optimal H2 model reduction of bilinear systems require multivariate Hermite interpolation in terms of the new Volterra series interpolation framework; and thus we extend the interpolation-based first-order necessary conditions for H2 optimality of LTI systems to the bilinear case. Finally, we show that multipoint interpolation on the truncated Volterra series representation of a bilinear system leads to an asymptotically optimal approach to H2 optimal model reduction, leading to an efficient model reduction algorithm. Several numerical examples illustrate the effectiveness of the proposed approach."
"Let $\mathrm{R}$ be a real closed field. The problem of obtaining tight bounds on the Betti numbers of semi-algebraic subsets of $\mathrm{R}^k$ in terms of the number and degrees of the defining polynomials has been an important problem in real algebraic geometry with the first results due to Ole{\u\i}nik and Petrovski{\u\i}, Thom and Milnor. These bounds are all exponential in the number of variables $k$. Motivated by several applications in real algebraic geometry, as well as in theoretical computer science, where such bounds have found applications, we consider in this paper the problem of bounding the equivariant Betti numbers of symmetric algebraic and semi-algebraic subsets of $\mathrm{R}^k$. We obtain several asymptotically tight upper bounds. In particular, we prove that if $S\subset \mathrm{R}^k$ is a semi-algebraic subset defined by a finite set of $s$ symmetric polynomials of degree at most $d$, then the sum of the $\mathfrak{S}_k$-equivariant Betti numbers of $S$ with coefficients in $\mathbb{Q}$ is bounded by $(skd)^{O(d)}$. Unlike the classical bounds on the ordinary Betti numbers of real algebraic varieties and semi-algebraic sets, the above bound is polynomial in $k$ when the degrees of the defining polynomials are bounded by a constant. As an application we improve the best known bound on the ordinary Betti numbers of the projection of a compact algebraic set improving for any fixed degree the best previously known bound for this problem due to Gabrielov, Vorobjov and Zell."
"Many privacy-type properties of security protocols can be modelled using trace equivalence properties in suitable process algebras. It has been shown that such properties can be decided for interesting classes of finite processes (i.e., without replication) by means of symbolic execution and constraint solving. However, this does not suffice to obtain practical tools. Current prototypes suffer from a classical combinatorial explosion problem caused by the exploration of many interleavings in the behaviour of processes. M\""odersheim et al. have tackled this problem for reachability properties using partial order reduction techniques. We revisit their work, generalize it and adapt it for equivalence checking. We obtain an optimization in the form of a reduced symbolic semantics that eliminates redundant interleavings on the fly."
"A recursive state estimation procedure is derived for a linear time varying system with both parametric uncertainties and stochastic measurement droppings. This estimator has a similar form as that of the Kalman filter with intermittent observations, but its parameters should be adjusted when a plant output measurement arrives. A new recursive form is derived for the pseudo-covariance matrix of estimation errors, which plays important roles in analyzing its asymptotic properties. Based on a Riemannian metric for positive definite matrices, some necessary and sufficient conditions have been obtained for the strict contractiveness of an iteration of this recursion. It has also been proved that under some controllability and observability conditions, as well as some weak requirements on measurement arrival probability, the gain matrix of this recursive robust state estimator converges in probability one to a stationary distribution. Numerical simulation results show that estimation accuracy of the suggested procedure is more robust against parametric modelling errors than the Kalman filter."
"Ergodic properties and asymptotic stationarity are investigated in this paper for the pseudo-covariance matrix (PCM) of a recursive state estimator which is robust against parametric uncertainties and is based on plant output measurements that may be randomly dropped. When the measurement dropping process is described by a Markov chain and the modified plant is both controllable and observable, it is proved that if the dropping probability is less than 1, this PCM converges to a stationary distribution that is independent of its initial values. A convergence rate is also provided. In addition, it has also been made clear that when the initial value of the PCM is set to the stabilizing solution of the algebraic Riccati equation related to the robust state estimator without measurement dropping, this PCM converges to an ergodic process. Based on these results, two approximations are derived for the probability distribution function of the stationary PCM, as well as a bound of approximation errors. A numerical example is provided to illustrate the obtained theoretical results."
"Some necessary and sufficient conditions are obtained for the controllability and observability of a networked system with linear time invariant (LTI) dynamics. The topology of this system is fixed but arbitrary, and every subsystem is permitted to have different dynamic input-output relations. These conditions essentially depend only on transmission zeros of every subsystem and the connection matrix among subsystems, which makes them attractive in the analysis and synthesis of a large scale networked system. As an application, these conditions are utilized to characterize systems whose steady state estimation accuracy with the distributed predictor developed in (Zhou, 2013) is equal to that of the lumped Kalman filter. Some necessary and sufficient conditions on system matrices are derived for this equivalence. It has been made clear that to guarantee this equivalence, the steady state update gain matrix of the Kalman filter must be block diagonal."
A bowtie is a graph consisting of two triangles with one vertex identified. We show that the class of all (countable) graphs not containing a bowtie as a subgraph have a Ramsey lift (expansion). This solves one of the old problems in the area and it is the first non-trivial Ramsey class with a non-trivial algebraic closure.
"We consider two-player non zero-sum infinite duration games played on weighted graphs. We extend the notion of secure equilibrium introduced by Chatterjee et al., from the Boolean setting to this quantitative setting. As for the Boolean setting, our notion of secure equilibrium refines the classical notion of Nash equilibrium. We prove that secure equilibria always exist in a large class of weighted games which includes common measures like sup, inf, lim sup, lim inf, mean-payoff, and discounted sum. Moreover we show that one can synthesize finite-memory strategy profiles with few memory. We also prove that the constrained existence problem for secure equilibria is decidable for sup, inf, lim sup, lim inf and mean-payoff measures. Our solutions rely on new results for zero-sum quantitative games with lexicographic objectives that are interesting on their own right."
"It is hypothesized by some thinkers that benign looking AI objectives may result in powerful AI drives that may pose an existential risk to human society. We analyze this scenario and find the underlying assumptions to be unlikely. We examine the alternative scenario of what happens when universal goals that are not human-centric are used for designing AI agents. We follow a design approach that tries to exclude malevolent motivations from AI agents, however, we see that objectives that seem benevolent may pose significant risk. We consider the following meta-rules: preserve and pervade life and culture, maximize the number of free minds, maximize intelligence, maximize wisdom, maximize energy production, behave like human, seek pleasure, accelerate evolution, survive, maximize control, and maximize capital. We also discuss various solution approaches for benevolent behavior including selfless goals, hybrid designs, Darwinism, universal constraints, semi-autonomy, and generalization of robot laws. A ""prime directive"" for AI may help in formulating an encompassing constraint for avoiding malicious behavior. We hypothesize that social instincts for autonomous robots may be effective such as attachment learning. We mention multiple beneficial scenarios for an advanced semi-autonomous AGI agent in the near future including space exploration, automation of industries, state functions, and cities. We conclude that a beneficial AI agent with intelligence beyond human-level is possible and has many practical use cases."
"Marsaglia proposed recently xorshift generators as a class of very fast, good-quality pseudorandom number generators. Subsequent analysis by Panneton and L'Ecuyer has lowered the expectations raised by Marsaglia's paper, showing several weaknesses of such generators, verified experimentally using the TestU01 suite. Nonetheless, many of the weaknesses of xorshift generators fade away if their result is scrambled by a non-linear operation (as originally suggested by Marsaglia). In this paper we explore the space of possible generators obtained by multiplying the result of a xorshift generator by a suitable constant. We sample generators at 100 equispaced points of their state space and obtain detailed statistics that lead us to choices of parameters that improve on the current ones. We then explore for the first time the space of high-dimensional xorshift generators, following another suggestion in Marsaglia's paper, finding choices of parameters providing periods of length $2^{1024} - 1$ and $2^{4096} - 1$. The resulting generators are of extremely high quality, faster than current similar alternatives, and generate long-period sequences passing strong statistical tests using only eight logical operations, one addition and one multiplication by a constant."
"We show that new definitions of the notion of ""projection"" on which some of the recent ""extended formulations"" works (such as Kaibel (2011); Fiorini et al. (2011; 2012); Kaibel and Walter (2013); Kaibel and Weltge (2013) for example) have been based can cause those works to over-reach in their conclusions in relating polytopes to one another when the sets of the descriptive variables for those polytopes are disjoint."
"This paper describes some biologically-inspired processes that could be used to build the sort of networks that we associate with the human brain. New to this paper, a 'refined' neuron will be proposed. This is a group of neurons that by joining together can produce a more analogue system, but with the same level of control and reliability that a binary neuron would have. With this new structure, it will be possible to think of an essentially binary system in terms of a more variable set of values. The paper also shows how recent research associated with the new model, can be combined with established theories, to produce a more complete picture. The propositions are largely in line with conventional thinking, but possibly with one or two more radical suggestions. An earlier cognitive model can be filled in with more specific details, based on the new research results, where the components appear to fit together almost seamlessly. The intention of the research has been to describe plausible 'mechanical' processes that can produce the appropriate brain structures and mechanisms, but that could be used without the magical 'intelligence' part that is still not fully understood. There are also some important updates from an earlier version of this paper."
"We present the Bayesian consensus filter (BCF) for tracking a moving target using a networked group of sensing agents and achieving consensus on the best estimate of the probability distributions of the target's states. Our BCF framework can incorporate nonlinear target dynamic models, heterogeneous nonlinear measurement models, non-Gaussian uncertainties, and higher-order moments of the locally estimated posterior probability distribution of the target's states obtained using Bayesian filters. If the agents combine their estimated posterior probability distributions using a logarithmic opinion pool, then the sum of Kullback--Leibler divergences between the consensual probability distribution and the local posterior probability distributions is minimized. Rigorous stability and convergence results for the proposed BCF algorithm with single or multiple consensus loops are presented. Communication of probability distributions and computational methods for implementing the BCF algorithm are discussed along with a numerical example."
"Around 2000, J.-Y. Girard developed a logical theory, called Ludics. This theory was a step in his program of Geometry of Interaction, the aim of which being to account for the dynamics of logical proofs. In Ludics, objects called designs keep only what is relevant for the cut elimination process, hence the dynamics of a proof: a design is an abstraction of a formal proof. The notion of behaviour is the counterpart in Ludics of the logical notions of formula or type. Formally a behaviour is a closed set of designs. Our aim is to explore the constructions of behaviours and to analyse their properties. In this paper a design is viewed as a set of coherent paths. We recall or give variants of properties concerning visitable paths, where a visitable path is a path in a design of a set of designs that may be traversed by interaction with a design of the orthogonal of the set. We are then able to answer the following question: which properties should satisfy a set of paths for being exactly the set of visitable paths of a behaviour? Such a set and its dual should be prefix-closed, daimon-closed and satisfy two saturation properties. This allows us to have a means for defining the whole set of visitable paths of a given set of designs without closing it explicitly, that is without computing the orthogonal of this set of designs. We finally apply all these results for making explicit the structure of a behaviour generated by constants and multiplicative/additive connectives. We end by proposing an oriented tensor for which we give basic properties."
"What does it mean to fully understand the behavior of a network of adaptive agents? The golden standard typically is the behavior of learning dynamics in potential games, where many evolutionary dynamics, e.g., replicator, are known to converge to sets of equilibria. Even in such classic settings many critical questions remain unanswered. We examine issues such as:   Point-wise convergence: Does the system actually equilibrate even in the presence of continuums of equilibria?   Computing regions of attraction: Given point-wise convergence can we compute the region of asymptotic stability of each equilibrium (e.g., estimate its volume, geometry)?   System invariants: Invariant functions remain constant along every system trajectory. This notion is orthogonal to the game theoretic concept of a potential function, which always strictly increases/decreases along system trajectories. Do dynamics in potential games exhibit invariant functions? If so, how many? How do these functions look like?   Based on these geometric characterizations, we propose a novel quantitative framework for analyzing the efficiency of potential games with many equilibria. The predictions of different equilibria are weighted by their probability to arise under evolutionary dynamics given uniformly random initial conditions. This average case analysis is shown to offer novel insights in classic game theoretic challenges, including quantifying the risk dominance in stag-hunt games and allowing for more nuanced performance analysis in networked coordination and congestion games with large gaps between price of stability and price of anarchy."
"Vector Fitting is a popular method of constructing rational approximants designed to fit given frequency response measurements. The original method, which we refer to as VF, is based on a least-squares fit to the measurements by a rational function, using an iterative reallocation of the poles of the approximant. We show that one can improve the performance of VF significantly, by using a particular choice of frequency sampling points and properly weighting their contribution based on quadrature rules that connect the least squares objective with an H2 error measure. Our modified approach, designated here as QuadVF, helps recover the original transfer function with better global fidelity (as measured with respect to the H2 norm), than the localized least squares approximation implicit in VF. We extend the new framework also to incorporate derivative information, leading to rational approximants that minimize system error with respect to a discrete Sobolev norm. We consider the convergence behavior of both VF and QuadVF as well, and evaluate potential numerical ill-conditioning of the underlying least-squares problems. We investigate briefly VF in the case of noisy measurements and propose a new formulation for the resulting approximation problem. Several numerical examples are provided to support the theoretical discussion."
"We formulate and solve the Slepian spatial-spectral concentration problem on the three-dimensional ball. Both the standard Fourier-Bessel and also the Fourier-Laguerre spectral domains are considered since the latter exhibits a number of practical advantages (spectral decoupling and exact computation). The Slepian spatial and spectral concentration problems are formulated as eigenvalue problems, the eigenfunctions of which form an orthogonal family of concentrated functions. Equivalence between the spatial and spectral problems is shown. The spherical Shannon number on the ball is derived, which acts as the analog of the space-bandwidth product in the Euclidean setting, giving an estimate of the number of concentrated eigenfunctions and thus the dimension of the space of functions that can be concentrated in both the spatial and spectral domains simultaneously. Various symmetries of the spatial region are considered that reduce considerably the computational burden of recovering eigenfunctions, either by decoupling the problem into smaller subproblems or by affording analytic calculations. The family of concentrated eigenfunctions forms a Slepian basis that can be used be represent concentrated signals efficiently. We illustrate our results with numerical examples and show that the Slepian basis indeeds permits a sparse representation of concentrated signals."
"The groundbreaking work of Rothvo{\ss} [arxiv:1311.2369] established that every linear program expressing the matching polytope has an exponential number of inequalities (formally, the matching polytope has exponential extension complexity). We generalize this result by deriving strong bounds on the polyhedral inapproximability of the matching polytope: for fixed $0 < \varepsilon < 1$, every polyhedral $(1 + \varepsilon / n)$-approximation requires an exponential number of inequalities, where $n$ is the number of vertices. This is sharp given the well-known $\rho$-approximation of size $O(\binom{n}{\rho/(\rho-1)})$ provided by the odd-sets of size up to $\rho/(\rho-1)$. Thus matching is the first problem in $P$, whose natural linear encoding does not admit a fully polynomial-size relaxation scheme (the polyhedral equivalent of an FPTAS), which provides a sharp separation from the polynomial-size relaxation scheme obtained e.g., via constant-sized odd-sets mentioned above.   Our approach reuses ideas from Rothvo{\ss} [arxiv:1311.2369], however the main lower bounding technique is different. While the original proof is based on the hyperplane separation bound (also called the rectangle corruption bound), we employ the information-theoretic notion of common information as introduced in Braun and Pokutta [http://eccc.hpi-web.de/report/2013/056/], which allows to analyze perturbations of slack matrices. It turns out that the high extension complexity for the matching polytope stem from the same source of hardness as for the correlation polytope: a direct sum structure."
"Since John von Neumann suggested utilizing Logistic map as a random number generator in 1947, a great number of encryption schemes based on Logistic map and/or its variants have been proposed. This paper re-evaluates the security of an image cipher based on transformed logistic maps and proves that the image cipher can be deciphered efficiently under two different conditions: 1) two pairs of known plain-images and the corresponding cipher-images with computational complexity of $O(2^{18}+L)$; 2) two pairs of chosen plain-images and the corresponding cipher-images with computational complexity of $O(L)$, where $L$ is the number of pixels in the plain-image. In contrast, the required condition in the previous deciphering method is eighty-seven pairs of chosen plain-images and the corresponding cipher-images with computational complexity of $O(2^{7}+L)$. In addition, three other security flaws existing in most Logistic-map-based ciphers are also reported."
"We present extremal constructions connected with the property of simplicial collapsibility.   (1) For each $d \ge 2$, there are collapsible (and shellable) simplicial $d$-complexes with only one free face. Also, there are non-evasive $d$-complexes with only two free faces. (Both results are optimal in all dimensions.)   (2) Optimal discrete Morse vectors need not be unique. We explicitly construct a contractible, but non-collapsible $3$-dimensional simplicial complex with face vector $f=(106,596,1064,573)$ that admits two distinct optimal discrete Morse vectors, $(1,1,1,0)$ and $(1,0,1,1)$. Indeed, we show that in every dimension $d\geq 3$ there are contractible, non-collapsible simplicial $d$-complexes that have $(1,0,\dots,0,1,1,0)$ and $(1,0,\dots,0,0,1,1)$ as distinct optimal discrete Morse vectors.   (3) We give a first explicit example of a (non-PL) $5$-manifold, with face vector $f=(5013,72300,290944,$ $495912,383136,110880)$, that is collapsible but not homeomorphic to a ball.   Furthermore, we discuss possible improvements and drawbacks of random approaches to collapsibility and discrete Morse theory. We will introduce randomized versions \texttt{random-lex-first} and \texttt{random-lex-last} of the \texttt{lex-first} and \texttt{lex-last} discrete Morse strategies of \cite{BenedettiLutz2014}, respectively --- and we will see that in many instances the \texttt{random-lex-last} strategy works significantly better than Benedetti--Lutz's (uniform) \texttt{random} strategy.   On the theoretical side, we prove that after repeated barycentric subdivisions, the discrete Morse vectors found by randomized algorithms have, on average, an exponential (in the number of barycentric subdivisions) number of critical cells asymptotically almost surely."
"We study an online problem in which a set of mobile servers have to be moved in order to efficiently serve a set of requests that arrive in an online fashion. More formally, there is a set of $n$ nodes and a set of $k$ mobile servers that are placed at some of the nodes. Each node can potentially host several servers and the servers can be moved between the nodes. There are requests $1,2,\ldots$ that are adversarially issued at nodes one at a time. An issued request at time $t$ needs to be served at all times $t' \geq t$. The cost for serving the requests is a function of the number of servers and requests at the different nodes. The requirements on how to serve the requests are governed by two parameters $\alpha\geq 1$ and $\beta\geq 0$. An algorithm needs to guarantee at all times that the total service cost remains within a multiplicative factor of $\alpha$ and an additive term $\beta$ of the current optimal service cost. We consider online algorithms for two different minimization objectives. We first consider the natural problem of minimizing the total number of server movements. We show that in this case for every $k$, the competitive ratio of every deterministic online algorithm needs to be at least $\Omega(n)$. Given this negative result, we then extend the minimization objective to also include the current service cost. We give almost tight bounds on the competitive ratio of the online problem where one needs to minimize the sum of the total number of movements and the current service cost. In particular, we show that at the cost of an additional additive term which is roughly linear in $k$, it is possible to achieve a multiplicative competitive ratio of $1+\varepsilon$ for every constant $\varepsilon>0$."
"Controllability and observability have long been recognized as fundamental structural properties of dynamical systems, but have recently seen renewed interest in the context of large, complex networks of dynamical systems. A basic problem is sensor and actuator placement: choose a subset from a finite set of possible placements to optimize some real-valued controllability and observability metrics of the network. Surprisingly little is known about the structure of such combinatorial optimization problems. In this paper, we show that several important classes of metrics based on the controllability and observability Gramians have a strong structural property that allows for either efficient global optimization or an approximation guarantee by using a simple greedy heuristic for their maximization. In particular, the mapping from possible placements to several scalar functions of the associated Gramian is either a modular or submodular set function. The results are illustrated on randomly generated systems and on a problem of power electronic actuator placement in a model of the European power grid."
"We prove the existence of a canonical form for semi-deterministic transducers with incomparable sets of output strings. Based on this, we develop an algorithm which learns semi-deterministic transducers given access to translation queries. We also prove that there is no learning algorithm for semi-deterministic transducers that uses only domain knowledge."
"We study a version of online edge coloring, where the goal is to color as many edges as possible using only a given number, $k$, of available colors. All of our results are with regard to competitive analysis. Previous attempts to identify optimal algorithms for this problem have failed, even for bipartite graphs. Thus, in this paper, we analyze even more restricted graph classes, paths and trees. For paths, we consider $k=2$, and for trees, we consider any $k \geq 2$.   We prove that a natural greedy algorithm called First-Fit is optimal among deterministic algorithms, on paths as well as trees. For paths, we give a randomized algorithm, which is optimal and better than the best possible deterministic algorithm. For trees, we prove that to obtain a better competitive ratio than First-Fit, the algorithm would have to be both randomized and unfair (i.e., reject edges that could have been colored), and even such algorithms cannot be much better than First-Fit."
"An improved characteristic set algorithm for solving Boolean polynomial systems is pro- posed. This algorithm is based on the idea of converting all the polynomials into monic ones by zero decomposition, and using additions to obtain pseudo-remainders. Three important techniques are applied in the algorithm. The first one is eliminating variables by new gener- ated linear polynomials. The second one is optimizing the strategy of choosing polynomial for zero decomposition. The third one is to compute add-remainders to eliminate the leading variable of new generated monic polynomials. By analyzing the depth of the zero decompo- sition tree, we present some complexity bounds of this algorithm, which are lower than the complexity bounds of previous characteristic set algorithms. Extensive experimental results show that this new algorithm is more efficient than previous characteristic set algorithms for solving Boolean polynomial systems."
"Estimating the number of signals is a fundamental problem in many scientific and engineering fields. As a well-known estimator based on random matrix theory (RMT), the RMT estimator estimates the number of signals via sequentially testing the likelihood of an eigenvalue as arising from a signal or from noise. However, the RMT estimator tends to down-estimate the number of signals as some signals will be buried in the interaction term among eigenvalues. In order to overcome this problem, we focus on developing novel RMT estimators by incorporating the Lawley theory into random matrix theory. Firstly, we derive a novel decision statistics for signal number estimation by incorporating the Lawley theory into random matrix theory, and then propose a signal-search RMT estimator for signal number estimation. Secondly, we analyze the effect of the interaction term among eigenvalues on the estimation performance of the signal-search RMT estimator and the RMT estimator. It shows that the signal-search RMT estimator has better detection performance than the RMT estimator when some signals are buried in the interaction term among eigenvalues, but has larger over-estimation probability than the RMT estimator when all signals are strong enough to be detected by the RMT estimator. Thirdly, in order to overcome the individual drawbacks of these two estimators, we derive the over-estimation probability of the signal-search RMT estimator and the down-estimation probability of the RMT estimator, and propose a signal-noise search RMT estimator which can adaptively select its decision criterion between the RMT estimator and the signal-search RMT estimator. Finally, simulation results show that the signal-noise search RMT estimator significantly outperforms the existing estimators including the RMT estimator, the classic AIC and MDL estimators, and the modified AIC estimator."
"Listening habits are strongly influenced by two opposing aspects, the desire for variety and the demand for uniformity in music. In this work we quantify these two notions in terms of musical instrumentation and production technologies that are typically involved in crafting popular music. We assign a ""complexity value"" to each music style. A style is complex if it shows the property of having both high variety and low uniformity in instrumentation. We find a strong inverse relation between variety and uniformity of music styles that is remarkably stable over the last half century. Individual styles, however, show dramatic changes in their ""complexity"" during that period. Styles like ""new wave"" or ""disco"" quickly climbed towards higher complexity in the 70s and fell back to low complexity levels shortly afterwards, whereas styles like ""folk rock"" remained at constant high complexity levels. We show that changes in the complexity of a style are related to its number of sales and to the number of artists contributing to that style. As a style attracts a growing number of artists, its instrumentational variety usually increases. At the same time the instrumentational uniformity of a style decreases, i.e. a unique stylistic and increasingly complex expression pattern emerges. In contrast, album sales of a given style typically increase with decreasing complexity. This can be interpreted as music becoming increasingly formulaic once commercial or mainstream success sets in."
"We present a language to specify syntax guided synthesis (SyGuS) problems. Syntax guidance is a prominent theme in contemporary program synthesis approaches, and SyGuS was first described in [1]. This paper describes concretely the input format of a SyGuS solver.   [1] Rajeev Alur, Rastislav Bodik, Garvit Juniwal, Milo M. K. Martin, Mukund Raghothaman, Sanjit A. Seshia, Rishabh Singh, Armando Solar-Lezama, Emina Torlak, and Abhishek Udupa. Syntax-guided synthesis. In FMCAD, pages 1--17, 2013."
"We define and propose a resource allocation architecture for cellular networks. The architecture combines content-aware, time-aware and location-aware resource allocation for next generation broadband wireless systems. The architecture ensures content-aware resource allocation by prioritizing real-time applications users over delay-tolerant applications users when allocating resources. It enables time-aware resource allocation via traffic-dependent pricing that varies during different hours of day (e.g. peak and off-peak traffic hours). Additionally, location-aware resource allocation is integrable in this architecture by including carrier aggregation of various frequency bands. The context-aware resource allocation is an optimal and flexible architecture that can be easily implemented in practical cellular networks. We highlight the advantages of the proposed network architecture with a discussion on the future research directions for context-aware resource allocation architecture. We also provide experimental results to illustrate a general proof of concept for this new architecture."
"An open problem in polarization theory is to determine the binary operations that always lead to polarization (in the general multilevel sense) when they are used in Ar{\i}kan style constructions. This paper, which is presented in two parts, solves this problem by providing a necessary and sufficient condition for a binary operation to be polarizing. This (first) part of the paper introduces the mathematical framework that we will use in the second part to characterize the polarizing operations. We define uniformity preserving, irreducible, ergodic and strongly ergodic operations and we study their properties. The concepts of a stable partition and the residue of a stable partition are introduced. We show that an ergodic operation is strongly ergodic if and only if all its stable partitions are their own residues. We also study the products of binary operations and the structure of their stable partitions. We show that the product of a sequence of binary operations is strongly ergodic if and only if all the operations in the sequence are strongly ergodic. In the second part of the paper, we provide a foundation of polarization theory based on the ergodic theory of binary operations that we develop in this part."
"We revisit a fundamental open problem in quantum information theory, namely whether it is possible to transmit quantum information at a rate exceeding the channel capacity if we allow for a non-vanishing probability of decoding error. Here we establish that the Rains information of any quantum channel is a strong converse rate for quantum communication: For any sequence of codes with rate exceeding the Rains information of the channel, we show that the fidelity vanishes exponentially fast as the number of channel uses increases. This remains true even if we consider codes that perform classical post-processing on the transmitted quantum data. As an application of this result, for generalized dephasing channels we show that the Rains information is also achievable, and thereby establish the strong converse property for quantum communication over such channels. Thus we conclusively settle the strong converse question for a class of quantum channels that have a non-trivial quantum capacity."
"An open problem in polarization theory is to determine the binary operations that always lead to polarization (in the general multilevel sense) when they are used in Ar{\i}kan style constructions. This paper, which is presented in two parts, solves this problem by providing a necessary and sufficient condition for a binary operation to be polarizing. This (second) part provides a foundation of polarization theory based on the ergodic theory of binary operations which we developed in the first part. We show that a binary operation is polarizing if and only if it is uniformity preserving and its right-inverse is strongly ergodic. The rate of polarization of single user channels is studied. It is shown that the exponent of any polarizing operation cannot exceed $\frac{1}{2}$, which is the exponent of quasigroup operations. We also study the polarization of multiple access channels (MAC). In particular, we show that a sequence of binary operations is MAC-polarizing if and only if each binary operation in the sequence is polarizing. It is shown that the exponent of any MAC-polarizing sequence cannot exceed $\frac{1}{2}$, which is the exponent of sequences of quasigroup operations."
This submission has been withdrawn by arXiv administrators because it contains excessive and unattributed reuse of content from other authors.
"Let ${\cal T}$ be a triangulation of a set ${\cal P}$ of $n$ points in the plane, and let $e$ be an edge shared by two triangles in ${\cal T}$ such that the quadrilateral $Q$ formed by these two triangles is convex. A {\em flip} of $e$ is the operation of replacing $e$ by the other diagonal of $Q$ to obtain a new triangulation of ${\cal P}$ from ${\cal T}$. The {\em flip distance} between two triangulations of ${\cal P}$ is the minimum number of flips needed to transform one triangulation into the other. The Flip Distance problem asks if the flip distance between two given triangulations of ${\cal P}$ is at most $k$, for some given $k \in N$. It is a fundamental and a challenging problem.   We present an algorithm for the {\sc Flip Distance} problem that runs in time $O(n + k \cdot c^{k})$, for a constant $c \leq 2 \cdot 14^{11}$, which implies that the problem is fixed-parameter tractable. We extend our results to triangulations of polygonal regions with holes, and to labeled triangulated graphs."
"This paper establishes that the strong converse holds for some classes of discrete memoryless multimessage multicast networks (DM-MMNs) whose corresponding cut-set bounds are tight, i.e., coincide with the set of achievable rate tuples. The strong converse for these classes of DM-MMNs implies that all sequences of codes with rate tuples belonging to the exterior of the cut-set bound have average error probabilities that necessarily tend to one (and are not simply bounded away from zero). Examples in the classes of DM-MMNs include wireless erasure networks, DM-MMNs consisting of independent discrete memoryless channels (DMCs) as well as single-destination DM-MMNs consisting of independent DMCs with destination feedback. Our elementary proof technique leverages properties of the R\'enyi divergence."
This submission has been withdrawn by arXiv administrators because it contains excessive and unattributed reuse of content from other authors.
"Understanding how the time-complexity of evolutionary algorithms (EAs) depend on their parameter settings and characteristics of fitness landscapes is a fundamental problem in evolutionary computation. Most rigorous results were derived using a handful of key analytic techniques, including drift analysis. However, since few of these techniques apply effortlessly to population-based EAs, most time-complexity results concern simplified EAs, such as the (1+1) EA.   This paper describes the level-based theorem, a new technique tailored to population-based processes. It applies to any non-elitist process where offspring are sampled independently from a distribution depending only on the current population. Given conditions on this distribution, our technique provides upper bounds on the expected time until the process reaches a target state.   We demonstrate the technique on several pseudo-Boolean functions, the sorting problem, and approximation of optimal solutions in combinatorial optimisation. The conditions of the theorem are often straightforward to verify, even for Genetic Algorithms and Estimation of Distribution Algorithms which were considered highly non-trivial to analyse. Finally, we prove that the theorem is nearly optimal for the processes considered. Given the information the theorem requires about the process, a much tighter bound cannot be proved."
"Overlap is a common phenomenon seen when structural components of a digital object are neither disjoint nor nested inside each other. Overlapping components resist reduction to a structural hierarchy, and tree-based indexing and query processing techniques cannot be used for them. Our solution to this data modeling problem is TGSA (Tree-like Graph for Structural Annotations), a novel extension of the XML data model for non-hierarchical structures. We introduce an algorithm for constructing TGSA from annotated documents; the algorithm can efficiently process non-hierarchical structures and is associated with formal proofs, ensuring that transformation of the document to the data model is valid. To enable high performance query analysis in large data repositories, we further introduce an extension of XML pre-post indexing for non-hierarchical structures, which can process both reachability and overlapping relationships."
"Citation networks represent the flow of information between agents. They are constrained in time and so form directed acyclic graphs which have a causal structure. Here we provide novel quantitative methods to characterise that structure by adapting methods used in the causal set approach to quantum gravity by considering the networks to be embedded in a Minkowski spacetime and measuring its dimension using Myrheim-Meyer and Midpoint-scaling estimates. We illustrate these methods on citation networks from the arXiv, supreme court judgements from the USA, and patents and find that otherwise similar citation networks have measurably different dimensions. We suggest that these differences can be interpreted in terms of the level of diversity or narrowness in citation behaviour."
"The smart grid spawns many innovative ideas, but many of them cannot be easily integrated into the existing power system due to power system constraints, such as the lack of capacity to transport renewable energy in remote areas to the urban centers. An energy delivery system can be built upon the traffic network and electric vehicles (EVs) are utilized as energy carriers to transport energy over a large geographical region. A generalized architecture called the vehicular energy network (VEN) is constructed and a mathematically tractable framework is developed. Dynamic wireless (dis)charging allows electric energy, as an energy packet, to be added and subtracted from EV batteries seamlessly. With proper routing, energy can be transported from the sources to destinations through EVs along appropriate vehicular routes. This paper gives a preliminary study of VEN. Models are developed to study its operational and economic feasibilities with real traffic data in the United Kingdom. Our study shows that a substantial amount of renewable energy can be transported from some remote wind farms to London under some reasonable settings and VEN is likely to be profitable in the near future. VEN can complement the power network and enhance its power delivery capability."
"The strong metric dimension of a graph was first introduced by Seb\""{o} and Tannier (Mathematics of Operations Research, 29(2), 383-393, 2004) as an alternative to the (weak) metric dimension of graphs previously introduced independently by Slater (Proc. 6th Southeastern Conference on Combinatorics, Graph Theory, and Computing, 549-559, 1975) and by Harary and Melter (Ars Combinatoria, 2, 191-195, 1976), and has since been investigated in several research papers. However, the exact worst-case computational complexity of computing the strong metric dimension has remained open beyond being NP-complete. In this communication, we show that the problem of computing the strong metric dimension of a graph of $n$ nodes admits a polynomial-time $2$-approximation, admits a $O^\ast\big(2^{\,0.287\,n}\big)$-time exact computation algorithm, admits a $O\big(1.2738^k+n\,k\big)$-time exact computation algorithm if the strong metric dimension is at most $k$, does not admit a polynomial time $(2-\varepsilon)$-approximation algorithm assuming the unique games conjecture is true, does not admit a polynomial time $(10\sqrt{5}-21-\varepsilon)$-approximation algorithm assuming P$\neq$NP, does not admit a $O^\ast\big(2^{o(n)}\big)$-time exact computation algorithm assuming the exponential time hypothesis is true, and does not admit a $O^\ast\big(n^{o(k)}\big)$-time exact computation algorithm if the strong metric dimension is at most $k$ assuming the exponential time hypothesis is true."
"We present the results of a comprehensive analysis of visualization paper keywords supplied for 4366 papers submitted to five main visualization conferences. We describe main keywords, topic areas, and 10-year historic trends from two datasets: (1) the standardized PCS taxonomy keywords in use for paper submissions for IEEE InfoVis, IEEE Vis-SciVis, IEEE VAST, EuroVis, and IEEE PacificVis since 2009 and (2) the author-chosen keywords for papers published in the IEEE Visualization conference series (now called IEEE VIS) since 2004. Our analysis of research topics in visualization can serve as a starting point to (a) help create a common vocabulary to improve communication among different visualization sub-groups, (b) facilitate the process of understanding differences and commonalities of the various research sub-fields in visualization, (c) provide an understanding of emerging new research trends, (d) facilitate the crucial step of finding the right reviewers for research submissions, and (e) it can eventually lead to a comprehensive taxonomy of visualization research. One additional tangible outcome of our work is an application that allows visualization researchers to easily browse the 2600+ keywords used for IEEE VIS papers during the past 10 years, aiming at more informed and, hence, more effective keyword selections for future visualization publications."
"We present a new and more efficient technique for computing the route that maximizes the probability of on-time arrival in stochastic networks, also known as the path-based stochastic on-time arrival (SOTA) problem. Our primary contribution is a pathfinding algorithm that uses the solution to the policy-based SOTA problem---which is of pseudo-polynomial-time complexity in the time budget of the journey---as a search heuristic for the optimal path. In particular, we show that this heuristic can be exceptionally efficient in practice, effectively making it possible to solve the path-based SOTA problem as quickly as the policy-based SOTA problem. Our secondary contribution is the extension of policy-based preprocessing to path-based preprocessing for the SOTA problem. In the process, we also introduce Arc-Potentials, a more efficient generalization of Stochastic Arc-Flags that can be used for both policy- and path-based SOTA. After developing the pathfinding and preprocessing algorithms, we evaluate their performance on two different real-world networks. To the best of our knowledge, these techniques provide the most efficient computation strategy for the path-based SOTA problem for general probability distributions, both with and without preprocessing."
"Systems biology is an emerging interdisciplinary area of research that focuses on study of complex interactions in a biological system, such as gene regulatory networks. The discovery of gene regulatory networks leads to a wide range of applications, such as pathways related to a disease that can unveil in what way the disease acts and provide novel tentative drug targets. In addition, the development of biological models from discovered networks or pathways can help to predict the responses to disease and can be much useful for the novel drug development and treatments. The inference of regulatory networks from biological data is still in its infancy stage. This paper proposes a recurrent neural network (RNN) based gene regulatory network (GRN) model hybridized with generalized extended Kalman filter for weight update in backpropagation through time training algorithm. The RNN is a complex neural network that gives a better settlement between the biological closeness and mathematical flexibility to model GRN. The RNN is able to capture complex, non-linear and dynamic relationship among variables. Gene expression data are inherently noisy and Kalman filter performs well for estimation even in noisy data. Hence, non-linear version of Kalman filter, i.e., generalized extended Kalman filter has been applied for weight update during network training. The developed model has been applied on DNA SOS repair network, IRMA network, and two synthetic networks from DREAM Challenge. We compared our results with other state-of-the-art techniques that show superiority of our model. Further, 5% Gaussian noise has been added in the dataset and result of the proposed model shows negligible effect of noise on the results."
"A variety of new measures of quantum Renyi mutual information and quantum Renyi conditional entropy have recently been proposed, and some of their mathematical properties explored. Here, we show that the Renyi mutual information attains operational meaning in the context of composite hypothesis testing, when the null hypothesis is a fixed bipartite state and the alternate hypothesis consists of all product states that share one marginal with the null hypothesis. This hypothesis testing problem occurs naturally in channel coding, where it corresponds to testing whether a state is the output of a given quantum channel or of a 'useless' channel whose output is decoupled from the environment. Similarly, we establish an operational interpretation of Renyi conditional entropy by choosing an alternative hypothesis that consists of product states that are maximally mixed on one system. Specialized to classical probability distributions, our results also establish an operational interpretation of Renyi mutual information and Renyi conditional entropy."
"This paper aims to develop a verification method for procedural programs via a transformation into Logically Constrained Term Rewriting Systems (LCTRSs). To this end, we extend transformation methods based on integer TRSs to handle arbitrary data types, global variables and arrays, as well as encode safety checks. Then we adapt existing rewriting induction methods to LCTRSs and propose a simple yet effective method to generalize equations. We show that we can automatically verify memory safety and prove correctness of realistic functions. Our approach proves equivalence between two implementations, so in contrast to other works, we do not require an explicit specification in a separate specification language."
"The $r$-rounds Even-Mansour block cipher is a generalization of the well known Even-Mansour block cipher to $r$ iterations. Attacks on this construction were described by Nikoli\'c et al. and Dinur et al., for $r = 2, 3$. These attacks are only marginally better than brute force, but are based on an interesting observation (due to Nikoli\'c et al.): for a ""typical"" permutation $P$, the distribution of $P(x) \oplus x$ is not uniform. This naturally raises the following question. Call permutations for which the distribution of $P(x) \oplus x$ is uniform ""balanced."" Is there a sufficiently large family of balanced permutations, and what is the security of the resulting Even-Mansour block cipher?   We show how to generate families of balanced permutations from the Luby-Rackoff construction, and use them to define a $2n$-bit block cipher from the $2$-rounds Even-Mansour scheme. We prove that this cipher is indistinguishable from a random permutation of $\{0, 1\}^{2n}$, for any adversary who has oracle access to the public permutations and to an encryption/decryption oracle, as long as the number of queries is $o (2^{n/2})$. As a practical example, we discuss the properties and the performance of a $256$-bit block cipher that is based on our construction, and uses AES as the public permutation."
"In this paper we revise and simplify the notion of observationally induced algebra introduced by Simpson and Schroeder for the purpose of modelling computational effects in the particular case where the ambient category is given by classical domain theory. As examples of the general framework we consider the various powerdomains. For the particular case of the Plotkin powerdomain the general recipe leads to a somewhat unexpected result which, however, makes sense from a Computer Science perspective. We analyze this ""deviation"" and show how to reobtain the original Plotkin powerdomain by imposing further conditions previously considered by R.~Heckmann and J.~Goubault-Larrecq."
"3D printers based on the Fused Decomposition Modeling create objects layer-by-layer dropping fused material. As a consequence, strong overhangs cannot be printed because the new-come material does not find a suitable support over the last deposed layer. In these cases, one can add some support structures (scaffolds) which make the object printable, to be removed at the end. In this paper we propose a level set method to create object-dependent support structures, specifically conceived to reduce both the amount of additional material and the printing time. We also review some open problems about 3D printing which can be of interests for the mathematical community."
"The study of ground state energies of local Hamiltonians has played a fundamental role in quantum complexity theory. In this paper, we take a new direction by introducing the physically motivated notion of ""ground state connectivity"" of local Hamiltonians, which captures problems in areas ranging from quantum stabilizer codes to quantum memories. We show that determining how ""connected"" the ground space of a local Hamiltonian is can range from QCMA-complete to NEXP-complete. As a result, we obtain a natural QCMA-complete problem, a goal which has generally proven difficult since the conception of QCMA over a decade ago. Our proofs rely on a new technical tool, the Traversal Lemma, which analyzes the Hilbert space a local unitary evolution must traverse under certain conditions. We show that this lemma is tight up to a polynomial factor with respect to the length of the unitary evolution in question."
"The colorful Carath\'eodory theorem, proved by B\'ar\'any in 1982, states that given d+1 sets of points S_1,...,S_{d+1} in R^d, with each S_i containing 0 in its convex hull, there exists a subset T of the union of the S_i's containing 0 in its convex hull and such that T contains at most one point from each S_i. An intriguing question -- still open -- is whether such a set T, whose existence is ensured, can be found in polynomial time. In 1997, B\'ar\'any and Onn defined colorful linear programming as algorithmic questions related to the colorful Carath\'eodory theorem. The question we just mentioned comes under colorful linear programming.   The traditional applications of colorful linear programming lie in discrete geometry. In this paper, we study its relations with other areas, such as game theory, operations research, and combinatorics. Regarding game theory, we prove that computing a Nash equilibrium in a bimatrix game is a colorful linear programming problem. We also formulate an optimization problem for colorful linear programming and show that as for usual linear programming, deciding and optimizing are computationally equivalent. We discuss then a colorful version of Dantzig's diet problem. We also propose a variant of the B\'ar\'any algorithm, which is an algorithm computing a set T whose existence is ensured by the colorful Carath\'eodory theorem. Our algorithm makes a clear connection with the simplex algorithm and we discuss its computational efficiency. Related complexity and combinatorial results are also provided."
"In this paper we study the $\ell_p$-analysis optimization ($0<p\leq1$) problem for cosparse signal recovery. We establish a bound for recovery error via the restricted $p$-isometry property over any subspace. We further prove that the nonconvex $\ell_q$-analysis optimization can do recovery with a lower sample complexity and in a wider range of cosparsity than its convex counterpart. In addition, we develop an iteratively reweighted method to solve the optimization problem under a variational framework. Empirical results of preliminary computational experiments illustrate that the nonconvex method outperforms its convex counterpart."
"In many different fields of science, it is useful to characterize physical states and processes as resources. Chemistry, thermodynamics, Shannon's theory of communication channels, and the theory of quantum entanglement are prominent examples. Questions addressed by a theory of resources include: Which resources can be converted into which other ones? What is the rate at which arbitrarily many copies of one resource can be converted into arbitrarily many copies of another? Can a catalyst help in making an impossible transformation possible? How does one quantify the resource? Here, we propose a general mathematical definition of what constitutes a resource theory. We prove some general theorems about how resource theories can be constructed from theories of processes wherein there is a special class of processes that are implementable at no cost and which define the means by which the costly states and processes can be interconverted one to another. We outline how various existing resource theories fit into our framework. Our abstract characterization of resource theories is a first step in a larger project of identifying universal features and principles of resource theories. In this vein, we identify a few general results concerning resource convertibility."
"According to Zipf's meaning-frequency law, words that are more frequent tend to have more meanings. Here it is shown that a linear dependency between the frequency of a form and its number of meanings is found in a family of models of Zipf's law for word frequencies. This is evidence for a weak version of the meaning-frequency law. Interestingly, that weak law (a) is not an inevitable of property of the assumptions of the family and (b) is found at least in the narrow regime where those models exhibit Zipf's law for word frequencies."
"Many modern and popular state of the art image denoising algorithms are trained and evaluated using images corrupted by artificial noise. These trained algorithms and their evaluations on synthetic data may lead to incorrect conclusions about their performances on real noise. In this paper we introduce a benchmark dataset of uncompressed color images corrupted by natural noise due to low-light conditions, together with spatially and intensity-aligned low noise images of the same scenes. The dataset contains over 120 scenes and more than 400 images, including both 16-bit RAW images and 8-bit BMP pixel and intensity-aligned images from 2 digital cameras (Canon S90 and Canon T3i) and a mobile phone (Xiaomi Mi3). We also introduce a method for estimating the true noise level in each of our images, since even the low noise images contain a small amount of noise. Finally, we exemplify the use of our dataset by evaluating four denoising algorithms: Active Random Field, BM3D, Bilevel MRF optimization, and Multi-Layer Perceptron. We show that while the Multi-Layer Perceptron and Bilevel MRF algorithms work as well as or even better than BM3D on synthetic noise, they lag behind on our dataset."
"The paper presents a novel approach for the analysis and control of a multi-agent system with non-identical agents and a path-graph topology. With the help of irrational wave transfer functions, the approach describes the interaction among the agents from the `local' perspective and identifies travelling waves in the system. It is shown that different dynamics of the agents creates a virtual boundary that causes a partial reflection of the travelling waves. Undesired effects due to the reflection of the waves, such as amplification/attenuation, long transients or string instability, can be compensated by the feedback controllers introduced in this paper. We show that the controllers achieve asymptotic and even string stability of the system."
"This paper proposes techniques to calibrate six-axis force-torque sensors that can be performed in situ, i.e., without removing the sensor from the hosting system. We assume that the force-torque sensor is attached to a rigid body equipped with an accelerometer. Then, the proposed calibration technique uses the measurements of the accelerometer, but requires neither the knowledge of the inertial parameters nor the orientation of the rigid body. The proposed method exploits the geometry induced by the model between the raw measurements of the sensor and the corresponding force-torque. The validation of the approach is performed by calibrating two six-axis force-torque sensors of the iCub humanoid robot."
"Dominant resource fairness (DRF) is a popular mechanism for multi-resource allocation in cloud computing systems. In this paper, we consider a problem of multi-resource fair allocation with bounded number of tasks. Firstly, we propose the lexicographically max-min normalized share (LMMNS) fair allocation mechanism, which is a natural generalization of DRF, and design a non-trivial optimal algorithm to find a LMMNS fair allocation, whose running time is linear in the number of users. Secondly, we prove that LMMNS satisfies envy-freeness (EF) and group strategy-proofness (GSP), and analysis the approximation ratios of LMMNS, by exploiting the properties of the optimal solution. Thirdly, we propose a modified version of LMMNS, which is the second mechanism satisfying sharing incentive, EF, and GSP. Finally, we have implemented LMMNS, and show that it has a good average-case performance, especially when the number of resources is 2."
"Due to various green initiatives, renewable energy will be massively incorporated into the future smart grid. However, the intermittency of the renewables may result in power imbalance, thus adversely affecting the stability of a power system. Frequency regulation may be used to maintain the power balance at all times. As electric vehicles (EVs) become popular, they may be connected to the grid to form a vehicle-to-grid (V2G) system. An aggregation of EVs can be coordinated to provide frequency regulation services. However, V2G is a dynamic system where the participating EVs come and go independently. Thus it is not easy to estimate the regulation capacities for V2G. In a preliminary study, we modeled an aggregation of EVs with a queueing network, whose structure allows us to estimate the capacities for regulation-up and regulation-down, separately. The estimated capacities from the V2G system can be used for establishing a regulation contract between an aggregator and the grid operator, and facilitating a new business model for V2G. In this paper, we extend our previous development by designing a smart charging mechanism which can adapt to given characteristics of the EVs and make the performance of the actual system follow the analytical model."
"Identification of inertial parameters is fundamental for the implementation of torque-based control in humanoids. At the same time, good models of friction and actuator dynamics are critical for the low-level control of joint torques. We propose a novel method to identify inertial, friction and motor parameters in a single procedure. The identification exploits the measurements of the PWM of the DC motors and a 6-axis force/torque sensor mounted inside the kinematic chain. The partial least-square (PLS) method is used to perform the regression. We identified the inertial, friction and motor parameters of the right arm of the iCub humanoid robot. We verified that the identified model can accurately predict the force/torque sensor measurements and the motor voltages. Moreover, we compared the identified parameters against the CAD parameters, in the prediction of the force/torque sensor measurements. Finally, we showed that the estimated model can effectively detect external contacts, comparing it against a tactile-based contact detection. The presented approach offers some advantages with respect to other state-of-the-art methods, because of its completeness (i.e. it identifies inertial, friction and motor parameters) and simplicity (only one data collection, with no particular requirements)."
"We have modified a commercial NOR flash memory array to enable high-precision tuning of individual floating-gate cells for analog computing applications. The modified array area per cell in a 180 nm process is about 1.5 um^2. While this area is approximately twice the original cell size, it is still at least an order of magnitude smaller than in the state-of-the-art analog circuit implementations. The new memory cell arrays have been successfully tested, in particular confirming that each cell may be automatically tuned, with ~1% precision, to any desired subthreshold readout current value within an almost three-orders-of-magnitude dynamic range, even using an unoptimized tuning algorithm. Preliminary results for a four-quadrant vector-by-matrix multiplier, implemented with the modified memory array gate-coupled with additional peripheral floating-gate transistors, show highly linear transfer characteristics over a broad range of input currents."
"In a network, a node is said to incur a delay if its encoding of each transmitted symbol involves only its received symbols obtained before the time slot in which the transmitted symbol is sent (hence the transmitted symbol sent in a time slot cannot depend on the received symbol obtained in the same time slot). A node is said to incur no delay if its received symbol obtained in a time slot is available for encoding its transmitted symbol sent in the same time slot. Under the classical model, every node in the network incurs a delay. In this paper, we investigate the multimessage multicast network (MMN) under a generalized-delay model which allows some nodes to incur no delay. We obtain the capacity regions for three classes of MMNs with zero-delay nodes, namely the deterministic network dominated by product distribution, the MMN consisting of independent DMCs and the wireless erasure network. In addition, we show that for any MMN belonging to one of the above three classes, the set of achievable rate tuples under the generalized-delay model and under the classical model are the same, which implies that the set of achievable rate tuples for the MMN does not depend on the delay amounts incurred by the nodes in the network."
"The period of a Morse oscillator and mathematical pendulum system are obtained, accurate to 100 significant digits, by forward period analysis (FPA). From these results, the long-term [0, 10^60] (time unit) solutions, which overlap from the Planck time to the age of the universe, are computed reliably and quickly with a parallel multiple-precision Taylor series (PMT) scheme. The application of FPA to periodic systems can reduce the computation loops of long-term reliable simulation from O(t^(1+1/M)) to O(lnt+t/h0) where T is the period, M the order and h0 a constant step-size. This scheme provides a way to generate reference solutions to test other schemes' long-term simulations."
"For a set $A$ of $n$ people and a set $B$ of $m$ items, with each person having a preference list that ranks all items from most wanted to least wanted, we consider the problem of matching every person with a unique item. A matching $M$ is called $\epsilon$-popular if for any other matching $M'$, the number of people who prefer $M'$ to $M$ is at most $\epsilon n$ plus the number of those who prefer $M$ to $M'$. In 2006, Mahdian showed that when randomly generating people's preference lists, if $m/n > 1.42$, then a 0-popular matching exists with $1-o(1)$ probability; and if $m/n < 1.42$, then a 0-popular matching exists with $o(1)$ probability. The ratio 1.42 can be viewed as a transition point, at which the probability rises from asymptotically zero to asymptotically one, for the case $\epsilon=0$. In this paper, we introduce an upper bound and a lower bound of the transition point in more general cases. In particular, we show that when randomly generating each person's preference list, if $\alpha(1-e^{-1/\alpha}) > 1-\epsilon$, then an $\epsilon$-popular matching exists with $1-o(1)$ probability (upper bound); and if $\alpha(1-e^{-(1+e^{1/\alpha})/\alpha}) < 1-2\epsilon$, then an $\epsilon$-popular matching exists with $o(1)$ probability (lower bound)."
"Consider n nodes connected to a single coordinator. Each node receives an individual online data stream of numbers and, at any point in time, the coordinator has to know the k nodes currently observing the largest values, for a given k between 1 and n. We design and analyze an algorithm that solves this problem while bounding the amount of messages exchanged between the nodes and the coordinator. Our algorithm employs the idea of using filters which, intuitively speaking, leads to few messages to be sent, if the new input is ""similar"" to the previous ones. The algorithm uses a number of messages that is on expectation by a factor of O((log {\Delta} + k) log n) larger than that of an offline algorithm that sets filters in an optimal way, where {\Delta} is upper bounded by the largest value observed by any node."
"Current generation Flash devices experience significant read-channel degradation from damage to the oxide layer during program and erase operations. Information about the read-channel degradation drives advanced signal processing methods in Flash to mitigate its effect. In this context, channel estimation must be ongoing since channel degradation evolves over time and as a function of the number of program/erase (P/E) cycles. This paper proposes a framework for ongoing model-based channel estimation using limited channel measurements (reads). This paper uses a channel model characterizing degradation resulting from retention time and the amount of charge programmed and erased. For channel histogram measurements, bin selection to achieve approximately equal-probability bins yields a good approximation to the original distribution using only ten bins (i.e. nine reads). With the channel model and binning strategy in place, this paper explores candidate numerical least squares algorithms and ultimately demonstrates the effectiveness of the Levenberg-Marquardt algorithm which provides both speed and accuracy."
"We prove that for writing the 3 by 3 permanent polynomial as a determinant of a matrix consisting only of zeros, ones, and variables as entries, a 7 by 7 matrix is required. Our proof is computer based and uses the enumeration of bipartite graphs. Furthermore, we analyze sequences of polynomials that are determinants of polynomially sized matrices consisting only of zeros, ones, and variables. We show that these are exactly the sequences in the complexity class of constant free polynomially sized (weakly) skew circuits."
"Random geometric networks consist of 1) a set of nodes embedded randomly in a bounded domain $\mathcal{V} \subseteq \mathbb{R}^d$ and 2) links formed probabilistically according to a function of mutual Euclidean separation. We quantify how often all paths in the network characterisable as topologically `shortest' contain a given node (betweenness centrality), deriving an expression in terms of a known integral whenever 1) the network boundary is the perimeter of a disk and 2) the network is extremely dense. Our method shows how similar formulas can be obtained for any convex geometry. Numerical corroboration is provided, as well as a discussion of our formula's potential use for cluster head election and boundary detection in densely deployed wireless ad hoc networks."
"We focus on the distribution regression problem: regressing to vector-valued outputs from probability measures. Many important machine learning and statistical tasks fit into this framework, including multi-instance learning and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). Despite the large number of available heuristics in the literature, the inherent two-stage sampled nature of the problem makes the theoretical analysis quite challenging, since in practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. To the best of our knowledge, the only existing technique with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which often performs poorly in practice), and the domain of the distributions to be compact Euclidean. In this paper, we study a simple, analytically computable, ridge regression-based alternative to distribution regression, where we embed the distributions to a reproducing kernel Hilbert space, and learn the regressor from the embeddings to the outputs. Our main contribution is to prove that this scheme is consistent in the two-stage sampled setup under mild conditions (on separable topological domains enriched with kernels): we present an exact computational-statistical efficiency trade-off analysis showing that our estimator is able to match the one-stage sampled minimax optimal rate [Caponnetto and De Vito, 2007; Steinwart et al., 2009]. This result answers a 17-year-old open question, establishing the consistency of the classical set kernel [Haussler, 1999; Gaertner et. al, 2002] in regression. We also cover consistency for more recent kernels on distributions, including those due to [Christmann and Steinwart, 2010]."
"Finding the infection sources in a network when we only know the network topology and infected nodes, but not the rates of infection, is a challenging combinatorial problem, and it is even more difficult in practice where the underlying infection spreading model is usually unknown a priori. In this paper, we are interested in finding a source estimator that is applicable to various spreading models, including the Susceptible-Infected (SI), Susceptible-Infected-Recovered (SIR), Susceptible-Infected-Recovered-Infected (SIRI), and Susceptible-Infected-Susceptible (SIS) models. We show that under the SI, SIR and SIRI spreading models and with mild technical assumptions, the Jordan center is the infection source associated with the most likely infection path in a tree network with a single infection source. This conclusion applies for a wide range of spreading parameters, while it holds for regular trees under the SIS model with homogeneous infection and recovery rates. Since the Jordan center does not depend on the infection, recovery and reinfection rates, it can be regarded as a universal source estimator. We also consider the case where there are k>1 infection sources, generalize the Jordan center definition to a k-Jordan center set, and show that this is an optimal infection source set estimator in a tree network for the SI model. Simulation results on various general synthetic networks and real world networks suggest that Jordan center-based estimators consistently outperform the betweenness, closeness, distance, degree, eigenvector, and pagerank centrality based heuristics, even if the network is not a tree."
"Path-velocity decomposition is an intuitive yet powerful approach to address the complexity of kinodynamic motion planning. The difficult trajectory planning problem is solved in two separate, simpler, steps: first, find a path in the configuration space that satisfies the geometric constraints (path planning), and second, find a time-parameterization of that path satisfying the kinodynamic constraints. A fundamental requirement is that the path found in the first step should be time-parameterizable. Most existing works fulfill this requirement by enforcing quasi-static constraints in the path planning step, resulting in an important loss in completeness. We propose a method that enables path-velocity decomposition to discover truly dynamic motions, i.e. motions that are not quasi-statically executable. At the heart of the proposed method is a new algorithm -- Admissible Velocity Propagation -- which, given a path and an interval of reachable velocities at the beginning of that path, computes exactly and efficiently the interval of all the velocities the system can reach after traversing the path while respecting the system kinodynamic constraints. Combining this algorithm with usual sampling-based planners then gives rise to a family of new trajectory planners that can appropriately handle kinodynamic constraints while retaining the advantages associated with path-velocity decomposition. We demonstrate the efficiency of the proposed method on some difficult kinodynamic planning problems, where, in particular, quasi-static methods are guaranteed to fail."
"Clique clustering is the problem of partitioning the vertices of a graph into disjoint clusters, where each cluster forms a clique in the graph, while optimizing some objective function. In online clustering, the input graph is given one vertex at a time, and any vertices that have previously been clustered together are not allowed to be separated. The goal is to maintain a clustering with an objective value close to the optimal solution. For the variant where we want to maximize the number of edges in the clusters, we propose an online strategy based on the doubling technique. It has an asymptotic competitive ratio at most 15.646 and an absolute competitive ratio at most 22.641. We also show that no deterministic strategy can have an asymptotic competitive ratio better than 6. For the variant where we want to minimize the number of edges between clusters, we show that the deterministic competitive ratio of the problem is $n-\omega(1)$, where n is the number of vertices in the graph."
"In this paper, we address two challenging problems in unsupervised subspace learning: 1) how to automatically identify the feature dimension of the learned subspace (i.e., automatic subspace learning), and 2) how to learn the underlying subspace in the presence of Gaussian noise (i.e., robust subspace learning). We show that these two problems can be simultaneously solved by proposing a new method (called principal coefficients embedding, PCE). For a given data set $\mathbf{D}\in \mathds{R}^{m\times n}$, PCE recovers a clean data set $\mathbf{D}_{0}\in \mathds{R}^{m\times n}$ from $\mathbf{D}$ and simultaneously learns a global reconstruction relation $\mathbf{C}\in \mathbf{R}^{n\times n}$ of $\mathbf{D}_{0}$. By preserving $\mathbf{C}$ into an $m^{\prime}$-dimensional space, the proposed method obtains a projection matrix that can capture the latent manifold structure of $\mathbf{D}_{0}$, where $m^{\prime}\ll m$ is automatically determined by the rank of $\mathbf{C}$ with theoretical guarantees. PCE has three advantages: 1) it can automatically determine the feature dimension even though data are sampled from a union of multiple linear subspaces in presence of the Gaussian noise, 2) Although the objective function of PCE only considers the Gaussian noise, experimental results show that it is robust to the non-Gaussian noise (\textit{e.g.}, random pixel corruption) and real disguises, 3) Our method has a closed-form solution and can be calculated very fast. Extensive experimental results show the superiority of PCE on a range of databases with respect to the classification accuracy, robustness and efficiency."
"We explore the problems of classification of composite object (images, speech signals) with low number of models per class. We study the question of improving recognition performance for medium-sized database (thousands of classes). The key issue of fast approximate nearest-neighbor methods widely applied in this task is their heuristic nature. It is possible to strongly prove their efficiency by using the theory of algorithms only for simple similarity measures and artificially generated tasks. On the contrary, in this paper we propose an alternative, statistically optimal greedy algorithm. At each step of this algorithm joint density (likelihood) of distances to previously checked models is estimated for each class. The next model to check is selected from the class with the maximal likelihood. The latter is estimated based on the asymptotic properties of the Kullback-Leibler information discrimination and mathematical model of piecewise-regular object with distribution of each regular segment of exponential type. Experimental results in face recognition for FERET dataset prove that the proposed method is much more effective than not only brute force and the baseline (directed enumeration method) but also approximate nearest neighbor methods from FLANN and NonMetricSpaceLib libraries (randomized kd-tree, composite index, perm-sort)."
"We consider the following interpolation problem. Suppose one is given a finite set $E \subset \mathbb{R}^d$, a function $f: E \rightarrow \mathbb{R}$, and possibly the gradients of $f$ at the points of $E$. We want to interpolate the given information with a function $F \in C^{1,1}(\mathbb{R}^d)$ with the minimum possible value of $\mathrm{Lip} (\nabla F)$. We present practical, efficient algorithms for constructing an $F$ such that $\mathrm{Lip} (\nabla F)$ is minimal, or for less computational effort, within a small dimensionless constant of being minimal."
"We study the minimization problem of a non-convex sparsity promoting penalty function, the transformed $l_1$ (TL1), and its application in compressed sensing (CS). The TL1 penalty interpolates $l_0$ and $l_1$ norms through a nonnegative parameter $a \in (0,+\infty)$, similar to $l_p$ with $p \in (0,1]$. TL1 is known in the statistics literature to enjoy three desired properties: unbiasedness, sparsity and Lipschitz continuity. We first consider the constrained minimization problem and prove the uniqueness of global minimizer and its equivalence to $l_0$ norm minimization if the sensing matrix $A$ satisfies a restricted isometry property (RIP) and if $a > a^*$, where $a^*$ depends only on $A$. The solution is stable under noisy measurement. For general sensing matrix $A$, we show that the support set of a local minimizer corresponds to linearly independent columns of $A$, and recall sufficient conditions for a critical point to be a local minimum. Next, we present difference of convex algorithms for TL1 (DCATL1) in computing TL1-regularized constrained and unconstrained problems in CS. For the unconstrained problem, we prove convergence of DCALT1 to a stationary point satisfying the first order optimality condition. Finally in numerical experiments, we identify the optimal value $a=1$, and compare DCATL1 with other CS algorithms on three classes of sensing matrices: Gaussian random matrices, over-sampled discrete cosine transform matrices (ODCT), and uniformly distributed M-sphere matrices. We find that for all three classes of sensing matrices, the performance of DCATL1 algorithm (initiated with $L_1$ minimization) always ranks near the top (if not the top), and is the most robust choice insensitive to RIP (incoherence) of the underlying CS problems."
"Recurrent networks with transfer functions that fulfill the Lipschitz continuity $L=1$ may be echo state networks if certain limitations on the recurrent connectivity are applied. It has been shown that it is sufficient if the largest singular value of the recurrent connectivity S is smaller than 1. The main achievement of this paper is a proof under which conditions the network is an echo state network even if $S=1$. It turns out that in this critical case the exact shape of the transfer function plays a decisive role in determining whether the network still fulfills the echo state condition. In addition, several examples with one neuron networks are outlined to illustrate effects of critical connectivity. Moreover, within the manuscript a mathematical definition for a critical echo state network is suggested."
"The exact nonnegative matrix factorization (exact NMF) problem is the following: given an $m$-by-$n$ nonnegative matrix $X$ and a factorization rank $r$, find, if possible, an $m$-by-$r$ nonnegative matrix $W$ and an $r$-by-$n$ nonnegative matrix $H$ such that $X = WH$. In this paper, we propose two heuristics for exact NMF, one inspired from simulated annealing and the other from the greedy randomized adaptive search procedure. We show that these two heuristics are able to compute exact nonnegative factorizations for several classes of nonnegative matrices (namely, linear Euclidean distance matrices, slack matrices, unique-disjointness matrices, and randomly generated matrices) and as such demonstrate their superiority over standard multi-start strategies. We also consider a hybridization between these two heuristics that allows us to combine the advantages of both methods. Finally, we discuss the use of these heuristics to gain insight on the behavior of the nonnegative rank, i.e., the minimum factorization rank such that an exact NMF exists. In particular, we disprove a conjecture on the nonnegative rank of a Kronecker product, propose a new upper bound on the extension complexity of generic $n$-gons and conjecture the exact value of (i) the extension complexity of regular $n$-gons and (ii) the nonnegative rank of a submatrix of the slack matrix of the correlation polytope."
"Matrix factorization is a popular approach for large-scale matrix completion. The optimization formulation based on matrix factorization can be solved very efficiently by standard algorithms in practice. However, due to the non-convexity caused by the factorization model, there is a limited theoretical understanding of this formulation. In this paper, we establish a theoretical guarantee for the factorization formulation to correctly recover the underlying low-rank matrix. In particular, we show that under similar conditions to those in previous works, many standard optimization algorithms converge to the global optima of a factorization formulation, and recover the true low-rank matrix. We study the local geometry of a properly regularized factorization formulation and prove that any stationary point in a certain local region is globally optimal. A major difference of our work from the existing results is that we do not need resampling in either the algorithm or its analysis. Compared to other works on nonconvex optimization, one extra difficulty lies in analyzing nonconvex constrained optimization when the constraint (or the corresponding regularizer) is not ""consistent"" with the gradient direction. One technical contribution is the perturbation analysis for non-symmetric matrix factorization."
"With the rapid development of mobile devices and the crowdsourcig platforms, the spatial crowdsourcing has attracted much attention from the database community, specifically, spatial crowdsourcing refers to sending a location-based request to workers according to their positions. In this paper, we consider an important spatial crowdsourcing problem, namely reliable diversity-based spatial crowdsourcing (RDB-SC), in which spatial tasks (such as taking videos/photos of a landmark or firework shows, and checking whether or not parking spaces are available) are time-constrained, and workers are moving towards some directions. Our RDB-SC problem is to assign workers to spatial tasks such that the completion reliability and the spatial/temporal diversities of spatial tasks are maximized. We prove that the RDB-SC problem is NP-hard and intractable. Thus, we propose three effective approximation approaches, including greedy, sampling, and divide-and-conquer algorithms. In order to improve the efficiency, we also design an effective cost-model-based index, which can dynamically maintain moving workers and spatial tasks with low cost, and efficiently facilitate the retrieval of RDB-SC answers. Through extensive experiments, we demonstrate the efficiency and effectiveness of our proposed approaches over both real and synthetic data sets."
"In this work we consider the learning setting where, in addition to the training set, the learner receives a collection of auxiliary hypotheses originating from other tasks. We focus on a broad class of ERM-based linear algorithms that can be instantiated with any non-negative smooth loss function and any strongly convex regularizer. We establish generalization and excess risk bounds, showing that, if the algorithm is fed with a good combination of source hypotheses, generalization happens at the fast rate $\mathcal{O}(1/m)$ instead of the usual $\mathcal{O}(1/\sqrt{m})$. On the other hand, if the source hypotheses combination is a misfit for the target task, we recover the usual learning rate. As a byproduct of our study, we also prove a new bound on the Rademacher complexity of the smooth loss class under weaker assumptions compared to previous works."
"Complex environments, processes and systems may exhibit several distinct modes of physical behaviour or operation. Thus, for example, in their design, a set of mathematical models may be needed, each model having its own domain of application and representing a particular mode of behaviour or operation of physical reality. The models may be of disparate kinds { discrete or continuous in data, time and space. Furthermore, some physical modes may not have a reliable model. Physical measurements determine modes of operation. We explore the question: What is a mode of behaviour? How do we specify algorithms and software that monitor or govern a complex physical situation with many modes? How do we specify a portfolio of modes, and the computational problem of transitioning from using one mode to another mode as physical modes change? We propose a general definition of an analogue-digital system with modes. We show how any diverse set of modes { with or without models { can be bound together, and how the transitions between modes can be determined, by constructing a topological data type based upon a simplicial complex. We illustrate the ideas of physical modes and our theory by reflecting on simple examples, including driverless racing cars."
"The transformed $l_1$ penalty (TL1) functions are a one parameter family of bilinear transformations composed with the absolute value function. When acting on vectors, the TL1 penalty interpolates $l_0$ and $l_1$ similar to $l_p$ norm ($p \in (0,1)$). In our companion paper, we showed that TL1 is a robust sparsity promoting penalty in compressed sensing (CS) problems for a broad range of incoherent and coherent sensing matrices. Here we develop an explicit fixed point representation for the TL1 regularized minimization problem. The TL1 thresholding functions are in closed form for all parameter values. In contrast, the $l_p$ thresholding functions ($p \in [0,1]$) are in closed form only for $p=0,1,1/2,2/3$, known as hard, soft, half, and 2/3 thresholding respectively. The TL1 threshold values differ in subcritical (supercritical) parameter regime where the TL1 threshold functions are continuous (discontinuous) similar to soft-thresholding (half-thresholding) functions. We propose TL1 iterative thresholding algorithms and compare them with hard and half thresholding algorithms in CS test problems. For both incoherent and coherent sensing matrices, a proposed TL1 iterative thresholding algorithm with adaptive subcritical and supercritical thresholds consistently performs the best in sparse signal recovery with and without measurement noise."
"This paper shows that the Hirschfeld-Gebelein-R\'enyi maximal correlation between the message and the ciphertext provides good secrecy guarantees for cryptosystems that use short keys. We first establish a bound on the eavesdropper's advantage in guessing functions of the message in terms of maximal correlation and the R\'enyi entropy of the message. This result implies that maximal correlation is stronger than the notion of entropic security introduced by Russell and Wang. We then show that a small maximal correlation $\rho$ can be achieved via a randomly generated cipher with key length $\approx2\log(1/\rho)$, independent of the message length, and by a stream cipher with key length $2\log(1/\rho)+\log n+2$ for a message of length $n$. We establish a converse showing that these ciphers are close to optimal. This is in contrast to entropic security for which there is a gap between the lower and upper bounds. Finally, we show that a small maximal correlation implies secrecy with respect to several mutual information based criteria but is not necessarily implied by them. Hence, maximal correlation is a stronger and more practically relevant measure of secrecy than mutual information."
"This work is motivated by a question at the heart of unsupervised learning approaches: Assume we are collecting a number K of (subjective) opinions about some event E from K different agents. Can we infer E from them? Prima facie this seems impossible, since the agents may be lying. We model this task by letting the events be distributed according to some distribution p and the task is to estimate p under unknown noise. Again, this is impossible without additional assumptions. We report here the finding of very natural such assumptions - the availability of multiple copies of the true data, each under independent and invertible (in the sense of matrices) noise, is already sufficient: If the true distribution and the observations are modeled on the same finite alphabet, then the number of such copies needed to determine p to the highest possible precision is exactly three! This result can be seen as a counterpart to independent component analysis. Therefore, we call our approach 'dependent component analysis'. In addition, we present generalizations of the model to different alphabet sizes at in- and output. A second result is found: the 'activation' of invertibility through multiple parallel uses."
"Prior distributions of binarized natural images are learned by using a Boltzmann machine. According the results of this study, there emerges a structure with two sublattices in the interactions, and the nearest-neighbor and next-nearest-neighbor interactions correspondingly take two discriminative values, which reflects the individual characteristics of the three sets of pictures that we process. Meanwhile, in a longer spatial scale, a longer-range, although still rapidly decaying, ferromagnetic interaction commonly appears in all cases. The characteristic length scale of the interactions is universally up to approximately four lattice spacings $\xi \approx 4$. These results are derived by using the mean-field method, which effectively reduces the computational time required in a Boltzmann machine. An improved mean-field method called the Bethe approximation also gives the same results, as well as the Monte Carlo method does for small size images. These reinforce the validity of our analysis and findings. Relations to criticality, frustration, and simple-cell receptive fields are also discussed."
"Network science have constantly been in the focus of research for the last decade, with considerable advances in the controllability of their structural. However, much less effort has been devoted to study that how to improve the controllability of complex networks. In this paper, a new algorithm is proposed to improve the controllability of complex networks by rewiring links regularly which transforms the network structure. Then it is demonstrated that our algorithm is very effective after numerical simulation experiment on typical network models (Erd\""os-R\'enyi and scale-free network). We find that our algorithm is mainly determined by the average degree and positive correlation of in-degree and out-degree of network and it has nothing to do with the network size. Furthermore, we analyze and discuss the correlation between controllability of complex networks and degree distribution index: power-law exponent and heterogeneity"
"The manuscript describes efficient algorithms for the computation of the CUR and ID decompositions. The methods used are based on simple modifications to the classical truncated pivoted QR decomposition, which means that highly optimized library codes can be utilized for implementation. For certain applications, further acceleration can be attained by incorporating techniques based on randomized projections. Numerical experiments demonstrate advantageous performance compared to existing techniques for computing CUR factorizations."
"We consider polynomial systems of Prony type, appearing in many areas of mathematics. Their robust numerical solution is considered to be difficult, especially in ""near-colliding"" situations. We consider a case when the structure of the system is a-priori fixed. We transform the nonlinear part of the Prony system into a Hankel-type polynomial system. Combining this representation with a recently discovered ""decimation"" technique, we present an algorithm which applies homotopy continuation to an appropriately chosen Hankel-type system as above. In this way, we are able to solve for the nonlinear variables of the original system with high accuracy when the data is perturbed."
"For large ranks, there is no good algorithm that decides whether a given lattice has an orthonormal basis. But when the lattice is given with enough symmetry, we can construct a provably deterministic polynomial-time algorithm to accomplish this, based on the work of Gentry and Szydlo. The techniques involve algorithmic algebraic number theory, analytic number theory, commutative algebra, and lattice basis reduction."
"We present a class of linear programming approximations for constrained optimization problems. In the case of mixed-integer polynomial optimization problems, if the intersection graph of the constraints has bounded tree-width our construction yields a class of linear size formulations that attain any desired tolerance. As a result, we obtain an approximation scheme for the ""AC-OPF"" problem on graphs with bounded tree-width. We also describe a more general construction for pure binary optimization problems where individual constraints are available through a membership oracle; if the intersection graph for the constraints has bounded tree-width our construction is of linear size and exact. This improves on a number of results in the literature, both from the perspective of formulation size and generality."
"Social network based trust relationships present a critical foundation for designing trustworthy systems, such as Sybil defenses, secure routing, and anonymous/censorshipresilient communications. A key issue in the design of such systems, is the revelation of users' trusted social contacts to an adversary-information that is considered sensitive in today's society.   In this work, we focus on the challenge of preserving the privacy of users' social contacts, while still enabling the design of social trust based applications. First, we propose LinkMirage, a community detection based algorithm for anonymizing links in social network topologies; LinkMirage preserves community structures in the social topology while anonymizing links within the communities. LinkMirage considers the evolution of the social network topologies, and minimizes privacy leakage due to temporal dynamics of the system.   Second, we define metrics for quantifying the privacy and utility of a time series of social topologies with anonymized links. We analyze the privacy and utility provided by LinkMirage both theoretically, as well as using real world social network topologies: a Facebook dataset with 870K links and a large-scale Google+ dataset with 940M links. We find that our approach significantly outperforms the existing state-of-art.   Finally, we demonstrate the applicability of LinkMirage in real-world applications such as Sybil defenses, reputation systems, anonymity systems and vertex anonymity. We also prototype LinkMirage as a Facebook application such that real world systems can bootstrap privacy-preserving trust relationships without the cooperation of the OSN operators."
"Probabilistic Boolean networks (PBNs) is a well-established computational framework for modelling biological systems. The steady-state dynamics of PBNs is of crucial importance in the study of such systems. However, for large PBNs, which often arise in systems biology, obtaining the steady-state distribution poses a significant challenge. In fact, statistical methods for steady-state approximation are the only viable means when dealing with large networks. In this paper, we revive the two-state Markov chain approach presented in the literature. We first identify a problem of generating biased results, due to the size of the initial sample with which the approach needs to start and we propose a few heuristics to avoid such a pitfall. Second, we conduct an extensive experimental comparison of the two-state Markov chain approach and another approach based on the Skart method and we show that statistically the two-state Markov chain has a better performance. Finally, we apply this approach to a large PBN model of apoptosis in hepatocytes."
"The growing demand for data has driven the Service Providers (SPs) to provide differential treatment of traffic to generate additional revenue streams from Content Providers (CPs). While SPs currently only provide best-effort services to their CPs, it is plausible to envision a model in near future, where CPs are willing to sponsor quality of service for their content in exchange of sharing a portion of their profit with SPs. This quality sponsoring becomes invaluable especially when the available resources are scarce such as in wireless networks, and can be accommodated in a non-neutral network. In this paper, we consider the problem of Quality-Sponsored Data (QSD) in a non-neutral network. In our model, SPs allow CPs to sponsor a portion of their resources, and price it appropriately to maximize their payoff. The payoff of the SP depends on the monetary revenue and the satisfaction of end-users both for the non-sponsored and sponsored content, while CPs generate revenue through advertisement. We analyze the market dynamics and equilibria in two different frameworks, i.e. sequential and bargaining game frameworks, and provide strategies for (i) SPs: to determine if and how to price resources, and (ii) CPs: to determine if and what quality to sponsor. The frameworks characterize different sets of equilibrium strategies and market outcomes depending on the parameters of the market."
"Given $n$ items with at most $d$ of which being positive, instead of testing these items individually, the theory of combinatorial group testing aims to identify all positive items using as few tests as possible. This paper is devoted to a fundamental and thirty-year-old problem in the nonadaptive group testing theory. A binary matrix is called $d$-disjunct if the boolean sum of arbitrary $d$ columns does not contain another column not in this collection. Let $T(d)$ denote the minimal $t$ such that there exists a $t\times n$ $d$-disjunct matrix with $n>t$. $T(d)$ can also be viewed as the minimal $t$ such that there exists a nonadaptive group testing scheme which is better than the trivial one that tests each item individually. It was known that $T(d)\ge\binom{d+2}{2}$ and was conjectured that $T(d)\ge(d+1)^2$. In this paper we narrow the gap by proving $T(d)/d^2\ge(15+\sqrt{33})/24$, a quantity in [6/7,7/8]."
"Gabidulin codes can be seen as the rank-metric equivalent of Reed-Solomon codes. It was recently proven, using subspace polynomials, that Gabidulin codes cannot be list decoded beyond the so-called Johnson radius. In another result, cyclic subspace codes were constructed by inspecting the connection between subspaces and their subspace polynomials. In this paper, these subspace codes are used to prove two bounds on the list size in decoding certain Gabidulin codes. The first bound is an existential one, showing that exponentially-sized lists exist for codes with specific parameters. The second bound presents exponentially-sized lists explicitly, for a different set of parameters. Both bounds rule out the possibility of efficiently list decoding several families of Gabidulin codes for any radius beyond half the minimum distance. Such a result was known so far only for non-linear rank-metric codes, and not for Gabidulin codes. Using a standard operation called lifting, identical results also follow for an important class of constant dimension subspace codes."
"In this note we deal with a new observer for nonlinear systems of dimension n in canonical observability form. We follow the standard high-gain paradigm, but instead of having an observer of dimension n with a gain that grows up to power n, we design an observer of dimension 2n-2 with a gain that grows up only to power 2."
"In non-coherent wideband fading channels where energy rather than spectrum is the limiting resource, peaky and non-peaky signaling schemes have long been considered species apart, as the first approaches asymptotically the capacity of a wideband AWGN channel with the same average SNR, whereas the second reaches a peak rate at some finite critical bandwidth and then falls to zero as bandwidth grows to infinity. In this paper it is shown that this distinction is in fact an artifact of the limited attention paid in the past to the product between the bandwidth and the fraction of time it is in use. This fundamental quantity, called bandwidth occupancy, measures average bandwidth usage over time. For all signaling schemes with the same bandwidth occupancy, achievable rates approach to the wideband AWGN capacity within the same gap as the bandwidth occupancy approaches its critical value, and decrease to zero as the occupancy goes to infinity. This unified analysis produces quantitative closed-form expressions for the ideal bandwidth occupancy, recovers the existing capacity results for (non-)peaky signaling schemes, and unveils a trade-off between the accuracy of approximating capacity with a generalized Taylor polynomial and the accuracy with which the optimal bandwidth occupancy can be bounded."
"Segmentation is a fundamental task for extracting semantically meaningful regions from an image. The goal of segmentation algorithms is to accurately assign object labels to each image location. However, image-noise, shortcomings of algorithms, and image ambiguities cause uncertainty in label assignment. Estimating the uncertainty in label assignment is important in multiple application domains, such as segmenting tumors from medical images for radiation treatment planning. One way to estimate these uncertainties is through the computation of posteriors of Bayesian models, which is computationally prohibitive for many practical applications. On the other hand, most computationally efficient methods fail to estimate label uncertainty. We therefore propose in this paper the Active Mean Fields (AMF) approach, a technique based on Bayesian modeling that uses a mean-field approximation to efficiently compute a segmentation and its corresponding uncertainty. Based on a variational formulation, the resulting convex model combines any label-likelihood measure with a prior on the length of the segmentation boundary. A specific implementation of that model is the Chan-Vese segmentation model (CV), in which the binary segmentation task is defined by a Gaussian likelihood and a prior regularizing the length of the segmentation boundary. Furthermore, the Euler-Lagrange equations derived from the AMF model are equivalent to those of the popular Rudin-Osher-Fatemi (ROF) model for image denoising. Solutions to the AMF model can thus be implemented by directly utilizing highly-efficient ROF solvers on log-likelihood ratio fields. We qualitatively assess the approach on synthetic data as well as on real natural and medical images. For a quantitative evaluation, we apply our approach to the icgbench dataset."
"We introduce natural strategic games on graphs, which capture the idea of coordination in a local setting. We study the existence of equilibria that are resilient to coalitional deviations of unbounded and bounded size (i.e., strong equilibria and k-equilibria respectively). We show that pure Nash equilibria and 2-equilibria exist, and give an example in which no 3-equilibrium exists. Moreover, we prove that strong equilibria exist for various special cases.   We also study the price of anarchy (PoA) and price of stability (PoS) for these solution concepts. We show that the PoS for strong equilibria is 1 in almost all of the special cases for which we have proven strong equilibria to exist. The PoA for pure Nash equilbria turns out to be unbounded, even when we fix the graph on which the coordination game is to be played. For the PoA for k-equilibria, we show that the price of anarchy is between 2(n-1)/(k-1) - 1 and 2(n-1)/(k-1). The latter upper bound is tight for $k=n$ (i.e., strong equilibria).   Finally, we consider the problems of computing strong equilibria and of determining whether a joint strategy is a k-equilibrium or strong equilibrium. We prove that, given a coordination game, a joint strategy s, and a number k as input, it is co-NP complete to determine whether s is a k-equilibrium. On the positive side, we give polynomial time algorithms to compute strong equilibria for various special cases."
"In previous work carried out in the setting of program algebra, including work in the area of instruction sequence size complexity, we chose instruction sets for Boolean registers that contain only instructions of a few of the possible kinds. In the current paper, we study instruction sequence size bounded functional completeness of all possible instruction sets for Boolean registers. We expect that the results of this study will turn out to be useful to adequately assess results of work that is concerned with lower bounds of instruction sequence size complexity."
"Given an undirected, edge-weighted graph G together with pairs of vertices, called pairs of terminals, the minimum multicut problem asks for a minimum-weight set of edges such that, after deleting these edges, the two terminals of each pair belong to different connected components of the graph. Relying on topological techniques, we provide a polynomial-time algorithm for this problem in the case where G is embedded on a fixed surface of genus g (e.g., when G is planar) and has a fixed number t of terminals. The running time is a polynomial of degree O(sqrt{g^2+gt}) in the input size.   In the planar case, our result corrects an error in an extended abstract by Bentz [Int. Workshop on Parameterized and Exact Computation, 109-119, 2012]. The minimum multicut problem is also a generalization of the multiway cut problem, a.k.a. multiterminal cut problem; even for this special case, no dedicated algorithm was known for graphs embedded on surfaces."
"This paper develops efficient algorithms for distributed average consensus with quantized communication using the alternating direction method of multipliers (ADMM). We first study the effects of probabilistic and deterministic quantizations on a distributed ADMM algorithm. With probabilistic quantization, this algorithm yields linear convergence to the desired average in the mean sense with a bounded variance. When deterministic quantization is employed, the distributed ADMM either converges to a consensus or cycles with a finite period after a finite-time iteration. In the cyclic case, local quantized variables have the same mean over one period and hence each node can also reach a consensus. We then obtain an upper bound on the consensus error which depends only on the quantization resolution and the average degree of the network. Finally, we propose a two-stage algorithm which combines both probabilistic and deterministic quantizations. Simulations show that the two-stage algorithm, without picking small algorithm parameter, has consensus errors that are typically less than one quantization resolution for all connected networks where agents' data can be of arbitrary magnitudes."
"Fox, Gromov, Lafforgue, Naor, and Pach proved a regularity lemma for semi-algebraic $k$-uniform hypergraphs of bounded complexity, showing that for each $\epsilon>0$ the vertex set can be equitably partitioned into a bounded number of parts (in terms of $\epsilon$ and the complexity) so that all but an $\epsilon$-fraction of the $k$-tuples of parts are homogeneous. We prove that the number of parts can be taken to be polynomial in $1/\epsilon$. Our improved regularity lemma can be applied to geometric problems and to the following general question on property testing: is it possible to decide, with query complexity polynomial in the reciprocal of the approximation parameter, whether a hypergraph has a given hereditary property? We give an affirmative answer for testing typical hereditary properties for semi-algebraic hypergraphs of bounded complexity."
"Soil water retention curve (SWRC) and saturated hydraulic conductivity (SHC) are key hydraulic properties for unsaturated zone hydrology and groundwater. In particular, SWRC provides useful information on entry pore-size distribution, and SHC is required for flow and transport modeling in the hydrologic cycle. Not only the SWRC and SHC measurements are time-consuming, but also scale dependent. This means as soil column volume increases, variability of the SWRC and SHC decreases. Although prediction of the SWRC and SHC from available parameters, such as textural data, organic matter, and bulk density have been under investigation for decades, up to now no research has focused on the effect of measurement scale on the soil hydraulic properties pedotransfer functions development. In the literature, several data mining approaches have been applied, such as multiple linear regression, artificial neural networks, group method of data handling. However, in this study we develop pedotransfer functions using a novel approach called contrast pattern aided regression (CPXR) and compare it with the multiple linear regression method. For this purpose, two databases including 210 and 213 soil samples are collected to develop and evaluate pedotransfer functions for the SWRC and SHC, respectively, from the UNSODA database. The 10-fold cross-validation method is applied to evaluate the accuracy and reliability of the proposed regression-based models. Our results show that including measurement scale parameters, such as sample internal diameter and length could substantially improve the accuracy of the SWRC and SHC pedotransfer functions developed using the CPXR method, while this is not the case when MLR is used. Moreover, the CPXR method yields remarkably more accurate soil water retention curve and saturated hydraulic conductivity predictions than the MLR approach."
"We prove the strong converse for the $N$-source Gaussian multiple access channel (MAC). In particular, we show that any rate tuple that can be supported by a sequence of codes with asymptotic average error probability less than one must lie in the Cover-Wyner capacity region. Our proof consists of the following. First, we perform an expurgation step to convert any given sequence of codes with asymptotic average error probability less than one to codes with asymptotic maximal error probability less than one. Second, we quantize the input alphabets with an appropriately chosen resolution. Upon quantization, we apply the wringing technique (by Ahlswede) on the quantized inputs to obtain further subcodes from the subcodes obtained in the expurgation step so that the resultant correlations among the symbols transmitted by the different sources vanish as the blocklength grows. Finally, we derive upper bounds on achievable sum-rates of the subcodes in terms of the type-II error of a binary hypothesis test. These upper bounds are then simplified through judicious choices of auxiliary output distributions. Our strong converse result carries over to the Gaussian interference channel under strong interference as long as the sum of the two asymptotic average error probabilities less than one."
"In mechanism design, the gold standard solution concepts are dominant strategy incentive compatibility and Bayesian incentive compatibility. These solution concepts relieve the (possibly unsophisticated) bidders from the need to engage in complicated strategizing. While incentive properties are simple to state, their proofs are specific to the mechanism and can be quite complex. This raises two concerns. From a practical perspective, checking a complex proof can be a tedious process, often requiring experts knowledgeable in mechanism design. Furthermore, from a modeling perspective, if unsophisticated agents are unconvinced of incentive properties, they may strategize in unpredictable ways.   To address both concerns, we explore techniques from computer-aided verification to construct formal proofs of incentive properties. Because formal proofs can be automatically checked, agents do not need to manually check the properties, or even understand the proof. To demonstrate, we present the verification of a sophisticated mechanism: the generic reduction from Bayesian incentive compatible mechanism design to algorithm design given by Hartline, Kleinberg, and Malekian. This mechanism presents new challenges for formal verification, including essential use of randomness from both the execution of the mechanism and from the prior type distributions. As an immediate consequence, our work also formalizes Bayesian incentive compatibility for the entire family of mechanisms derived via this reduction. Finally, as an intermediate step in our formalization, we provide the first formal verification of incentive compatibility for the celebrated Vickrey-Clarke-Groves mechanism."
"Causal Graph Dynamics extend Cellular Automata to arbitrary, bounded-degree, time-varying graphs. The whole graph evolves in discrete time steps, and this global evolution is required to have a number of physics-like symmetries: shift-invariance (it acts everywhere the same) and causality (information has a bounded speed of propagation). We add a further physics-like symmetry, namely reversibility."
"We discuss how to efficiently forward data in vehicular networks. Existing solutions do not make full use of trajectory planning of nearby vehicles, or social attributes. The development of onboard navigation system provides drivers some traveling route information. The main novelty of our approach is to envision sharing partial traveling information to the encountered vehicles for better service. Our data forwarding algorithm utilizes this lightweight information under the delusive paths privacy preservation together with the social community structure in vehicular networks. We assume that data transmission is carried by vehicles and road side units (RSUs), while cellular network manages and coordinates relevant global information. The approximate destination set is the set of RSUs that are often passed by the destination vehicle. RSU importance is raised by summing encounter ratios of RSUs in the same connected component. We first define a concept of space-time approachability which is derived from shared partial traveling route and encounter information. It describes the capability of a vehicle to advance messages toward destination. Then, we design a novel data forwarding algorithm, called approachability based algorithm, which combines the space-time approachability with the social community attribute in vehicular networks. We evaluate our approachability based algorithm on data sets from San Francisco Cabspotting and Shanghai Taxi Movement. Results show that the partially shared traveling information plays a positive role in data forwarding in vehicular networks. Approachability based data forwarding algorithm achieves a better performance than existing social based algorithms in vehicular networks."
"Designing mechanical devices, called linkages, that draw a given plane curve has been a topic that interested engineers and mathematicians for hundreds of years, and recently also computer scientists. Already in 1876, Kempe proposed a procedure for solving the problem in full generality, but his constructions tend to be extremely complicated. We provide a novel algorithm that produces much simpler linkages, but works only for parametric curves. Our approach is to transform the problem into a factorization task over some noncommutative algebra. We show how to compute such a factorization, and how to use it to construct a linkage tracing a given curve."
"This paper proposes a general adaptive procedure for budget-limited predictor design in high dimensions called two-stage Sampling, Prediction and Adaptive Regression via Correlation Screening (SPARCS). SPARCS can be applied to high dimensional prediction problems in experimental science, medicine, finance, and engineering, as illustrated by the following. Suppose one wishes to run a sequence of experiments to learn a sparse multivariate predictor of a dependent variable $Y$ (disease prognosis for instance) based on a $p$ dimensional set of independent variables $\mathbf X=[X_1,\ldots, X_p]^T$ (assayed biomarkers). Assume that the cost of acquiring the full set of variables $\mathbf X$ increases linearly in its dimension. SPARCS breaks the data collection into two stages in order to achieve an optimal tradeoff between sampling cost and predictor performance. In the first stage we collect a few ($n$) expensive samples $\{y_i,\mathbf x_i\}_{i=1}^n$, at the full dimension $p\gg n$ of $\mathbf X$, winnowing the number of variables down to a smaller dimension $l < p$ using a type of cross-correlation or regression coefficient screening. In the second stage we collect a larger number $(t-n)$ of cheaper samples of the $l$ variables that passed the screening of the first stage. At the second stage, a low dimensional predictor is constructed by solving the standard regression problem using all $t$ samples of the selected variables. SPARCS is an adaptive online algorithm that implements false positive control on the selected variables, is well suited to small sample sizes, and is scalable to high dimensions. We establish asymptotic bounds for the Familywise Error Rate (FWER), specify high dimensional convergence rates for support recovery, and establish optimal sample allocation rules to the first and second stages."
"This article investigates the problem of dynamic spectrum access with statistical quality of service (QoS) provisioning for dynamic canonical networks, in which the channel states are time-varying from slot to slot. In the existing work with time-varying environment, the commonly used optimization objective is to maximize the expectation of a certain metric (e.g., throughput or achievable rate). However, it is realized that expectation alone is not enough since some applications are sensitive to the channel fluctuations. Effective capacity is a promising metric for time-varying service process since it characterizes the packet delay violating probability (regarded as an important statistical QoS index), by taking into account not only the expectation but also other high-order statistic. We formulate the interactions among the users in the time-varying environment as a non-cooperative game, in which the utility function is defined as the achieved effective capacity. We prove that it is an ordinal potential game which has at least one pure strategy Nash equilibrium. In addition, we propose a multi-agent learning algorithm which is proved to achieve stable solutions with uncertain, dynamic and incomplete information constraints. The convergence of the proposed learning algorithm is verified by simulation results. Also, it is shown that the proposed multi-agent learning algorithm achieves satisfactory performance."
"Technology of autonomous vehicles (AVs) is getting mature and many AVs will appear on the roads in the near future. AVs become connected with the support of various vehicular communication technologies and they possess high degree of control to respond to instantaneous situations cooperatively with high efficiency and flexibility. In this paper, we propose a new public transportation system based on AVs. It manages a fleet of AVs to accommodate transportation requests, offering point-to-point services with ride sharing. We focus on the two major problems of the system: scheduling and admission control. The former is to configure the most economical schedules and routes for the AVs to satisfy the admissible requests while the latter is to determine the set of admissible requests among all requests to produce maximum profit. The scheduling problem is formulated as a mixed-integer linear program and the admission control problem is cast as a bilevel optimization, which embeds the scheduling problem as the major constraint. By utilizing the analytical properties of the problem, we develop an effective genetic-algorithm-based method to tackle the admission control problem. We validate the performance of the algorithm with real-world transportation service data."
"A lot of works have shown that frobenius-norm based representation (FNR) is competitive to sparse representation and nuclear-norm based representation (NNR) in numerous tasks such as subspace clustering. Despite the success of FNR in experimental studies, less theoretical analysis is provided to understand its working mechanism. In this paper, we fill this gap by building the theoretical connections between FNR and NNR. More specially, we prove that: 1) when the dictionary can provide enough representative capacity, FNR is exactly NNR even though the data set contains the Gaussian noise, Laplacian noise, or sample-specified corruption, 2) otherwise, FNR and NNR are two solutions on the column space of the dictionary."
In this paper we study multi robot cooperative task allocation issue in a situation where a swarm of robots is deployed in a confined unknown environment where the number of colored spots which represent tasks and the ratios of them are unknown. The robots should cover this spots as far as possible to do cleaning and sampling actions desirably. It means that they should discover the spots cooperatively and spread proportional to the spots area and avoid from remaining idle. We proposed 4 self-organized distributed methods which are called hybrid methods for coping with this scenario. In two diffe?rent experiments the performance of the methods is analyzed. We compared them with each other and investigated their scalability and robustness in term of single point of failure.
"Vector Fitting (VF) is a popular method of constructing rational approximants that provides a least squares fit to frequency response measurements. In an earlier work, we provided an analysis of VF for scalar-valued rational functions and established a connection with optimal $H_2$ approximation. We build on this work and extend the previous framework to include the construction of effective rational approximations to matrix-valued functions, a problem which presents significant challenges that do not appear in the scalar case. Transfer functions associated with multi-input/multi-output (MIMO) dynamical systems typify the class of functions that we consider here. Others have also considered extensions of VF to matrix-valued functions and related numerical implementations are readily available. However to our knowledge, a detailed analysis of numerical issues that arise does not yet exist. We offer such an analysis including critical implementation details here.   One important issue that arises for VF on matrix-valued functions that has remained largely unaddressed is the control of the McMillan degree of the resulting rational approximant; the McMillan degree can grow very high in the case of large input/output dimensions. We introduce two new mechanisms for controlling the McMillan degree of the final approximant, one based on alternating least-squares minimization and one based on ancillary system-theoretic reduction methods. Motivated in part by our earlier work on the scalar VF problem as well as by recent innovations for computing optimal $H_2$ approximation, we establish a connection with optimal $H_2$ approximation, and are able to improve significantly the fidelity of VF through numerical quadrature, with virtually no increase in cost or complexity. We provide several numerical examples to support the theoretical discussion and proposed algorithms."
"A smart city provides its people with high standard of living through advanced technologies and transport is one of the major foci. With the advent of autonomous vehicles (AVs), an AV-based public transportation system has been proposed recently, which is capable of providing new forms of transportation services with high efficiency, high flexibility, and low cost. For the benefit of passengers, multitenancy can increase market competition leading to lower service charge and higher quality of service. In this paper, we study the pricing issue of the multi-tenant AV public transportation system and three types of services are defined. The pricing process for each service type is modeled as a combinatorial auction, in which the service providers, as bidders, compete for offering transportation services. The winners of the auction are determined through an integer linear program. To prevent the bidders from raising their bids for higher returns, we propose a strategy-proof Vickrey-Clarke-Groves-based charging mechanism, which can maximize the social welfare, to settle the final charges for the customers. We perform extensive simulations to verify the analytical results and evaluate the performance of the charging mechanism."
"We discover new hereditary classes of graphs that are minimal (with respect to set inclusion) of unbounded clique-width. The new examples include split permutation graphs and bichain graphs. Each of these classes is characterised by a finite list of minimal forbidden induced subgraphs. These, therefore, disprove a conjecture due to Daligault, Rao and Thomasse from 2010 claiming that all such minimal classes must be defined by infinitely many forbidden induced subgraphs.   In the same paper, Daligault, Rao and Thomasse make another conjecture that every hereditary class of unbounded clique-width must contain a labelled infinite antichain. We show that the two example classes we consider here satisfy this conjecture. Indeed, they each contain a canonical labelled infinite antichain, which leads us to propose a stronger conjecture: that every hereditary class of graphs that is minimal of unbounded clique-width contains a canonical labelled infinite antichain."
"This paper investigates the effectiveness of factorial speech processing models in noise-robust automatic speech recognition tasks. For this purpose, the paper proposes an idealistic approach for modeling state-conditional observation distribution of factorial models based on weighted stereo samples. This approach is an extension to previous single pass retraining for ideal model compensation which is extended here to support multiple audio sources. Non-stationary noises can be considered as one of these audio sources with multiple states. Experiments of this paper over the set A of the Aurora 2 dataset show that recognition performance can be improved by this consideration. The improvement is significant in low signal to noise energy conditions, up to 4% absolute word recognition accuracy. In addition to the power of the proposed method in accurate representation of state-conditional observation distribution, it has an important advantage over previous methods by providing the opportunity to independently select feature spaces for both source and corrupted features. This opens a new window for seeking better feature spaces appropriate for noisy speech, independent from clean speech features."
"Low-rank matrix completion (LRMC) problems arise in a wide variety of applications. Previous theory mainly provides conditions for completion under missing-at-random samplings. This paper studies deterministic conditions for completion. An incomplete $d \times N$ matrix is finitely rank-$r$ completable if there are at most finitely many rank-$r$ matrices that agree with all its observed entries. Finite completability is the tipping point in LRMC, as a few additional samples of a finitely completable matrix guarantee its unique completability. The main contribution of this paper is a deterministic sampling condition for finite completability. We use this to also derive deterministic sampling conditions for unique completability that can be efficiently verified. We also show that under uniform random sampling schemes, these conditions are satisfied with high probability if $O(\max\{r,\log d\})$ entries per column are observed. These findings have several implications on LRMC regarding lower bounds, sample and computational complexity, the role of coherence, adaptive settings and the validation of any completion algorithm. We complement our theoretical results with experiments that support our findings and motivate future analysis of uncharted sampling regimes."
"When applied on some particular quantum entangled states, measurements are universal for quantum computing. In particular, despite the fondamental probabilistic evolution of quantum measurements, any unitary evolution can be simulated by a measurement-based quantum computer (MBQC). We consider the extended version of the MBQC where each measurement can occur not only in the (X,Y)-plane of the Bloch sphere but also in the (X,Z)- and (Y,Z)-planes. The existence of a gflow in the underlying graph of the computation is a necessary and sufficient condition for a certain kind of determinism. We extend the focused gflow (a gflow in a particular normal form) defined for the (X,Y)-plane to the extended case, and we provide necessary and sufficient conditions for the existence of such normal forms."
"We present a heuristic algorithm to compute the ideal class group, and a generator of a principal ideal in $\mathbb{Q}(\zeta_{p^s})$ in time $2^{O(n^{1/2+\varepsilon})}$ for $n:= deg(K)$ and arbitrarily small $\varepsilon$. This yields an attack on the schemes relying on the hardness of finding a short generator of a principal ideal such as such as the homomorphic encryption scheme of Vercauteren and Smart, and the multilinear maps of Garg, Gentry and Halevi. We rely on the work from Cramer, Ducas, Peikert and Regev. They proved that finding a short generator polynomially reduces to finding an arbitrary one. The complexity is better than when we rely on the work of Biasse and Fieker on the PIP, which yields an attack in time $2^{n^{2/3+\varepsilon}}$ for arbitrarily small $\varepsilon >0$.   $\textbf{Since Sep. 30 2016}$ We present practical improvements to our methods. Moreover, we describe a variant that solves the PIP in time $2^{O\left(n^{\frac{2}{3} - \frac{a}{3}+\varepsilon}\right)}$ for arbitrarily small $\varepsilon > 0$ given a precomputation in $2^{O(n^a)}$. When $1/2 < a < 1$, we achieve run times ranging from $2^{\tilde{O}(n^{1/2})}$ to $2^{\tilde{O}(n^{1/3})}$. Combined with the recent reduction of $\gamma$-SVP to the PIP under reasonable assumptions on the class group of Cramer, Ducas and Wesolowski, our algorithm solves $\gamma$-SVP in $\mathbb{Q}(\zeta_{p^s})$ for $\gamma\in e^{\tilde{O}(\sqrt{n})}$ in heuristic time $2^{O\left(n^{\frac{2}{3} - \frac{a}{3}+\varepsilon}\right)}$ for arbitrarily small $\varepsilon > 0$ given a precomputation on $\mathbb{Q}(\zeta_{p^s})$ in $2^{O(n^a)}$. When $1/2 < a < 1$, we can leverage the precomputation to achieve a better asymptotic run time than the BKZ algorithm.   $\textbf{Since Oct. 7 2016}$ We added a discussion in Appendix about the differences between the current version of this work and the original document of March 2015."
"In a group testing scheme, a set of tests is designed to identify a small number $t$ of defective items that are present among a large number $N$ of items. Each test takes as input a group of items and produces a binary output indicating whether any defective item is present in the group. In a non-adaptive scheme designing a testing scheme is equivalent to the construction of a disjunct matrix, an $M \times N$ binary matrix where the union of supports of any $t$ columns does not contain the support of any other column. In this paper we consider the scenario where defective items are random and follow simple probability distributions. In particular we consider the cases where 1) each item can be defective independently with probability $\frac{t}{N}$ and 2) each $t$-set of items can be defective with uniform probability. In both cases our aim is to design a testing matrix that successfully identifies the set of defectives with high probability. Both of these models have been studied in the literature before and it is known that $O(t\log N)$ tests are necessary as well as sufficient (via random coding) in both cases. Our main focus is explicit deterministic construction of the test matrices amenable to above scenarios. One of the most popular ways of constructing test matrices relies on \emph{constant-weight error-correcting codes} and their minimum distance. We go beyond the minimum distance analysis and connect the average distance of a constant weight code to the parameters of the resulting test matrix. With our relaxed requirements, we show that using explicit constant-weight codes (e.g., based on algebraic geometry codes) we may achieve a number of tests equal to $O(t \frac{\log^2 N}{ \log t})$ for both the first and the second cases."
"Secure spectrum auctions can revolutionize the spectrum utilization of cellular networks and satisfy the ever increasing demand for resources. In this paper, a multi-tier dynamic spectrum sharing system is studied for efficient sharing of spectrum with commercial wireless system providers (WSPs), with an emphasis on federal spectrum sharing. The proposed spectrum sharing system optimizes usage of spectrum resources, manages intra-WSP and inter-WSP interference and provides essential level of security, privacy, and obfuscation to enable the most efficient and reliable usage of the shared spectrum. It features an intermediate spectrum auctioneer responsible for allocating resources to commercial WSPs by running secure spectrum auctions. The proposed secure spectrum auction, MTSSA, leverages Paillier cryptosystem to avoid possible fraud and bid-rigging. Numerical simulations are provided to compare the performance of MTSSA, in the considered spectrum sharing system, with other spectrum auction mechanisms for realistic cellular systems."
"It has been challenging to find ways to educate people to have better environmental consciousness. In some cases, people do not know what the right behaviors are to protect the environment. Game engine has been used in the AEC industry for visualization. However, it has barely been used in environmental consciousness education, for example, what operation can reduce building energy consumption, what items are recyclables. As social psychology studies show that video game can influence human behavior, a good designed game should provide the game player with right incentives and guide the users to make wiser choices for better environmental protection. This paper discussed a method to use serious game engines to educate the players the right actions that should be taken under in different scenarios. These actions in real life will results in a better environmental protection. The game proposed in this study is for residential home operation. Other scenarios such as restaurant operation, grocery store operations are discussed as expansion of this study. The game players points will be calculated based on their performance on different choices and when they surpass a certain level, different rewards will be gained in order for them to adjust their current living style. The purpose of the game is to raise the environmental consciousness among the game players and educate them the right actions they can make to better protect the environment while they are spending time on games."
"We study the approximate message-passing decoder for sparse superposition coding on the additive white Gaussian noise channel and extend our preliminary work [1]. We use heuristic statistical-physics-based tools such as the cavity and the replica methods for the statistical analysis of the scheme. While superposition codes asymptotically reach the Shannon capacity, we show that our iterative decoder is limited by a phase transition similar to the one that happens in Low Density Parity check codes. We consider two solutions to this problem, that both allow to reach the Shannon capacity: i) a power allocation strategy and ii) the use of spatial coupling, a novelty for these codes that appears to be promising. We present in particular simulations suggesting that spatial coupling is more robust and allows for better reconstruction at finite code lengths. Finally, we show empirically that the use of a fast Hadamard-based operator allows for an efficient reconstruction, both in terms of computational time and memory, and the ability to deal with very large messages."
"How low can the joint entropy of $n$ $d$-wise independent (for $d\ge2$) discrete random variables be, subject to given constraints on the individual distributions (say, no value may be taken by a variable with probability greater than $p$, for $p<1$)? This question has been posed and partially answered in a recent work of Babai.   In this paper we improve some of his bounds, prove new bounds in a wider range of parameters and show matching upper bounds in some special cases. In particular, we prove tight lower bounds for the min-entropy (as well as the entropy) of pairwise and three-wise independent balanced binary variables for infinitely many values of $n$."
"This paper introduces RankMap, a platform-aware end-to-end framework for efficient execution of a broad class of iterative learning algorithms for massive and dense datasets. Our framework exploits data structure to factorize it into an ensemble of lower rank subspaces. The factorization creates sparse low-dimensional representations of the data, a property which is leveraged to devise effective mapping and scheduling of iterative learning algorithms on the distributed computing machines. We provide two APIs, one matrix-based and one graph-based, which facilitate automated adoption of the framework for performing several contemporary learning applications. To demonstrate the utility of RankMap, we solve sparse recovery and power iteration problems on various real-world datasets with up to 1.8 billion non-zeros. Our evaluations are performed on Amazon EC2 and IBM iDataPlex servers using up to 244 cores. The results demonstrate up to two orders of magnitude improvements in memory usage, execution speed, and bandwidth compared with the best reported prior work, while achieving the same level of learning accuracy."
"We consider an special dynamics of a quantum walk (QW) on a line. Initially, the walker localized at the origin of the line with arbitrary chirality, evolves to an asymptotic stationary state. In this stationary state a measurement is performed and the state resulting from this measurement is used to start a second QW evolution to achieve a second asymptotic stationary state. In previous works, we developed the thermodynamics associated with the entanglement between the coin and position degrees of freedom in the QW. Here we study the application of the first and second laws of thermodynamics to the process between the two stationary states mentioned above. We show that: i) the entropy change has upper and lower bounds that are obtained analytically as a function of the initial conditions. ii) the energy change is associated to a heat-transfer process."
"Attributes act as intermediate representations that enable parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function that measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. Label embedding enjoys a built-in ability to leverage alternative sources of information instead of or in addition to attributes, such as e.g. class hierarchies or textual descriptions. Moreover, label embedding encompasses the whole range of learning settings from zero-shot learning to regular learning with a large number of labeled examples."
"The Wiener index of a graph is the sum of all pairwise shortest-path distances between its vertices. In this paper we study the novel problem of finding a minimum Wiener connector: given a connected graph $G=(V,E)$ and a set $Q\subseteq V$ of query vertices, find a subgraph of $G$ that connects all query vertices and has minimum Wiener index.   We show that The Minimum Wiener Connector admits a polynomial-time (albeit impractical) exact algorithm for the special case where the number of query vertices is bounded. We show that in general the problem is NP-hard, and has no PTAS unless $\mathbf{P} = \mathbf{NP}$. Our main contribution is a constant-factor approximation algorithm running in time $\widetilde{O}(|Q||E|)$.   A thorough experimentation on a large variety of real-world graphs confirms that our method returns smaller and denser solutions than other methods, and does so by adding to the query set $Q$ a small number of important vertices (i.e., vertices with high centrality)."
"Yannakakis showed that the matching problem does not have a small symmetric linear program. Rothvo{\ss} recently proved that any, not necessarily symmetric, linear program also has exponential size. It is natural to ask whether the matching problem can be expressed compactly in a framework such as semidefinite programming (SDP) that is more powerful than linear programming but still allows efficient optimization. We answer this question negatively for symmetric SDPs: any symmetric SDP for the matching problem has exponential size.   We also show that an O(k)-round Lasserre SDP relaxation for the metric traveling salesperson problem yields at least as good an approximation as any symmetric SDP relaxation of size $n^k$.   The key technical ingredient underlying both these results is an upper bound on the degree needed to derive polynomial identities that hold over the space of matchings or traveling salesperson tours."
"Android is an operating system that has been used in a majority of mobile devices. Each application in Android runs in an instance of the Dalvik virtual machine, which is a register-based virtual machine (VM). Most applications for Android are developed using Java, compiled to Java bytecode and then translated to DEX bytecode using the dx tool in the Android SDK. In this work, we aim to develop a type-based method for certifying non-interference properties of DEX bytecode, following a methodology that has been developed for Java bytecode certification by Barthe et al. To this end, we develop a formal operational semantics of the Dalvik VM, a type system for DEX bytecode, and prove the soundness of the type system with respect to a notion of non-interference. We then study the translation process from Java bytecode to DEX bytecode, as implemented in the dx tool in the Android SDK. We show that an abstracted version of the translation from Java bytecode to DEX bytecode preserves the non-interference property. More precisely, we show that if the Java bytecode is typable in Barthe et al's type system (which guarantees non-interference) then its translation is typable in our type system. This result opens up the possibility to leverage existing bytecode verifiers for Java to certify non-interference properties of Android bytecode."
"In this paper, we propose a variational multiphase image segmentation model based on fuzzy membership functions and L1-norm fidelity. Then we apply the alternating direction method of multipliers to solve an equivalent problem. All the subproblems can be solved efficiently. Specifically, we propose a fast method to calculate the fuzzy median. Experimental results and comparisons show that the L1-norm based method is more robust to outliers such as impulse noise and keeps better contrast than its L2-norm counterpart. Theoretically, we prove the existence of the minimizer and analyze the convergence of the algorithm."
"In this paper we consider the special case where a discrete signal ${\bf x}$ of length N is known to vanish outside a support interval of length $m < N$. If the support length $m$ of ${\bf x}$ or a good bound of it is a-priori known we derive a sublinear deterministic algorithm to compute ${\bf x}$ from its discrete Fourier transform. In case of exact Fourier measurements we require only ${\cal O}(m \log m)$ arithmetical operations. For noisy measurements, we propose a stable ${\cal O}(m \log N)$ algorithm."
"Knowledge reduction of dynamic covering information systems involves with the time in practical situations. In this paper, we provide incremental approaches to computing the type-1 and type-2 characteristic matrices of dynamic coverings because of varying attribute values. Then we present incremental algorithms of constructing the second and sixth approximations of sets by using characteristic matrices. We employ experimental results to illustrate that the incremental approaches are effective to calculate approximations of sets in dynamic covering information systems. Finally, we perform knowledge reduction of dynamic covering information systems with the incremental approaches."
"Word $W$ is said to encounter word $V$ provided there is a homomorphism $\phi$ mapping letters to nonempty words so that $\phi(V)$ is a substring of $W$. For example, taking $\phi$ such that $\phi(h)=c$ and $\phi(u)=ien$, we see that ""science"" encounters ""huh"" since $cienc=\phi(huh)$. The density of $V$ in $W$, $\delta(V,W)$, is the proportion of substrings of $W$ that are homomorphic images of $V$. So the density of ""huh"" in ""science"" is $2/{8 \choose 2}$. A word is doubled if every letter that appears in the word appears at least twice.   The dichotomy: Let $V$ be a word over any alphabet, $\Sigma$ a finite alphabet with at least 2 letters, and $W_n \in \Sigma^n$ chosen uniformly at random. Word $V$ is doubled if and only if $\mathbb{E}(\delta(V,W_n)) \rightarrow 0$ as $n \rightarrow \infty$.   We further explore convergence for nondoubled words and concentration of the limit distribution for doubled words around its mean."
"We present a single-node, multi-GPU programmable graph processing library that allows programmers to easily extend single-GPU graph algorithms to achieve scalable performance on large graphs with billions of edges. Directly using the single-GPU implementations, our design only requires programmers to specify a few algorithm-dependent concerns, hiding most multi-GPU related implementation details. We analyze the theoretical and practical limits to scalability in the context of varying graph primitives and datasets. We describe several optimizations, such as direction optimizing traversal, and a just-enough memory allocation scheme, for better performance and smaller memory consumption. Compared to previous work, we achieve best-of-class performance across operations and datasets, including excellent strong and weak scalability on most primitives as we increase the number of GPUs in the system."
"The minimal free resolution of the Jacobian ideals of the determinant polynomial were computed by Lascoux, and it is an active area of research to understand the Jacobian ideals of the permanent. As a step in this direction we compute several new cases and completely determine the linear strand of the minimal free resolutions of the ideals generated by sub-permanents. Our motivation is to lay the groundwork for the use of commutative algebra in algebraic complexity theory, building on the use of Hilbert functions in work of Gupta-Kamath-Kayal-Saptharishi. We compute several such Hilbert functions relevant for complexity theory."
"For two integers $r, \ell \geq 0$, a graph $G = (V, E)$ is an $(r,\ell)$-graph if $V$ can be partitioned into $r$ independent sets and $\ell$ cliques. In the parameterized $(r,\ell)$-Vertex Deletion problem, given a graph $G$ and an integer $k$, one has to decide whether at most $k$ vertices can be removed from $G$ to obtain an $(r,\ell)$-graph. This problem is NP-hard if $r+\ell \geq 1$ and encompasses several relevant problems such as Vertex Cover and Odd Cycle Transversal. The parameterized complexity of $(r,\ell)$-Vertex Deletion was known for all values of $(r,\ell)$ except for $(2,1)$, $(1,2)$, and $(2,2)$. We prove that each of these three cases is FPT and, furthermore, solvable in single-exponential time, which is asymptotically optimal in terms of $k$. We consider as well the version of $(r,\ell)$-Vertex Deletion where the set of vertices to be removed has to induce an independent set, and provide also a parameterized complexity dichotomy for this problem."
"In this paper, we construct the first families of asymmetric quantum convolutional codes (AQCC)'s. These new AQCC's are constructed by means of the CSS-type construction applied to suitable families of classical convolutional codes, which are also constructed here. The new codes have noncatastrophic generator matrices and they present great asymmetry. Since our constructions are performed algebraically, it is possible to derive several families of such codes and not only codes with specific parameters. Additionally, several different types of such codes are obtained."
"Although most business application data is stored in relational databases, programming languages and wire formats in integration middleware systems are not table-centric. Due to costly format conversions, data-shipments and faster computation, the trend is to ""push-down"" the integration operations closer to the storage representation.   We address the alternative case of defining declarative, table-centric integration semantics within standard integration systems. For that, we replace the current operator implementations for the well-known Enterprise Integration Patterns by equivalent ""in-memory"" table processing, and show a practical realization in a conventional integration system for a non-reliable, ""data-intensive"" messaging example. The results of the runtime analysis show that table-centric processing is promising already in standard, ""single-record"" message routing and transformations, and can potentially excel the message throughput for ""multi-record"" table messages."
"We present a deterministic $(1+o(1))$-approximation $O(n^{1/2+o(1)}+D^{1+o(1)})$-time algorithm for solving the single-source shortest paths problem on distributed weighted networks (the CONGEST model); here $n$ is the number of nodes in the network and $D$ is its (hop) diameter. This is the first non-trivial deterministic algorithm for this problem. It also improves (i) the running time of the randomized $(1+o(1))$-approximation $\tilde O(n^{1/2}D^{1/4}+D)$-time algorithm of Nanongkai [STOC 2014] by a factor of as large as $n^{1/8}$, and (ii) the $O(\epsilon^{-1}\log \epsilon^{-1})$-approximation factor of Lenzen and Patt-Shamir's $\tilde O(n^{1/2+\epsilon}+D)$-time algorithm [STOC 2013] within the same running time. Our running time matches the known time lower bound of $\Omega(n^{1/2}/\log n + D)$ [Das Sarma et al. STOC 2011] modulo some lower-order terms, thus essentially settling the status of this problem which was raised at least a decade ago [Elkin, SIGACT News 2004]. It also implies a $(2+o(1))$-approximation $O(n^{1/2+o(1)}+D^{1+o(1)})$-time algorithm for approximating a network's weighted diameter which almost matches the lower bound by Holzer et al. [PODC 2012]. In achieving this result, we develop two techniques which might be of independent interest and useful in other settings: (i) a deterministic process that replaces the ""hitting set argument"" commonly used for shortest paths computation in various settings, and (ii) a simple, deterministic, construction of an $(n^{o(1)}, o(1))$-hop set of size $O(n^{1+o(1)})$. We combine these techniques with many distributed algorithmic techniques, some of which from problems that are not directly related to shortest paths, e.g. ruling sets [Goldberg et al. STOC 1987], source detection [Lenzen, Peleg PODC 2013], and partial distance estimation [Lenzen, Patt-Shamir PODC 2015]."
"Bayesian max-margin models have shown superiority in various practical applications, such as text categorization, collaborative prediction, social network link prediction and crowdsourcing, and they conjoin the flexibility of Bayesian modeling and predictive strengths of max-margin learning. However, Monte Carlo sampling for these models still remains challenging, especially for applications that involve large-scale datasets. In this paper, we present the stochastic subgradient Hamiltonian Monte Carlo (HMC) methods, which are easy to implement and computationally efficient. We show the approximate detailed balance property of subgradient HMC which reveals a natural and validated generalization of the ordinary HMC. Furthermore, we investigate the variants that use stochastic subsampling and thermostats for better scalability and mixing. Using stochastic subgradient Markov Chain Monte Carlo (MCMC), we efficiently solve the posterior inference task of various Bayesian max-margin models and extensive experimental results demonstrate the effectiveness of our approach."
"We consider optimization of generalized performance metrics for binary classification by means of surrogate losses. We focus on a class of metrics, which are linear-fractional functions of the false positive and false negative rates (examples of which include $F_{\beta}$-measure, Jaccard similarity coefficient, AM measure, and many others). Our analysis concerns the following two-step procedure. First, a real-valued function $f$ is learned by minimizing a surrogate loss for binary classification on the training sample. It is assumed that the surrogate loss is a strongly proper composite loss function (examples of which include logistic loss, squared-error loss, exponential loss, etc.). Then, given $f$, a threshold $\widehat{\theta}$ is tuned on a separate validation sample, by direct optimization of the target performance metric. We show that the regret of the resulting classifier (obtained from thresholding $f$ on $\widehat{\theta}$) measured with respect to the target metric is upperbounded by the regret of $f$ measured with respect to the surrogate loss. We also extend our results to cover multilabel classification and provide regret bounds for micro- and macro-averaging measures. Our findings are further analyzed in a computational study on both synthetic and real data sets."
"Fixed parameter tractable algorithms for bounded treewidth are known to exist for a wide class of graph optimization problems. While most research in this area has been focused on exact algorithms, it is hard to find decompositions of treewidth sufficiently small to make these al- gorithms fast enough for practical use. Consequently, tree decomposition based algorithms have limited applicability to large scale optimization. However, by first reducing the input graph so that a small width tree decomposition can be found, we can harness the power of tree decomposi- tion based techniques in a heuristic algorithm, usable on graphs of much larger treewidth than would be tractable to solve exactly. We propose a solution merging heuristic to the Steiner Tree Problem that applies this idea. Standard local search heuristics provide a natural way to generate subgraphs with lower treewidth than the original instance, and subse- quently we extract an improved solution by solving the instance induced by this subgraph. As such the fixed parameter tractable algorithm be- comes an efficient tool for our solution merging heuristic. For a large class of sparse benchmark instances the algorithm is able to find small width tree decompositions on the union of generated solutions. Subsequently it can often improve on the generated solutions fast."
"We propose a novel approach to reduce the computational cost of evaluation of convolutional neural networks, a factor that has hindered their deployment in low-power devices such as mobile phones. Inspired by the loop perforation technique from source code optimization, we speed up the bottleneck convolutional layers by skipping their evaluation in some of the spatial positions. We propose and analyze several strategies of choosing these positions. We demonstrate that perforation can accelerate modern convolutional networks such as AlexNet and VGG-16 by a factor of 2x - 4x. Additionally, we show that perforation is complementary to the recently proposed acceleration method of Zhang et al."
"This paper explores a PAC (probably approximately correct) learning model in cooperative games. Specifically, we are given $m$ random samples of coalitions and their values, taken from some unknown cooperative game; can we predict the values of unseen coalitions? We study the PAC learnability of several well-known classes of cooperative games, such as network flow games, threshold task games, and induced subgraph games. We also establish a novel connection between PAC learnability and core stability: for games that are efficiently learnable, it is possible to find payoff divisions that are likely to be stable using a polynomial number of samples."
"We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ~0.25M images, ~0.76M questions, and ~10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa)."
"A graph $G$ has maximal local edge-connectivity $k$ if the maximum number of edge-disjoint paths between every pair of distinct vertices $x$ and $y$ is at most $k$. We prove Brooks-type theorems for $k$-connected graphs with maximal local edge-connectivity $k$, and for any graph with maximal local edge-connectivity 3. We also consider several related graph classes defined by constraints on connectivity. In particular, we show that there is a polynomial-time algorithm that, given a 3-connected graph $G$ with maximal local connectivity 3, outputs an optimal colouring for $G$. On the other hand, we prove, for $k \ge 3$, that $k$-colourability is NP-complete when restricted to minimally $k$-connected graphs, and 3-colourability is NP-complete when restricted to $(k-1)$-connected graphs with maximal local connectivity $k$. Finally, we consider a parameterization of $k$-colourability based on the number of vertices of degree at least $k+1$, and prove that, even when $k$ is part of the input, the corresponding parameterized problem is FPT."
"Let $G=(V,E)$ be a graph of density $p$ on $n$ vertices. Following Erd\H{o}s, \L uczak and Spencer, an $m$-vertex subgraph $H$ of $G$ is called {\em full} if $H$ has minimum degree at least $p(m - 1)$. Let $f(G)$ denote the order of a largest full subgraph of $G$. If $p\binom{n}{2}$ is a non-negative integer, define \[ f(n,p) = \min\{f(G) : \vert V(G)\vert = n, \ \vert E(G)\vert = p\binom{n}{2} \}.\] Erd\H{o}s, \L uczak and Spencer proved that for $n \geq 2$, \[ (2n)^{\frac{1}{2}} - 2 \leq f(n, {\frac{1}{2}}) \leq 4n^{\frac{2}{3}}(\log n)^{\frac{1}{3}}.\] In this paper, we prove the following lower bound: for $n^{-\frac{2}{3}} <p_n <1-n^{-\frac{1}{7}}$, \[ f(n,p) \geq \frac{1}{4}(1-p)^{\frac{2}{3}}n^{\frac{2}{3}} -1.\] Furthermore we show that this is tight up to a multiplicative constant factor for infinitely many $p$ near the elements of $\{\frac{1}{2},\frac{2}{3},\frac{3}{4},\dots\}$. In contrast, we show that for any $n$-vertex graph $G$, either $G$ or $G^c$ contains a full subgraph on $\Omega(\frac{n}{\log n})$ vertices. Finally, we discuss full subgraphs of random and pseudo-random graphs, and several open problems."
"With the rise of Software Defined Networks (SDN), there is growing interest in dynamic and centralized traffic engineering, where decisions about forwarding paths are taken dynamically from a network-wide perspective. Frequent path reconfiguration can significantly improve the network performance, but should be handled with care, so as to minimize disruptions that may occur during network updates.   In this paper we introduce Time4, an approach that uses accurate time to coordinate network updates. Time4 is a powerful tool in softwarized environments, that can be used for various network update scenarios. Specifically, we characterize a set of update scenarios called flow swaps, for which Time4 is the optimal update approach, yielding less packet loss than existing update approaches. We define the lossless flow allocation problem, and formally show that in environments with frequent path allocation, scenarios that require simultaneous changes at multiple network devices are inevitable.   We present the design, implementation, and evaluation of a Time4-enabled OpenFlow prototype. The prototype is publicly available as open source. Our work includes an extension to the OpenFlow protocol that has been adopted by the Open Networking Foundation (ONF), and is now included in OpenFlow 1.5. Our experimental results show the significant advantages of Time4 compared to other network update approaches, and demonstrate an SDN use case that is infeasible without Time4."
"This paper explores the relationship between author-level bibliometric indicators and the researchers the ""measure"", exemplified across five academic seniorities and four disciplines. Using cluster methodology, the disciplinary and seniority appropriateness of author-level indicators is examined. Publication and citation data for 741 researchers across Astronomy, Environmental Science, Philosophy and Public Health was collected in Web of Science (WoS). Forty-four indicators of individual performance were computed using the data. A two-step cluster analysis using IBM SPSS version 22 was performed, followed by a risk analysis and ordinal logistic regression to explore cluster membership. Indicator scores were contextualized using the individual researcher's curriculum vitae. Four different clusters based on indicator scores ranked researchers as low, middle, high and extremely high performers. The results show that different indicators were appropriate in demarcating ranked performance in different disciplines. In Astronomy the h2 indicator, sum pp top prop in Environmental Science, Q2 in Philosophy and e-index in Public Health. The regression and odds analysis showed individual level indicator scores were primarily dependent on the number of years since the researcher's first publication registered in WoS, number of publications and number of citations. Seniority classification was secondary therefore no seniority appropriate indicators were confidently identified. Cluster methodology proved useful in identifying disciplinary appropriate indicators providing the preliminary data preparation was thorough but needed to be supplemented by other analyses to validate the results. A general disconnection between the performance of the researcher on their curriculum vitae and the performance of the researcher based on bibliometric indicators was observed."
"Metal-oxide memristors have emerged as promising candidates for hardware implementation of artificial synapses - the key components of high-performance, analog neuromorphic networks - due to their excellent scaling prospects. Since some advanced cognitive tasks require spiking neuromorphic networks, which explicitly model individual neural pulses (spikes) in biological neural systems, it is crucial for memristive synapses to support the spike-time-dependent plasticity (STDP), which is believed to be the primary mechanism of Hebbian adaptation. A major challenge for the STDP implementation is that, in contrast to some simplistic models of the plasticity, the elementary change of a synaptic weight in an artificial hardware synapse depends not only on the pre-synaptic and post-synaptic signals, but also on the initial weight (memristor's conductance) value. Here we experimentally demonstrate, for the first time, STDP protocols that ensure self-adaptation of the average memristor conductance, making the plasticity stable, i.e. insensitive to the initial state of the devices. The experiments have been carried out with 200-nm Al2O3/TiO2-x memristors integrated into 12x12 crossbars. The experimentally observed self-adaptive STDP behavior has been complemented with numerical modeling of weight dynamics in a simple system with a leaky-integrate-and-fire neuron with a random spike-train input, using a compact model of memristor plasticity, fitted for quantitatively correct description of our memristors."
"CPUs and operating systems are moving from 32 to 64 bits, and hence it is important to have good pseudorandom number generators designed to fully exploit these word lengths. However, existing 64-bit very long period generators based on linear recurrences modulo 2 are not completely optimized in terms of the equidistribution properties. Here we develop 64-bit maximally equidistributed pseudorandom number generators that are optimal in this respect and have speeds equivalent to 64-bit Mersenne Twisters. We provide a table of specific parameters with period lengths from $2^{607}-1$ to $2^{44497}-1$. (An online appendix is available at http://www.ritsumei.ac.jp/~harase/memt-64-app.pdf)"
"We present an innovative method called FilExSeC (Filaments Extraction, Selection and Classification), a data mining tool developed to investigate the possibility to refine and optimize the shape reconstruction of filamentary structures detected with a consolidated method based on the flux derivative analysis, through the column-density maps computed from Herschel infrared Galactic Plane Survey (Hi-GAL) observations of the Galactic plane. The present methodology is based on a feature extraction module followed by a machine learning model (Random Forest) dedicated to select features and to classify the pixels of the input images. From tests on both simulations and real observations the method appears reliable and robust with respect to the variability of shape and distribution of filaments. In the cases of highly defined filament structures, the presented method is able to bridge the gaps among the detected fragments, thus improving their shape reconstruction. From a preliminary ""a posteriori"" analysis of derived filament physical parameters, the method appears potentially able to add a sufficient contribution to complete and refine the filament reconstruction."
"Linear dimensionality reduction techniques are powerful tools for image analysis as they allow the identification of important features in a data set. In particular, nonnegative matrix factorization (NMF) has become very popular as it is able to extract sparse, localized and easily interpretable features by imposing an additive combination of nonnegative basis elements. Nonnegative matrix underapproximation (NMU) is a closely related technique that has the advantage to identify features sequentially. In this paper, we propose a variant of NMU that is particularly well suited for image analysis as it incorporates the spatial information, that is, it takes into account the fact that neighboring pixels are more likely to be contained in the same features, and favors the extraction of localized features by looking for sparse basis elements. We show that our new approach competes favorably with comparable state-of-the-art techniques on synthetic, facial and hyperspectral image data sets."
"Limit theory means an advanced analysis of the asymptotic behavior of discrete problems when some finite parameter tends to infinity. Namely, we are looking for limits of finite problems for which we can prove that the solutions of the finite problems converge to the solution of the limit problem. We show this simple but powerful technique on three problems: the Manickam-Mikl\'os-Singhi Conjecture, the Kikuta-Ruckle Conjecture and Alpern's Caching Game."
Heisenberg-like and Fisher-information-based uncertainty relations which extend and generalize previous similar expressions are obtained for $N$-fermion $d$-dimensional systems. The contributions of both spatial and spin degrees of freedom are taken into account. The accuracy of some of these generalized spinned uncertainty-like relations is numerically examined for a large number of atomic and molecular systems.
"In this paper, we address two optimisation problems arising in the context of city logistics and two-level transportation systems. The two-echelon vehicle routing problem and the two-echelon location routing problem seek to produce vehicle itineraries to deliver goods to customers, with transits through intermediate facilities. To efficiently solve these problems, we propose a hybrid metaheuristic which combines enumerative local searches with destroy-and-repair principles, as well as some tailored operators to optimise the selections of intermediate facilities. We conduct extensive computational experiments to investigate the contribution of these operators to the search performance, and measure the performance of the method on both problem classes. The proposed algorithm finds the current best known solutions, or better ones, for 95% of the two-echelon vehicle routing problem benchmark instances. Overall, for both problems, it achieves high-quality solutions within short computing times. Finally, for future reference, we resolve inconsistencies between different versions of benchmark instances, document their differences, and provide them all online in a unified format."
"We consider a communication network consisting of nodes and directed edges that connect the nodes. The network may contain cycles. The communications are slotted where the duration of each time slot is equal to the maximum propagation delay experienced by the edges. The edges with negligible delays are allowed to be operated before the other edges in each time slot. For any pair of adjacent edges $(\ell, i)$ and $(i,j)$ where $(\ell,i)$ terminates at node $i$ and $(i,j)$ originates from node $i$, we say $(\ell,i)$ incurs zero delay on $(i,j)$ if $(\ell,i)$ is operated before $(i,j)$; otherwise, we say $(\ell,i)$ incurs a unit delay on $(i,j)$. In the classical model, every edge incurs a unit delay on every adjacent edge and the cut-set bound is a well-known outer bound on the capacity region. In this paper, we investigate the multimessage multicast network (MMN) consisting of independent channels where each channel is associated with a set of edges and each edge may incur zero delay on some other edges. Our result reveals that the capacity region of the MMN with independent channels and zero-delay edges lies within the classical cut-set bound despite a violation of the unit-delay assumption."
"Navigational graph queries are an important class of queries that canextract implicit binary relations over the nodes of input graphs. Most of the navigational query languages used in the RDF community, e.g. property paths in W3C SPARQL 1.1 and nested regular expressions in nSPARQL, are based on the regular expressions. It is known that regular expressions have limited expressivity; for instance, some natural queries, like same generation-queries, are not expressible with regular expressions. To overcome this limitation, in this paper, we present cfSPARQL, an extension of SPARQL query language equipped with context-free grammars. The cfSPARQL language is strictly more expressive than property paths and nested expressions. The additional expressivity can be used for modelling graph similarities, graph summarization and ontology alignment. Despite the increasing expressivity, we show that cfSPARQL still enjoys a low computational complexity and can be evaluated efficiently."
"It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate ""reasonable bounds"" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks."
"The topological structure of complex networks has fascinated researchers for several decades, resulting in the discovery of many universal properties and reoccurring characteristics of different kinds of networks. However, much less is known today about the network dynamics: indeed, complex networks in reality are not static, but rather dynamically evolve over time.   Our paper is motivated by the empirical observation that network evolution patterns seem far from random, but exhibit structure. Moreover, the specific patterns appear to depend on the network type, contradicting the existence of a ""one fits it all"" model. However, we still lack observables to quantify these intuitions, as well as metrics to compare graph evolutions. Such observables and metrics are needed for extrapolating or predicting evolutions, as well as for interpolating graph evolutions.   To explore the many faces of graph dynamics and to quantify temporal changes, this paper suggests to build upon the concept of centrality, a measure of node importance in a network. In particular, we introduce the notion of centrality distance, a natural similarity measure for two graphs which depends on a given centrality, characterizing the graph type. Intuitively, centrality distances reflect the extent to which (non-anonymous) node roles are different or, in case of dynamic graphs, have changed over time, between two graphs.   We evaluate the centrality distance approach for five evolutionary models and seven real-world social and physical networks. Our results empirically show the usefulness of centrality distances for characterizing graph dynamics compared to a null-model of random evolution, and highlight the differences between the considered scenarios. Interestingly, our approach allows us to compare the dynamics of very different networks, in terms of scale and evolution speed."
"Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning."
"We define and study the problem of predicting the solution to a linear program (LP) given only partial information about its objective and constraints. This generalizes the problem of learning to predict the purchasing behavior of a rational agent who has an unknown objective function, that has been studied under the name ""Learning from Revealed Preferences"". We give mistake bound learning algorithms in two settings: in the first, the objective of the LP is known to the learner but there is an arbitrary, fixed set of constraints which are unknown. Each example is defined by an additional known constraint and the goal of the learner is to predict the optimal solution of the LP given the union of the known and unknown constraints. This models the problem of predicting the behavior of a rational agent whose goals are known, but whose resources are unknown. In the second setting, the objective of the LP is unknown, and changing in a controlled way. The constraints of the LP may also change every day, but are known. An example is given by a set of constraints and partial information about the objective, and the task of the learner is again to predict the optimal solution of the partially known LP."
"Recently, the reference functions for the synthesis and analysis of the autostereoscopic multiview and integral images in three-dimensional displays we introduced. In the current paper, we propose the wavelets to analyze such images. The wavelets are built on the reference functions as on the scaling functions of the wavelet analysis. The continuous wavelet transform was successfully applied to the testing wireframe binary objects. The restored locations correspond to the structure of the testing wireframe binary objects."
"Finding a fixed point to a nonexpansive operator, i.e., $x^*=Tx^*$, abstracts many problems in numerical linear algebra, optimization, and other areas of scientific computing. To solve fixed-point problems, we propose ARock, an algorithmic framework in which multiple agents (machines, processors, or cores) update $x$ in an asynchronous parallel fashion. Asynchrony is crucial to parallel computing since it reduces synchronization wait, relaxes communication bottleneck, and thus speeds up computing significantly. At each step of ARock, an agent updates a randomly selected coordinate $x_i$ based on possibly out-of-date information on $x$. The agents share $x$ through either global memory or communication. If writing $x_i$ is atomic, the agents can read and write $x$ without memory locks.   Theoretically, we show that if the nonexpansive operator $T$ has a fixed point, then with probability one, ARock generates a sequence that converges to a fixed points of $T$. Our conditions on $T$ and step sizes are weaker than comparable work. Linear convergence is also obtained.   We propose special cases of ARock for linear systems, convex optimization, machine learning, as well as distributed and decentralized consensus problems. Numerical experiments of solving sparse logistic regression problems are presented."
"We present a deep layered architecture that generalizes convolutional neural networks (ConvNets). The architecture, called SimNets, is driven by two operators: (i) a similarity function that generalizes inner-product, and (ii) a log-mean-exp function called MEX that generalizes maximum and average. The two operators applied in succession give rise to a standard neuron but in ""feature space"". The feature spaces realized by SimNets depend on the choice of the similarity operator. The simplest setting, which corresponds to a convolution, realizes the feature space of the Exponential kernel, while other settings realize feature spaces of more powerful kernels (Generalized Gaussian, which includes as special cases RBF and Laplacian), or even dynamically learned feature spaces (Generalized Multiple Kernel Learning). As a result, the SimNet contains a higher abstraction level compared to a traditional ConvNet. We argue that enhanced expressiveness is important when the networks are small due to run-time constraints (such as those imposed by mobile applications). Empirical evaluation validates the superior expressiveness of SimNets, showing a significant gain in accuracy over ConvNets when computational resources at run-time are limited. We also show that in large-scale settings, where computational complexity is less of a concern, the additional capacity of SimNets can be controlled with proper regularization, yielding accuracies comparable to state of the art ConvNets."
"In this paper, we present a mathematical and algorithmic framework for the continuation of point clouds by persistence diagrams. A key property used in the method is that the persistence map, which assigns a persistence diagram to a point cloud, is differentiable. This allows us to apply the Newton-Raphson continuation method in this setting. Given an original point cloud $P$, its persistence diagram $D$, and a target persistence diagram $D'$, we gradually move from $D$ to $D'$, by successively computing intermediate point clouds until we finally find a point cloud $P'$ having $D'$ as its persistence diagram. Our method can be applied to a wide variety of situations in topological data analysis where it is necessary to solve an inverse problem, from persistence diagrams to point cloud data."
"We study an optimal control problem aimed at achieving a desired tradeoff between the network coherence and communication requirements in the distributed controller. Our objective is to add a certain number of edges to an undirected network, with a known graph Laplacian, in order to optimally enhance closed-loop performance. To promote controller sparsity, we introduce $\ell_1$-regularization into the optimal ${\cal H}_2$ formulation and cast the design problem as a semidefinite program. We derive a Lagrange dual, provide interpretation of dual variables, and exploit structure of the optimality conditions for undirected networks to develop customized proximal gradient and Newton algorithms that are well-suited for large problems. We illustrate that our algorithms can solve the problems with more than million edges in the controller graph in a few minutes, on a PC. We also exploit structure of connected resistive networks to demonstrate how additional edges can be systematically added in order to minimize the ${\cal H}_2$ norm of the closed-loop system."
We propose a localized approach to multiple kernel learning that can be formulated as a convex optimization problem over a given cluster structure. For which we obtain generalization error guarantees and derive an optimization algorithm based on the Fenchel dual representation. Experiments on real-world datasets from the application domains of computational biology and computer vision show that convex localized multiple kernel learning can achieve higher prediction accuracies than its global and non-convex local counterparts.
"We study the quadratic residue weight enumerators of the dual projective Reed-Solomon codes of dimensions $5$ and $q-4$ over the finite field $\mathbb{F}_q$. Our main results are formulas for the coefficients of the the quadratic residue weight enumerators for such codes. If $q=p^v$ and we fix $v$ and vary $p$ then our formulas for the coefficients of the dimension $q-4$ code involve only polynomials in $p$ and the trace of the $q$th and $(q/p^2)$th Hecke operators acting on spaces of cusp forms for the congruence groups $\operatorname{SL}_2 (\mathbb{Z}), \Gamma_0(2)$, and $\Gamma_0(4)$. The main tool we use is the Eichler-Selberg trace formula, which gives along the way a variation of a theorem of Birch on the distribution of rational point counts for elliptic curves with prescribed $2$-torsion over a fixed finite field."
"We study a non-convex low-rank promoting penalty function, the transformed Schatten-1 (TS1), and its applications in matrix completion. The TS1 penalty, as a matrix quasi-norm defined on its singular values, interpolates the rank and the nuclear norm through a nonnegative parameter a. We consider the unconstrained TS1 regularized low-rank matrix recovery problem and develop a fixed point representation for its global minimizer. The TS1 thresholding functions are in closed analytical form for all parameter values. The TS1 threshold values differ in subcritical (supercritical) parameter regime where the TS1 threshold functions are continuous (discontinuous). We propose TS1 iterative thresholding algorithms and compare them with some state-of-the-art algorithms on matrix completion test problems. For problems with known rank, a fully adaptive TS1 iterative thresholding algorithm consistently performs the best under different conditions with ground truth matrix being multivariate Gaussian at varying covariance. For problems with unknown rank, TS1 algorithms with an additional rank estimation procedure approach the level of IRucL-q which is an iterative reweighted algorithm, non-convex in nature and best in performance."
"A new solver featuring time-space adaptation and error control has been recently introduced to tackle the numerical solution of stiff reaction-diffusion systems. Based on operator splitting, finite volume adaptive multiresolution and high order time integrators with specific stability properties for each operator, this strategy yields high computational efficiency for large multidimensional computations on standard architectures such as powerful workstations. However, the data structure of the original implementation, based on trees of pointers, provides limited opportunities for efficiency enhancements, while posing serious challenges in terms of parallel programming and load balancing. The present contribution proposes a new implementation of the whole set of numerical methods including Radau5 and ROCK4, relying on a fully different data structure together with the use of a specific library, TBB, for shared-memory, task-based parallelism with work-stealing. The performance of our implementation is assessed in a series of test-cases of increasing difficulty in two and three dimensions on multi-core and many-core architectures, demonstrating high scalability."
"Many applications in vision require estimation of thin structures such as boundary edges, surfaces, roads, blood vessels, neurons, etc. Unlike most previous approaches, we simultaneously detect and delineate thin structures with sub-pixel localization and real-valued orientation estimation. This is an ill-posed problem that requires regularization. We propose an objective function combining detection likelihoods with a prior minimizing curvature of the center-lines or surfaces. Unlike simple block-coordinate descent, we develop a novel algorithm that is able to perform joint optimization of location and detection variables more effectively. Our lower bound optimization algorithm applies to quadratic or absolute curvature. The proposed early vision framework is sufficiently general and it can be used in many higher-level applications. We illustrate the advantage of our approach on a range of 2D and 3D examples."
"This paper evaluates four algorithms for denoising raw Electrooculography (EOG) data based on the Signal to Noise Ratio (SNR). The SNR is computed using the eigenvalue method. The filtering algorithms are a) Finite Impulse Response (FIR) bandpass filters, b) Stationary Wavelet Transform, c) Empirical Mode Decomposition (EMD) d) FIR Median Hybrid Filters. An EOG dataset has been prepared where the subject is asked to perform letter cancelation test on 20 subjects."
"We prove that any polynomial-time $\alpha(n)$-approximation algorithm for the $n$-vertex metric asymmetric Traveling Salesperson Problem yields a polynomial-time $O(\alpha(C))$-approximation algorithm for the mixed and windy Capacitated Arc Routing Problem, where $C$ is the number of weakly connected components in the subgraph induced by the positive-demand arcs---a small number in many applications. In conjunction with known results, we obtain constant-factor approximations for $C\in O(\log n)$ and $O(\log C/\log\log C)$-approximations in general. Experiments show that our algorithm, together with several heuristic enhancements, outperforms many previous polynomial-time heuristics. Finally, since the solution quality achievable in polynomial time appears to mainly depend on $C$ and since $C=1$ in almost all benchmark instances, we propose the Ob benchmark set, simulating cities that are divided into several components by a river."
"We prove that every simple bridgeless cubic graph with n >= 8 vertices has a travelling salesman tour of length at most 1.3n - 2, which can be constructed in polynomial time."
"In a physical neural system, where storage and processing are intimately intertwined, the rules for adjusting the synaptic weights can only depend on variables that are available locally, such as the activity of the pre- and post-synaptic neurons, resulting in local learning rules. A systematic framework for studying the space of local learning rules is obtained by first specifying the nature of the local variables, and then the functional form that ties them together into each learning rule. Such a framework enables also the systematic discovery of new learning rules and exploration of relationships between learning rules and group symmetries. We study polynomial local learning rules stratified by their degree and analyze their behavior and capabilities in both linear and non-linear units and networks. Stacking local learning rules in deep feedforward networks leads to deep local learning. While deep local learning can learn interesting representations, it cannot learn complex input-output functions, even when targets are available for the top layer. Learning complex input-output functions requires local deep learning where target information is communicated to the deep layers through a backward learning channel. The nature of the communicated information about the targets and the structure of the learning channel partition the space of learning algorithms. We estimate the learning channel capacity associated with several algorithms and show that backpropagation outperforms them by simultaneously maximizing the information rate and minimizing the computational cost, even in recurrent networks. The theory clarifies the concept of Hebbian learning, establishes the power and limitations of local learning rules, introduces the learning channel which enables a formal analysis of the optimality of backpropagation, and explains the sparsity of the space of learning rules discovered so far."
"We use a recently discovered constrained de Finetti reduction (aka ""Post-Selection Lemma"") to study the parallel repetition of multi-player non-local games under no-signalling strategies. Since the technique allows us to reduce general strategies to independent plays, we obtain parallel repetition (corresponding to winning all rounds) in the same way as exponential concentration of the probability to win a fraction larger than the value of the game.   Our proof technique leads us naturally to a relaxation of no-signalling (NS) strategies, which we dub sub-no-signalling (SNOS). While for two players the two concepts coincide, they differ for three or more players. Our results are most complete and satisfying for arbitrary number of sub-no-signalling players, where we get universal parallel repetition and concentration for any game, while the no-signalling case is obtained as a corollary, but only for games with ""full support""."
"Design mining is the use of computational intelligence techniques to iteratively search and model the attribute space of physical objects evaluated directly through rapid prototyping to meet given objectives. It enables the exploitation of novel materials and processes without formal models or complex simulation. In this paper, we focus upon the coevolutionary nature of the design process when it is decomposed into concurrent sub-design threads due to the overall complexity of the task. Using an abstract, tuneable model of coevolution we consider strategies to sample sub-thread designs for whole system testing, how best to construct and use surrogate models within the coevolutionary scenario, and the effects of access to multiple whole system (physical) testing equipment on performance. Drawing on our findings, the paper then describes the effective design of an array of six heterogeneous vertical-axis wind turbines."
"Consider a two-party correlation that can be generated by performing local measurements on a bipartite quantum system. A question of fundamental importance is to understand how many resources, which we quantify by the dimension of the underlying quantum system, are needed to reproduce this correlation. In this Letter, we identify an easy-to-compute lower bound on the smallest Hilbert space dimension needed to generate a given two-party quantum correlation. We show that our bound is tight on many well-known correlations and discuss how it can rule out correlations of having a finite-dimensional quantum representation. We show that our bound is multiplicative under product correlations and also that it can witness the non-convexity of certain restricted-dimensional quantum correlations."
"In this work we establish and investigate connections between causes for query answers in databases, database repairs wrt. denial constraints, and consistency-based diagnosis. The first two are relatively new research areas in databases, and the third one is an established subject in knowledge representation. We show how to obtain database repairs from causes, and the other way around. Causality problems are formulated as diagnosis problems, and the diagnoses provide causes and their responsibilities. The vast body of research on database repairs can be applied to the newer problems of computing actual causes for query answers and their responsibilities. These connections, which are interesting per se, allow us, after a transition -inspired by consistency-based diagnosis- to computational problems on hitting sets and vertex covers in hypergraphs, to obtain several new algorithmic and complexity results for database causality."
"We consider the problem of how to assign treatment in a randomized experiment, in which the correlation among the outcomes is informed by a network available pre-intervention. Working within the potential outcome causal framework, we develop a class of models that posit such a correlation structure among the outcomes. Then we leverage these models to develop rerandomization strategies for allocating treatment optimally, by minimizing the mean squared error of the estimated average treatment effect. Analytical decompositions of the mean squared error, due both to the model and to the randomization distribution, provide insights into aspects of the optimal designs. In particular, the analysis suggests new notions of balance based on specific network quantities, in addition to classical covariate balance. The resulting balanced, optimal rerandomization strategies are still design unbiased, in situations where the model used to derive them does not hold. We illustrate how the proposed treatment allocation strategies improve on allocations that ignore the network structure, with extensive simulations."
"Multilayer networks are a useful data structure for simultaneously capturing multiple types of relationships between a set of nodes. In such networks, each relational definition gives rise to a layer. While each layer provides its own set of information, community structure across layers can be collectively utilized to discover and quantify underlying relational patterns between nodes. To concisely extract information from a multilayer network, we propose to identify and combine sets of layers with meaningful similarities in community structure. In this paper, we describe the ""strata multilayer stochastic block model'' (sMLSBM), a probabilistic model for multilayer community structure. The central extension of the model is that there exist groups of layers, called ""strata'', which are defined such that all layers in a given stratum have community structure described by a common stochastic block model (SBM). That is, layers in a stratum exhibit similar node-to-community assignments and SBM probability parameters. Fitting the sMLSBM to a multilayer network provides a joint clustering that yields node-to-community and layer-to-stratum assignments, which cooperatively aid one another during inference. We describe an algorithm for separating layers into their appropriate strata and an inference technique for estimating the SBM parameters for each stratum. We demonstrate our method using synthetic networks and a multilayer network inferred from data collected in the Human Microbiome Project."
"In this paper we consider two metric covering/clustering problems - \textit{Minimum Cost Covering Problem} (MCC) and $k$-clustering. In the MCC problem, we are given two point sets $X$ (clients) and $Y$ (servers), and a metric on $X \cup Y$. We would like to cover the clients by balls centered at the servers. The objective function to minimize is the sum of the $\alpha$-th power of the radii of the balls. Here $\alpha \geq 1$ is a parameter of the problem (but not of a problem instance). MCC is closely related to the $k$-clustering problem. The main difference between $k$-clustering and MCC is that in $k$-clustering one needs to select $k$ balls to cover the clients.   For any $\eps > 0$, we describe quasi-polynomial time $(1 + \eps)$ approximation algorithms for both of the problems. However, in case of $k$-clustering the algorithm uses $(1 + \eps)k$ balls. Prior to our work, a $3^{\alpha}$ and a ${c}^{\alpha}$ approximation were achieved by polynomial-time algorithms for MCC and $k$-clustering, respectively, where $c > 1$ is an absolute constant. These two problems are thus interesting examples of metric covering/clustering problems that admit $(1 + \eps)$-approximation (using $(1+\eps)k$ balls in case of $k$-clustering), if one is willing to settle for quasi-polynomial time. In contrast, for the variant of MCC where $\alpha$ is part of the input, we show under standard assumptions that no polynomial time algorithm can achieve an approximation factor better than $O(\log |X|)$ for $\alpha \geq \log |X|$."
"This paper investigates the information-theoretic limits of energy-harvesting (EH) channels in the finite blocklength regime. The EH process is characterized by a sequence of i.i.d. random variables with finite variances. We use the save-and-transmit strategy proposed by Ozel and Ulukus (2012) together with Shannon's non-asymptotic achievability bound to obtain lower bounds on the achievable rates for both additive white Gaussian noise channels and discrete memoryless channels under EH constraints. The first-order terms of the lower bounds of the achievable rates are equal to $C$ and the second-order (backoff from capacity) terms are proportional to $-\sqrt{ \frac{\log n}{n}}$, where $n$ denotes the blocklength and $C$ denotes the capacity of the EH channel, which is the same as the capacity without the EH constraints. The constant of proportionality of the backoff term is found and qualitative interpretations are provided."
"We present necessary and sufficient optimality conditions for finite time optimal control problems for a class of hybrid systems described by linear complementarity models. Although these optimal control problems are difficult in general due to the presence of complementarity constraints, we provide a set of structural assumptions ensuring that the tangent cone of the constraints possesses geometric regularity properties. These imply that the classical Karush-Kuhn-Tucker conditions of nonlinear programming theory are both necessary and sufficient for local optimality, which is not the case for general mathematical programs with complementarity constraints. We also present sufficient conditions for global optimality.   We proceed to show that the dynamics of every continuous piecewise affine system can be written as the optimizer of a mathematical program which results in a linear complementarity model satisfying our structural assumptions. Hence, our stationarity results apply to a large class of hybrid systems with piecewise affine dynamics. We present simulation results showing the substantial benefits possible from using a nonlinear programming approach to the optimal control problem with complementarity constraints instead of a more traditional mixed-integer formulation."
"This paper proposes and investigates a Boolean gossiping model as a simplified but non-trivial probabilistic Boolean network. With positive node interactions, in view of standard theories from Markov chains, we prove that the node states asymptotically converge to an agreement at a binary random variable, whose distribution is characterized for large-scale networks by mean-field approximation. Using combinatorial analysis, we also successfully count the number of communication classes of the positive Boolean network explicitly in terms of the topology of the underlying interaction graph, where remarkably minor variation in local structures can drastically change the number of network communication classes. With general Boolean interaction rules, emergence of absorbing network Boolean dynamics is shown to be determined by the network structure with necessary and sufficient conditions established regarding when the Boolean gossiping process defines absorbing Markov chains. These results illustrate possibilities of relating detailed dynamical properties of Boolean networks to graphical properties of the underlying interactions."
"A MapReduce algorithm can be described by a mapping schema, which assigns inputs to a set of reducers, such that for each required output there exists a reducer that receives all the inputs that participate in the computation of this output. Reducers have a capacity, which limits the sets of inputs that they can be assigned. However, individual inputs may vary in terms of size. We consider, for the first time, mapping schemas where input sizes are part of the considerations and restrictions. One of the significant parameters to optimize in any MapReduce job is communication cost between the map and reduce phases. The communication cost can be optimized by minimizing the number of copies of inputs sent to the reducers. The communication cost is closely related to the number of reducers of constrained capacity that are used to accommodate appropriately the inputs, so that the requirement of how the inputs must meet in a reducer is satisfied. In this work, we consider a family of problems where it is required that each input meets with each other input in at least one reducer. We also consider a slightly different family of problems in which, each input of a list, X, is required to meet each input of another list, Y, in at least one reducer. We prove that finding an optimal mapping schema for these families of problems is NP-hard, and present a bin-packing-based approximation algorithm for finding a near optimal mapping schema."
"We introduce a variation of the scheduling with precedence constraints problem that has applications to molecular folding and production management. We are given a bipartite graph $H=(B,S)$. Vertices in $B$ are thought of as goods or services that must be \emph{bought} to produce items in $S$ that are to be \emph{sold}. An edge from $j\in S$ to $i\in B$ indicates that the production of $j$ requires the purchase of $i$. Each vertex in $B$ has a cost, and each vertex in $S$ results in some gain. The goal is to obtain an ordering of $B\cup S$ that respects the precedence constraints and maximizes the minimal net profit encountered as the vertices are processed. We call this optimal value the \emph{budget} or \emph{capital} investment required for the bipartite graph, and refer to our problem as \emph{the bipartite graph ordering problem}.   The problem is equivalent to a version of an NP-complete molecular folding problem that has been studied recently [12]. Work on the molecular folding problem has focused on heuristic algorithms and exponential-time exact algorithms for the un-weighted problem where costs are $\pm 1$ and when restricted to graphs arising from RNA folding.   The bipartite graph present work seeks exact algorithms for solving the bipartite ordering problem. We demonstrate an algorithm that computes the optimal ordering in time $O^*(2^n)$ when $n$ is the number of vertices in the input bipartite graph. Our main result is a general strategy that can be used to find an optimal ordering in polynomial time for bipartite graphs that satisfy certain properties. We apply the technique to a variety of graph classes, obtaining polynomial-time solutions to the bipartite graph ordering problem for bipartite permutation graphs, trivially perfect, co-bipartite graphs, and trees."
"Recent work has proposed a promising approach to improving scalability of program synthesis by allowing the user to supply a syntactic template that constrains the space of potential programs. Unfortunately, creating templates often requires nontrivial effort from the user, which impedes the usability of the synthesizer. We present a solution to this problem in the context of recursive transformations on algebraic data-types. Our approach relies on polymorphic synthesis constructs: a small but powerful extension to the language of syntactic templates, which makes it possible to define a program space in a concise and highly reusable manner, while at the same time retains the scalability benefits of conventional templates. This approach enables end-users to reuse predefined templates from a library for a wide variety of problems with little effort. The paper also describes a novel optimization that further improves the performance and scalability of the system. We evaluated the approach on a set of benchmarks that most notably includes desugaring functions for lambda calculus, which force the synthesizer to discover Church encodings for pairs and boolean operations."
"We consider distributed optimization over orthogonal collision channels in spatial random access networks. Users are spatially distributed and each user is in the interference range of a few other users. Each user is allowed to transmit over a subset of the shared channels with a certain attempt probability. We study both the non-cooperative and cooperative settings. In the former, the goal of each user is to maximize its own rate irrespective of the utilities of other users. In the latter, the goal is to achieve proportionally fair rates among users. Simple distributed learning algorithms are developed to solve these problems. The efficiencies of the proposed algorithms are demonstrated via both theoretical analysis and simulation results."
"It is a well-known fact that feedback does not increase the capacity of point-to-point memoryless channels, however, its effect in secure communications is not fully understood yet. In this work, two achievable schemes for the wiretap channel with generalized feedback are presented. The first scheme, which uses the feedback signal to generate a shared secret key between the legitimate users, encrypts the message to be sent at the bit level. The second scheme, which is based on joint source-channel coding, correlates the codewords to be transmitted with the feedback signal in the hope of ""hiding"" them from the eavesdropper's observations. The first achievable scheme provides new capacity results for a class of channels as well as some new insights into the secret key agreement problem. Moreover, these results recover previously reported rate regions from the literature and thus, they can be seen as a generalization that unifies several results in the field."
"In this paper, we introduce a novel approach for optimal resource allocation with frequency reuse for users with elastic and inelastic traffic in cellular networks. In our model, we represent users' applications running on different user equipments (UE)s by logarithmic and sigmoid utility functions. We applied utility proportional fairness allocation policy, i.e. the resources are allocated among users with fairness in utility percentage of the application running on each mobile station. Our objective is to allocate the cellular system resources to mobile users optimally from a multi-cell network. In our model, a minimum quality-of-service (QoS) is guaranteed to every user subscribing for the mobile service with priority given to users with real-time applications. We show that the novel resource allocation optimization problem with frequency reuse is convex and therefore the optimal solution is tractable. We present a distributed algorithm to allocate the resources optimally from Mobility Management Entity (MME) to base stations (BS)s sectors. Finally, we present the simulation results for the performance of our rate allocation algorithm."
"To meet the growing spectrum demands, future cellular systems are expected to share the spectrum of other services such as radar. In this paper, we consider a network multiple-input multiple-output (MIMO) with partial cooperation model where radar stations cooperate with cellular base stations (BS)s to deliver messages to intended mobile users. So the radar stations act as BSs in the cellular system. However, due to the high power transmitted by radar stations for detection of far targets, the cellular receivers could burnout when receiving these high radar powers. Therefore, we propose a new projection method called small singular values space projection (SSVSP) to mitigate these harmful high power and enable radar stations to collaborate with cellular base stations. In addition, we formulate the problem into a MIMO interference channel with general constraints (MIMO-IFC-GC). Finally, we provide a solution to minimize the weighted sum mean square error minimization problem (WSMMSE) with enforcing power constraints on both radar and cellular stations."
"While many of the architectural details of future exascale-class high performance computer systems are still a matter of intense research, there appears to be a general consensus that they will be strongly heterogeneous, featuring ""standard"" as well as ""accelerated"" resources. Today, such resources are available as multicore processors, graphics processing units (GPUs), and other accelerators such as the Intel Xeon Phi. Any software infrastructure that claims usefulness for such environments must be able to meet their inherent challenges: massive multi-level parallelism, topology, asynchronicity, and abstraction. The ""General, Hybrid, and Optimized Sparse Toolkit"" (GHOST) is a collection of building blocks that targets algorithms dealing with sparse matrix representations on current and future large-scale systems. It implements the ""MPI+X"" paradigm, has a pure C interface, and provides hybrid-parallel numerical kernels, intelligent resource management, and truly heterogeneous parallelism for multicore CPUs, Nvidia GPUs, and the Intel Xeon Phi. We describe the details of its design with respect to the challenges posed by modern heterogeneous supercomputers and recent algorithmic developments. Implementation details which are indispensable for achieving high efficiency are pointed out and their necessity is justified by performance measurements or predictions based on performance models. The library code and several applications are available as open source. We also provide instructions on how to make use of GHOST in existing software packages, together with a case study which demonstrates the applicability and performance of GHOST as a component within a larger software stack."
"We consider a hidden Markov model, where the signal process, given by a diffusion, is only indirectly observed through some noisy measurements. The article develops a variational method for approximating the hidden states of the signal process given the full set of observations. This, in particular, leads to systematic approximations of the smoothing densities of the signal process. The paper then demonstrates how an efficient inference scheme, based on this variational approach to the approximation of the hidden states, can be designed to estimate the unknown parameters of stochastic differential equations. Two examples at the end illustrate the efficacy and the accuracy of the presented method."
"The research of influence propagation in social networks via word-of-mouth processes has been given considerable attention in recent years. Arguably, the most fundamental problem in this domain is influence maximization, where the goal is to identify a seed set of individuals that can trigger a large cascade of influence in the network. While there has been significant progress regarding this problem and its variants, one basic shortcoming of the models is that they lack the flexibility in the way the budget is allocated to individuals. Indeed, budget allocation is a critical issue in advertising and viral marketing. Taking the other point of view, known models allowing flexible budget allocation do not take into account the influence spread in the network. We introduce a generalized model that captures both budgets and influence propagation simultaneously.   For the offline setting, we identify a large family of budget-based propagation functions that admit tight approximation guarantee. This family extends most of the previously studied influence models, including the well-known Triggering model. We establish that any function in this family implies an instance of a monotone submodular function maximization over the integer lattice subject to a knapsack constraint. This problem is known to admit an optimal (1-1/e)-approximation. We also study the price of anarchy of the multi-player game that extends the model and establish tight results.   For the online setting, in which an unknown subset of agents arrive in a random order and the algorithm needs to make an irrevocable budget allocation in each step, we develop a 1/(15e)-competitive algorithm. This setting extends the secretary problem, and its variant, the submodular knapsack secretary problem. Notably, our algorithm improves over the best known approximation for the latter problem, even though it applies to a more general setting."
"Dynamic spectrum sharing paradigm is envisaged to meet the growing demand for the Radio Frequency (RF) spectrum. There exist several technical, regulatory, and business impediments for adopting the new paradigm. In this regard, we underscore the need of characterizing and quantifying the use of spectrum by each of the individual transmitters and receivers.   We propose MUSE, a methodology to characterize and quantify the use of spectrum in the space, time, and frequency dimensions. MUSE characterizes the use of spectrum by a transmitter at a point in terms of the RF power occupied by the transmitter. It characterizes the use of spectrum by a receiver at a point in terms of the constraints on the RF-power that can be occupied by any of the transmitters in the system in order to ensure successful reception. It divides the spectrum-space into discrete unit-spectrum-spaces and quantifies the spectrum used by the individual transceivers in the discretized spectrum space.   We characterize the performance of the spectrum management functions in the discretized spectrum-space and illustrate maximizing the use of spectrum. In order to address the challenges for the dynamic spectrum sharing paradigm, we emphasize on articulating, defining, and enforcing the spectrum-access rights in the discretized spectrum-space."
"The use of reverse time chaos allows the realization of hardware chaotic systems that can operate at speeds equivalent to existing state of the art while requiring significantly less complex circuitry. Matched filter decoding is possible for the reverse time system since it exhibits a closed form solution formed partially by a linear basis pulse. Coefficients have been calculated and are used to realize the matched filter digitally as a finite impulse response filter. Numerical simulations confirm that this correctly implements a matched filter that can be used for detection of the chaotic signal. In addition, the direct form of the filter has been implemented in hardware description language and demonstrates performance in agreement with numerical results."
"Let $X_{\Sigma}$ be a smooth complete toric variety defined by a fan $\Sigma$ and let $V=V(I)$ be a subscheme of $X_{\Sigma}$ defined by an ideal $I$ homogeneous with respect to the grading on the total coordinate ring of $X_{\Sigma}$. We show a new expression for the Segre class $s(V,X_{\Sigma})$ in terms of the projective degrees of a rational map specified by the generators of $I$ when each generator corresponds to a numerically effective (nef) divisor. Restricting to the case where $X_{\Sigma}$ is a smooth projective toric variety and dehomogenizing the total homogeneous coordinate ring of $X_{\Sigma}$ via a dehomogenizing ideal we also give an expression for the projective degrees of this rational map in terms of the dimension of an explicit quotient ring. Under an additional technical assumption we construct what we call a general dehomogenizing ideal and apply this construction to give effective algorithms to compute the Segre class $s(V,X_{\Sigma})$, the Chern-Schwartz-MacPherson class $c_{SM}(V)$ and the topological Euler characteristic $\chi(V)$ of $V$. These algorithms can, in particular, be used for subschemes of any product of projective spaces $\mathbb{P}^{n_1} \times \cdots \times \mathbb{P}^{n_j}$ or for subschemes of many other projective toric varieties. Running time bounds for several of the algorithms are given and the algorithms are tested on a variety of examples. In all applicable cases our algorithms to compute these characteristic classes are found to offer significantly increased performance over other known algorithms."
"This paper describes an architecture that combines the complementary strengths of probabilistic graphical models and declarative programming to enable robots to represent and reason with logic-based and probabilistic descriptions of uncertainty and domain knowledge. An action language is extended to support non-boolean fluents and non-deterministic causal laws. This action language is used to describe tightly-coupled transition diagrams at two levels of granularity, refining a coarse-resolution transition diagram of the domain to obtain a fine-resolution transition diagram. The coarse-resolution system description, and a history that includes (prioritized) defaults, are translated into an Answer Set Prolog (ASP) program. For any given goal, inference in the ASP program provides a plan of abstract actions. To implement each such abstract action probabilistically, the part of the fine-resolution transition diagram relevant to this action is identified, and a probabilistic representation of the uncertainty in sensing and actuation is included and used to construct a partially observable Markov decision process (POMDP). The policy obtained by solving the POMDP is invoked repeated to implement the abstract action as a sequence of concrete actions, with the corresponding observations being used for subsequent reasoning in the coarse-resolution. The architecture is evaluated in simulation and on a mobile robot moving objects in an indoor domain, to show that it supports reasoning with violation of defaults, noisy observations and unreliable actions, in complex domains."
"A central question in information theory is to determine the maximum success probability that can be achieved in sending a fixed number of messages over a noisy channel. This was first studied in the pioneering work of Shannon who established a simple expression characterizing this quantity in the limit of multiple independent uses of the channel. Here we consider the general setting with only one use of the channel. We observe that the maximum success probability can be expressed as the maximum value of a submodular function. Using this connection, we establish the following results:   1. There is a simple greedy polynomial-time algorithm that computes a code achieving a (1-1/e)-approximation of the maximum success probability. Moreover, for this problem it is NP-hard to obtain an approximation ratio strictly better than (1-1/e).   2. Shared quantum entanglement between the sender and the receiver can increase the success probability by a factor of at most 1/(1-1/e). In addition, this factor is tight if one allows an arbitrary non-signaling box between the sender and the receiver.   3. We give tight bounds on the one-shot performance of the meta-converse of Polyanskiy-Poor-Verdu."
"Feature learning and deep learning have drawn great attention in recent years as a way of transforming input data into more effective representations using learning algorithms. Such interest has grown in the area of music information retrieval (MIR) as well, particularly in music audio classification tasks such as auto-tagging. In this paper, we present a two-stage learning model to effectively predict multiple labels from music audio. The first stage learns to project local spectral patterns of an audio track onto a high-dimensional sparse space in an unsupervised manner and summarizes the audio track as a bag-of-features. The second stage successively performs the unsupervised learning on the bag-of-features in a layer-by-layer manner to initialize a deep neural network and finally fine-tunes it with the tag labels. Through the experiment, we rigorously examine training choices and tuning parameters, and show that the model achieves high performance on Magnatagatune, a popularly used dataset in music auto-tagging."
"Discrete combinatorial optimization has a central role in many scientific disciplines, however, for hard problems we lack linear time algorithms that would allow us to solve very large instances. Moreover, it is still unclear what are the key features that make a discrete combinatorial optimization problem hard to solve. Here we study random K-satisfiability problems with $K=3,4$, which are known to be very hard close to the SAT-UNSAT threshold, where problems stop having solutions. We show that the backtracking survey propagation algorithm, in a time practically linear in the problem size, is able to find solutions very close to the threshold, in a region unreachable by any other algorithm. All solutions found have no frozen variables, thus supporting the conjecture that only unfrozen solutions can be found in linear time, and that a problem becomes impossible to solve in linear time when all solutions contain frozen variables."
"The Boolean constraint satisfaction problem 3-SAT is arguably the canonical NP-complete problem. In contrast, 2-SAT can not only be decided in polynomial time, but in fact in deterministic linear time. In 2006, Bravyi proposed a physically motivated generalization of k-SAT to the quantum setting, defining the problem ""quantum k-SAT"". He showed that quantum 2-SAT is also solvable in polynomial time on a classical computer, in particular in deterministic time O(n^4), assuming unit-cost arithmetic over a field extension of the rational numbers, where n is number of variables. In this paper, we present an algorithm for quantum 2-SAT which runs in linear time, i.e. deterministic time O(n+m) for n and m the number of variables and clauses, respectively. Our approach exploits the transfer matrix techniques of Laumann et al. [QIC, 2010] used in the study of phase transitions for random quantum 2-SAT, and bears similarities with both the linear time 2-SAT algorithms of Even, Itai, and Shamir (based on backtracking) [SICOMP, 1976] and Aspvall, Plass, and Tarjan (based on strongly connected components) [IPL, 1979]."
"Humans are heterogenous and the behaviors of individuals could be different from that at the population level. We conduct an in-depth study of the temporal patterns of cellphone conversation activities of 73'339 anonymous cellphone users with the same truncated Weibull distribution of inter-call durations. We find that the individual call events exhibit a pattern of bursts, in which high activity periods are alternated with low activity periods. Surprisingly, the number of events in high activity periods are found to conform to a power-law distribution at the population level, but follow an exponential distribution at the individual level, which is a hallmark of absence of memory in individual call activities. Such exponential distribution is also observed for the number of events in low activity periods. Together with the exponential distributions of inter-call durations within bursts and of the intervals between consecutive bursts, we demonstrate that the individual call activities are driven by two independent Poisson processes, which can be combined within a minimal model in terms of a two-state first-order Markov chain giving very good agreement with the empirical distributions using the parameters estimated from real data for about half of the individuals in our sample. By measuring directly the distributions of call rates across the population, which exhibit power-law tails, we explain the difference with previous population level studies, purporting the existence of power-law distributions, via the ""Superposition of Distributions"" mechanism: The superposition of many exponential distributions of activities with a power-law distribution of their characteristic scales leads to a power-law distribution of the activities at the population level."
"We study social welfare of learning outcomes in mechanisms with admission. In our repeated game there are $n$ bidders and $m$ mechanisms, and in each round each mechanism is available for each bidder only with a certain probability. Our scenario is an elementary case of simple mechanism design with incomplete information, where availabilities are bidder types. It captures natural applications in online markets with limited supply and can be used to model access of unreliable channels in wireless networks.   If mechanisms satisfy a smoothness guarantee, existing results show that learning outcomes recover a significant fraction of the optimal social welfare. These approaches, however, have serious drawbacks in terms of plausibility and computational complexity. Also, the guarantees apply only when availabilities are stochastically independent among bidders.   In contrast, we propose an alternative approach where each bidder uses a single no-regret learning algorithm and applies it in all rounds. This results in what we call availability-oblivious coarse correlated equilibria. It exponentially decreases the learning burden, simplifies implementation (e.g., as a method for channel access in wireless devices), and thereby addresses some of the concerns about Bayes-Nash equilibria and learning outcomes in Bayesian settings. Our main results are general composition theorems for smooth mechanisms when valuation functions of bidders are lattice-submodular. They rely on an interesting connection to the notion of correlation gap of submodular functions over product lattices."
"Vehicular networks allow vehicles to share information and are expected to be an integral part in future intelligent transportation system (ITS). In order to guide and validate the design process, analytical expressions of key performance metrics such as packet reception probabilities and throughput are necessary, in particular for accident-prone scenarios such as intersections. In this paper, we analyze the impact of interference in an intersection scenario with two perpendicular roads using tools from stochastic geometry. We present a general procedure to analytically determine the packet reception probability and throughput of a selected link, taking into account the geographical clustering of vehicles close to the intersection. We consider both Aloha and CSMA MAC protocols, and show how the procedure can be used to model different propagation environments of practical relevance. We show how different path loss functions and fading distributions can be incorporated in the analysis to model propagation conditions typical to both rural and urban intersections. Our results indicate that the procedure is general and flexible to deal with a variety of scenarios. Thus, it can serve as a useful design tool for communication system engineers, complementing simulations and experiments, to obtain quick insights into the network performance."
"Given a symmetric nonnegative matrix $A$, symmetric nonnegative matrix factorization (symNMF) is the problem of finding a nonnegative matrix $H$, usually with much fewer columns than $A$, such that $A \approx HH^T$. SymNMF can be used for data analysis and in particular for various clustering tasks. In this paper, we propose simple and very efficient coordinate descent schemes to solve this problem, and that can handle large and sparse input matrices. The effectiveness of our methods is illustrated on synthetic and real-world data sets, and we show that they perform favorably compared to recent state-of-the-art methods."
"Human history has been marked by social instability and conflict, often driven by the irreconcilability of opposing sets of beliefs, ideologies, and religious dogmas. The dynamics of belief systems has been studied mainly from two distinct perspectives, namely how cognitive biases lead to individual belief rigidity and how social influence leads to social conformity. Here we propose a unifying framework that connects cognitive and social forces together in order to study the dynamics of societal belief evolution. Each individual is endowed with a network of interacting beliefs that evolves through interaction with other individuals in a social network. The adoption of beliefs is affected by both internal coherence and social conformity. Our framework explains how social instabilities can arise in otherwise homogeneous populations, how small numbers of zealots with highly coherent beliefs can overturn societal consensus, and how belief rigidity protects fringe groups and cults against invasion from mainstream beliefs, allowing them to persist and even thrive in larger societies. Our results suggest that strong consensus may be insufficient to guarantee social stability, that the cognitive coherence of belief-systems is vital in determining their ability to spread, and that coherent belief-systems may pose a serious problem for resolving social polarization, due to their ability to prevent consensus even under high levels of social exposure. We therefore argue that the inclusion of cognitive factors into a social model is crucial in providing a more complete picture of collective human dynamics."
"In this manuscript, we formulate the problem of denoising Time Differences of Arrival (TDOAs) in the TDOA space, i.e. the Euclidean space spanned by TDOA measurements. The method consists of pre-processing the TDOAs with the purpose of reducing the measurement noise. The complete set of TDOAs (i.e., TDOAs computed at all microphone pairs) is known to form a redundant set, which lies on a linear subspace in the TDOA space. Noise, however, prevents TDOAs from lying exactly on this subspace. We therefore show that TDOA denoising can be seen as a projection operation that suppresses the component of the noise that is orthogonal to that linear subspace. We then generalize the projection operator also to the cases where the set of TDOAs is incomplete. We analytically show that this operator improves the localization accuracy, and we further confirm that via simulation."
"The XCRUSH family of non-Feistel, ARX block ciphers is designed to make efficient use of modern 64-bit general-purpose processors using a small number of encryption rounds which are simple to implement in software. The avalanche function, which applies one data-dependent, key-dependent rotation per 64-bit word of plaintext per round, allows XCRUSH to achieve an almost totally diffuse 256-bit block after just the first two rounds. Designed for speed in software, 3-round XCRUSH is measured at ~7.3 cycles/byte single-threaded on an Intel Haswell processor. A pseudorandom number generator, constructed using the avalanche function, serves as a key scheduling algorithm. No security claims are made in this paper."
"The Shape Interaction Matrix (SIM) is one of the earliest approaches to performing subspace clustering (i.e., separating points drawn from a union of subspaces). In this paper, we revisit the SIM and reveal its connections to several recent subspace clustering methods. Our analysis lets us derive a simple, yet effective algorithm to robustify the SIM and make it applicable to realistic scenarios where the data is corrupted by noise. We justify our method by intuitive examples and the matrix perturbation theory. We then show how this approach can be extended to handle missing data, thus yielding an efficient and general subspace clustering algorithm. We demonstrate the benefits of our approach over state-of-the-art subspace clustering methods on several challenging motion segmentation and face clustering problems, where the data includes corrupted and missing measurements."
"Background: Big Data is a relatively new field of research and technology, and literature reports a wide variety of concepts labeled with Big Data. The maturity of a research field can be measured in the number of publications containing empirical results. In this paper we present the current status of empirical research in Big Data. Method: We employed a systematic mapping method with which we mapped the collected research according to the labels Variety, Volume and Velocity. In addition, we addressed the application areas of Big Data. Results: We found that 151 of the assessed 1778 contributions contain a form of empirical result and can be mapped to one or more of the 3 V's and 59 address an application area. Conclusions: The share of publications containing empirical results is well below the average compared to computer science research as a whole. In order to mature the research on Big Data, we recommend applying empirical methods to strengthen the confidence in the reported results. Based on our trend analysis we consider Volume and Variety to be the most promising uncharted area in Big Data."
"Let $X$ be a finite set of points in $\mathbb{R}^d$. The Tukey depth of a point $q$ with respect to $X$ is the minimum number $\tau_X(q)$ of points of $X$ in a halfspace containing $q$. In this paper we prove a depth version of Carath\'eodory's theorem. In particular, we prove that there exists a constant $c$ (that depends only on $d$ and $\tau_X(q)$) and pairwise disjoint sets $X_1,\dots, X_{d+1} \subset X$ such that the following holds. Each $X_i$ has at least $c|X|$ points, and for every choice of points $x_i$ in $X_i$, $q$ is a convex combination of $x_1,\dots, x_{d+1}$. We also prove depth versions of Helly's and Kirchberger's theorems."
"Methods for identification of dynamical patterns in networks suffer from effects of arbitrary time scales that need to be imposed a priori. Here we develop a principled method to identify patterns on dynamics that take place on network systems, as well as on the dynamics that shape the network themselves, without requiring the stipulation of relevant time scales, which instead are determined solely from data. Our approach is based on a variable-order hidden Markov chain model that generalizes the stochastic block model for discrete time-series as well as temporal networks, without requiring the aggregation of events into discrete intervals. We formulate an efficient nonparametric Bayesian framework that can infer the most appropriate Markov order and number of communities, based solely on statistical evidence and without overfitting."
"It has long been conjectured that hypotheses spaces suitable for data that is compositional in nature, such as text or images, may be more efficiently represented with deep hierarchical networks than with shallow ones. Despite the vast empirical evidence supporting this belief, theoretical justifications to date are limited. In particular, they do not account for the locality, sharing and pooling constructs of convolutional networks, the most successful deep learning architecture to date. In this work we derive a deep network architecture based on arithmetic circuits that inherently employs locality, sharing and pooling. An equivalence between the networks and hierarchical tensor factorizations is established. We show that a shallow network corresponds to CP (rank-1) decomposition, whereas a deep network corresponds to Hierarchical Tucker decomposition. Using tools from measure theory and matrix algebra, we prove that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require exponential size in order to be realized (or even approximated) by a shallow network. Since log-space computation transforms our networks into SimNets, the result applies directly to a deep learning architecture demonstrating promising empirical performance. The construction and theory developed in this paper shed new light on various practices and ideas employed by the deep learning community."
"The capacity of the linear time-varying (LTV) channel, a continuous-time LTV filter with additive white Gaussian noise, is characterized by waterfilling in the time-frequency plane. Similarly, the rate distortion function for a related nonstationary source is characterized by reverse waterfilling in the time-frequency plane. Constraints on the average energy or on the squared-error distortion, respectively, are used. The source is formed by the white Gaussian noise response of the same LTV filter as before. The proofs of both waterfilling theorems rely on a Szego theorem for a class of operators associated with the filter. A self-contained proof of the Szego theorem is given. The waterfilling theorems compare well with the classical results of Gallager and Berger. In the case of a nonstationary source, it is observed that the part of the classical power spectral density is taken by the Wigner-Ville spectrum. The present approach is based on the spread Weyl symbol of the LTV filter, and is asymptotic in nature. For the spreading factor, a lower bound is suggested by means of an uncertainty inequality."
"In this paper we extend the classical notion of strong and weak backdoor sets for SAT and CSP by allowing that different instantiations of the backdoor variables result in instances that belong to different base classes; the union of the base classes forms a heterogeneous base class. Backdoor sets to heterogeneous base classes can be much smaller than backdoor sets to homogeneous ones, hence they are much more desirable but possibly harder to find. We draw a detailed complexity landscape for the problem of detecting strong and weak backdoor sets into heterogeneous base classes for SAT and CSP."
"State-of-the-art methods reconstruct three dimensional (3D) face shape from a single image either by fitting 3D face model to the input image or by warping to a reference shape. However, they are often difficult to apply in real-world applications due to expensive on-line optimization or to their unstable performance across poses and expressions. This letter approaches the 3D face reconstruction problem as a regression problem rather than a model fitting problem. Given an input face image along with annotated landmarks, a series of coarse-to-fine shape adjustments to the initial 3D face shape are computed through cascaded regressors based on the deviations between the input landmarks and the landmarks rendered from the reconstructed 3D faces. The cascaded regressors are off-line learned from a set of 3D faces and their corresponding 2D face images of various poses and expressions. By treating the landmarks that are invisible in large pose angles as missing data, the proposed method can handle arbitrary view face images in a unified way. Experiments on the BFM and BU3DFE databases demonstrate that the proposed method can reconstruct 3D faces more efficiently and more accurately than existing methods from 2D face images of arbitrary poses and expressions."
"The Lovasz Local Lemma (LLL) is a cornerstone principle in the probabilistic method of combinatorics, and a seminal algorithm of Moser & Tardos (2010) provided an efficient randomized algorithm to implement it. This algorithm could be parallelized to give an algorithm that uses polynomially many processors and $O(\log^3 n)$ time, stemming from $O(\log n)$ adaptive computations of a maximal independent set (MIS). Chung et. al. (2014) developed faster local and parallel algorithms, potentially running in time $O(\log^2 n)$, but these algorithms work under significantly more stringent conditions than the LLL.   We give a new parallel algorithm, that works under essentially the same conditions as the original algorithm of Moser & Tardos, but uses only a single MIS computation, thus running in $O(\log^2 n)$ time. This conceptually new algorithm also gives a clean combinatorial description of a satisfying assignment which might be of independent interest. Our techniques extend to the deterministic LLL algorithm given by Chandrasekaran et al (2013) leading to an NC-algorithm running in time $O(\log^2 n)$ as well.   We also provide improved bounds on the run-times of the sequential and parallel resampling-based algorithms originally developed by Moser & Tardos. These bounds extend to any problem instance in which the tighter Shearer LLL criterion is satisfied. We also improve on the analysis of Kolipaka & Szegedy (2011) to give tighter concentration results. Interestingly, we are able to give bounds which are independent of the (somewhat mysterious) weighting function used in formulations of the asymmetric LLL."
"In an illiquid stock, traders can collude and place orders on a predetermined price and quantity at a fixed schedule. This is usually done to manipulate the price of the stock or to create artificial liquidity in the stock, which may mislead genuine investors. Here, the problem is to identify such group of colluding traders. We modeled the problem instance as a graph, where each trader corresponds to a vertex of the graph and trade corresponds to edges of the graph. Further, we assign weights on edges depending on total volume, total number of trades, maximum change in the price and commonality between two vertices. Spectral clustering algorithms are used on the constructed graph to identify colluding group(s). We have compared our results with simulated data to show the effectiveness of spectral clustering to detecting colluding groups. Moreover, we also have used parameters of real data to test the effectiveness of our algorithm."
"To characterize economic development and diagnose the economic health condition, several popular indices such as gross domestic product (GDP), industrial structure and income growth are widely applied. However, computing these indices based on traditional economic census is usually costly and resources consuming, and more importantly, following a long time delay. In this paper, we analyzed nearly 200 million users' activities for four consecutive years in the largest social network (Sina Microblog) in China, aiming at exploring latent relationships between the online social activities and local economic status. Results indicate that online social activity has a strong correlation with local economic development and industrial structure, and more interestingly, allows revealing the macro-economic structure instantaneously with nearly no cost. Beyond, this work also provides a new venue to identify risky signal in local economic structure."
"In a variety of research areas, the weighted bag of vectors and the histogram are widely used descriptors for complex objects. Both can be expressed as discrete distributions. D2-clustering pursues the minimum total within-cluster variation for a set of discrete distributions subject to the Kantorovich-Wasserstein metric. D2-clustering has a severe scalability issue, the bottleneck being the computation of a centroid distribution, called Wasserstein barycenter, that minimizes its sum of squared distances to the cluster members. In this paper, we develop a modified Bregman ADMM approach for computing the approximate discrete Wasserstein barycenter of large clusters. In the case when the support points of the barycenters are unknown and of low cardinality, our method achieves high accuracy empirically at a much reduced computational cost. The strengths and weaknesses of our method and its alternatives are examined through experiments; and scenarios for their respective usage are recommended. Moreover, we develop both serial and parallelized versions of the algorithm. By experimenting with large-scale data, we demonstrate the computational efficiency of the new methods and investigate their convergence properties and numerical stability. The clustering results obtained on several datasets in different domains are highly competitive in comparison with some widely used methods' in the corresponding areas."
"Let $M$ be a matroid without loops or coloops and let $T(M;x,y)$ be its Tutte polynomial. In 1999 Merino and Welsh conjectured that $$\max(T(M;2,0), T(M;0,2))\geq T(M;1,1)$$ holds for graphic matroids. Ten years later, Conde and Merino proposed a multiplicative version of the conjecture which implies the original one. In this paper we prove the multiplicative conjecture for the family of lattice path matroids (generalizing earlier results on uniform and Catalan matroids). In order to do this, we introduce and study particular lattice path matroids, called snakes, used as building bricks to indeed establish a strengthening of the multiplicative conjecture as well as a complete characterization of the cases in which equality holds."
"Recent studies have shown that a Deep Convolutional Neural Network (DCNN) pretrained on a large image dataset can be used as a universal image descriptor, and that doing so leads to impressive performance for a variety of image classification tasks. Most of these studies adopt activations from a single DCNN layer, usually the fully-connected layer, as the image representation. In this paper, we proposed a novel way to extract image representations from two consecutive convolutional layers: one layer is utilized for local feature extraction and the other serves as guidance to pool the extracted features. By taking different viewpoints of convolutional layers, we further develop two schemes to realize this idea. The first one directly uses convolutional layers from a DCNN. The second one applies the pretrained CNN on densely sampled image regions and treats the fully-connected activations of each image region as convolutional feature activations. We then train another convolutional layer on top of that as the pooling-guidance convolutional layer. By applying our method to three popular visual classification tasks, we find our first scheme tends to perform better on the applications which need strong discrimination on subtle object patterns within small regions while the latter excels in the cases that require discrimination on category-level patterns. Overall, the proposed method achieves superior performance over existing ways of extracting image representations from a DCNN."
"Optimal regret bounds for Multi-Armed Bandit problems are now well documented. They can be classified into two categories based on the growth rate with respect to the time horizon $T$: (i) small, distribution-dependent, bounds of order of magnitude $\ln(T)$ and (ii) robust, distribution-free, bounds of order of magnitude $\sqrt{T}$. The Bandits with Knapsacks model, an extension to the framework allowing to model resource consumption, lacks this clear-cut distinction. While several algorithms have been shown to achieve asymptotically optimal distribution-free bounds on regret, there has been little progress toward the development of small distribution-dependent regret bounds. We partially bridge the gap by designing a general-purpose algorithm with distribution-dependent regret bounds that are optimal in several important cases that cover many practical applications, including dynamic pricing with limited supply, online bid optimization for sponsored search auctions, and dynamic procurement."
"Segregation is widespread in all realms of human society. Several influential studies have argued that intolerance is not a prerequisite for a segregated society, and that segregation can arise even when people generally prefer diversity. We investigated this paradox experimentally, by letting groups of high-school students play four different real-time interactive games. Incentives for neighbor similarity produced segregation, but incentives for neighbor dissimilarity and neighborhood diversity prevented it. The participants continued to move while their game scores were below optimal, but their individual moves did not consistently take them to the best alternative position. These small differences between human and simulated agents produced different segregation patterns than previously predicted, thus challenging conclusions about segregation arising from these models."
"Recommender systems benefit us in tackling the problem of information overload by predicting our potential choices among diverse niche objects. So far, a variety of personalized recommendation algorithms have been proposed and most of them are based on similarities, such as collaborative filtering and mass diffusion. Here, we propose a novel vertex similarity index named CosRA, which combines advantages of both the cosine index and the resource-allocation (RA) index. By applying the CosRA index to real recommender systems including MovieLens, Netflix and RYM, we show that the CosRA-based method has better performance in accuracy, diversity and novelty than some benchmark methods. Moreover, the CosRA index is free of parameters, which is a significant advantage in real applications. Further experiments show that the introduction of two turnable parameters cannot remarkably improve the overall performance of the CosRA index."
"With the rapid development of mobile devices and crowdsourcing platforms, the spatial crowdsourcing has attracted much attention from the database community. Specifically, the spatial crowdsourcing refers to sending location-based requests to workers, based on their current positions. In this paper, we consider a spatial crowdsourcing scenario, in which each worker has a set of qualified skills, whereas each spatial task (e.g., repairing a house, decorating a room, and performing entertainment shows for a ceremony) is time-constrained, under the budget constraint, and required a set of skills. Under this scenario, we will study an important problem, namely multi-skill spatial crowdsourcing (MS-SC), which finds an optimal worker-and-task assignment strategy, such that skills between workers and tasks match with each other, and workers' benefits are maximized under the budget constraint. We prove that the MS-SC problem is NP-hard and intractable. Therefore, we propose three effective heuristic approaches, including greedy, g-divide-and-conquer and cost-model-based adaptive algorithms to get worker-and-task assignments. Through extensive experiments, we demonstrate the efficiency and effectiveness of our MS-SC processing approaches on both real and synthetic data sets."
"Underlay in-band device-to-device (D2D) communication can improve the spectrum efficiency of cellular networks. However, the coexistence of D2D and cellular users causes inter-cell and intra-cell interference. The former can be effectively managed through inter-cell interference coordination and, therefore, is not considered in this work. Instead, we focus on the intra-cell interference and propose a D2D mode selection scheme to manage it inside a finite cellular network region. The potential D2D users are controlled by the base station (BS) to operate in D2D mode based on the average interference generated to the BS. Using stochastic geometry, we study the outage probability experienced at the BS and a D2D receiver, and spectrum reuse ratio, which quantifies the average fraction of successfully transmitting D2D users. The analysis shows that the outage probability at the D2D receiver varies for different locations. Additionally, without impairing the performance at the BS, if the path-loss exponent on the cellular link is slightly lower than that on the D2D link, the spectrum reuse ratio can have negligible decrease while the D2D users' average number of successful transmissions increases with increasing D2D node density. This indicates that an increasing level of D2D communication can be beneficial in future networks.."
"A group of players which contain n sellers and n buyers bargain over the partitions of n pies. A seller(/buyer) has to reach an agreement with a buyer (/seller) on the division of a pie. The players bargain in a system like the stock market: each seller(buyer) can either offer a selling(buying) price to all buyers(sellers) or accept a price offered by another buyer(seller). The offered prices are known to all. Once a player accepts a price offered by another one, the division of a pie between them is determined. Each player has a constant discounting factor and the discounting factors of all players are common knowledge. In this article, we prove that the equilibrium of this bargaining problem is a unanimous division rate, which is equivalent to Nash bargaining equilibrium of a two-player bargaining game in which the discounting factors of two players are the average of n buyers and the average of n sellers respectively. This result shows the relevance between bargaining equilibrium and general equilibrium of markets."
"Local caching is an effective scheme for leveraging the memory of the mobile terminal (MT) and short range communications to save the bandwidth usage and reduce the download delay in the cellular communication system. Specifically, the MTs first cache in their local memories in off-peak hours and then exchange the requested files with each other in the vicinity during peak hours. However, prior works largely overlook MTs' heterogeneity in file preferences and their selfish behaviours. In this paper, we practically categorize the MTs into different interest groups according to the MTs' preferences. Each group of MTs aims to increase the probability of successful file discovery from the neighbouring MTs (from the same or different groups). Hence, we define the groups' utilities as the probability of successfully discovering the file in the neighbouring MTs, which should be maximized by deciding the caching strategies of different groups. By modelling MTs' mobilities as homogeneous Poisson point processes (HPPPs), we analytically characterize MTs' utilities in closed-form. We first consider the fully cooperative case where a centralizer helps all groups to make caching decisions. We formulate the problem as a weighted-sum utility maximization problem, through which the maximum utility trade-offs of different groups are characterized. Next, we study two benchmark cases under selfish caching, namely, partial and no cooperation, with and without inter-group file sharing, respectively. The optimal caching distributions for these two cases are derived. Finally, numerical examples are presented to compare the utilities under different cases and show the effectiveness of the fully cooperative local caching compared to the two benchmark cases."
"This study simulates the evolution of artificial economies in order to understand the tax relevance of administrative boundaries in the quality of life of its citizens. The modeling involves the construction of a computational algorithm, which includes citizens, bounded into families; firms and governments; all of them interacting in markets for goods, labor and real estate. The real estate market allows families to move to dwellings with higher quality or lower price when the families capitalize property values. The goods market allows consumers to search on a flexible number of firms choosing by price and proximity. The labor market entails a matching process between firms (location) and candidates (qualification). The government may be configured into one, four or seven distinct sub-national governments. The role of government is to collect taxes on the value added of firms in its territory and invest the taxes into higher levels of quality of life for residents. The model does not have a credit market. The results suggest that the configuration of administrative boundaries is relevant to the levels of quality of life arising from the reversal of taxes. The model with seven regions is more dynamic, with higher GDP values, but more unequal and heterogeneous across regions. The simulation with only one region is more homogeneously poor. The study seeks to contribute to a theoretical and methodological framework as well as to describe, operationalize and test computer models of public finance analysis, with explicitly spatial and dynamic emphasis. Several alternatives of expansion of the model for future research are described. Moreover, this study adds to the existing literature in the realm of simple microeconomic computational models, specifying structural relationships between local governments and firms, consumers and dwellings mediated by distance."
"We study homomorphism problems of signed graphs from a computational point of view. A signed graph $(G,\Sigma)$ is a graph $G$ where each edge is given a sign, positive or negative; $\Sigma\subseteq E(G)$ denotes the set of negative edges. Thus, $(G, \Sigma)$ is a $2$-edge-coloured graph with the property that the edge-colours, $\{+, -\}$, form a group under multiplication. Central to the study of signed graphs is the operation of switching at a vertex, that results in changing the sign of each incident edge. We study two types of homomorphisms of a signed graph $(G,\Sigma)$ to a signed graph $(H,\Pi)$: ec-homomorphisms and s-homomorphisms. Each is a standard graph homomorphism of $G$ to $H$ with some additional constraint. In the former, edge-signs are preserved. In the latter, edge-signs are preserved after the switching operation has been applied to a subset of vertices of $G$.   We prove a dichotomy theorem for s-homomorphism problems for a large class of (fixed) target signed graphs $(H,\Pi)$. Specifically, as long as $(H,\Pi)$ does not contain a negative (respectively a positive) loop, the problem is polynomial-time solvable if the core of $(H,\Pi)$ has at most two edges, and is NP-complete otherwise. (Note that this covers all simple signed graphs.) The same dichotomy holds if $(H,\Pi)$ has no negative digons, and we conjecture that it holds always. In our proofs, we reduce s-homomorphism problems to certain ec-homomorphism problems, for which we are able to show a dichotomy. In contrast, we prove that a dichotomy theorem for ec-homomorphism problems (even when restricted to bipartite target signed graphs) would settle the dichotomy conjecture of Feder and Vardi."
"Energy games are infinite two-player games played in weighted arenas with quantitative objectives that restrict the consumption of a resource modeled by the weights, e.g., a battery that is charged and drained. Typically, upper and/or lower bounds on the battery capacity are part of the problem description. Here, we consider the problem of determining upper bounds on the average accumulated energy or on the capacity while satisfying a given lower bound, i.e., we do not determine whether a given bound is sufficient to meet the specification, but if there exists a sufficient bound to meet it.   In the classical setting with positive and negative weights, we show that the problem of determining the existence of a sufficient bound on the long-run average accumulated energy can be solved in doubly-exponential time. Then, we consider recharge games: here, all weights are negative, but there are recharge edges that recharge the energy to some fixed capacity. We show that bounding the long-run average energy in such games is complete for exponential time. Then, we consider the existential version of the problem, which turns out to be solvable in polynomial time: here, we ask whether there is a recharge capacity that allows the system player to win the game.   We conclude by studying tradeoffs between the memory needed to implement strategies and the bounds they realize. We give an example showing that memory can be traded for bounds and vice versa. Also, we show that increasing the capacity allows to lower the average accumulated energy."
"Randomness extractors, widely used in classical and quantum cryptography and other fields of computer science, e.g., derandomization, are functions which generate almost uniform randomness from weak sources of randomness. In the quantum setting one must take into account the quantum side information held by an adversary which might be used to break the security of the extractor. In the case of seeded extractors the presence of quantum side information has been extensively studied. For multi-source extractors one can easily see that high conditional min-entropy is not sufficient to guarantee security against arbitrary side information, even in the classical case. Hence, the interesting question is under which models of (both quantum and classical) side information multi-source extractors remain secure. In this work we suggest a natural model of side information, which we call the Markov model, and prove that any multi-source extractor remains secure in the presence of quantum side information of this type (albeit with weaker parameters). This improves on previous results in which more restricted models were considered and the security of only some types of extractors was shown."
"Software evolution is a fundamental process that transcends the realm of technical artifacts and permeates the entire organizational structure of a software project. By means of a longitudinal empirical study of 18 large open-source projects, we examine and discuss the evolutionary principles that govern the coordination of developers. By applying a network-analytic approach, we found that the implicit and self-organizing structure of developer coordination is ubiquitously described by non-random organizational principles that defy conventional software-engineering wisdom. In particular, we found that: (a) developers form scale-free networks, in which the majority of coordination requirements arise among an extremely small number of developers, (b) developers tend to accumulate coordination requirements with more and more developers over time, presumably limited by an upper bound, and (c) initially developers are hierarchically arranged, but over time, form a hybrid structure, in which core developers are hierarchically arranged and peripheral developers are not. Our results suggest that the organizational structure of large projects is constrained to evolve towards a state that balances the costs and benefits of developer coordination, and the mechanisms used to achieve this state depend on the project's scale."
"In this paper, we propose a novel data-driven approach for removing trends (detrending) from nonstationary, fractal and multifractal time series. We consider real-valued time series relative to measurements of an underlying dynamical system that evolves through time. We assume that such a dynamical process is predictable to a certain degree by means of a class of recurrent networks called Echo State Network (ESN), which are capable to model a generic dynamical process. In order to isolate the superimposed (multi)fractal component of interest, we define a data-driven filter by leveraging on the ESN prediction capability to identify the trend component of a given input time series. Specifically, the (estimated) trend is removed from the original time series and the residual signal is analyzed with the multifractal detrended fluctuation analysis procedure to verify the correctness of the detrending procedure. In order to demonstrate the effectiveness of the proposed technique, we consider several synthetic time series consisting of different types of trends and fractal noise components with known characteristics. We also process a real-world dataset, the sunspot time series, which is well-known for its multifractal features and has recently gained attention in the complex systems field. Results demonstrate the validity and generality of the proposed detrending method based on ESNs."
"Structure-from-motion (SfM) largely relies on feature tracking. In image sequences, if disjointed tracks caused by objects moving in and out of the field of view, occasional occlusion, or image noise, are not handled well, corresponding SfM could be affected. This problem becomes severer for large-scale scenes, which typically requires to capture multiple sequences to cover the whole scene. In this paper, we propose an efficient non-consecutive feature tracking (ENFT) framework to match interrupted tracks distributed in different subsequences or even in different videos. Our framework consists of steps of solving the feature `dropout' problem when indistinctive structures, noise or large image distortion exists, and of rapidly recognizing and joining common features located in different subsequences. In addition, we contribute an effective segment-based coarse-to-fine SfM algorithm for robustly handling large datasets. Experimental results on challenging video data demonstrate the effectiveness of the proposed system."
"The satisfiability problem of the branching time logic CTL is studied in terms of computational complexity. Tight upper and lower bounds are provided for each temporal operator fragment. In parallel, the minimal model size is studied with a suitable notion of minimality. Thirdly, flat CTL is investigated, i.e., formulas with very low temporal operator nesting depth. A sharp dichotomy is shown in terms of complexity and minimal models: Temporal depth one has low expressive power, while temporal depth two is equivalent to full CTL."
"A right ideal (left ideal, two-sided ideal) is a non-empty language $L$ over an alphabet $\Sigma$ such that $L=L\Sigma^*$ ($L=\Sigma^*L$, $L=\Sigma^*L\Sigma^*$). Let $k=3$ for right ideals, 4 for left ideals and 5 for two-sided ideals. We show that there exist sequences ($L_n \mid n \ge k $) of right, left, and two-sided regular ideals, where $L_n$ has quotient complexity (state complexity) $n$, such that $L_n$ is most complex in its class under the following measures of complexity: the size of the syntactic semigroup, the quotient complexities of the left quotients of $L_n$, the number of atoms (intersections of complemented and uncomplemented left quotients), the quotient complexities of the atoms, and the quotient complexities of reversal, star, product (concatenation), and all binary boolean operations. In that sense, these ideals are ""most complex"" languages in their classes, or ""universal witnesses"" to the complexity of the various operations."
"In this paper, we present an algorithm for computing a fundamental matrix of formal solutions of completely integrable Pfaffian systems with normal crossings in several variables. This algorithm is a generalization of a method developed for the bivariate case based on a combination of several reduction techniques and is implemented in the computer algebra system Maple."
"We formulate the conditional Kolmogorov complexity of x given y at precision r, where x and y are points in Euclidean spaces and r is a natural number. We demonstrate the utility of this notion in two ways.   1. We prove a point-to-set principle that enables one to use the (relativized, constructive) dimension of a single point in a set E in a Euclidean space to establish a lower bound on the (classical) Hausdorff dimension of E. We then use this principle, together with conditional Kolmogorov complexity in Euclidean spaces, to give a new proof of the known, two-dimensional case of the Kakeya conjecture. This theorem of geometric measure theory, proved by Davies in 1971, says that every plane set containing a unit line segment in every direction has Hausdorff dimension 2.   2. We use conditional Kolmogorov complexity in Euclidean spaces to develop the lower and upper conditional dimensions dim(x|y) and Dim(x|y) of x given y, where x and y are points in Euclidean spaces. Intuitively these are the lower and upper asymptotic algorithmic information densities of x conditioned on the information in y. We prove that these conditional dimensions are robust and that they have the correct information-theoretic relationships with the well-studied dimensions dim(x) and Dim(x) and mutual dimensions mdim(x:y) and Mdim(x:y)."
"We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance.   SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/."
"Given a constraint satisfaction problem (CSP) on $n$ variables, $x_1, x_2, \dots, x_n \in \{\pm 1\}$, and $m$ constraints, a global cardinality constraint has the form of $\sum_{i = 1}^{n} x_i = (1-2p)n$, where $p \in (\Omega(1), 1 - \Omega(1))$ and $pn$ is an integer. Let $AVG$ be the expected number of constraints satisfied by randomly choosing an assignment to $x_1, x_2, \dots, x_n$, complying with the global cardinality constraint. The CSP above average with the global cardinality constraint problem asks whether there is an assignment (complying with the cardinality constraint) that satisfies more than $(AVG+t)$ constraints, where $t$ is an input parameter.   In this paper, we present an algorithm that finds a valid assignment satisfying more than $(AVG+t)$ constraints (if there exists one) in time $(2^{O(t^2)} + n^{O(d)})$. Therefore, the CSP above average with the global cardinality constraint problem is fixed-parameter tractable."
"Network densification through the deployment of large number of small cells has been considered as the dominant driver for wireless evolution into 5G. However, it has increased the complexity of mobility management, and operators have been facing the technical challenges in handover (HO) parameter optimization. The trade-off between the HO failure (HOF) rate and the ping-pong (PP) rate has further complicated the challenges. In this article, we proposed ZEro handover failure with Unforced and automatic time-to-execute Scaling (ZEUS) HO. ZEUS HO assures HO signaling when a user equipment (UE) is in a good radio link condition and executes the HO at an optimal time. We analyzed the HO performance of Long-Term Evolution (LTE) and ZEUS theoretically using a geometry-based model, considering the most important HO parameter, i.e., HO margin (HOM). We derived the probabilities of HOF and PP from the analysis. The numerical results demonstrated that ZEUS HO can achieve zero HOF rate without increasing the PP rate, solving the trade-off. Furthermore, we showed that the ZEUS HO can accomplish zero HOF rate and zero PP rate simultaneously with an extension of keeping fast moving users out of small cells."
"Compressed sensing is a powerful tool in applications such as magnetic resonance imaging (MRI). It enables accurate recovery of images from highly undersampled measurements by exploiting the sparsity of the images or image patches in a transform domain or dictionary. In this work, we focus on blind compressed sensing (BCS), where the underlying sparse signal model is a priori unknown, and propose a framework to simultaneously reconstruct the underlying image as well as the unknown model from highly undersampled measurements. Specifically, our model is that the patches of the underlying image(s) are approximately sparse in a transform domain. We also extend this model to a union of transforms model that better captures the diversity of features in natural images. The proposed block coordinate descent type algorithms for blind compressed sensing are highly efficient, and are guaranteed to converge to at least the partial global and partial local minimizers of the highly non-convex BCS problems. Our numerical experiments show that the proposed framework usually leads to better quality of image reconstructions in MRI compared to several recent image reconstruction methods. Importantly, the learning of a union of sparsifying transforms leads to better image reconstructions than a single adaptive transform."
"Online Social Networks (OSNs) provide a venue for virtual interactions and relationships between individuals. In some communities, OSNs also facilitate arranging online meetings and relationships. FetLife, the worlds largest anonymous social network for the BDSM, fetish and kink communities, provides a unique example of an OSN that serves as an interaction space, community organizing tool, and sexual market. In this paper, we present a first look at the characteristics of European members of Fetlife, comprising 504,416 individual nodes with 1,912,196 connections. We looked at user characteristics in terms of gender, sexual orientation, and preferred role. We further examined the topological and structural properties of groups, as well as the type of interactions and relations between their members. Our results suggest there are important differences between the FetLife community and conventional OSNs. The network can be characterised by complex gender based interactions both from a sexual market and platonic viewpoint which point to a truly fascinating social network."
"On-line social networks publish information on a high volume of real-world events almost instantly, becoming a primary source for breaking news. Some of these real-world events can end up having a very strong impact on on-line social networks. The effect of such events can be analyzed from several perspectives, one of them being the intensity and characteristics of the collective activity that it produces in the social platform. We research 5,234 real-world news events encompassing 43 million messages discussed on the Twitter microblogging service for approximately 1 year. We show empirically that exogenous news events naturally create collective patterns of bursty behavior in combination with long periods of inactivity in the network. This type of behavior agrees with other patterns previously observed in other types of natural collective phenomena, as well as in individual human communications. In addition, we propose a methodology to classify news events according to the different levels of intensity in activity that they produce. In particular, we analyze the most highly active events and observe a consistent and strikingly different collective reaction from users when they are exposed to such events. This reaction is independent of an event's reach and scope. We further observe that extremely high-activity events have characteristics that are quite distinguishable at the beginning stages of their outbreak. This allows us to predict with high precision, the top 8% of events that will have the most impact in the social network by just using the first 5% of the information of an event's lifetime evolution. This strongly implies that high-activity events are naturally prioritized collectively by the social network, engaging users early on, way before they are brought to the mainstream audience."
"In this paper we focus on improving object detection performance in terms of recall. We propose a post-detection stage during which we explore the image with the objective of recovering missed detections. This exploration is performed by sampling object proposals in the image. We analyze four different strategies to perform this sampling, giving special attention to strategies that exploit spatial relations between objects. In addition, we propose a novel method to discover higher-order relations between groups of objects. Experiments on the challenging KITTI dataset show that our proposed relations-based proposal generation strategies can help improving recall at the cost of a relatively low amount of object proposals."
"Cloud storage services like Dropbox and Google Drive are widely used by individuals and businesses. Two attractive features of these services are 1) the automatic synchronization of files between multiple client devices and 2) the possibility to share files with other users. However, privacy of cloud data is a growing concern for both individuals and businesses. Encrypting data on the client-side before uploading it is an effective privacy safeguard, but it requires all client devices to have the decryption key. Current solutions derive these keys solely from user-chosen passwords, which have low entropy and are easily guessed.   We present OmniShare, the first scheme to allow client-side encryption with high-entropy keys whilst providing an intuitive key distribution mechanism to enable access from multiple client devices. Instead of passwords, we use low bandwidth uni-directional out-of-band (OOB) channels, such as QR codes, to authenticate new devices. To complement these OOB channels, the cloud storage itself is used as a communication channel between devices in our protocols. We rely on a directory-based key hierarchy with individual file keys to limit the consequences of key compromise and allow efficient sharing of files without requiring re-encryption. OmniShare is open source software and currently available for Android and Windows with other platforms in development. We describe the design and implementation of OmniShare, and explain how we evaluated its security using formal methods, its performance via real-world benchmarks, and its usability through a cognitive walkthrough."
"Automated facial identification and facial expression recognition have been topics of active research over the past few decades. Facial and expression recognition find applications in human-computer interfaces, subject tracking, real-time security surveillance systems and social networking. Several holistic and geometric methods have been developed to identify faces and expressions using public and local facial image databases. In this work we present the evolution in facial image data sets and the methodologies for facial identification and recognition of expressions such as anger, sadness, happiness, disgust, fear and surprise. We observe that most of the earlier methods for facial and expression recognition aimed at improving the recognition rates for facial feature-based methods using static images. However, the recent methodologies have shifted focus towards robust implementation of facial/expression recognition from large image databases that vary with space (gathered from the internet) and time (video recordings). The evolution trends in databases and methodologies for facial and expression recognition can be useful for assessing the next-generation topics that may have applications in security systems or personal identification systems that involve ""Quantitative face"" assessments."
"There are a variety of existing conditions for a degree sequence to be graphic. When a degree sequence satisfies any of these conditions, there exists a graph that realizes the sequence. We formulate several novel sufficient graphicality criteria that depend on the number of elements in the sequence, corresponding to the number of nodes in an associated graph, and the mean degree of the sequence. These conditions, which are stated in terms of bidegree sequences for directed graphs, are easier to apply than classic necessary and sufficient graphicality conditions involving multiple inequalities. They are also more flexible than more recent graphicality conditions, in that they imply graphicality of some degree sequences not covered by those conditions. The form of our results will allow them to be easily used for the generation of graphs with particular degree sequences for applications."
"We present a deep learning framework for probabilistic pixel-wise semantic segmentation, which we term Bayesian SegNet. Semantic segmentation is an important tool for visual scene understanding and a meaningful measure of uncertainty is essential for decision making. Our contribution is a practical system which is able to predict pixel-wise class labels with a measure of model uncertainty. We achieve this by Monte Carlo sampling with dropout at test time to generate a posterior distribution of pixel class labels. In addition, we show that modelling uncertainty improves segmentation performance by 2-3% across a number of state of the art architectures such as SegNet, FCN and Dilation Network, with no additional parametrisation. We also observe a significant improvement in performance for smaller datasets where modelling uncertainty is more effective. We benchmark Bayesian SegNet on the indoor SUN Scene Understanding and outdoor CamVid driving scenes datasets."
"In the present article we describe a few simple and efficient finite volume type schemes on moving grids in one spatial dimension. The underlying finite volume scheme is conservative and it is accurate up to the second order in space. The main novelty consists in the motion of the grid. This new dynamic aspect can be used to resolve better the areas with high solution gradients or any other special features. No interpolation procedure is employed, thus an unnecessary solution smearing is avoided. Thus, our method enjoys excellent conservation properties. The resulting grid is completely redistributed according the choice of the so-called monitor function. Several more or less universal choices of the monitor function are provided. Finally, the performance of the proposed algorithm is illustrated on several examples stemming from the simple linear advection to the simulation of complex shallow water waves."
"We show that the Minimal Length-Bounded L-But problem can be computed in linear time with respect to L and the tree-width of the input graph as parameters. In this problem the task is to find a set of edges of a graph such that after removal of this set, the shortest path between two prescribed vertices is at least L long. We derive an FPT algorithm for a more general multi-commodity length bounded cut problem when parameterized by the number of terminals also.   For the former problem we show a W[1]-hardness result when the parameterization is done by the path-width only (instead of the tree-width) and that this problem does not admit polynomial kernel when parameterized by tree-width and L. We also derive an FPT algorithm for the Minimal Length-Bounded Cut problem when parameterized by the tree-depth. Thus showing an interesting paradigm for this problem and parameters tree-depth and path-width."
"Rolling element bearing faults in rotating systems are observed as impulses in the vibration signals, which are usually buried in noises. In order to effectively detect the fault of bearings, a novel spectrum searching method is proposed. The structural information of spectrum (SIOS) on a predefined basis is constructed through a searching algorithm, such that the harmonics of impulses generated by faults can be clearly identified and analyzed. Local peaks of the spectrum are located on a certain bin of the basis, and then the SIOS can interpret the spectrum via the number and energy of harmonics related to frequency bins of the basis. Finally bearings can be diagnosed based on the SIOS by identifying its dominant components. Mathematical formulation is developed to guarantee the correct construction of the SISO through searching. The effectiveness of the proposed method is verified with a simulation signal and a benchmark study of bearings."
"We provide a novel method for constructing asymptotics (to arbitrary accuracy) for the number of directed graphs that realize a fixed bidegree sequence $d = a \times b$ with maximum degree $d_{max}=O(S^{\frac{1}{2}-\tau})$ for an arbitrarily small positive number $\tau$, where $S$ is the number edges specified by $d$. Our approach is based on two key steps, graph partitioning and degree preserving switches. The former idea allows us to relate enumeration results for given sequences to those for sequences that are especially easy to handle, while the latter facilitates expansions based on numbers of shared neighbors of pairs of nodes. While we focus primarily on directed graphs allowing loops, our results can be extended to other cases, including bipartite graphs, as well as directed and undirected graphs without loops. In addition, we can relax the constraint that $d_{max} = O(S^{\frac{1}{2}-\tau})$ and replace it with $a_{max} b_{max} = O(S^{1-\tau})$. where $a_{max}$ and $b_{max}$ are the maximum values for $a$ and $b$ respectively. The previous best results, from Greenhill et al., only allow for $d_{max} = o(S^{\frac{1}{3}})$ or alternatively $a_{max} b_{max} = o(S^{\frac{2}{3}})$. Since in many real world networks, $d_{max}$ scales larger than $o(S^{\frac{1}{3}})$, we expect that this work will be helpful for various applications."
"In this paper, we develop a randomized algorithm and theory for learning a sparse model from large-scale and high-dimensional data, which is usually formulated as an empirical risk minimization problem with a sparsity-inducing regularizer. Under the assumption that there exists a (approximately) sparse solution with high classification accuracy, we argue that the dual solution is also sparse or approximately sparse. The fact that both primal and dual solutions are sparse motivates us to develop a randomized approach for a general convex-concave optimization problem. Specifically, the proposed approach combines the strength of random projection with that of sparse learning: it utilizes random projection to reduce the dimensionality, and introduces $\ell_1$-norm regularization to alleviate the approximation error caused by random projection. Theoretical analysis shows that under favored conditions, the randomized algorithm can accurately recover the optimal solutions to the convex-concave optimization problem (i.e., recover both the primal and dual solutions)."
"I researched the ability of browsers to counterfeit the behaviour of installed software. In full screen mode browsers can counterfeit almost anything, including BSOD, formatting the hard drive and fake login screens. I found one category of behaviour which could not be counterfeited by a remote website. On examination every solution in that category was a secret known by the computer user and her browser. That is, remote websites cannot counterfeit what they do not know. Neither Bob nor Mallory know secrets shared between the computer user and her computer. This transformed game theory research into cryptography research. On successful verification of a TLS certificate's digital signature the browser should present the 'user-browser' shared secret together with the TLS certificate's identity credentials. This allows the user to authenticate both her browser and the identity specified in the TLS certificate. Following these conclusions, an authentication mechanism for manufactured goods is presented."
"The paper presents an introduction to Artificial Intelligence (AI) in an accessible and informal but precise form. The paper focuses on the algorithmic aspects of the discipline, presenting the main techniques used in AI systems groped in symbolic and subsymbolic. The last part of the paper is devoted to the discussion ongoing among experts in the field and the public at large about on the advantages and disadvantages of AI and in particular on the possible dangers. The personal opinion of the author on this subject concludes the paper.   -----   L'articolo presenta un'introduzione all'Intelligenza Artificiale (IA) in forma divulgativa e informale ma precisa. L'articolo affronta prevalentemente gli aspetti informatici della disciplina, presentando le principali tecniche usate nei sistemi di IA divise in simboliche e subsimboliche. L'ultima parte dell'articolo presenta il dibattito in corso tra gli esperi e il pubblico su vantaggi e svantaggi dell'IA e in particolare sui possibili pericoli. L'articolo termina con l'opinione dell'autore al riguardo."
"Face alignment, which is the task of finding the locations of a set of facial landmark points in an image of a face, is useful in widespread application areas. Face alignment is particularly challenging when there are large variations in pose (in-plane and out-of-plane rotations) and facial expression. To address this issue, we propose a cascade in which each stage consists of a mixture of regression experts. Each expert learns a customized regression model that is specialized to a different subset of the joint space of pose and expressions. The system is invariant to a predefined class of transformations (e.g., affine), because the input is transformed to match each expert's prototype shape before the regression is applied. We also present a method to include deformation constraints within the discriminative alignment framework, which makes our algorithm more robust. Our algorithm significantly outperforms previous methods on publicly available face alignment datasets."
"This survey is devoted to the dynamic analysis of the Costas loop. In particular the acquisition process is analyzed in great detail. Acquision is most conventiently described by a number of frequency and time parameters such as lock-in range, lock-in time, pull-in range, pull-in time, and hold-in range. While for the classical PLL equations for all these parameters have been derived (many of them are approximations, some even crude approximations), this has not yet been carried out for the Costas loop. It is the aim of this analysis to close this gap. The paper starts with an overview on mathematical and physical models (exact and simplified) of the different variants of the Costas loop, cf. Section~1. In Sections 2--5 equations for the above mentioned key parameters are derived. Finally, the hold-in range of the Costas loop for the case where a lead-lag filter is used for the loop filter is analyzed, cf. Appendix."
"Let M(n, d) be the maximum size of a permutation array on n symbols with pairwise Hamming distance at least d. We use various combinatorial, algebraic, and computational methods to improve lower bounds for M(n, d). We compute the Hamming distances of affine semilinear groups and projective semilinear groups, and unions of cosets of AGL(1,q) and PGL(2,q) with Frobenius maps to obtain new, improved lower bounds for M(n,d). We give new randomized algorithms. We give better lower bounds for M(n,d) also using new theorems concerning the contraction operation. For example, we prove a quadratic lower bound for M(n,n-2) for all n=2 (mod 3) such that n+1 is a prime power."
"In this paper we provide new compact integral expressions and associated simple asymptotic approximations for converse and achievability bounds in the finite blocklength regime. The chosen converse and random coding union bounds were taken from the recent work of Polyanskyi-Poor-Verdu, and are investigated under parallel AWGN channels, the AWGN channels, the BI-AWGN channel, and the BSC. The technique we use, which is a generalization of some recent results available from the literature, is to map the probabilities of interest into a Laplace integral, and then solve (or approximate) the integral by use of a steepest descent technique. The proposed results are particularly useful for short packet lengths, where the normal approximation may provide unreliable results."
"Despite the widespread installation of accelerometers in almost all mobile phones and wearable devices, activity recognition using accelerometers is still immature due to the poor recognition accuracy of existing recognition methods and the scarcity of labeled training data. We consider the problem of human activity recognition using triaxial accelerometers and deep learning paradigms. This paper shows that deep activity recognition models (a) provide better recognition accuracy of human activities, (b) avoid the expensive design of handcrafted features in existing systems, and (c) utilize the massive unlabeled acceleration samples for unsupervised feature extraction. Moreover, a hybrid approach of deep learning and hidden Markov models (DL-HMM) is presented for sequential activity recognition. This hybrid approach integrates the hierarchical representations of deep activity recognition models with the stochastic modeling of temporal sequences in the hidden Markov models. We show substantial recognition improvement on real world datasets over state-of-the-art methods of human activity recognition using triaxial accelerometers."
"Emotional content is a key element in user-generated videos. However, it is difficult to understand emotions conveyed in such videos due to the complex and unstructured nature of user-generated content and the sparsity of video frames that express emotion. In this paper, for the first time, we study the problem of transferring knowledge from heterogeneous external sources, including image and textual data, to facilitate three related tasks in video emotion understanding: emotion recognition, emotion attribution and emotion-oriented summarization. Specifically, our framework (1) learns a video encoding from an auxiliary emotional image dataset in order to improve supervised video emotion recognition, and (2) transfers knowledge from an auxiliary textual corpus for zero-shot \pl{recognition} of emotion classes unseen during training. The proposed technique for knowledge transfer facilitates novel applications of emotion attribution and emotion-oriented summarization. A comprehensive set of experiments on multiple datasets demonstrate the effectiveness of our framework."
"In vehicle traffic networks, congestion on one outgoing link of a diverging junction often impedes flow to other outgoing links, a phenomenon known as the first-in-first-out (FIFO) property. Simplified traffic models that do not account for the FIFO property result in monotone dynamics for which powerful analysis techniques exist. FIFO models are in general not monotone, but have been shown to be mixed monotone - a generalization of monotonicity that enables similarly powerful analysis techniques. In this paper, we study traffic flow models for which the FIFO property is only partial, that is, flows at diverging junctions exhibit a combination of FIFO and non-FIFO phenomena. We show that mixed monotonicity extends to this wider class of models and establish conditions that guarantee convergence to an equilibrium."
"Modern data is messy and high-dimensional, and it is often not clear a priori what are the right questions to ask. Instead, the analyst typically needs to use the data to search for interesting analyses to perform and hypotheses to test. This is an adaptive process, where the choice of analysis to be performed next depends on the results of the previous analyses on the same data. Ultimately, which results are reported can be heavily influenced by the data. It is widely recognized that this process, even if well-intentioned, can lead to biases and false discoveries, contributing to the crisis of reproducibility in science. But while %the adaptive nature of exploration any data-exploration renders standard statistical theory invalid, experience suggests that different types of exploratory analysis can lead to disparate levels of bias, and the degree of bias also depends on the particulars of the data set. In this paper, we propose a general information usage framework to quantify and provably bound the bias and other error metrics of an arbitrary exploratory analysis. We prove that our mutual information based bound is tight in natural settings, and then use it to give rigorous insights into when commonly used procedures do or do not lead to substantially biased estimation. Through the lens of information usage, we analyze the bias of specific exploration procedures such as filtering, rank selection and clustering. Our general framework also naturally motivates randomization techniques that provably reduces exploration bias while preserving the utility of the data analysis. We discuss the connections between our approach and related ideas from differential privacy and blinded data analysis, and supplement our results with illustrative simulations."
"The regularization and output consistency behavior of dropout and layer-wise pretraining for learning deep networks have been fairly well studied. However, our understanding of how the asymptotic convergence of backpropagation in deep architectures is related to the structural properties of the network and other design choices (like denoising and dropout rate) is less clear at this time. An interesting question one may ask is whether the network architecture and input data statistics may guide the choices of learning parameters and vice versa. In this work, we explore the association between such structural, distributional and learnability aspects vis-\`a-vis their interaction with parameter convergence rates. We present a framework to address these questions based on convergence of backpropagation for general nonconvex objectives using first-order information. This analysis suggests an interesting relationship between feature denoising and dropout. Building upon these results, we obtain a setup that provides systematic guidance regarding the choice of learning parameters and network sizes that achieve a certain level of convergence (in the optimization sense) often mediated by statistical attributes of the inputs. Our results are supported by a set of experimental evaluations as well as independent empirical observations reported by other groups."
"Images of scenes have various objects as well as abundant attributes, and diverse levels of visual categorization are possible. A natural image could be assigned with fine-grained labels that describe major components, coarse-grained labels that depict high level abstraction or a set of labels that reveal attributes. Such categorization at different concept layers can be modeled with label graphs encoding label information. In this paper, we exploit this rich information with a state-of-art deep learning framework, and propose a generic structured model that leverages diverse label relations to improve image classification performance. Our approach employs a novel stacked label prediction neural network, capturing both inter-level and intra-level label semantics. We evaluate our method on benchmark image datasets, and empirical results illustrate the efficacy of our model."
"Sigma Delta quantization, a quantization method which first surfaced in the 1960s, has now been used widely in various digital products such as cameras, cell phones, radars, etc. The method samples an input signal at a rate higher than the Nyquist rate, thus achieves great robustness to quantization noise. Compressed Sensing (CS) is a frugal acquisition method that utilizes the possible sparsity of the signals to reduce the required number of samples for a lossless acquisition. One can deem the reduced number as an effective dimensionality of the set of sparse signals and accordingly, define an effective oversampling rate as the ratio between the actual sampling rate and the effective dimensionality. A natural conjecture is that the error of Sigma Delta quantization, previously shown to decay with the vanilla oversampling rate, should now decay with the effective oversampling rate when carried out in the regime of compressed sensing. Confirming this intuition is one of the main goals in this direction.   The study of quantization in CS has so far been limited to proving error convergence results for Gaussian and sub-Gaussian sensing matrices, as the number of bits and/or the number of samples grow to infinity. In this paper, we provide a first result for the more realistic Fourier sensing matrices. The major idea is to randomly permute the Fourier samples before feeding them into the quantizer. We show that the random permutation can effectively increase the low frequency power of the measurements, thus enhance the quality of $\Sigma\Delta$ quantization."
"P. polycephalum may be considered as a spatially represented parallel unconventional computing substrate, but how can this `computer' be programmed? In this paper we examine and catalogue individual low-level mechanisms which may be used to induce network formation and adaptation in a multi-agent model of P. polycephalum. These mechanisms include those intrinsic to the model (particle sensor angle, rotation angle, and scaling parameters) and those mediated by the environment (stimulus loca- tion, distance, angle, concentration, engulfment and consumption of nutrients, and the presence of simulated light irradiation, repellents and obstacles). The mechanisms in- duce a concurrent integration of chemoattractant and chemorepellent gradients diffusing within the 2D lattice upon which the agent population resides, stimulating growth, move- ment, morphological adaptation and network minimisation. Chemoattractant gradients, and their modulation by the engulfment and consumption of nutrients by the model population, represent an efficient outsourcing of spatial computation. The mechanisms may prove useful in understanding the search strategies and adaptation of distributed organisms within their environment, in understanding the minimal requirements for com- plex adaptive behaviours, and in developing methods of spatially programming parallel unconventional computers and robotic devices."
"A long standing question asks whether $\mathbb{Z}$ is uniformly 2-repetitive [Justin 1972, Pirillo and Varricchio, 1994], that is, whether there is an infinite sequence over a finite subset of $\mathbb{Z}$ avoiding two consecutive blocks of same size and same sum or not. Cassaigne \emph{et al.} [2014] showed that $\mathbb{Z}$ is not uniformly 3-repetitive. We show that $\mathbb{Z}^2$ is not uniformly 2-repetitive. Moreover, this problem is related to a question from M\""akel\""a in combinatorics on words and we answer to a weak version of it."
"We present a framework using the Quantized Tensor Train (QTT) decomposition to accurately and efficiently solve volume and boundary integral equations in three dimensions. We describe how the QTT decomposition can be used as a hierarchical compression and inversion scheme for matrices arising from the discretization of integral equations. For a broad range of problems, computational and storage costs of the inversion scheme are extremely modest $O(\log N)$ and once the inverse is computed, it can be applied in $O(N \log N)$.   We analyze the QTT ranks for hierarchically low rank matrices and discuss its relationship to commonly used hierarchical compression techniques such as FMM and HSS. We prove that the QTT ranks are bounded for translation-invariant systems and argue that this behavior extends to non-translation invariant volume and boundary integrals.   For volume integrals, the QTT decomposition provides an efficient direct solver requiring significantly less memory compared to other fast direct solvers. We present results demonstrating the remarkable performance of the QTT-based solver when applied to both translation and non-translation invariant volume integrals in 3D.   For boundary integral equations, we demonstrate that using a QTT decomposition to construct preconditioners for a Krylov subspace method leads to an efficient and robust solver with a small memory footprint. We test the QTT preconditioners in the iterative solution of an exterior elliptic boundary value problem (Laplace) formulated as a boundary integral equation in complex, multiply connected geometries."
"Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies."
"Current approaches for fine-grained recognition do the following: First, recruit experts to annotate a dataset of images, optionally also collecting more structured data in the form of part annotations and bounding boxes. Second, train a model utilizing this data. Toward the goal of solving fine-grained recognition, we introduce an alternative approach, leveraging free, noisy data from the web and simple, generic methods of recognition. This approach has benefits in both performance and scalability. We demonstrate its efficacy on four fine-grained datasets, greatly exceeding existing state of the art without the manual collection of even a single label, and furthermore show first results at scaling to more than 10,000 fine-grained categories. Quantitatively, we achieve top-1 accuracies of 92.3% on CUB-200-2011, 85.4% on Birdsnap, 93.4% on FGVC-Aircraft, and 80.8% on Stanford Dogs without using their annotated training sets. We compare our approach to an active learning approach for expanding fine-grained datasets."
"To understand, predict, and control complex networked systems, a prerequisite is to reconstruct the network structure from observable data. Despite recent progress in network reconstruction, binary-state dynamics that are ubiquitous in nature, technology and society still present an outstanding challenge in this field. Here we offer a framework for reconstructing complex networks with binary-state dynamics by developing a universal data-based linearization approach that is applicable to systems with linear, nonlinear, discontinuous, or stochastic dynamics governed by monotonous functions. The linearization procedure enables us to convert the network reconstruction into a sparse signal reconstruction problem that can be resolved through convex optimization. We demonstrate generally high reconstruction accuracy for a number of complex networks associated with distinct binary-state dynamics from using binary data contaminated by noise and missing data. Our framework is completely data driven, efficient and robust, and does not require any a priori knowledge about the detailed dynamical process on the network. The framework represents a general paradigm for reconstructing, understanding, and exploiting complex networked systems with binary-state dynamics."
"Galois connections are a foundational tool for structuring abstraction in semantics and their use lies at the heart of the theory of abstract interpretation. Yet, mechanization of Galois connections remains limited to restricted modes of use, preventing their general application in mechanized metatheory and certified programming.   This paper presents constructive Galois connections, a variant of Galois connections that is effective both on paper and in proof assistants; is complete with respect to a large subset of classical Galois connections; and enables more general reasoning principles, including the ""calculational"" style advocated by Cousot.   To design constructive Galois connection we identify a restricted mode of use of classical ones which is both general and amenable to mechanization in dependently-typed functional programming languages. Crucial to our metatheory is the addition of monadic structure to Galois connections to control a ""specification effect"". Effectful calculations may reason classically, while pure calculations have extractable computational content. Explicitly moving between the worlds of specification and implementation is enabled by our metatheory.   To validate our approach, we provide two case studies in mechanizing existing proofs from the literature: one uses calculational abstract interpretation to design a static analyzer, the other forms a semantic basis for gradual typing. Both mechanized proofs closely follow their original paper-and-pencil counterparts, employ reasoning principles not captured by previous mechanization approaches, support the extraction of verified algorithms, and are novel."
"We present a new type system combining occurrence typing, previously used to type check programs in dynamically-typed languages such as Racket, JavaScript, and Ruby, with dependent refinement types. We demonstrate that the addition of refinement types allows the integration of arbitrary solver-backed reasoning about logical propositions from external theories. By building on occurrence typing, we can add our enriched type system as an extension of Typed Racket---adding dependency and refinement reuses the existing formalism while increasing its expressiveness.   Dependent refinement types allow Typed Racket programmers to express rich type relationships, ranging from data structure invariants such as red-black tree balance to preconditions such as vector bounds. Refinements allow programmers to embed the propositions that occurrence typing in Typed Racket already reasons about into their types. Further, extending occurrence typing to refinements allows us to make the underlying formalism simpler and more powerful.   In addition to presenting the design of our system, we present a formal model of the system, show how to integrate it with theories over both linear arithmetic and bitvectors, and evaluate the system in the context of the full Typed Racket implementation. Specifically, we take safe vector access as a case study, and examine all vector accesses in a 56,000 line corpus of Typed Racket programs. Our system is able to prove that 50% of these are safe with no new annotation, and with a few annotations and modifications, we can capture close to 80%."
"We present a new algorithm for finding a near optimal low-rank approximation of a matrix $A$ in $O(nnz(A))$ time. Our method is based on a recursive sampling scheme for computing a representative subset of $A$'s columns, which is then used to find a low-rank approximation.   This approach differs substantially from prior $O(nnz(A))$ time algorithms, which are all based on fast Johnson-Lindenstrauss random projections. It matches the guarantees of these methods while offering a number of advantages.   Not only are sampling algorithms faster for sparse and structured data, but they can also be applied in settings where random projections cannot. For example, we give new single-pass streaming algorithms for the column subset selection and projection-cost preserving sample problems. Our method has also been used to give the fastest algorithms for provably approximating kernel matrices [MM16]."
"Many mobile applications running on smartphones and wearable devices would potentially benefit from the accuracy and scalability of deep CNN-based machine learning algorithms. However, performance and energy consumption limitations make the execution of such computationally intensive algorithms on mobile devices prohibitive. We present a GPU-accelerated library, dubbed CNNdroid, for execution of trained deep CNNs on Android-based mobile devices. Empirical evaluations show that CNNdroid achieves up to 60X speedup and 130X energy saving on current mobile devices. The CNNdroid open source library is available for download at https://github.com/ENCP/CNNdroid"
"We propose a novel pilot sequence design to mitigate pilot contamination in multi-cell multiuser massive multiple-input multiple-output networks. Our proposed design generates pilot sequences in the multi-cell network and devises power allocation at base stations (BSs) for downlink transmission. The pilot sequences together with the power allocation ensure that the user capacity of the network is achieved and the pre-defined signal-to-interference-plus-noise ratio (SINR) requirements of all users are met. To realize our design, we first derive new closed-form expressions for the user capacity and the user capacity region. Built upon these expressions, we then develop a new algorithm to obtain the required pilot sequences and power allocation. We further determine the minimum number of antennas required at BSs to achieve certain SINR requirements of all users. Numerical results are presented to corroborate our analysis and to examine the impact of key parameters, such as the pilot sequence length and the total number of users, on the network performance. A pivotal conclusion is reached that our design achieves a larger user capacity region than the existing designs and needs less antennas at the BS to fulfill the pre-defined SINR requirements of all users in the network than the existing designs."
"Instance segmentation is the problem of detecting and delineating each distinct object of interest appearing in an image. Current instance segmentation approaches consist of ensembles of modules that are trained independently of each other, thus missing opportunities for joint learning. Here we propose a new instance segmentation paradigm consisting in an end-to-end method that learns how to segment instances sequentially. The model is based on a recurrent neural network that sequentially finds objects and their segmentations one at a time. This net is provided with a spatial memory that keeps track of what pixels have been explained and allows occlusion handling. In order to train the model we designed a principled loss function that accurately represents the properties of the instance segmentation problem. In the experiments carried out, we found that our method outperforms recent approaches on multiple person segmentation, and all state of the art approaches on the Plant Phenotyping dataset for leaf counting."
"Massive graph data sets are pervasive in contemporary application domains. Hence, graph database systems are becoming increasingly important. In the experimental study of these systems, it is vital that the research community has shared solutions for the generation of database instances and query workloads having predictable and controllable properties. In this paper, we present the design and engineering principles of gMark, a domain- and query language-independent graph instance and query workload generator. A core contribution of gMark is its ability to target and control the diversity of properties of both the generated instances and the generated workloads coupled to these instances. Further novelties include support for regular path queries, a fundamental graph query paradigm, and schema-driven selectivity estimation of queries, a key feature in controlling workload chokepoints. We illustrate the flexibility and practical usability of gMark by showcasing the framework's capabilities in generating high quality graphs and workloads, and its ability to encode user-defined schemas across a variety of application domains."
"The presented research work considers a mathematical model for energy of the signal at the output of an ideal DAC, in presence of sampling clock jitter. When sampling clock jitter occurs, the energy of the signal at the output of ideal DAC does not satisfies a Parseval identity. Nevertheless, an estimation of the signal energy is here shown by a direct method involving sinc functions."
"Three-qubit quantum gates are key ingredients for quantum error correction and quantum information processing. We generate quantum-control procedures to design three types of three-qubit gates, namely Toffoli, Controlled-Not-Not and Fredkin gates. The design procedures are applicable to a system comprising three nearest-neighbor-coupled superconducting artificial atoms. For each three-qubit gate, the numerical simulation of the proposed scheme achieves 99.9% fidelity, which is an accepted threshold fidelity for fault-tolerant quantum computing. We test our procedure in the presence of decoherence-induced noise as well as show its robustness against random external noise generated by the control electronics. The three-qubit gates are designed via the machine learning algorithm called Subspace-Selective Self-Adaptive Differential Evolution (SuSSADE)."
"We present a low complexity experimental RF-based indoor localization system based on the collection and processing of WiFi RSSI signals and processing using a RSS-based multi-lateration algorithm to determine a robotic mobile node's location. We use a real indoor wireless testbed called w-iLab.t that is deployed in Zwijnaarde, Ghent, Belgium. One of the unique attributes of this testbed is that it provides tools and interfaces using Global Environment for Network Innovations (GENI) project to easily create reproducible wireless network experiments in a controlled environment. We provide a low complexity algorithm to estimate the location of the mobile robots in the indoor environment. In addition, we provide a comparison between some of our collected measurements with their corresponding location estimation and the actual robot location. The comparison shows an accuracy between 0.65 and 5 meters."
"This paper is concerned with stochastic SIR and SEIR epidemic models on random networks in which individuals may rewire away from infected neighbors at some rate $\omega$ (and reconnect to non-infectious individuals with probability $\alpha$ or else simply drop the edge if $\alpha=0$), so-called preventive rewiring. The models are denoted SIR-$\omega$ and SEIR-$\omega$, and we focus attention on the early stages of an outbreak, where we derive expression for the basic reproduction number $R_0$ and the expected degree of the infectious nodes $E(D_I)$ using two different approximation approaches. The first approach approximates the early spread of an epidemic by a branching process, whereas the second one uses pair approximation. The expressions are compared with the corresponding empirical means obtained from stochastic simulations of SIR-$\omega$ and SEIR-$\omega$ epidemics on Poisson and scale-free networks. Without rewiring of exposed nodes, the two approaches predict the same epidemic threshold and the same $E(D_I)$ for both types of epidemics, the latter being very close to the mean degree obtained from simulated epidemics over Poisson networks. Above the epidemic threshold, pairwise models overestimate the value of $R_0$ computed from simulations, which turns out to be very close to the one predicted by the branching process approximation. When exposed individuals also rewire with $\alpha > 0$ (perhaps unaware of being infected), the two approaches give different epidemic thresholds, with the branching process approximation being more in agreement with simulations."
"Counterfeiting of manufactured goods is presented as the theft of intellectual property, patents, copyright etc. accompanied by identity theft. The purpose of the identity theft is to facilitate the intellectual property theft. Without it the intellectual property theft would be obvious and the products would be confiscated and destroyed. Authentication solutions, to prevent identity theft, were then developed for the two categories of manufactured goods i.e. goods which can be subjected to destructive screening strategies and goods which cannot e.g. pharmaceutical drugs and currencies, respectively. The solutions developed were found to be analogous to digital signatures. Tamper proof packaging on pharmaceutical drugs is analogous to encryption because it prevents Mallory from interfering with the product. Breaking the tamper proof packaging is a one-way function. Concealed inside the packaging a one-time password, which can be used to authenticate the product over the internet. The name of the authentication website must be common knowledge, just like a public key for authenticating digital signatures. Otherwise the counterfeiters will specify their own authentication website. This solution can be altered for currencies i.e. the one-way function, equivalent to opening the tamper proof packaging, becomes the method of manufacture of the currency."
"We propose fast O(N) preconditioning, where N is the number of gridpoints on the prediction horizon, for iterative solution of (non)-linear systems appearing in model predictive control methods such as forward-difference Newton-Krylov methods. The Continuation/GMRES method for nonlinear model predictive control, suggested by T. Ohtsuka in 2004, is a specific application of the Newton-Krylov method, which uses the GMRES iterative algorithm to solve a forward difference approximation of the optimality equations on every time step."
"This paper investigates a novel problem of generating images from visual attributes. We model the image as a composite of foreground and background and develop a layered generative model with disentangled latent variables that can be learned end-to-end using a variational auto-encoder. We experiment with natural images of faces and birds and demonstrate that the proposed models are capable of generating realistic and diverse samples with disentangled latent representations. We use a general energy minimization algorithm for posterior inference of latent variables given novel images. Therefore, the learned generative models show excellent quantitative and visual results in the tasks of attribute-conditioned image reconstruction and completion."
"The human face constantly conveys information, both consciously and subconsciously. However, as basic as it is for humans to visually interpret this information, it is quite a big challenge for machines. Conventional semantic facial feature recognition and analysis techniques are already in use and are based on physiological heuristics, but they suffer from lack of robustness and high computation time. This thesis aims to explore ways for machines to learn to interpret semantic information available in faces in an automated manner without requiring manual design of feature detectors, using the approach of Deep Learning. This thesis provides a study of the effects of various factors and hyper-parameters of deep neural networks in the process of determining an optimal network configuration for the task of semantic facial feature recognition. This thesis explores the effectiveness of the system to recognize the various semantic features (like emotions, age, gender, ethnicity etc.) present in faces. Furthermore, the relation between the effect of high-level concepts on low level features is explored through an analysis of the similarities in low-level descriptors of different semantic features. This thesis also demonstrates a novel idea of using a deep network to generate 3-D Active Appearance Models of faces from real-world 2-D images.   For a more detailed report on this work, please see [arXiv:1512.00743v1]."
"A research frontier has emerged in scientific computation, founded on the principle that numerical error entails epistemic uncertainty that ought to be subjected to statistical analysis. This viewpoint raises several interesting challenges, including the design of statistical methods that enable the coherent propagation of probabilities through a (possibly deterministic) computational pipeline. This paper examines thoroughly the case for probabilistic numerical methods in statistical computation and a specific case study is presented for Markov chain and Quasi Monte Carlo methods. A probabilistic integrator is equipped with a full distribution over its output, providing a measure of epistemic uncertainty that is shown to be statistically valid at finite computational levels, as well as in asymptotic regimes. The approach is motivated by expensive integration problems, where, as in krigging, one is willing to expend, at worst, cubic computational effort in order to gain uncertainty quantification. There, probabilistic integrators enjoy the ""best of both worlds"", leveraging the sampling efficiency of Monte Carlo methods whilst providing a principled route to assessment of the impact of numerical error on scientific conclusions. Several substantial applications are provided for illustration and critical evaluation, including examples from statistical modelling, computer graphics and uncertainty quantification in oil reservoir modelling."
"Creativity, together with the making of ideas into fruition, is essential for progress. Today the evolution from an idea to its application can be facilitated by the implementation of Fabrication Laboratories, or FabLabs, having affordable digital tools for prototyping. FabLabs aiming at scientific research and invention are now starting to be established inside Universities, Research Centers and Schools. We review the setting up of the ICTP Scientific FabLab in Trieste, Italy, give concrete examples on the use in physics, and propose to replicate world-wide this class of multi-purpose workplaces within academia as a support for physics and math education and for community development."
"We present ASIST, a technique for transforming point clouds by replacing objects with their semantically equivalent counterparts. Transformations of this kind have applications in virtual reality, repair of fused scans, and robotics. ASIST is based on a unified formulation of semantic labeling and object replacement; both result from minimizing a single objective. We present numerical tools for the efficient solution of this optimization problem. The method is experimentally assessed on new datasets of both synthetic and real point clouds, and is additionally compared to two recent works on object replacement on data from the corresponding papers."
"We study the asymptotic dynamical properties of Boolean networks without local negative cycle. While the properties of Boolean networks without local cycle or without local positive cycle are rather well understood, recent literature raises the following two questions about networks without local negative cycle. Do they have at least one fixed point? Should all their attractors be fixed points? The two main results of this paper are negative answers to both questions: we show that and-nets without local negative cycle may have no fixed point, and that Boolean networks without local negative cycle may have antipodal attractive cycles."
"A lower bound on the capacity of the split-step Fourier channel is derived. The channel under study is a concatenation of smaller segments, within which three operations are performed on the signal, namely, nonlinearity, linearity, and noise addition. Simulation results indicate that for a fixed number of segments, our lower bound saturates in the high-power regime and that the larger the number of segments is, the higher is the saturation point. We also obtain an alternative lower bound, which is less tight but has a simple closed-form expression. This bound allows us to conclude that the saturation point grows unbounded with the number of segments. Specifically, it grows as $c+(1/2)\log(K)$, where $K$ is the number of segments and $c$ is a constant. The connection between our channel model and the nonlinear Schr\""odinger equation is discussed."
"Commonly used in computer vision and other applications, robust PCA represents an algorithmic attempt to reduce the sensitivity of classical PCA to outliers. The basic idea is to learn a decomposition of some data matrix of interest into low rank and sparse components, the latter representing unwanted outliers. Although the resulting optimization problem is typically NP-hard, convex relaxations provide a computationally-expedient alternative with theoretical support. However, in practical regimes performance guarantees break down and a variety of non-convex alternatives, including Bayesian-inspired models, have been proposed to boost estimation quality. Unfortunately though, without additional a priori knowledge none of these methods can significantly expand the critical operational range such that exact principal subspace recovery is possible. Into this mix we propose a novel pseudo-Bayesian algorithm that explicitly compensates for design weaknesses in many existing non-convex approaches leading to state-of-the-art performance with a sound analytical foundation. Surprisingly, our algorithm can even outperform convex matrix completion despite the fact that the latter is provided with perfect knowledge of which entries are not corrupted."
"In many problems in machine learning and operations research, we need to optimize a function whose input is a random variable or a probability density function, i.e. to solve optimization problems in an infinite dimensional space. On the other hand, online learning has the advantage of dealing with streaming examples, and better model a changing environ- ment. In this paper, we extend the celebrated online gradient descent algorithm to Hilbert spaces (function spaces), and analyze the convergence guarantee of the algorithm. Finally, we demonstrate that our algorithms can be useful in several important problems."
"We provide an algorithm to check whether two rational space curves are related by a similarity. The algorithm exploits the relationship between the curvatures and torsions of two similar curves, which is formulated in a computer algebra setting. Helical curves, where curvature and torsion are proportional, need to be distinguished as a special case. The algorithm is easy to implement, as it involves only standard computer algebra techniques, such as greatest common divisors and resultants, and Gr\""obner basis for the special case of helical curves. Details on the implementation and experimentation carried out using the computer algebra system Maple 18 are provided."
"We propose a distributed approach to train deep neural networks (DNNs), which has guaranteed convergence theoretically and great scalability empirically: close to 6 times faster on instance of ImageNet data set when run with 6 machines. The proposed scheme is close to optimally scalable in terms of number of machines, and guaranteed to converge to the same optima as the undistributed setting. The convergence and scalability of the distributed setting is shown empirically across different datasets (TIMIT and ImageNet) and machine learning tasks (image classification and phoneme extraction). The convergence analysis provides novel insights into this complex learning scheme, including: 1) layerwise convergence, and 2) convergence of the weights in probability."
"Sparse modeling has been widely and successfully used in many applications such as computer vision, machine learning, and pattern recognition. Accompanied with those applications, significant research has studied the theoretical limits and algorithm design for convex relaxations in sparse modeling. However, theoretical analyses on non-negative versions of sparse modeling are limited in the literature either to a noiseless setting or a scenario with a specific statistical noise model such as Gaussian noise. This paper studies the performance of non-negative sparse modeling in a more general scenario where the observed signals have an unknown arbitrary distortion, especially focusing on non-negativity constrained and L1-penalized least squares, and gives an exact bound for which this problem can recover the correct signal elements. We pose two conditions to guarantee the correct signal recovery: minimum coefficient condition (MCC) and nonlinearity vs. subset coherence condition (NSCC). The former defines the minimum weight for each of the correct atoms present in the signal and the latter defines the tolerable deviation from the linear model relative to the positive subset coherence (PSC), a novel type of ""coherence"" metric. We provide rigorous performance guarantees based on these conditions and experimentally verify their precise predictive power in a hyperspectral data unmixing application."
"We consider the design of linear precoders and receivers in a Multiple-Input Single-Output (MISO) Broadcast Channel (BC). We aim at minimizing the transmit power while fullfiling a set of per-user Quality-of-Service (QoS) constraints expressed in terms of per-user average rate requirements. The Channel State Information (CSI) is assumed to be perfectly known at the receivers but only partially at the transmitter. To solve the problem we transform the QoS constraints into Minimum Mean Square Error (MMSE) constraints. We then leverage the MSE duality between the BC and the Multiple Access Channel (MAC), as well as standard interference functions in the dual MAC, to perform power minimization by means of an Alternating Optimization (AO) algorithm. Problem feasibility is also studied to determine whether the QoS constraints can be fulfilled or not. Finally, we present an algorithm to balance the average rates and manage situations that may be unfeasible or lead to an unacceptably high transmit power."
"Stochastic Gradient Descent (SGD) has become one of the most popular optimization methods for training machine learning models on massive datasets. However, SGD suffers from two main drawbacks: (i) The noisy gradient updates have high variance, which slows down convergence as the iterates approach the optimum, and (ii) SGD scales poorly in distributed settings, typically experiencing rapidly decreasing marginal benefits as the number of workers increases. In this paper, we propose a highly parallel method, CentralVR, that uses error corrections to reduce the variance of SGD gradient updates, and scales linearly with the number of worker nodes. CentralVR enjoys low iteration complexity, provably linear convergence rates, and exhibits linear performance gains up to hundreds of cores for massive datasets. We compare CentralVR to state-of-the-art parallel stochastic optimization methods on a variety of models and datasets, and find that our proposed methods exhibit stronger scaling than other SGD variants."
"Mined Semantic Analysis (MSA) is a novel concept space model which employs unsupervised learning to generate semantic representations of text. MSA represents textual structures (terms, phrases, documents) as a bag-of-concepts where concepts are derived from concept rich encyclopedic corpora. Traditional concept space models exploit only target corpus content to construct the concept space. MSA, alternatively, uncovers implicit relations between concepts by mining for their associations (e.g., mining Wikipedia's ""See also"" link graph). We evaluate MSA's performance on benchmark data sets for measuring lexical semantic relatedness. Empirical results show competitive performance of MSA compared to prior state of-the-art methods. Additionally, we introduce the first analytical study to examine statistical significance of results reported by different semantic relatedness methods. Our study shows that, the nuances of results across top performing methods could be statistically insignificant. The study positions MSA as one of state-of-the-art methods for measuring semantic relatedness."
"In this paper, we analyze the limiting spectral distribution of the adjacency matrix of a random graph ensemble, proposed by Chung and Lu, in which a given expected degree sequence $\overline{w}_n^{^{T}} = (w^{(n)}_1,\ldots,w^{(n)}_n)$ is prescribed on the ensemble. Let $\mathbf{a}_{i,j} =1$ if there is an edge between the nodes $\{i,j\}$ and zero otherwise, and consider the normalized random adjacency matrix of the graph ensemble: $\mathbf{A}_n$ $=$ $ [\mathbf{a}_{i,j}/\sqrt{n}]_{i,j=1}^{n}$. The empirical spectral distribution of $\mathbf{A}_n$ denoted by $\mathbf{F}_n(\mathord{\cdot})$ is the empirical measure putting a mass $1/n$ at each of the $n$ real eigenvalues of the symmetric matrix $\mathbf{A}_n$. Under some technical conditions on the expected degree sequence, we show that with probability one, $\mathbf{F}_n(\mathord{\cdot})$ converges weakly to a deterministic distribution $F(\mathord{\cdot})$. Furthermore, we fully characterize this distribution by providing explicit expressions for the moments of $F(\mathord{\cdot})$. We apply our results to well-known degree distributions, such as power-law and exponential. The asymptotic expressions of the spectral moments in each case provide significant insights about the bulk behavior of the eigenvalue spectrum."
"Domain adaptation is the supervised learning setting in which the training and test data are sampled from different distributions: training data is sampled from a source domain, whilst test data is sampled from a target domain. This paper proposes and studies an approach, called feature-level domain adaptation (FLDA), that models the dependence between the two domains by means of a feature-level transfer model that is trained to describe the transfer from source to target domain. Subsequently, we train a domain-adapted classifier by minimizing the expected loss under the resulting transfer model. For linear classifiers and a large family of loss functions and transfer models, this expected loss can be computed or approximated analytically, and minimized efficiently. Our empirical evaluation of FLDA focuses on problems comprising binary and count data in which the transfer can be naturally modeled via a dropout distribution, which allows the classifier to adapt to differences in the marginal probability of features in the source and the target domain. Our experiments on several real-world problems show that FLDA performs on par with state-of-the-art domain-adaptation techniques."
"Minimizing empirical risk subject to a set of constraints can be a useful strategy for learning restricted classes of functions, such as monotonic functions, submodular functions, classifiers that guarantee a certain class label for some subset of examples, etc. However, these restrictions may result in a very large number of constraints. Projected stochastic gradient descent (SGD) is often the default choice for large-scale optimization in machine learning, but requires a projection after each update. For heavily-constrained objectives, we propose an efficient extension of SGD that stays close to the feasible region while only applying constraints probabilistically at each iteration. Theoretical analysis shows a compelling trade-off between per-iteration work and the number of iterations needed on problems with a large number of constraints."
"We study implicational formulas in the context of proof complexity of intuitionistic propositional logic (IPC). On the one hand, we give an efficient transformation of tautologies to implicational tautologies that preserves the lengths of intuitionistic extended Frege (EF) or substitution Frege (SF) proofs up to a polynomial. On the other hand, EF proofs in the implicational fragment of IPC polynomially simulate full intuitionistic logic for implicational tautologies. The results also apply to other fragments of other superintuitionistic logics under certain conditions.   In particular, the exponential lower bounds on the length of intuitionistic EF proofs by Hrube\v{s} \cite{hru:lbint}, generalized to exponential separation between EF and SF systems in superintuitionistic logics of unbounded branching by Je\v{r}\'abek \cite{ej:sfef}, can be realized by implicational tautologies."
"Borderline personality disorder and narcissistic personality disorder are important nosographic entities and have been subject of intensive investigations. The currently prevailing psychodynamic theory for mental disorders is based on the repertoire of defense mechanisms employed. Another line of research is concerned with the study of psychological traumas and dissociation as a defensive response. Both theories can be used to shed light on some aspects of pathological mental functioning, and have many points of contact. This work merges these two psychological theories, and builds a model of mental function in a relational context called Quadripolar Relational Model. The model, which is enriched with ideas borrowed from the field of computer science, leads to a new therapeutic proposal for psychological traumas and personality disorders."
"This is the announcement, and the long summary, of a series of articles on the algorithmic study of Thurston maps. We describe branched coverings of the sphere in terms of group-theoretical objects called bisets, and develop a theory of decompositions of bisets.   We introduce a canonical ""Levy"" decomposition of an arbitrary Thurston map into homeomorphisms, metrically-expanding maps and maps doubly covered by torus endomorphisms. The homeomorphisms decompose themselves into finite-order and pseudo-Anosov maps, and the expanding maps decompose themselves into rational maps.   As an outcome, we prove that it is decidable when two Thurston maps are equivalent. We also show that the decompositions above are computable, both in theory and in practice."
"Several methods have been proposed for large-scale 3D reconstruction from large, unorganized image collections. A large reconstruction problem is typically divided into multiple components which are reconstructed independently using structure from motion (SFM) and later merged together. Incremental SFM methods are most popular for the basic structure recovery of a single component. They are robust and effective but are strictly sequential in nature. We present a multistage approach for SFM reconstruction of a single component that breaks the sequential nature of the incremental SFM methods. Our approach begins with quickly building a coarse 3D model using only a fraction of features from given images. The coarse model is then enriched by localizing remaining images and matching and triangulating remaining features in subsequent stages. These stages are made efficient and highly parallel by leveraging the geometry of the coarse model. Our method produces similar quality models as compared to incremental SFM methods while being notably fast and parallel."
"We study the second-order asymptotics of information transmission using random Gaussian codebooks and nearest neighbor (NN) decoding over a power-limited stationary memoryless additive non-Gaussian noise channel. We show that the dispersion term depends on the non-Gaussian noise only through its second and fourth moments, thus complementing the capacity result (Lapidoth, 1996), which depends only on the second moment. Furthermore, we characterize the second-order asymptotics of point-to-point codes over $K$-sender interference networks with non-Gaussian additive noise. Specifically, we assume that each user's codebook is Gaussian and that NN decoding is employed, i.e., that interference from the $K-1$ unintended users (Gaussian interfering signals) is treated as noise at each decoder. We show that while the first-order term in the asymptotic expansion of the maximum number of messages depends on the power of the interferring codewords only through their sum, this does not hold for the second-order term."
"$ $In its usual form, Grover's quantum search algorithm uses $O(\sqrt{N})$ queries and $O(\sqrt{N} \log N)$ other elementary gates to find a solution in an $N$-bit database. Grover in 2002 showed how to reduce the number of other gates to $O(\sqrt{N}\log\log N)$ for the special case where the database has a unique solution, without significantly increasing the number of queries. We show how to reduce this further to $O(\sqrt{N}\log^{(r)} N)$ gates for any constant $r$, and sufficiently large $N$. This means that, on average, the gates between two queries barely touch more than a constant number of the $\log N$ qubits on which the algorithm acts. For a very large $N$ that is a power of 2, we can choose $r$ such that the algorithm uses essentially the minimal number $\frac{\pi}{4}\sqrt{N}$ of queries, and only $O(\sqrt{N}\log(\log^{\star} N))$ other gates."
"In this paper we present a novel methodology based on a topological entropy, the so-called persistent entropy, for addressing the comparison between discrete piecewise linear functions. The comparison is certified by the stability theorem for persistent entropy. The theorem is used in the implementation of a new algorithm. The algorithm transforms a discrete piecewise linear function into a filtered simplicial complex that is analyzed with persistent homology and persistent entropy. Persistent entropy is used as discriminant feature for solving the supervised classification problem of real long length noisy signals of DC electrical motors. The quality of classification is stated in terms of the area under receiver operating characteristic curve (AUC=94.52%)."
"One of the challenges of modern engineering, and robotics in particular, is designing complex systems, composed of many subsystems, rigorously and with optimality guarantees. This paper introduces a theory of co-design that describes ""design problems"", defined as tuples of ""functionality space"", ""implementation space"", and ""resources space"", together with a feasibility relation that relates the three spaces. Design problems can be interconnected together to create ""co-design problems"", which describe possibly recursive co-design constraints among subsystems. A co-design problem induces a family of optimization problems of the type ""find the minimal resources needed to implement a given functionality""; the solution is an antichain (Pareto front) of resources. A special class of co-design problems are Monotone Co-Design Problems (MCDPs), for which functionality and resources are complete partial orders and the feasibility relation is monotone and Scott continuous. The induced optimization problems are multi-objective, nonconvex, nondifferentiable, noncontinuous, and not even defined on continuous spaces; yet, there exists a complete solution. The antichain of minimal resources can be characterized as a least fixed point, and it can be computed using Kleene's algorithm. The computation needed to solve a co-design problem can be bounded by a function of a graph property that quantifies the interdependence of the subproblems. These results make us much more optimistic about the problem of designing complex systems in a rigorous way."
"With the rapid development of mobile devices, the spatial crowdsourcing has recently attracted much attention from the database community. Specifically, the spatial crowdsourcing refers to a system that automatically assigns a number of location-based workers with spatial tasks nearby . Previous works on the spatial crowdsourcing usually designed task assignment strategies that maximize some assignment scores. However, their assignment strategies only considered the existing workers and tasks in the spatial crowdsourcing system, which might achieve local optimality, due to the unavailability of future workers/tasks that may join the system. Thus, in this paper, our goal is to achieve globally optimal task assignments at the current timestamp, by taking into account not only the existing, but also those future workers/tasks. We formalize an important problem, namely prediction-based spatial crowdsourcing (PB-SC), which finds a global optimal strategy for worker-and-task assignments for multiple rounds, based on both existing and predicted task/worker locations, such that the total assignment quality score of multiple rounds is maximized, under the constraint of the travel budget. The PB-SC problem is very challenging, in terms of the prediction accuracy and efficiency. In this paper, we design an effective prediction method to estimate spatial distributions of workers and tasks in the future, and then utilize the predicted ones in our procedure of worker-and-task assignments. We prove that, the PB-SC problem is NP-hard, and thus intractable. Therefore, we propose efficient approximation algorithms, greedy and divide-and-conquer, to deal with the PB-SC problem, by considering both current and future task/worker distributions. Through extensive experiments, we demonstrate the efficiency and effectiveness of our PB-SC processing approaches on real/synthetic data."
"Quantum networks must classically exchange complex metadata between devices in order to carry out information for protocols such as teleportation, super-dense coding, and quantum key distribution. Demonstrating the integration of these new communication methods with existing network protocols, channels, and data forwarding mechanisms remains an open challenge. Software-defined networking (SDN) offers robust and flexible strategies for managing diverse network devices and uses. We adapt the principles of SDN to the deployment of quantum networks, which are composed from unique devices that operate according to the laws of quantum mechanics. We show how quantum metadata can be managed within a software-defined network using the OpenFlow protocol, and we describe how OpenFlow management of classical optical channels is compatible with emerging quantum communication protocols. We next give an example specification of the metadata needed to manage and control QPHY behavior and we extend the OpenFlow interface to accommodate this quantum metadata. We conclude by discussing near-term experimental efforts that can realize SDN's principles for quantum communication."
"The Traveling Salesperson problem asks for the shortest cyclic tour visiting a set of cities given their pairwise distances and belongs to the NP-hard complexity class, which means that with all known algorithms in the worst case instances are not solveable in polynomial time, i.e., the problem is hard. Though that does not mean, that there are not subsets of the problem which are easy to solve. To examine numerically transitions from an easy to a hard phase, a random ensemble of cities in the Euclidean plane given a parameter {\sigma}, which governs the hardness, is introduced. Here, a linear programming approach together with suitable cutting planes is applied. Such algorithms operate outside the space of feasible solutions and are often used in practical application but rarely studied in physics so far. We observe several transitions. To characterize these transitions, scaling assumptions from continuous phase transitions are applied"
"This paper makes two contributions to Bayesian machine learning algorithms. Firstly, we propose stochastic natural gradient expectation propagation (SNEP), a novel alternative to expectation propagation (EP), a popular variational inference algorithm. SNEP is a black box variational algorithm, in that it does not require any simplifying assumptions on the distribution of interest, beyond the existence of some Monte Carlo sampler for estimating the moments of the EP tilted distributions. Further, as opposed to EP which has no guarantee of convergence, SNEP can be shown to be convergent, even when using Monte Carlo moment estimates. Secondly, we propose a novel architecture for distributed Bayesian learning which we call the posterior server. The posterior server allows scalable and robust Bayesian learning in cases where a dataset is stored in a distributed manner across a cluster, with each compute node containing a disjoint subset of data. An independent Monte Carlo sampler is run on each compute node, with direct access only to the local data subset, but which targets an approximation to the global posterior distribution given all data across the whole cluster. This is achieved by using a distributed asynchronous implementation of SNEP to pass messages across the cluster. We demonstrate SNEP and the posterior server on distributed Bayesian learning of logistic regression and neural networks.   Keywords: Distributed Learning, Large Scale Learning, Deep Learning, Bayesian Learning, Variational Inference, Expectation Propagation, Stochastic Approximation, Natural Gradient, Markov chain Monte Carlo, Parameter Server, Posterior Server."
"A generalized model of games is proposed, in which cooperative games and non-cooperative games are special cases. Some games that are neither cooperative nor non-cooperative can be expressed and analyzed. The model is based on relationships and supposed relationships between players. A relationship is a numerical value that denotes how one player cares for the payoffs of another player, while a supposed relationship is another numerical value that denotes a player's belief about the relationship between two players. The players choose their strategies by taking into consideration not only the material payoffs but also relationships and their change. Two games, a prisoners' dilemma and a repeated ultimatum game, are analyzed as examples of application of this model."
"Andries Brouwer maintains a public database of existence results for strongly regular graphs on $n\leq 1300$ vertices. We implemented most of the infinite families of graphs listed there in the open-source software Sagemath, as well as provided constructions of the ""sporadic"" cases, to obtain a graph for each set of parameters with known examples. Besides providing a convenient way to verify these existence results from the actual graphs, it also extends the database to higher values of $n$."
"Motivated by Tverberg-type problems in topological combinatorics and by classical results about embeddings (maps without double points), we study the question whether a finite simplicial complex K can be mapped into R^d without higher-multiplicity intersections. We focus on conditions for the existence of almost r-embeddings, i.e., maps from K to R^d without r-intersection points among any set of r pairwise disjoint simplices of K.   Generalizing the classical Haefliger-Weber embeddability criterion, we show that a well-known necessary deleted product condition for the existence of almost r-embeddings is sufficient in a suitable r-metastable range of dimensions (r d > (r+1) dim K +2). This significantly extends one of the main results of our previous paper (which treated the special case where d=rk and dim K=(r-1)k, for some k> 3)."
"This paper investigates the scaling exponent of polar codes for binary-input energy-harvesting (EH) channels with infinite-capacity batteries. The EH process is characterized by a sequence of i.i.d. random variables with finite variances. The scaling exponent $\mu$ of polar codes for a binary-input memoryless channel (BMC) characterizes the closest gap between the capacity and non-asymptotic rates achieved by polar codes with error probabilities no larger than some non-vanishing $\varepsilon\in(0,1)$. It has been shown that for any $\varepsilon\in(0,1)$, the scaling exponent $\mu$ for any binary-input memoryless symmetric channel (BMSC) with $I(q_{Y|X})\in(0,1)$ lies between 3.579 and 4.714 , where the upper bound $4.714$ was shown by an explicit construction of polar codes. Our main result shows that $4.714$ remains to be a valid upper bound on the scaling exponent for any binary-input EH channel, i.e., a BMC subject to additional EH constraints. Our result thus implies that the EH constraints do not worsen the rate of convergence to capacity if polar codes are employed. The main result is proved by leveraging the following three existing results: scaling exponent analyses for BMSCs, construction of polar codes designed for binary-input memoryless asymmetric channels, and the save-and-transmit strategy for EH channels."
"With 3D sensing becoming cheaper, environment-aware and visually-guided robot arms capable of safely working in collaboration with humans will become common. However, a reliable calibration is needed, both for camera internal calibration, as well as Eye-to-Hand calibration, to make sure the whole system functions correctly. We present a framework, using a novel combination of well proven methods, allowing a quick automatic calibration for the integration of systems consisting of the robot and a varying number of 3D cameras by using a standard checkerboard calibration grid. Our approach allows a quick camera-to-robot recalibration after any changes to the setup, for example when cameras or robot have been repositioned. Modular design of the system ensures flexibility regarding a number of sensors used as well as different hardware choices. The framework has been proven to work by practical experiments to analyze the quality of the calibration versus the number of positions of the checkerboard used for each of the calibration procedures."
"A vertex coloring of a graph is nonrepetitive if there is no path in the graph whose first half receives the same sequence of colors as the second half. While every tree can be nonrepetitively colored with a bounded number of colors (4 colors is enough), Fiorenzi, Ochem, Ossona de Mendez, and Zhu recently showed that this does not extend to the list version of the problem, that is, for every $\ell \geq 1$ there is a tree that is not nonrepetitively $\ell$-choosable. In this paper we prove the following positive result, which complements the result of Fiorenzi et al.: There exists a function $f$ such that every tree of pathwidth $k$ is nonrepetitively $f(k)$-choosable. We also show that such a property is specific to trees by constructing a family of pathwidth-2 graphs that are not nonrepetitively $\ell$-choosable for any fixed $\ell$."
"We propose a novel deep neural network architecture for speech recognition that explicitly employs knowledge of the background environmental noise within a deep neural network acoustic model. A deep neural network is used to predict the acoustic environment in which the system in being used. The discriminative embedding generated at the bottleneck layer of this network is then concatenated with traditional acoustic features as input to a deep neural network acoustic model. Through a series of experiments on Resource Management, CHiME-3 task, and Aurora4, we show that the proposed approach significantly improves speech recognition accuracy in noisy and highly reverberant environments, outperforming multi-condition training, noise-aware training, i-vector framework, and multi-task learning on both in-domain noise and unseen noise."
"Nowadays, neural networks play an important role in the task of relation classification. By designing different neural architectures, researchers have improved the performance to a large extent in comparison with traditional methods. However, existing neural networks for relation classification are usually of shallow architectures (e.g., one-layer convolutional neural networks or recurrent networks). They may fail to explore the potential representation space in different abstraction levels. In this paper, we propose deep recurrent neural networks (DRNNs) for relation classification to tackle this challenge. Further, we propose a data augmentation method by leveraging the directionality of relations. We evaluated our DRNNs on the SemEval-2010 Task~8, and achieve an F1-score of 86.1%, outperforming previous state-of-the-art recorded results."
"An arbitrarily varying wiretap channel (AVWTC) with a type constraint on the allowed state sequences is considered, and a single-letter characterization of its correlated-random (CR) assisted semantic-security (SS) capacity is derived. The allowed state sequences are the ones in a typical set around a single constraining type. SS is established by showing that the mutual information between the message and the eavesdropper's observations is negligible even when maximized over all message distributions, choices of state sequences and realizations of the CR-code. Both the achievability and the converse proofs of the type constrained coding theorem rely on stronger claims than actually required. The direct part establishes a novel single-letter lower bound on the CR-assisted SS-capacity of an AVWTC with state sequences constrained by any convex and closed set of state probability mass functions. This bound achieves the best known single-letter secrecy rates for a corresponding compound wiretap channel over the same constraint set. In contrast to other single-letter results in the AVWTC literature, this work does not assume the existence of a best channel to the eavesdropper. Instead, SS follows by leveraging the heterogeneous version of the stronger soft-covering lemma and a CR-code reduction argument. Optimality is a consequence of an max-inf upper bound on the CR-assisted SS-capacity of an AVWTC with state sequences constrained to any collection of type-classes. When adjusted to the aforementioned compound WTC, the upper bound simplifies to a max-min structure, thus strengthening the previously best known single-letter upper bound by Liang et al. that has a min-max form. The proof of the upper bound uses a novel distribution coupling argument."
"In this paper, we study the key properties of multi-antenna two-tier networks under different system configurations. Based on stochastic geometry, we derive the expressions and approximations for the users' average data rate. Through the more tractable approximations, the theoretical analysis can be greatly simplified. We find that the differences in density and transmit power between two tiers, together with range expansion bias significantly affect the users' data rate. Besides, for the purpose of area spectral efficiency (ASE) maximization, we find that the optimal number of active users for each tier is approximately fixed portion of the sum of the number of antennas plus one. Interestingly, the optimal settings are insensitive to different configurations between two tiers. Last but not the least, if the number of antennas of macro base stations (MBSs) is sufficiently larger than that of small cell base stations (SBSs), we find that range expansion will improve ASE."
"Gaussian random fields are a powerful tool for modeling environmental processes. For high dimensional samples, classical approaches for estimating the covariance parameters require highly challenging and massive computations, such as the evaluation of the Cholesky factorization or solving linear systems. Recently, Anitescu, Chen and Stein \cite{M.Anitescu} proposed a fast and scalable algorithm which does not need such burdensome computations. The main focus of this article is to study the asymptotic behavior of the algorithm of Anitescu et al. (ACS) for regular and irregular grids in the increasing domain setting. Consistency, minimax optimality and asymptotic normality of this algorithm are proved under mild differentiability conditions on the covariance function. Despite the fact that ACS's method entails a non-concave maximization, our results hold for any stationary point of the objective function. A numerical study is presented to evaluate the efficiency of this algorithm for large data sets."
"We show that the average stability notion introduced by \cite{kearns1999algorithmic, bousquet2002stability} is invariant to data preconditioning, for a wide class of generalized linear models that includes all known exp-concave losses. In other words, when analyzing the stability rate of a given algorithm, we may assume the optimal preconditioning of the data. This implies that, at least from a statistical perspective, explicit regularization is not required in order to compensate for ill-conditioned data, which stands in contrast to a widely common approach that includes a regularization for analyzing the sample complexity of generalized linear models. Several important implications of our findings include: a) We demonstrate that the excess risk of empirical risk minimization (ERM) is controlled by the preconditioned stability rate. This immediately yields a relatively short and elegant proof for the fast rates attained by ERM in our context. b) We strengthen the recent bounds of \cite{hardt2015train} on the stability rate of the Stochastic Gradient Descent algorithm."
"This paper develops a power management scheme that jointly optimizes the real power consumption of programmable loads and reactive power outputs of photovoltaic (PV) inverters in distribution networks. The premise is to determine the optimal demand response schedule that accounts for the stochastic availability of solar power, as well as to control the reactive power generation or consumption of PV inverters adaptively to the real power injections of all PV units. These uncertain real power injections by PV units are modeled as random variables taking values from a finite number of possible scenarios. Through the use of second order cone relaxation of the power flow equations, a convex stochastic program is formulated. The objectives are to minimize the negative user utility, cost of power provision, and thermal losses, while constraining voltages to remain within specified levels. To find the global optimum point, a decentralized algorithm is developed via the alternating direction method of multipliers that results in closed-form updates per node and per scenario, rendering it suitable to implement in distribution networks with large number of scenarios. Numerical tests and comparisons with an alternative deterministic approach are provided for typical residential distribution networks that confirm the efficiency of the algorithm."
"We study a first-order, purely functional quantum extension of lambda calculus, that is, an extension of lambda calculus to express some quantum features, where the quantum memory is abstracted out. This calculus is a typed extension of the first-order linear-algebraic lambda-calculus. The type is linear on superpositions, so to forbid from cloning them, while allows to clone basis vectors."
"We propose a new method to recover global information about a network of interconnected dynamical systems based on observations made at a small number (possibly one) of its nodes. In contrast to classical identification of full graph topology, we focus on the identification of the spectral graph-theoretic properties of the network, a framework that we call spectral network identification.   The main theoretical results connect the spectral properties of the network to the spectral properties of the dynamics, which are well-defined in the context of the so-called Koopman operator and can be extracted from data through the Dynamic Mode Decomposition algorithm. These results are obtained for networks of diffusively-coupled units that admit a stable equilibrium state. For large networks, a statistical approach is considered, which focuses on spectral moments of the network and is well-suited to the case of heterogeneous populations.   Our framework provides efficient numerical methods to infer global information on the network from sparse local measurements at a few nodes. Numerical simulations show for instance the possibility of detecting the mean number of connections or the addition of a new vertex using measurements made at one single node, that need not be representative of the other nodes' properties."
"Consider the continuous distributed monitoring model in which $n$ distributed nodes, receiving individual data streams, are connected to a designated server. The server is asked to continuously monitor a function defined over the values observed across all streams while minimizing the communication. We study a variant in which the server is equipped with a broadcast channel and is supposed to keep track of an approximation of the set of nodes currently observing the $k$ largest values. Such an approximate set is exact except for some imprecision in an $\varepsilon$-neighborhood of the $k$-th largest value. This approximation of the Top-$k$-Position Monitoring Problem is of interest in cases where marginal changes (e.g. due to noise) in observed values can be ignored so that monitoring an approximation is sufficient and can reduce communication.   This paper extends our results from [IPDPS'15], where we have developed a filter-based online algorithm for the (exact) Top-k-Position Monitoring Problem. There we have presented a competitive analysis of our algorithm against an offline adversary that also is restricted to filter-based algorithms. Our new algorithms as well as their analyses use new methods. We analyze their competitiveness against adversaries that use both exact and approximate filter-based algorithms, and observe severe differences between the respective powers of these adversaries."
"Convex clustering, a convex relaxation of k-means clustering and hierarchical clustering, has drawn recent attentions since it nicely addresses the instability issue of traditional nonconvex clustering methods. Although its computational and statistical properties have been recently studied, the performance of convex clustering has not yet been investigated in the high-dimensional clustering scenario, where the data contains a large number of features and many of them carry no information about the clustering structure. In this paper, we demonstrate that the performance of convex clustering could be distorted when the uninformative features are included in the clustering. To overcome it, we introduce a new clustering method, referred to as Sparse Convex Clustering, to simultaneously cluster observations and conduct feature selection. The key idea is to formulate convex clustering in a form of regularization, with an adaptive group-lasso penalty term on cluster centers. In order to optimally balance the tradeoff between the cluster fitting and sparsity, a tuning criterion based on clustering stability is developed. In theory, we provide an unbiased estimator for the degrees of freedom of the proposed sparse convex clustering method. Finally, the effectiveness of the sparse convex clustering is examined through a variety of numerical experiments and a real data application."
"This paper studies recursive composite hypothesis testing in a network of sparsely connected agents. The network objective is to test a simple null hypothesis against a composite alternative concerning the state of the field, modeled as a vector of (continuous) unknown parameters determining the parametric family of probability measures induced on the agents' observation spaces under the hypotheses. Specifically, under the alternative hypothesis, each agent sequentially observes an independent and identically distributed time-series consisting of a (nonlinear) function of the true but unknown parameter corrupted by Gaussian noise, whereas, under the null, they obtain noise only. Two distributed recursive generalized likelihood ratio test type algorithms of the \emph{consensus+innovations} form are proposed, namely $\mathcal{CIGLRT-L}$ and $\mathcal{CIGLRT-NL}$, in which the agents estimate the underlying parameter and in parallel also update their test decision statistics by simultaneously processing the latest local sensed information and information obtained from neighboring agents. For $\mathcal{CIGLRT-NL}$, for a broad class of nonlinear observation models and under a global observability condition, algorithm parameters which ensure asymptotically decaying probabilities of errors~(probability of miss and probability of false detection) are characterized. For $\mathcal{CIGLRT-L}$, a linear observation model is considered and upper bounds on large deviations decay exponent for the error probabilities are obtained."
"Separating hash families are useful combinatorial structures which are generalizations of many well-studied objects in combinatorics, cryptography and coding theory. In this paper, using tools from graph theory and additive number theory, we solve several open problems and conjectures concerning bounds and constructions for separating hash families. Firstly, we discover that the cardinality of a separating hash family satisfies a Johnson-type inequality. As a result, we obtain a new upper bound, which is superior to all previous ones. Secondly, we present a construction for an infinite class of perfect hash families. It is based on the Hamming graphs in coding theory and generalizes many constructions that appeared before. It provides an affirmative answer to both Bazrafshan-Trung's open problem on separating hash families and Alon-Stav's conjecture on parent-identifying codes. Thirdly, let $p_t(N,q)$ denote the maximal cardinality of a $t$-perfect hash family of length $N$ over an alphabet of size $q$. Walker II and Colbourn conjectured that $p_3(3,q)=o(q^2)$. We verify this conjecture by proving $q^{2-o(1)}<p_3(3,q)=o(q^2)$. Our proof can be viewed as an application of Ruzsa-Szemer{\'e}di's (6,3)-theorem. We also prove $q^{2-o(1)}<p_4(4,q)=o(q^2)$. Two new notions in graph theory and additive number theory, namely rainbow cycles and $R$-sum-free sets, are introduced to prove this result. These two bounds support a question of Blackburn, Etzion, Stinson and Zaverucha. Finally, we establish a bridge between perfect hash families and hypergraph Tur{\'a}n problems. This connection has not been noticed before. As a consequence, many new results and problems arise."
"Rooted phylogenetic networks are used to describe evolutionary histories that contain non-treelike evolutionary events such as hybridization and horizontal gene transfer. In some cases, such histories can be described by a phylogenetic base-tree with additional linking arcs, which can for example represent gene transfer events. Such phylogenetic networks are called tree-based. Here, we consider two possible generalizations of this concept to nonbinary networks, which we call tree-based and strictly-tree-based nonbinary phylogenetic networks. We give simple graph-theoretic characterizations of tree-based and strictly-tree-based nonbinary phylogenetic networks. Moreover, we show for each of these two classes that it can be decided in polynomial time whether a given network is contained in the class. Our approach also provides a new view on tree-based binary phylogenetic networks. Finally, we discuss two examples of nonbinary phylogenetic networks in biology and show how our results can be applied to them."
"We analyze the asymptotic convergence of all infinite products of matrices taken in a given finite set, by looking only at finite or periodic products. It is known that when the matrices of the set have a common nonincreasing polyhedral norm, all infinite products converge to zero if and only if all infinite periodic products with period smaller than a certain value converge to zero, and bounds exist on that value.   We provide a stronger bound holding for both polyhedral norms and polyhedral seminorms. In the latter case, the matrix products do not necessarily converge to 0, but all trajectories of the associated system converge to a common invariant space. We prove our bound to be tight, in the sense that for any polyhedral seminorm, there is a set of matrices such that not all infinite products converge, but every periodic product with period smaller than our bound does converge.   Our technique is based on an analysis of the combinatorial structure of the face lattice of the unit ball of the nonincreasing seminorm. The bound we obtain is equal to half the size of the largest antichain in this lattice. Explicitly evaluating this quantity may be challenging in some cases. We therefore link our problem with the Sperner property: the property that, for some graded posets, -- in this case the face lattice of the unit ball -- the size of the largest antichain is equal to the size of the largest rank level.   We show that some sets of matrices with invariant polyhedral seminorms lead to posets that do not have that Sperner property. However, this property holds for the polyhedron obtained when treating sets of stochastic matrices, and our bound can then be easily evaluated in that case. In particular, we show that for the dimension of the space $n \geq 8$, our bound is smaller than the previously known bound by a multiplicative factor of $\frac{3}{2 \sqrt{\pi n}}$."
"Several studies assert that the random access procedure of the Long Term Evolution (LTE) cellular standard may not be effective whenever a massive number of simultaneous connection attempts are performed by terminals, as may happen in a typical Internet of Things or Smart City scenario. Nevertheless, simulation studies in real deployment scenarios are missing because many system-level simulators do not implement the LTE random access procedure in detail. In this paper, we propose a patch for the LTE module of ns-3, one of the most prominent open-source network simulators, to improve the accuracy of the routine that simulates the LTE Random Access Channel (RACH). The patched version of the random access procedure is compared with the default one and the issues arising from massive simultaneous access from mobile terminals in LTE are assessed via a simulation campaign."
"The decidability of the distributed version of the Ramadge and Wonham controller synthesis problemwhere both the plant and the controllers are modelled as Zielonka automataand the controllers have causal memoryis a challenging open problem.There exists three classes of plants for which the existence of a correct controller with causal memory has been shown decidable: when the dependency graph of actions is series-parallel, when the processes are connectedly communicating and when the dependency graph of processes is a tree. We provide a unified proof of these three results,and design a class of plants, called broadcast games, with a decidable controller synthesis problem.This leads to new examples of decidable architectures."
"We present a game semantics for intuitionistic type theory. Specifically, we propose categories with families of a new variant of games and strategies for both extensional and intensional variants of the type theory with dependent function, dependent pair, and identity types as well as universes. Our games and strategies generalize the existing notion of games and strategies and achieve an interpretation of dependent types and the hierarchy of universes in an intuitive manner. We believe that it is a significant step towards a computational and intensional interpretation of the type theory."
"Rank aggregation systems collect ordinal preferences from individuals to produce a global ranking that represents the social preference. Rank-breaking is a common practice to reduce the computational complexity of learning the global ranking. The individual preferences are broken into pairwise comparisons and applied to efficient algorithms tailored for independent paired comparisons. However, due to the ignored dependencies in the data, naive rank-breaking approaches can result in inconsistent estimates. The key idea to produce accurate and consistent estimates is to treat the pairwise comparisons unequally, depending on the topology of the collected data. In this paper, we provide the optimal rank-breaking estimator, which not only achieves consistency but also achieves the best error bound. This allows us to characterize the fundamental tradeoff between accuracy and complexity. Further, the analysis identifies how the accuracy depends on the spectral gap of a corresponding comparison graph."
"Semiconstrained systems were recently suggested as a generalization of constrained systems, commonly used in communication and data-storage applications that require certain offending subsequences be avoided. In an attempt to apply techniques from constrained systems, we study sequences of constrained systems that are contained in, or contain, a given semiconstrained system, while approaching its capacity. In the case of contained systems we describe to such sequences resulting in constant-to-constant bit-rate block encoders and sliding-block encoders. Surprisingly, in the case of containing systems we show that a ""generic"" semiconstrained system is never contained in a proper fully-constrained system."
"In network cooperation strategies, nodes work together with the aim of increasing transmission rates or reliability. This paper demonstrates that enabling cooperation between the transmitters of a two-user multiple access channel, via a cooperation facilitator that has access to both messages, always results in a network whose maximal- and average-error sum-capacities are the same---even when those capacities differ in the absence of cooperation and the information shared with the encoders is negligible. From this result, it follows that if a multiple access channel with no transmitter cooperation has different maximal- and average-error sum-capacities, then the maximal-error sum-capacity of the network consisting of this channel and a cooperation facilitator is not continuous with respect to the output edge capacities of the facilitator. This shows that there exist networks where sharing even a negligible number of bits per channel use with the encoders yields a non-negligible benefit."
"This paper develops an active sensing method to estimate the relative weight (or trust) agents place on their neighbors' information in a social network. The model used for the regression is based on the steady state equation in the linear DeGroot model under the influence of stubborn agents, i.e., agents whose opinions are not influenced by their neighbors. This method can be viewed as a \emph{social RADAR}, where the stubborn agents excite the system and the latter can be estimated through the reverberation observed from the analysis of the agents' opinions. The social network sensing problem can be interpreted as a blind compressed sensing problem with a sparse measurement matrix. We prove that the network structure will be revealed when a sufficient number of stubborn agents independently influence a number of ordinary (non-stubborn) agents. We investigate the scenario with a deterministic or randomized DeGroot model and propose a consistent estimator of the steady states for the latter scenario. Simulation results on synthetic and real world networks support our findings."
"We establish the first known upper bound on the exact and Wyner's common information of $n$ continuous random variables in terms of the dual total correlation between them (which is a generalization of mutual information). In particular, we show that when the pdf of the random variables is log-concave, there is a constant gap of $n^{2}\log e+9n\log n$ between this upper bound and the dual total correlation lower bound that does not depend on the distribution. The upper bound is obtained using a computationally efficient dyadic decomposition scheme for constructing a discrete common randomness variable $W$ from which the $n$ random variables can be simulated in a distributed manner. We then bound the entropy of $W$ using a new measure, which we refer to as the erosion entropy."
"In this paper we report the likelihood estimates that are performed on time series using a echo state network with orthogonal recurrent connectivity. The results indicate that the optimal performance depends on the way of balancing the input strength with the recurrent activity, which also has an influence on the network with regard to the quality of the short term prediction versus prediction that accounts for influences that date back a long time in the input history. Finally, sensitivity of such networks against noise/finite accuracy of network states in the recurrent layer is investigated. In addition a measure that bases on mutual information is introduced in order to best quantify the performance of the network with the time series."
"We study the optimal control problem of allocating campaigning resources over the campaign duration and degree classes in a social network. Information diffusion is modeled as a Susceptible-Infected epidemic and direct recruitment of susceptible nodes to the infected (informed) class is used as a strategy to accelerate the spread of information. We formulate an optimal control problem for optimizing a net reward function, a linear combination of the reward due to information spread and cost due to application of controls. The time varying resource allocation and seeds for the epidemic are jointly optimized. A problem variation includes a fixed budget constraint. We prove the existence of a solution for the optimal control problem, provide conditions for uniqueness of the solution, and prove some structural results for the controls (e.g. controls are non-increasing functions of time). The solution technique uses Pontryagin's Maximum Principle and the forward-backward sweep algorithm (and its modifications) for numerical computations. Our formulations lead to large optimality systems with up to about 200 differential equations and allow us to study the effect of network topology (Erdos-Renyi/scale-free) on the controls. Results reveal that the allocation of campaigning resources to various degree classes depends not only on the network topology but also on system parameters such as cost/abundance of resources. The optimal strategies lead to significant gains over heuristic strategies for various model parameters. Our modeling approach assumes uncorrelated network, however, we find the approach useful for real networks as well. This work is useful in product advertising, political and crowdfunding campaigns in social networks."
"Cooperation strategies allow communication devices to work together to improve network capacity. Consider a network consisting of a $k$-user multiple access channel (MAC) and a node that is connected to all $k$ encoders via rate-limited bidirectional links, referred to as the ""cooperation facilitator"" (CF). Define the cooperation benefit as the sum-capacity gain resulting from the communication between the encoders and the CF and the cooperation rate as the total rate the CF shares with the encoders. This work demonstrates the existence of a class of $k$-user MACs where the ratio of the cooperation benefit to cooperation rate tends to infinity as the cooperation rate tends to zero. Examples of channels in this class include the binary erasure MAC for $k=2$ and the $k$-user Gaussian MAC for any $k\geq 2$."
"Larsen and Skou characterized probabilistic bisimilarity over reactive probabilistic systems with a logic including true, negation, conjunction, and a diamond modality decorated with a probabilistic lower bound. Later on, Desharnais, Edalat, and Panangaden showed that negation is not necessary to characterize the same equivalence. In this paper, we prove that the logical characterization holds also when conjunction is replaced by disjunction, with negation still being not necessary. To this end, we introduce reactive probabilistic trees, a fully abstract model for reactive probabilistic systems that allows us to demonstrate expressiveness of the disjunctive probabilistic modal logic, as well as of the previously mentioned logics, by means of a compactness argument."
"We investigate centrality and root-inference properties in a class of growing random graphs known as sublinear preferential attachment trees. We show that a continuous time branching processes called the Crump-Mode-Jagers (CMJ) branching process is well-suited to analyze such random trees, and prove that almost surely, a unique terminal tree centroid emerges, having the property that it becomes more central than any other fixed vertex in the limit of the random growth process. Our result generalizes and extends previous work establishing persistent centrality in uniform and linear preferential attachment trees. We also show that centrality may be utilized to generate a finite-sized $1-\epsilon$ confidence set for the root node, for any $\epsilon > 0$ in a certain subclass of sublinear preferential attachment trees."
"Uplink-downlink decoupling in which users can be associated to different base stations in the uplink and downlink of heterogeneous small cell networks (SCNs) has attracted significant attention recently. However, most existing works focus on simple association mechanisms in LTE SCNs that operate only in the licensed band. In contrast, in this paper, the problem of resource allocation with uplink-downlink decoupling is studied for an SCN that incorporates LTE in the unlicensed band (LTE-U). Here, the users can access both licensed and unlicensed bands while being associated to different base stations. This problem is formulated as a noncooperative game that incorporates user association, spectrum allocation, and load balancing. To solve this problem, a distributed algorithm based on the machine learning framework of echo state networks is proposed using which the small base stations autonomously choose their optimal bands allocation strategies while having only limited information on the network's and users' states. It is shown that the proposed algorithm converges to a stationary mixed-strategy distribution which constitutes a mixed strategy Nash equilibrium for the studied game. Simulation results show that the proposed approach yields significant gains, in terms of the sum-rate of the 50th percentile of users, that reach up to 60% and 78%, respectively, compared to Q-learning and Q-learning without decoupling. The results also show that ESN significantly provides a considerable reduction of information exchange for the wireless network."
"Media-based modulation (MBM) is a recently proposed modulation scheme which uses radio frequency (RF) mirrors at the transmit antenna(s) in order to create different channel fade realizations based on their ON/OFF status. These complex fade realizations constitute the modulation alphabet. MBM has the advantage of increased spectral efficiency and performance. In this paper, we investigate the performance of some physical layer techniques when applied to MBM. Particularly, we study the performance of $i)$ MBM with generalized spatial modulation (GSM), $ii)$ MBM with mirror activation pattern (MAP) selection based on an Euclidean distance (ED) based metric, and $iii)$ MBM with feedback based phase compensation and constellation rotation. Our results show that, for the same spectral efficiency, GSM-MBM can achieve better performance compared to MIMO-MBM. Also, it is found that MBM with ED-based MAP selection results in improved bit error performance, and that phase compensation and MBM constellation rotation increases the ED between the MBM constellation points and improves the performance significantly. We also analyze the diversity orders achieved by the ED-based MAP selection scheme and the phase compensation and constellation rotation (PC-CR) scheme. The diversity orders predicted by the analysis are validated through simulations."
"In this work, we investigate optimal geographical caching in heterogeneous cellular networks. Different types of base stations (BSs) have different cache capacities. The content library is containing files with popularities following a probability distribution. The performance metric is the total hit probability, which is the probability that a user at an arbitrary location in the plane will find the content that it requires in one of the BSs that it is covered by. We consider the problem of optimally placing content in all BSs jointly. As this problem is not convex, we provide a heuristic scheme by finding the optimal placement policies for different types of BSs using the information coming from the other types. We demonstrate that these individual optimization problems are convex and we provide an analytical solution. As an illustration, we find the optimal placement policy of the small base stations (SBSs) depending on the placement policy of the macro base stations (MBSs). We show how the hit probability evolves as the deployment density of the SBSs varies. We show that the heuristic of placing the most popular content in the MBSs is almost optimal after deploying the SBSs with optimal placement policies. Also, for the SBSs no such heuristic can be used; the optimal placement is significantly better than storing the most popular content. Finally, we show that solving the individual problems to find the optimal placement policies for different types of BSs iteratively, namely repeatedly updating the placement policies, does not improve the performance."
"We study the footprint function, with respect to a monomial order, of complete intersection graded ideals in a polynomial ring with coefficients in a field. For graded ideals of dimension one, whose initial ideal is a complete intersection, we give a formula for the footprint function and a sharp lower bound for the corresponding minimum distance function. This allows us to recover a formula for the minimum distance of an affine cartesian code and the fact that in this case the minimum distance and the footprint functions coincide. Then we present an extension of a result of Alon and F\""uredi, about coverings of the cube $\{0,1\}^n$ by affine hyperplanes, in terms of the regularity of a vanishing ideal."
"This paper focuses on the problem of recursive nonlinear least squares parameter estimation in multi-agent networks, in which the individual agents observe sequentially over time an independent and identically distributed (i.i.d.) time-series consisting of a nonlinear function of the true but unknown parameter corrupted by noise. A distributed recursive estimator of the \emph{consensus} + \emph{innovations} type, namely $\mathcal{CIWNLS}$, is proposed, in which the agents update their parameter estimates at each observation sampling epoch in a collaborative way by simultaneously processing the latest locally sensed information~(\emph{innovations}) and the parameter estimates from other agents~(\emph{consensus}) in the local neighborhood conforming to a pre-specified inter-agent communication topology. Under rather weak conditions on the connectivity of the inter-agent communication and a \emph{global observability} criterion, it is shown that at every network agent, the proposed algorithm leads to consistent parameter estimates. Furthermore, under standard smoothness assumptions on the local observation functions, the distributed estimator is shown to yield order-optimal convergence rates, i.e., as far as the order of pathwise convergence is concerned, the local parameter estimates at each agent are as good as the optimal centralized nonlinear least squares estimator which would require access to all the observations across all the agents at all times. In order to benchmark the performance of the proposed distributed $\mathcal{CIWNLS}$ estimator with that of the centralized nonlinear least squares estimator, the asymptotic normality of the estimate sequence is established and the asymptotic covariance of the distributed estimator is evaluated. Finally, simulation results are presented which illustrate and verify the analytical findings."
"Desktops and laptops can be maliciously exploited to violate privacy. There are two main types of attack scenarios: active and passive. In this paper, we consider the passive scenario where the adversary does not interact actively with the device, but he is able to eavesdrop on the network traffic of the device from the network side. Most of the Internet traffic is encrypted and thus passive attacks are challenging. Previous research has shown that information can be extracted from encrypted multimedia streams. This includes video title classification of non HTTP adaptive streams (non-HAS). This paper presents an algorithm for encrypted HTTP adaptive video streaming title classification. We show that an external attacker can identify the video title from video HTTP adaptive streams (HAS) sites such as YouTube. To the best of our knowledge, this is the first work that shows this. We provide a large data set of 10000 YouTube video streams of 100 popular video titles (each title downloaded 100 times) as examples for this task. The dataset was collected under real-world network conditions. We present several machine algorithms for the task and run a through set of experiments, which shows that our classification accuracy is more than 95%. We also show that our algorithms are able to classify video titles that are not in the training set as unknown and some of the algorithms are also able to eliminate false prediction of video titles and instead report unknown. Finally, we evaluate our algorithms robustness to delays and packet losses at test time and show that a solution that uses SVM is the most robust against these changes given enough training data. We provide the dataset and the crawler for future research."
"We consider the problem of aggregating data in a dynamic graph, that is, aggregating the data that originates from all nodes in the graph to a specific node, the sink. We are interested in giving lower bounds for this problem, under different kinds of adversaries. In our model, nodes are endowed with unlimited memory and unlimited computational power. Yet, we assume that communications between nodes are carried out with pairwise interactions, where nodes can exchange control information before deciding whether they transmit their data or not, given that each node is allowed to transmit its data at most once. When a node receives a data from a neighbor, the node may aggregate it with its own data. We consider three possible adversaries: the online adaptive adversary, the oblivious adversary , and the randomized adversary that chooses the pairwise interactions uniformly at random. For the online adaptive and the oblivious adversary, we give impossibility results when nodes have no knowledge about the graph and are not aware of the future. Also, we give several tight bounds depending on the knowledge (be it topology related or time related) of the nodes. For the randomized adversary, we show that the Gathering algorithm, which always commands a node to transmit, is optimal if nodes have no knowledge at all. Also, we propose an algorithm called Waiting Greedy, where a node either waits or transmits depending on some parameter, that is optimal when each node knows its future pairwise interactions with the sink."
"We propose a fully automatic method for fitting a 3D morphable model to single face images in arbitrary pose and lighting. Our approach relies on geometric features (edges and landmarks) and, inspired by the iterated closest point algorithm, is based on computing hard correspondences between model vertices and edge pixels. We demonstrate that this is superior to previous work that uses soft correspondences to form an edge-derived cost surface that is minimised by nonlinear optimisation."
"In a secondary spectrum market primaries set prices for their unused channels to the secondaries. The payoff of a primary depends on the availability of unused channels of its competitors. We consider a model were a primary can acquire its competitor's channel state information (C-CSI) at a cost. We formulate a game between two primaries where each primary decides whether to acquire C-CSI or not and then selects its price based on that. We first characterize the Nash Equilibrium (NE) of this game for a symmetric model where the C-CSI is perfect. We show that the payoff of a primary is independent of the C-CSI acquisition cost. We then generalize our analysis to allow for imperfect estimation and cases where the two primaries have different C-CSI costs or different channel availabilities. Our results show interestingly that the payoff of a primary increases when there is estimation error. We also show that surprisingly, the expected payoff of a primary may decrease when the C-CSI acquisition cost decreases when primaries have different availabilities."
"Quantifying synergy among stochastic variables is an important open problem in information theory. Information synergy occurs when multiple sources together predict an outcome variable better than the sum of single-source predictions. It is an essential phenomenon in biology such as in neuronal networks and cellular regulatory processes, where different information flows integrate to produce a single response, but also in social cooperation processes as well as in statistical inference tasks in machine learning. Here we propose a metric of synergistic entropy and synergistic information from first principles. The proposed measure relies on so-called synergistic random variables (SRVs) which are constructed to have zero mutual information about individual source variables but non-zero mutual information about the complete set of source variables. We prove several basic and desired properties of our measure, including bounds and additivity properties. In addition, we prove several important consequences of our measure, including the fact that different types of synergistic information may co-exist between the same sets of variables. A numerical implementation is provided, which we use to demonstrate that synergy is associated with resilience to noise. Our measure may be a marked step forward in the study of multivariate information theory and its numerous applications."
"We consider the problem of providing privacy, in the private information retrieval (PIR) sense, to users requesting data from a distributed storage system (DSS). The DSS uses a Maximum Distance Separable (MDS) code to store the data reliably on unreliable storage nodes. Some of these nodes can be spies which report to a third party, such as an oppressive regime, which data is being requested by the user. An information theoretic PIR scheme ensures that a user can satisfy its request while revealing, to the spy nodes, no information on which data is being requested. A user can achieve PIR by downloading all the data in the DSS. However, this is not a feasible solution due to its high communication cost. We construct PIR schemes with low download communication cost. When there is one spy node, we construct PIR schemes with download cost $\frac{1}{1-R}$ per unit of requested data ($R$ is the code rate), achieving the information theoretic limit for linear schemes. An important property of these schemes is their universality since they depend on the code rate, but not on the MDS code itself. When there are two spy nodes for rate $R\leq 1/2$ rates, we devise PIR schemes that have download cost independent of the total size of the data in the DSS."
"We present a comparative evaluation of various techniques for action recognition while keeping as many variables as possible controlled. We employ two categories of Riemannian manifolds: symmetric positive definite matrices and linear subspaces. For both categories we use their corresponding nearest neighbour classifiers, kernels, and recent kernelised sparse representations. We compare against traditional action recognition techniques based on Gaussian mixture models and Fisher vectors (FVs). We evaluate these action recognition techniques under ideal conditions, as well as their sensitivity in more challenging conditions (variations in scale and translation). Despite recent advancements for handling manifolds, manifold based techniques obtain the lowest performance and their kernel representations are more unstable in the presence of challenging conditions. The FV approach obtains the highest accuracy under ideal conditions. Moreover, FV best deals with moderate scale and translation changes."
"We propose a hierarchical approach to multi-action recognition that performs joint classification and segmentation. A given video (containing several consecutive actions) is processed via a sequence of overlapping temporal windows. Each frame in a temporal window is represented through selective low-level spatio-temporal features which efficiently capture relevant local dynamics. Features from each window are represented as a Fisher vector, which captures first and second order statistics. Instead of directly classifying each Fisher vector, it is converted into a vector of class probabilities. The final classification decision for each frame is then obtained by integrating the class probabilities at the frame level, which exploits the overlapping of the temporal windows. Experiments were performed on two datasets: s-KTH (a stitched version of the KTH dataset to simulate multi-actions), and the challenging CMU-MMAC dataset. On s-KTH, the proposed approach achieves an accuracy of 85.0%, significantly outperforming two recent approaches based on GMMs and HMMs which obtained 78.3% and 71.2%, respectively. On CMU-MMAC, the proposed approach achieves an accuracy of 40.9%, outperforming the GMM and HMM approaches which obtained 33.7% and 38.4%, respectively. Furthermore, the proposed system is on average 40 times faster than the GMM based approach."
"The current paper proposes a novel dynamic neural network model for categorization of complex human action visual patterns. The Multiple Spatio-Temporal Scales Recurrent Neural Network (MSTRNN) adds recurrent connectivity to a prior model, the Multiple Spatio-Temporal Scales Neural Network (MSTNN). By developing adequate recurrent contextual dynamics, the MSTRNN can learn to extract latent spatio-temporal structures from input image sequences more effectively than the MSTNN. Two experiments with the MSTRNN are detailed. The first experiment involves categorizing a set of human movement patterns consisting of sequences of action primitives. The MSTRNN is able to extract long-ranged correlations in video images better than the MSTNN. Time series analysis on neural activation values obtained from the recurrent structure shows that the MSTRNN accumulates extracted spatio-temporal features which discriminate action sequences. The second experiment requires that the model categorize a set of object-directed actions, and demonstrates that the MSTRNN can learn to extract structural relationships between actions and action-directed-objects (ADOs). Analysis of characteristics employed in categorizing both object-directed actions and pantomime actions indicates that the model network develops categorical memories by organizing relational structures between each action and appropriate ADO. Such relational structure may be necessary for categorizing human actions with an adequate ability to generalize."
"In this paper we introduce, and characterize, a class of graph parameters obtained from tensor invariants of the symplectic group. These parameters are similar to partition functions of vertex models, as introduced by de la Harpe and Jones, [P. de la Harpe, V.F.R. Jones, Graph invariants related to statistical mechanical models: examples and problems, Journal of Combinatorial Theory, Series B 57 (1993) 207-227]. Yet they give a completely different class of graph invariants. We moreover show that certain evaluations of the cycle partition polynomial, as defined by Martin [P. Martin, Enum\'erations eul\'eriennes dans les multigraphes et invariants de Tutte-Grothendieck, Diss. Institut National Polytechnique de Grenoble-INPG; Universit\'e Joseph-Fourier-Grenoble I, 1977], give examples of graph parameters that can be obtained this way."
"A quantum channel physically is a unitary interaction between the information carrying system and an environment, which is initialized in a pure state before the interaction. Conventionally, this state, as also the parameters of the interaction, is assumed to be fixed and known to the sender and receiver. Here, following the model introduced by us earlier [Karumanchi et al., arXiv[quant-ph]:1407.8160], we consider a benevolent third party, i.e. a helper, controlling the environment state, and how the helper's presence changes the communication game. In particular, we define and study the classical capacity of a unitary interaction with helper, indeed two variants, one where the helper can only prepare separable states across many channel uses, and one without this restriction. Furthermore, the two even more powerful scenarios of pre-shared entanglement between helper and receiver, and of classical communication between sender and helper (making them conferencing encoders) are considered."
"We introduce a novel framework for an approxi- mate recovery of data matrices which are low-rank on graphs, from sampled measurements. The rows and columns of such matrices belong to the span of the first few eigenvectors of the graphs constructed between their rows and columns. We leverage this property to recover the non-linear low-rank structures efficiently from sampled data measurements, with a low cost (linear in n). First, a Resrtricted Isometry Property (RIP) condition is introduced for efficient uniform sampling of the rows and columns of such matrices based on the cumulative coherence of graph eigenvectors. Secondly, a state-of-the-art fast low-rank recovery method is suggested for the sampled data. Finally, several efficient, parallel and parameter-free decoders are presented along with their theoretical analysis for decoding the low-rank and cluster indicators for the full data matrix. Thus, we overcome the computational limitations of the standard linear low-rank recovery methods for big datasets. Our method can also be seen as a major step towards efficient recovery of non- linear low-rank structures. For a matrix of size n X p, on a single core machine, our method gains a speed up of $p^2/k$ over Robust Principal Component Analysis (RPCA), where k << p is the subspace dimension. Numerically, we can recover a low-rank matrix of size 10304 X 1000, 100 times faster than Robust PCA."
"The ordered set is one of the most important data type in both theoretical algorithm design and analysis and practical programming. In this paper we study the set operations on two ordered sets, including Union, Intersect and Difference, based on four types of balanced Binary Search Trees (BST) including AVL trees, red-black trees, weight balanced trees and treaps. We introduced only one subroutine Join that needs to be implemented differently for each balanced BST, and on top of which we can implement generic, simple and efficient parallel functions for ordered sets. We first prove the work-efficiency of these Join-based set functions using a generic proof working for all the four types of balanced BSTs.   We also implemented and tested our algorithm on all the four balancing schemes. Interestingly the implementations on all four data structures and three set functions perform similarly in time and speedup (more than 45x on 64 cores). We also compare the performance of our implementation to other existing libraries and algorithms."
"Future wireless networks need to offer orders of magnitude more capacity to address the predicted growth in mobile traffic demand. Operators to enhance the capacity of cellular networks are increasingly using WiFi to offload traffic from their core networks. This paper deals with the efficient and flexible management of a heterogeneous networking environment offering wireless access to multimode terminals. This wireless access is evaluated under disruptive usage scenarios, such as flash crowds, which can mean unwanted severe congestion on a specific operator network whilst the remaining available capacity from other access technologies is not being used. To address these issues, we propose a scalable network assisted distributed solution that is administered by centralized policies, and an embedded reputation system, by which initially selfish operators are encouraged to cooperate under the threat of churn. Our solution after detecting a congested technology, including within its wired backhaul, automatically offloads and balances the flows amongst the access resources from all the existing technologies, following some quality metrics. Our results show that the smart integration of access networks can yield an additional wireless quality for mobile flows up to thirty eight percent beyond that feasible from the best effort standalone operation of each wireless access technology. It is also evidenced that backhaul constraints are conveniently reflected on the way the flow access to wireless media is granted. Finally, we have analyzed the sensitivity of the handover decision algorithm running in each terminal agent to consecutive flash crowds, as well as its centralized feature that controls the connection quality offered by a heterogeneous access infrastructure owned by distinct operators."
"We propose Sketched Online Newton (SON), an online second order learning algorithm that enjoys substantially improved regret guarantees for ill-conditioned data. SON is an enhanced version of the Online Newton Step, which, via sketching techniques enjoys a running time linear in the dimension and sketch size. We further develop sparse forms of the sketching methods (such as Oja's rule), making the computation linear in the sparsity of features. Together, the algorithm eliminates all computational obstacles in previous second order online learning approaches."
"This paper devises a general modeling and analyzing framework for a heterogeneous wireless network (HetNet) in which several wireless subnetworks coexist and use multiple radio access technologies (multi-RATs). The coexisting coverage and network capacity in such a multi-RAT HetNet are hardly investigated in prior works. To characterize the coexisting interactions in a multi-RAT HetNet, in this paper we consider a HetNet consisting of K-tier APs and two different RATs, RAT-L and RAT-U, are adopted in the HetNet. RAT-L is adopted by the access points (APs) in the first K-1 tiers and APs in the Kth tier only use RAT-U. Both noncrossing-RAT and crossing-RAT user association scenarios are considered. In each scenario, the void probability and channel access probability of the APs in each tier are first found and then the tight lower bounds and their lowest limits on the proposed coexisting coverage and network capacity are derived. We show that multi-RAT networks in general can achieve higher link coverage and capacity by using opportunistic CSMA/CA that avoids/alleviates severe interfering between all coexisting APs. Also, crossing-RAT user association is shown to achieve much higher coexisting coverage and network capacity than noncrossing-RAT user association. Finally, numerical simulations for the LTE-U and WiFi networks coexisting in the HetNet validate our findings."
"In opinion formation games with directed graphs, a bounded price of anarchy is only known for weighted Eulerian graphs. Thus, we bound the price of anarchy for a more general class of directed graphs with conditions intuitively meaning that each node does not influence the others more than she is influenced, where the bounds depend on such difference (in a ratio). We also show that there exists an example just slightly violating the conditions with an unbounded price of anarchy."
"Sparse matrix multiplication is traditionally performed in memory and scales to large matrices using the distributed memory of multiple nodes. In contrast, we scale sparse matrix multiplication beyond memory capacity by implementing sparse matrix dense matrix multiplication (SpMM) in a semi-external memory (SEM) fashion; i.e., we keep the sparse matrix on commodity SSDs and dense matrices in memory. Our SEM-SpMM incorporates many in-memory optimizations for large power-law graphs. It outperforms the in-memory implementations of Trilinos and Intel MKL and scales to billion-node graphs, far beyond the limitations of memory. Furthermore, on a single large parallel machine, our SEM-SpMM operates as fast as the distributed implementations of Trilinos using five times as much processing power. We also run our implementation in memory (IM-SpMM) to quantify the overhead of keeping data on SSDs. SEM-SpMM achieves almost 100% performance of IM-SpMM on graphs when the dense matrix has more than four columns; it achieves at least 65% performance of IM-SpMM on all inputs. We apply our SpMM to three important data analysis tasks--PageRank, eigensolving, and non-negative matrix factorization--and show that our SEM implementations significantly advance the state of the art."
"Joint segmentation and classification of fine-grained actions is important for applications of human-robot interaction, video surveillance, and human skill evaluation. However, despite substantial recent progress in large-scale action classification, the performance of state-of-the-art fine-grained action recognition approaches remains low. We propose a model for action segmentation which combines low-level spatiotemporal features with a high-level segmental classifier. Our spatiotemporal CNN is comprised of a spatial component that uses convolutional filters to capture information about objects and their relationships, and a temporal component that uses large 1D convolutional filters to capture information about how object relationships change across time. These features are used in tandem with a semi-Markov model that models transitions from one action to another. We introduce an efficient constrained segmental inference algorithm for this model that is orders of magnitude faster than the current approach. We highlight the effectiveness of our Segmental Spatiotemporal CNN on cooking and surgical action datasets for which we observe substantially improved performance relative to recent baseline methods."
"We study the infinite horizon optimal control problem for N-network queueing systems, which consist of two customer classes and two server pools, under average (ergodic) criteria in the Halfin-Whitt regime. We consider three control objectives: 1) minimizing the queueing (and idleness) cost, 2) minimizing the queueing cost while imposing a constraint on idleness at each server pool, and 3) minimizing the queueing cost while requiring fairness on idleness. The running costs can be any nonnegative convex functions having at most polynomial growth.   For all three problems we establish asymptotic optimality, namely, the convergence of the value functions of the diffusion-scaled state process to the corresponding values of the controlled diffusion limit. We also present a simple state-dependent priority scheduling policy under which the diffusion-scaled state process is geometrically ergodic in the Halfin-Whitt regime, and some results on convergence of mean empirical measures which facilitate the proofs."
"The auxiliary function of a classical channel appears in two fundamental quantities that upper and lower bound the error probability, respectively. A crucial property of the auxiliary function is its concavity, which leads to several important results in finite block length analysis. In this paper, we prove that the auxiliary function of a classical-quantum channel also enjoys the same concave property, extending an earlier partial result to its full generality. The key component in our proof is a beautiful result of geometric means of operators."
"Crowdsourcing systems are popular for solving large-scale labelling tasks with low-paid (or even non-paid) workers. We study the problem of recovering the true labels from the possibly erroneous crowdsourced labels under the popular Dawid-Skene model. To address this inference problem, several algorithms have recently been proposed, but the best known guarantee is still significantly larger than the fundamental limit. In our previous work, we closed this gap under a canonical assumption where each worker is assigned only two tasks, i.e., $r=2$, and each task is assigned to sufficiently but constantly many workers, $\ell \ge C_r$. In this work, we further remove the condition on r and show that for all $r \ge 1$, Belief Propagation exactly matches a lower bound on the fundamental limit if $\ell \ge C_r$. The guaranteed optimality of BP is the strongest in the sense that it is information-theoretically impossible for any other algorithm to correctly label a larger fraction of the tasks. In the general setting, regardless of the number of workers assigned to a task, we establish the dominance result on BP that it outperforms all existing algorithms with provable guarantees. Experimental results suggest that BP is close to optimal for all regimes considered, while all other algorithms show suboptimal performances in certain regimes."
"Motivated by applications in clustering and information retrieval, we extend the classical Hotelling setting to deal with multi-facility location duels. In the classical Hotelling setting, customers' locations are taken from the uniform distribution on the $[0,1]$ segment, and there are two facility owners, each needs to decide on the location of her (single) facility, aiming to maximize the proportion of customers closer to it. We extend the Hotelling setting to multi-unit facility location games, where there are $n$ players, where player $i$ may control ${\bf several}$ facilities. We first analyze competition among the owner of $k$ facilities to the owner of $l$ facilities, for arbitrary $(l,k)$, where $l\leq k$. Our message for this extended Hotelling duel is quite striking: in no equilibrium of any such $(l,k)$ facility location duel a facility will materialize in a location which is not part of the social welfare maximizing locations of the player who has $k$ facilities, if she were to locate her facilities under no competition. This is obtained despite the lack of pure strategy equilibrium in any $(l,k)$ duel whenever $l \neq k$. Moreover, for the $n$-player setting, we provide sufficient and necessary conditions for a pure strategy profile to be an equilibrium in such game. We then show that a pure-strategy equilibrium exist if and only if there is no dominant player who controls more than half of the facilities; in the latter case, under some conditions, a mixed strategy equilibrium of the form obtained in the $(l,k)$ duel does exist."
"A recent study of bugs in real-world concurrent and distributed systems found that, while implementations of individual protocols tend to be robust, the composition of multiple protocols and its interplay with internal computation is the culprit for most errors. Multiparty Session Types and Choreographic Programming are methodologies for developing correct-by-construction concurrent and distributed software, based on global descriptions of communication flows. However, protocol composition is either limited or left unchecked. Inspired by these two methodologies, in this work we present a new language model for the safe composition of protocols, called Procedural Choreographies (PC). Protocols in PC are procedures, parameterised on the processes that enact them. Procedures define communications declaratively using global descriptions, and programs are written by invoking and composing these procedures. An implementation in terms of a process model is then mechanically synthesised, guaranteeing correctness and deadlock-freedom. We study PC in the settings of synchronous and asynchronous communications, and illustrate its expressivity with some representative examples."
"The distribution of machine learning tasks on the user's devices offers several advantages for application purposes: scalability, reduction of deployment costs and privacy. We propose a basic brick, Distributed Median Elimination, which can be used to distribute the best arm identification task in various schemes. In comparison to Median Elimination run on a single player, we showed a near optimal speed-up factor. This speed-up factor is reached with a near optimal communication cost. Experiments illustrate and complete the analysis. In comparison to {\sc Median Elimination} performed on each player, according to the analysis Distributed Median Elimination shows practical improvements."
"In this paper, We study network centrality based on dynamic influence propagation models in social networks. We consider two basic centrality measures: (a) single node influence (SNI) centrality, which uses each node's influence spread as its own centrality measure; (b) Shapley centrality, which uses the Shapley value of the influence spread function --- formulated based on a fundamental corporative-game-theoretical concept --- to measure the significance of nodes. We present a comprehensive comparative study of these two centrality measures. Algorithmically, we provide scalable algorithms for approximating them for a large family of social-influence instances. Mathematically, we present axiomatic characterizations, which precisely capture the essence of these two centrality measures and their fundamental differences. Empirically, we demonstrate their similarity and differences in a number of real-world social networks, as well as the efficiency of our scalable algorithms. Our results shed light on their applicability: SNI centrality is suitable for assessing individual influence in isolation while Shapley centrality assesses individual's performance in group influence settings."
"First-order stochastic methods are the state-of-the-art in large-scale machine learning optimization owing to efficient per-iteration complexity. Second-order methods, while able to provide faster convergence, have been much less explored due to the high cost of computing the second-order information. In this paper we develop second-order stochastic methods for optimization problems in machine learning that match the per-iteration cost of gradient based methods, and in certain settings improves upon the overall running time upon the state-of-the-art. Furthermore, our algorithm has the desirable property of being implementable in time linear in the sparsity of the input data."
"Cliques are frequently used to model communities: a community is a set of nodes where each pair is equally likely to be connected. But studying real-world communities reveals that they have more structure than that. In particular, the nodes can be ordered in such a way that (almost) all edges in the community lie below a hyperbola. In this paper we present three new models for communities that capture this phenomenon. Our models explain the structure of the communities differently, but we also prove that they are identical in their expressive power. Our models fit to real-world data much better than traditional block models or previously-proposed hyperbolic models, both of which are a special case of our model. Our models also allow for intuitive interpretation of the parameters, enabling us to summarize the shapes of the communities in graphs effectively."
"In this review we argue for the creation of a physics of moving systems -- a locomotion ""robophysics"" -- which we define as the pursuit of the discovery of principles of self generated motion. Robophysics can provide an important intellectual complement to the discipline of robotics, largely the domain of researchers from engineering and computer science. The essential idea is that we must complement study of complex robots in complex situations with systematic study of simplified robophysical devices in controlled laboratory settings and simplified theoretical models. We must thus use the methods of physics to examine successful and failed locomotion in simplified (abstracted) devices using parameter space exploration, systematic control, and techniques from dynamical systems. Using examples from our and other's research, we will discuss how such robophysical studies have begun to aid engineers in the creation of devices that begin to achieve life-like locomotor abilities on and within complex environments, have inspired interesting physics questions in low dimensional dynamical systems, geometric mechanics and soft matter physics, and have been useful to develop models for biological locomotion in complex terrain. The rapidly decreasing cost of constructing sophisticated robot models with easy access to significant computational power bodes well for scientists and engineers to engage in a discipline which can readily integrate experiment, theory and computation."
"Delay games are two-player games of infinite duration in which one player may delay her moves to obtain a lookahead on her opponent's moves. Recently, such games with quantitative winning conditions in weak MSO with the unbounding quantifier were studied, but their properties turned out to be unsatisfactory. In particular, unbounded lookahead is in general necessary. Here, we study delay games with winning conditions given by Prompt-LTL, Linear Temporal Logic equipped with a parameterized eventually operator whose scope is bounded. Our main result shows that solving Prompt-LTL delay games is complete for triply-exponential time. Furthermore, we give tight triply-exponential bounds on the necessary lookahead and on the scope of the parameterized eventually operator. Thus, we identify Prompt-LTL as the first known class of well-behaved quantitative winning conditions for delay games. Finally, we show that applying our techniques to delay games with \omega-regular winning conditions answers open questions in the cases where the winning conditions are given by non-deterministic, universal, or alternating automata."
"In this paper, we consider a recent cellular network connection paradigm, known as user-provided network (UPN), where users share their connectivity and act as an access point for other users. To incentivize user participation in this network, we allow the users to trade their data plan and obtain some profits by selling and buying leftover data capacities (caps) from each other. We formulate the buyers and sellers association for data trading as a matching game. In this game, buyers and sellers rank one another based on preference functions that capture buyers' data demand and QoS requirements, sellers' available data and energy resources. We show that these preferences are interdependent and influenced by the existing network-wide matching. For this reason, the game can be classified as a one-to-many matching game with externalities. To solve this game, a distributed algorithm that combines notions from matching theory and market equilibrium is proposed. The algorithm enables the players to self-organize into a stable matching and dynamic adaptation of price to data demand and supply. The properties of the resulting matching are discussed. Moreover, the price benchmark for the users to join the UPN and the operator gain are also determined. Simulation results show that the proposed algorithm yields an improvement of the average utility per user up to 25% and 50% relative to random matching and worst case utility, respectively."
"We consider the problem of exploration of networks, some of whose edges are faulty. A mobile agent, situated at a starting node and unaware of which edges are faulty, has to explore the connected fault-free component of this node by visiting all of its nodes. The cost of the exploration is the number of edge traversals. For a given network and given starting node, the overhead of an exploration algorithm is the worst-case ratio (taken over all fault configurations) of its cost to the cost of an optimal algorithm which knows where faults are situated. An exploration algorithm, for a given network and given starting node, is called perfectly competitive if its overhead is the smallest among all exploration algorithms not knowing the location of faults. We design a perfectly competitive exploration algorithm for any ring, and show that, for networks modeled by hamiltonian graphs, the overhead of any DFS exploration is at most 10/9 times larger than that of a perfectly competitive algorithm. Moreover, for hamiltonian graphs of size at least 24, this overhead is less than 6% larger than that of a perfectly competitive algorithm."
"Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.   We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent."
"The halting probability of a Turing machine is the probability that the machine will halt if it starts with a random stream written on its one-way input tape. When the machine is universal, this probability is referred to as Chaitin's omega number, and is the most well known example of a real which is random in the sense of Martin-L\""{o}f. Although omega numbers depend on the underlying universal Turing machine, they are robust in the sense that they all have the same Turing degree, namely the degree of the halting problem. In this paper we give precise bounds on the redundancy growth rate that is generally required for the computation of an omega number from another omega number. We show that for each $\epsilon >1$, any pair of omega numbers compute each other with redundancy $\epsilon \log n$. On the other hand, this is not true for $\epsilon=1$. In fact, we show that for each omega number there exists another omega number which is not computable from the first one with redundancy $\log n$. This latter result improves an older result of Frank Stephan."
"There is a graph reduction system so that every optimal 1-planar graph can be reduced to an irreducible extended wheel graph, provided the reductions are applied such that the given graph class is preserved. A graph is optimal 1-planar if it can be drawn in the plane with at most one crossing per edge and is optimal if it has the maximum of 4n-8 edges.   We show that the reduction system is context-sensitive so that the preservation of the graph class can be granted by local conditions which can be tested in constant time. Every optimal 1-planar graph G can be reduced to every extended wheel graph whose size is in a range from the (second) smallest one to some upper bound that depends on G. There is a reduction to the smallest extended wheel graph if G is not 5-connected, but not conversely. The reduction system has side effects and is non-deterministic and non-confluent. Nevertheless, reductions can be computed in linear time."
"Here, we study the flow of energy between coupled simulators in a co-simulation environment using the concept of power bonds. We introduce energy residuals which are a direct expression of the coupling errors and hence the accuracy of co-simulation results. We propose a novel Energy-Conservation-based Co-Simulation method (ECCO) for adaptive macro step size control to improve accuracy and efficiency. In contrast to most other co-simulation algorithms, this method is non-iterative and only requires knowledge of the current coupling data. Consequently, it allows for significant speed ups and the protection of sensitive information contained within simulator models. A quarter car model with linear and nonlinear damping serves as a co-simulation benchmark and verifies the capabilities of the energy residual concept: Reductions in the errors of up to 93% are achieved at no additional computational cost."
"We study a scheduling problem arising in demand response management in smart grid. Consumers send in power requests with a flexible feasible time interval during which their requests can be served. The grid controller, upon receiving power requests, schedules each request within the specified interval. The electricity cost is measured by a convex function of the load in each timeslot. The objective is to schedule all requests with the minimum total electricity cost. Previous work has studied cases where jobs have unit power requirement and unit duration. We extend the study to arbitrary power requirement and duration, which has been shown to be NP-hard. We give the first online algorithm for the general problem, and prove that the competitive ratio is asymptotically optimal. We also prove that the problem is fixed parameter tractable."
"The performance of long-haul high speed coherent optical fiber communication systems is significantly degraded by the laser phase noise and the equalization enhanced phase noise (EEPN). In this paper, the analysis of the one-tap normalized least-mean-square (LMS) carrier phase recovery (CPR) is carried out and the close-form expression is investigated for quadrature phase shift keying (QPSK) coherent optical fiber communication systems, in compensating both laser phase noise and equalization enhanced phase noise. Numerical simulations have also been implemented to verify the theoretical analysis. It is found that the one-tap normalized least-mean-square algorithm gives the same analytical expression for predicting CPR bit-error-rate (BER) floors as the traditional differential carrier phase recovery, when both the laser phase noise and the equalization enhanced phase noise are taken into account."
"The increasingly dense deployments of wireless CSMA networks arising from applications of Internet-of-things call for an improvement to mitigate the interference among simultaneous transmitting wireless devices. For cost efficiency and backward compatibility with legacy transceiver hardware, a simple approach to address interference is by appropriately configuring the carrier sensing thresholds in wireless CSMA protocols, particularly in dense wireless networks. Most prior studies of the configuration of carrier sensing thresholds are based on a simplified conflict graph model, whereas this paper considers a realistic signal-to-interference-and-noise ratio model. We provide a comprehensive study for two effective wireless CSMA protocols: Cumulative-interference-Power Carrier Sensing and Incremental-interference-Power Carrier Sensing, in two aspects: (1) static approach that sets a universal carrier sensing threshold to ensure interference-safe transmissions regardless of network topology, and (2) adaptive approach that adjusts the carrier sensing thresholds dynamically based on the feedback of nearby transmissions. We also provide simulation studies to evaluate the starvation ratio, fairness, and goodput of our approaches."
We investigate active learning with access to two distinct oracles: Label (which is standard) and Search (which is not). The Search oracle models the situation where a human searches a database to seed or counterexample an existing solution. Search is stronger than Label while being natural to implement in many situations. We show that an algorithm using both oracles can provide exponentially large problem-dependent improvements over Label alone.
"Analog-to-digital converters (ADCs) stand for a significant part of the total power consumption in a massive MIMO base station. One-bit ADCs are one way to reduce power consumption. This paper presents an analysis of the spectral efficiency of single-carrier and OFDM transmission in massive MIMO systems that use one-bit ADCs. A closed-form achievable rate, i.e., a lower bound on capacity, is derived for a wideband system with a large number of channel taps that employs low-complexity linear channel estimation and symbol detection. Quantization results in two types of error in the symbol detection. The circularly symmetric error becomes Gaussian in massive MIMO and vanishes as the number of antennas grows. The amplitude distortion, which severely degrades the performance of OFDM, is caused by variations between symbol durations in received interference energy. As the number of channel taps grows, the amplitude distortion vanishes and OFDM has the same performance as single-carrier transmission. A main conclusion of this paper is that wideband massive MIMO systems work well with one-bit ADCs."
"Electrical power systems are one of the most important infrastructures that support our society. However, their vulnerabilities have raised great concern recently due to several large-scale blackouts around the world. In this paper, we investigate the robustness of power systems against cascading failures initiated by a random attack. This is done under a simple yet useful model based on global and equal redistribution of load upon failures. We provide a complete understanding of system robustness by i) deriving an expression for the final system size as a function of the size of initial attacks; ii) deriving the critical attack size after which system breaks down completely; iii) showing that complete system breakdown takes place through a first-order (i.e., discontinuous) transition in terms of the attack size; and iv) establishing the optimal load-capacity distribution that maximizes robustness. In particular, we show that robustness is maximized when the difference between the capacity and initial load is the same for all lines; i.e., when all lines have the same redundant space regardless of their initial load. This is in contrast with the intuitive and commonly used setting where capacity of a line is a fixed factor of its initial load."
"We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese."
"In regularized risk minimization, the associated optimization problem becomes particularly difficult when both the loss and regularizer are nonsmooth. Existing approaches either have slow or unclear convergence properties, are restricted to limited problem subclasses, or require careful setting of a smoothing parameter. In this paper, we propose a continuation algorithm that is applicable to a large class of nonsmooth regularized risk minimization problems, can be flexibly used with a number of existing solvers for the underlying smoothed subproblem, and with convergence results on the whole algorithm rather than just one of its subproblems. In particular, when accelerated solvers are used, the proposed algorithm achieves the fastest known rates of $O(1/T^2)$ on strongly convex problems, and $O(1/T)$ on general convex problems. Experiments on nonsmooth classification and regression tasks demonstrate that the proposed algorithm outperforms the state-of-the-art."
"We study the truthful facility assignment problem, where a set of agents with private most-preferred points on a metric space are assigned to facilities that lie on the metric space, under capacity constraints on the facilities. The goal is to produce such an assignment that minimizes the social cost, i.e., the total distance between the most-preferred points of the agents and their corresponding facilities in the assignment, under the constraint of truthfulness, which ensures that agents do not misreport their most-preferred points.   We propose a resource augmentation framework, where a truthful mechanism is evaluated by its worst-case performance on an instance with enhanced facility capacities against the optimal mechanism on the same instance with the original capacities. We study a very well-known mechanism, Serial Dictatorship, and provide an exact analysis of its performance. Although Serial Dictatorship is a purely combinatorial mechanism, our analysis uses linear programming; a linear program expresses its greedy nature as well as the structure of the input, and finds the input instance that enforces the mechanism have its worst-case performance. Bounding the objective of the linear program using duality arguments allows us to compute tight bounds on the approximation ratio. Among other results, we prove that Serial Dictatorship has approximation ratio $g/(g-2)$ when the capacities are multiplied by any integer $g \geq 3$. Our results suggest that even a limited augmentation of the resources can have wondrous effects on the performance of the mechanism and in particular, the approximation ratio goes to 1 as the augmentation factor becomes large. We complement our results with bounds on the approximation ratio of Random Serial Dictatorship, the randomized version of Serial Dictatorship, when there is no resource augmentation."
"Various algorithms for text-independent speaker recognition have been developed through the decades, aiming to improve both accuracy and efficiency. This paper presents a novel PCA/LDA-based approach that is faster than traditional statistical model-based methods and achieves competitive results. First, the performance based on only PCA and only LDA is measured; then a mixed model, taking advantages of both methods, is introduced. A subset of the TIMIT corpus composed of 200 male speakers, is used for enrollment, validation and testing. The best results achieve 100%; 96% and 95% classification rate at population level 50; 100 and 200, using 39-dimensional MFCC features with delta and double delta. These results are based on 12-second text-independent speech for training and 4-second data for test. These are comparable to the conventional MFCC-GMM methods, but require significantly less time to train and operate."
"For odd n, the alternating group on n elements is generated by the permutations that jump an element from any odd position to position 1. We prove Hamiltonicity of the associated directed Cayley graph for all odd n not equal to 5. (A result of Rankin implies that the graph is not Hamiltonian for n=5.) This solves a problem arising in rank modulation schemes for flash memory. Our result disproves a conjecture of Horovitz and Etzion, and proves another conjecture of Yehezkeally and Schwartz."
"Recently, there has been immense interest in using unmanned aerial vehicles (UAVs) for civilian operations. As a result, unmanned aerial systems traffic management is needed to ensure the safety and liveness of potentially thousands of UAVs flying simultaneously. Currently, the analysis of large multi-agent systems cannot tractably provide safety and liveness guarantees if the agents' set of maneuvers is unrestricted. In this paper, platoons of UAVs flying on air highways is proposed to impose an airspace structure that allows for tractable analysis. For the air highway placement problem, the fast marching method is used to produce a sequence of air highways that minimizes the cost of flying from an origin to any destination. The placement of air highways can be updated in real-time to accommodate sudden airspace changes. Within platoons traveling on air highways, each vehicle is modeled as a hybrid system. Using Hamilton-Jacobi reachability, safety and liveness are guaranteed for all mode transitions. For a single altitude range, the proposed approach guarantees safety for one safety breach per vehicle; in the unlikely event of multiple safety breaches, safety can be guaranteed over multiple altitude ranges. The satisfaction of safety and liveness requirements is demonstrated through simulations of three scenarios."
"Streaming interactive proofs (SIPs) are a framework for outsourced computation. A computationally limited streaming client (the verifier) hands over a large data set to an untrusted server (the prover) in the cloud and the two parties run a protocol to confirm the correctness of result with high probability. SIPs are particularly interesting for problems that are hard to solve (or even approximate) well in a streaming setting. The most notable of these problems is finding maximum matchings, which has received intense interest in recent years but has strong lower bounds even for constant factor approximations.   In this paper, we present efficient streaming interactive proofs that can verify maximum matchings exactly. Our results cover all flavors of matchings (bipartite/non-bipartite and weighted). In addition, we also present streaming verifiers for approximate metric TSP. In particular, these are the first efficient results for weighted matchings and for metric TSP in any streaming verification model."
"The increasing complexity of deep neural networks (DNNs) has made it challenging to exploit existing large-scale data processing pipelines for handling massive data and parameters involved in DNN training. Distributed computing platforms and GPGPU-based acceleration provide a mainstream solution to this computational challenge. In this paper, we propose DeepSpark, a distributed and parallel deep learning framework that exploits Apache Spark on commodity clusters. To support parallel operations, DeepSpark automatically distributes workloads and parameters to Caffe/Tensorflow-running nodes using Spark, and iteratively aggregates training results by a novel lock-free asynchronous variant of the popular elastic averaging stochastic gradient descent based update scheme, effectively complementing the synchronized processing capabilities of Spark. DeepSpark is an on-going project, and the current release is available at http://deepspark.snu.ac.kr."
"Vector approximate message passing (VAMP) is a computationally simple approach to the recovery of a signal $\mathbf{x}$ from noisy linear measurements $\mathbf{y}=\mathbf{Ax}+\mathbf{w}$. Like the AMP proposed by Donoho, Maleki, and Montanari in 2009, VAMP is characterized by a rigorous state evolution (SE) that holds under certain large random matrices and that matches the replica prediction of optimality. But while AMP's SE holds only for large i.i.d. sub-Gaussian $\mathbf{A}$, VAMP's SE holds under the much larger class: right-rotationally invariant $\mathbf{A}$. To run VAMP, however, one must specify the statistical parameters of the signal and noise. This work combines VAMP with Expectation-Maximization to yield an algorithm, EM-VAMP, that can jointly recover $\mathbf{x}$ while learning those statistical parameters. The fixed points of the proposed EM-VAMP algorithm are shown to be stationary points of a certain constrained free-energy, providing a variational interpretation of the algorithm. Numerical simulations show that EM-VAMP is robust to highly ill-conditioned $\mathbf{A}$ with performance nearly matching oracle-parameter VAMP."
"We study the envy-free pricing problem in multi-unit markets with budgets, where there is a seller who brings multiple units of a good, while several buyers bring monetary endowments. Our goal is to compute an envy-free (item) price and allocation, i.e. an outcome where all the demands of the buyers are met given their budget constraints, which additionally achieves a desirable objective. We analyze markets with linear valuations, where the buyers are price takers and price makers, respectively.   In the price taking scenario, for the problem of computing a welfare maximizing envy-free pricing we provide a polynomial time algorithm, while for the problem of computing a revenue optimal envy-free pricing we provide an FPTAS and exact algorithm (which is polynomial for a constant number of types of buyers).   In the price making scenario, where the buyers can strategize, we show a general impossibility of designing strategyproof and efficient mechanisms even with public budgets. On the positive side, we provide an optimal strategyproof mechanism whose approximation ratio is a function of the market share, $s^*$, which can roughly be understood as the maximum purchasing power of any individual buyer in the market. When the market is even mildly competitive$-$i.e. with no buyer having a market share higher than 50%$-$the approximation ratio of our mechanism is at most $2$ for revenue and at most $1/(1-s^*)$ for welfare. Moreover, this mechanism is optimal among all the strategyproof mechanisms for both objectives on competitive markets.   Finally, for multi-unit markets with general valuations in the price taking model, we provide fully polynomial time approximation schemes as well as hardness results for computing envy-free pricings that maximize revenue and welfare, respectively."
"More than a dozen wireless technologies are used by air traffic communication systems during different flight phases. From a conceptual perspective, all of them are insecure as security was never part of their design. Recent contributions from academic and hacking communities have exploited this inherent vulnerability to demonstrate attacks on some of these technologies. However, not all of these contributions have resonated widely within aviation circles. At the same time, the security community lacks certain aviation domain knowledge, preventing aviation authorities from giving credence to their findings.   In this paper, we aim to reconcile the view of the security community and the perspective of aviation professionals concerning the safety of air traffic communication technologies. To achieve this, we first provide a systematization of the applications of wireless technologies upon which civil aviation relies. Based on these applications, we comprehensively analyze vulnerabilities, attacks, and countermeasures. We categorize the existing research on countermeasures into approaches that are applicable in the short term and research of secure new technologies deployable in the long term.   Since not all of the required aviation knowledge is codified in academic publications, we additionally examine existing aviation standards and survey 242 international aviation experts. Besides their domain knowledge, we also analyze the awareness of members of the aviation community concerning the security of wireless systems and collect their expert opinions on the potential impact of concrete attack scenarios using these technologies."
"PAR-constrained sequences are widely used in communication systems and radars due to various practical needs; specifically, sequences are required to be unimodular or of low peak-to-average power ratio (PAR). For unimodular sequence design, plenty of efforts have been devoted to obtaining good correlation properties. Regarding channel estimation, however, sequences of such properties do not necessarily help produce optimal estimates. Tailored unimodular sequences for the specific criterion concerned are desirable especially when the prior knowledge of the channel is taken into account as well. In this paper, we formulate the problem of optimal unimodular sequence design for minimum mean square error estimation of the channel impulse response and conditional mutual information maximization, respectively. Efficient algorithms based on the majorization-minimization framework are proposed for both problems with guaranteed convergence. As the unimodular constraint is a special case of the low PAR constraint, optimal sequences of low PAR are also considered. Numerical examples are provided to show the performance of the proposed training sequences, with the efficiency of the derived algorithms demonstrated."
"Convolutional rectifier networks, i.e. convolutional neural networks with rectified linear activation and max or average pooling, are the cornerstone of modern deep learning. However, despite their wide use and success, our theoretical understanding of the expressive properties that drive these networks is partial at best. On the other hand, we have a much firmer grasp of these issues in the world of arithmetic circuits. Specifically, it is known that convolutional arithmetic circuits possess the property of ""complete depth efficiency"", meaning that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require exponential size in order to be implemented (or even approximated) by a shallow network. In this paper we describe a construction based on generalized tensor decompositions, that transforms convolutional arithmetic circuits into convolutional rectifier networks. We then use mathematical tools available from the world of arithmetic circuits to prove new results. First, we show that convolutional rectifier networks are universal with max pooling but not with average pooling. Second, and more importantly, we show that depth efficiency is weaker with convolutional rectifier networks than it is with convolutional arithmetic circuits. This leads us to believe that developing effective methods for training convolutional arithmetic circuits, thereby fulfilling their expressive potential, may give rise to a deep learning architecture that is provably superior to convolutional rectifier networks but has so far been overlooked by practitioners."
"We introduce a framework suitable for describing pattern recognition task using the mathematical language of density matrices. In particular, we provide a one-to-one correspondence between patterns and pure density operators. This correspondence enables us to: i) represent the Nearest Mean Classifier (NMC) in terms of quantum objects, ii) introduce a Quantum Classifier (QC). By comparing the QC with the NMC on different 2D datasets, we show the first classifier can provide additional information that are particularly beneficial on a classical computer with respect to the second classifier."
"As an emerging research direction, online streaming feature selection deals with sequentially added dimensions in a feature space while the number of data instances is fixed. Online streaming feature selection provides a new, complementary algorithmic methodology to enrich online feature selection, especially targets to high dimensionality in big data analytics. This paper introduces the first comprehensive open-source library for use in MATLAB that implements the state-of-the-art algorithms of online streaming feature selection. The library is designed to facilitate the development of new algorithms in this exciting research direction and make comparisons between the new methods and existing ones available."
"Stochastic gradient methods for machine learning and optimization problems are usually analyzed assuming data points are sampled \emph{with} replacement. In practice, however, sampling \emph{without} replacement is very common, easier to implement in many cases, and often performs better. In this paper, we provide competitive convergence guarantees for without-replacement sampling, under various scenarios, for three types of algorithms: Any algorithm with online regret guarantees, stochastic gradient descent, and SVRG. A useful application of our SVRG analysis is a nearly-optimal algorithm for regularized least squares in a distributed setting, in terms of both communication complexity and runtime complexity, when the data is randomly partitioned and the condition number can be as large as the data size per machine (up to logarithmic factors). Our proof techniques combine ideas from stochastic optimization, adversarial online learning, and transductive learning theory, and can potentially be applied to other stochastic optimization and learning problems."
"Hierarchical Classification (HC) is a supervised learning problem where unlabeled instances are classified into a taxonomy of classes. Several methods that utilize the hierarchical structure have been developed to improve the HC performance. However, in most cases apriori defined hierarchical structure by domain experts is inconsistent; as a consequence performance improvement is not noticeable in comparison to flat classification methods. We propose a scalable data-driven filter based rewiring approach to modify an expert-defined hierarchy. Experimental comparisons of top-down HC with our modified hierarchy, on a wide range of datasets shows classification performance improvement over the baseline hierarchy (i:e:, defined by expert), clustered hierarchy and flattening based hierarchy modification approaches. In comparison to existing rewiring approaches, our developed method (rewHier) is computationally efficient, enabling it to scale to datasets with large numbers of classes, instances and features. We also show that our modified hierarchy leads to improved classification performance for classes with few training samples in comparison to flat and state-of-the-art HC approaches."
"This paper assesses the performance of mobile messaging and VoIP connections. We investigate the CPU usage of WhatsApp and IMO under different scenarios. This analysis also enabled a comparison of the performance of these applications on two Android operating system (OS) versions: KitKat or Lollipop. Two models of smartphones were considered, viz. Galaxy Note 4 and Galaxy S4. The applications behavior was statistically investigated for both sending and receiving VoIP calls. Connections have been examined over 3G and WiFi. The handset model plays a decisive role in CPU usage of the application. t-tests showed that IMO has a better performance that WhatsApp whatever be the Android at a significance level 1%, on Galaxy Note 4. In contrast, WhatsApp requires less CPU than IMO on Galaxy S4 whatever be the OS and access (3G/WiFi). Galaxy Note 4 using WiFi always outperformed S4 in terms of processing efficiency."
"The approximation of a high-dimensional vector by a small combination of column vectors selected from a fixed matrix has been actively debated in several different disciplines. In this paper, a sampling approach based on the Monte Carlo method is presented as an efficient solver for such problems. Especially, the use of simulated annealing (SA), a metaheuristic optimization algorithm, for determining degrees of freedom (the number of used columns) by cross validation is focused on and tested. Test on a synthetic model indicates that our SA-based approach can find a nearly optimal solution for the approximation problem and, when combined with the CV framework, it can optimize the generalization ability. Its utility is also confirmed by application to a real-world supernova data set."
"Dictionaries are collections of vectors used for representations of random vectors in Euclidean spaces. Recent research on optimal dictionaries is focused on constructing dictionaries that offer sparse representations, i.e., $\ell_0$-optimal representations. Here we consider the problem of finding optimal dictionaries with which representations of samples of a random vector are optimal in an $\ell_2$-sense: optimality of representation is defined as attaining the minimal average $\ell_2$-norm of the coefficients used to represent the random vector. With the help of recent results on rank-$1$ decompositions of symmetric positive semidefinite matrices, we provide an explicit description of $\ell_2$-optimal dictionaries as well as their algorithmic constructions in polynomial time."
"The comparative study on three carrier phase estimation (CPE) approaches, involving a one-tap normalized least-mean-square (NLMS) algorithm, a block-wise average algorithm, and a Viterbi-Viterbi algorithm has been carried out in the long-haul high-capacity dispersion-unmanaged coherent optical communication systems. The close-form expressions and analytical predictions for bit-error-rate behaviors in these CPE methods have been analyzed by considering both the laser phase noise and the equalization enhanced phase noise. It is found that the Viterbi-Viterbi algorithm outperforms the one-tap NLMS and the block-wise average algorithms for a small phase noise variance (or effective phase noise variance), while the three CPE methods converge to a similar performance for a large phase noise variance (or effective phase noise variance). In addition, the differences between the three CPE approaches become smaller for higher-level modulation formats."
"CPU caches introduce variations into the execution time of programs that can be exploited by adversaries to recover private information about users or cryptographic keys. Establishing the security of countermeasures against this threat often requires intricate reasoning about the interactions of program code, memory layout, and hardware architecture and has so far only been done for restricted cases. In this paper we devise novel techniques that provide support for bit-level and arithmetic reasoning about pointers in the presence of dynamic memory allocation. These techniques enable us to perform the first rigorous analysis of widely deployed software countermeasures against cache attacks on modular exponentiation, based on executable code."
"Additive utility function models are widely used in multiple criteria decision analysis. In such models, a numerical value is associated to each alternative involved in the decision problem. It is computed by aggregating the scores of the alternative on the different criteria of the decision problem. The score of an alternative is determined by a marginal value function that evolves monotonically as a function of the performance of the alternative on this criterion. Determining the shape of the marginals is not easy for a decision maker. It is easier for him/her to make statements such as ""alternative $a$ is preferred to $b$"". In order to help the decision maker, UTA disaggregation procedures use linear programming to approximate the marginals by piecewise linear functions based only on such statements. In this paper, we propose to infer polynomials and splines instead of piecewise linear functions for the marginals. In this aim, we use semidefinite programming instead of linear programming. We illustrate this new elicitation method and present some experimental results."
"For many machine learning problems, data is abundant and it may be prohibitive to make multiple passes through the full training set. In this context, we investigate strategies for dynamically increasing the effective sample size, when using iterative methods such as stochastic gradient descent. Our interest is motivated by the rise of variance-reduced methods, which achieve linear convergence rates that scale favorably for smaller sample sizes. Exploiting this feature, we show -- theoretically and empirically -- how to obtain significant speed-ups with a novel algorithm that reaches statistical accuracy on an $n$-sample in $2n$, instead of $n \log n$ steps."
"Information-Centric Networking (ICN) is a new networking paradigm, which replaces the widely used host-centric networking paradigm in communication networks (e.g., Internet, mobile ad hoc networks) with an information-centric paradigm, which prioritizes the delivery of named content, oblivious of the contents origin. Content and client security are more intrinsic in the ICN paradigm versus the current host centric paradigm where they have been instrumented as an after thought. By design, the ICN paradigm inherently supports several security and privacy features, such as provenance and identity privacy, which are still not effectively available in the host-centric paradigm. However, given its nascency, the ICN paradigm has several open security and privacy concerns, some that existed in the old paradigm, and some new and unique. In this article, we survey the existing literature in security and privacy research sub-space in ICN. More specifically, we explore three broad areas: security threats, privacy risks, and access control enforcement mechanisms.   We present the underlying principle of the existing works, discuss the drawbacks of the proposed approaches, and explore potential future research directions. In the broad area of security, we review attack scenarios, such as denial of service, cache pollution, and content poisoning. In the broad area of privacy, we discuss user privacy and anonymity, name and signature privacy, and content privacy. ICN's feature of ubiquitous caching introduces a major challenge for access control enforcement that requires special attention. In this broad area, we review existing access control mechanisms including encryption-based, attribute-based, session-based, and proxy re-encryption-based access control schemes. We conclude the survey with lessons learned and scope for future work."
"We introduce a new weakly-convex penalty function for signals with a group behavior. The penalty promotes signals with a few number of active groups, where within each group, only a few high magnitude coefficients are active. We derive the threshold function associated with the proposed penalty and study its properties. We discuss how the proposed penalty/threshold function can be useful for signals with isolated non-zeros, such as audio with isolated harmonics along the frequency axis, or reflection functions in exploration seismology where the non-zeros occur on the boundaries of subsoil layers. We demonstrate the use of the proposed penalty/threshold functions in a convex denoising and a non-convex deconvolution formulation. We provide convergent algorithms for both formulations and compare the performance with state-of-the-art methods."
"The basic active disturbance rejection control (BADRC) algorithm with only one order higher extended state observer (ESO) proves to be robust to both internal and external disturbances. An advantage of BADRC is that in many applications it can achieve high disturbance attenuation level without requiring a detailed model of the plant or disturbance. However, this can be regarded as a disadvantage when the disturbance characteristic is known since the BADRC algorithm cannot exploit such information. This paper proposes an internal model based ADRC (IADRC) method, which can take advantage of knowing disturbance characteristic to achieve perfect estimation of the disturbance under some mild assumptions. The effectiveness of the proposed method is validated by comprehensive simulations and comparisons with the BADRC algorithm."
"Matrix factorization (MF) is employed by many popular algorithms, e.g., collaborative filtering. The emerging GPU technology, with massively multicore and high intra-chip memory bandwidth but limited memory capacity, presents an opportunity for accelerating MF much further when appropriately exploiting the GPU architectural characteristics.   This paper presents cuMF, a CUDA-based matrix factorization library that implements memory-optimized alternate least square (ALS) method to solve very large-scale MF. CuMF uses a variety set of techniques to maximize the performance on either single or multiple GPUs. These techniques include smart access of sparse data leveraging GPU memory hierarchy, using data parallelism in conjunction with model parallelism, minimizing the communication overhead between computing units, and utilizing a novel topology-aware parallel reduction scheme.   With only a single machine with four Nvidia GPU cards, cuMF can be 6-10 times as fast, and 33-100 times as cost-efficient, compared with the state-of-art distributed CPU solutions. Moreover, this cuMF can solve the largest matrix factorization problem ever reported yet in current literature, while maintaining impressively good performance."
"Robots assisting disabled or elderly people in activities of daily living must perform complex manipulation tasks. These tasks are dependent on the user's environment and preferences. Thus, learning from demonstration (LfD) is a promising choice that would allow the non-expert user to teach the robot different tasks. Unfortunately, learning general solutions from raw demonstrations requires a significant amount of data. Performing this number of physical demonstrations is unfeasible for a disabled user. In this paper we propose an approach where the user demonstrates the manipulation task in a virtual environment. The collected demonstrations are used to train an LSTM recurrent neural network that can act as the controller for the robot. We show that the controller learned from virtual demonstrations can be used to successfully perform the manipulation tasks on a physical robot."
"In the risk assessment of cascading outages, the rationality of simulation and efficiency of computation are both of great significance. To overcome the drawback of sampling-based methods that huge computation resources are required and the shortcoming of initial contingency selection practices that the dependencies in sequences of outages are omitted, this paper proposes a novel risk assessment approach by searching on Markovian Tree. The Markovian tree model is reformulated from the quasi-dynamic multi-timescale simulation model proposed recently to ensure reasonable modeling and simulation of cascading outages. Then a tree search scheme is established to avoid duplicated simulations on same cascade paths, significantly saving computation time. To accelerate the convergence of risk assessment, a risk estimation index is proposed to guide the search for states with major contributions to the risk, and the risk assessment is realized based on the risk estimation index with a forward tree search and backward update algorithm. The effectiveness of the proposed method is illustrated on a 4-node power system, and its convergence profile as well as efficiency is demonstrated on the RTS-96 test system."
"We consider the action of mapping class groups, by pre- and post-composition, on branched coverings, and encode them algebraically as ""mapping class bisets"". We show how the mapping class biset of maps preserving a multicurve decomposes into mapping class bisets of smaller complexity, called ""small mapping class bisets"".   We phrase the decision problem of ""Thurston equivalence"" between branched self-coverings of the sphere in terms of the conjugacy and centralizer problems in a mapping class biset, and use our decomposition results on mapping class bisets to reduce these decision problems to small mapping class bisets.   This is the main step in our proof of decidability of Thurston equivalence, since decidability of conjugacy and centralizer problems in the small mapping class bisets are well understood in terms of linear algebra, group theory and complex analysis.   Branched coverings themselves are also encoded into bisets, with actions of the fundamental groups. We characterize those bisets that arise from branched coverings between topological spheres, and extend this correspondence to maps between spheres with multicurves, whose algebraic counterparts are ""sphere trees of bisets"".   A concrete outcome of our investigations is the construction of a Thurston map with infinitely generated centralizer --- while centralizers of homeomorphisms are always finitely generated."
"The article examines in some detail the convergence rate and mean-square-error performance of momentum stochastic gradient methods in the constant step-size and slow adaptation regime. The results establish that momentum methods are equivalent to the standard stochastic gradient method with a re-scaled (larger) step-size value. The size of the re-scaling is determined by the value of the momentum parameter. The equivalence result is established for all time instants and not only in steady-state. The analysis is carried out for general strongly convex and smooth risk functions, and is not limited to quadratic risks. One notable conclusion is that the well-known bene ts of momentum constructions for deterministic optimization problems do not necessarily carry over to the adaptive online setting when small constant step-sizes are used to enable continuous adaptation and learn- ing in the presence of persistent gradient noise. From simulations, the equivalence between momentum and standard stochastic gradient methods is also observed for non-differentiable and non-convex problems."
"Object proposals greatly benefit object detection task in recent state-of-the-art works. However, the existing object proposals usually have low localization accuracy at high intersection over union threshold. To address it, we apply saliency detection to each bounding box to improve their quality in this paper. We first present a geodesic saliency detection method in contour, which is designed to find closed contours. Then, we apply it to each candidate box with multi-sizes, and refined boxes can be easily produced in the obtained saliency maps which are further used to calculate saliency scores for proposal ranking. Experiments on PASCAL VOC 2007 test dataset demonstrate the proposed refinement approach can greatly improve existing models."
"We consider the online version of the isotonic regression problem. Given a set of linearly ordered points (e.g., on the real line), the learner must predict labels sequentially at adversarially chosen positions and is evaluated by her total squared loss compared against the best isotonic (non-decreasing) function in hindsight. We survey several standard online learning algorithms and show that none of them achieve the optimal regret exponent; in fact, most of them (including Online Gradient Descent, Follow the Leader and Exponential Weights) incur linear regret. We then prove that the Exponential Weights algorithm played over a covering net of isotonic functions has a regret bounded by $O\big(T^{1/3} \log^{2/3}(T)\big)$ and present a matching $\Omega(T^{1/3})$ lower bound on regret. We provide a computationally efficient version of this algorithm. We also analyze the noise-free case, in which the revealed labels are isotonic, and show that the bound can be improved to $O(\log T)$ or even to $O(1)$ (when the labels are revealed in isotonic order). Finally, we extend the analysis beyond squared loss and give bounds for entropic loss and absolute loss."
"Exchangeable models for countable vertex-labeled graphs cannot replicate the large sample behaviors of sparsity and power law degree distribution observed in many network datasets. Out of this mathematical impossibility emerges the question of how network data can be modeled in a way that reflects known empirical behaviors and respects basic statistical principles. We address this question by observing that edges, not vertices, act as the statistical units in networks constructed from interaction data, making a theory of edge-labeled networks more natural for many applications. In this context we introduce the concept of {\em edge exchangeability}, which unlike its vertex exchangeable counterpart admits models for networks with sparse and/or power law structure. Our characterization of edge exchangeable networks gives rise to a class of nonparametric models, akin to graphon models in the vertex exchangeable setting. Within this class, we identify a tractable family of distributions with a clear interpretation and suitable theoretical properties, whose significance in estimation, prediction, and testing we demonstrate."
"Several min-max relations in graph theory can be expressed in the framework of the Erd\H{o}s-P\'osa property. Typically, this property reveals a connection between packing and covering problems on graphs. We describe some recent techniques for proving this property that are related to tree-like decompositions. We also provide an unified presentation of the current state of the art on this topic."
"In this report, we aim at the development of a decentralized abstraction framework for multi-agent systems under coupled constraints, with the possibility for a varying degree of decentralization. The methodology is based on the analysis employed in our recent work, where decentralized abstractions based exclusively on the information of each agent's neighbors were derived. In the first part of this report, we define the notion each agent's m-neighbor set, which constitutes a measure for the employed degree of decentralization. Then, sufficient conditions are provided on the space and time discretization that provides the abstract system's model, which guarantee the extraction of a transition system with quantifiable transition possibilities."
"Motivated by challenges to existing multiuser transmission methods in a low signal to noise ratio (SNR) regime, and emergence of massive numbers of low data rate ehealth and internet of things (IoT) devices, in this paper we show that it is beneficial to incorporate knowledge of modulation type into multiuser transmit precoder design. Particularly, we propose a transmit precoding (beamforming) specific to BPSK modulation, which has maximum power efficiency and capacity in poor channel conditions. To be more specific, in a multiuser scenario, an objective function is formulated based on the weighted sum of error probabilities of BPSK modulated users. Convex optimization is used to transform and solve this ill-behaved non-convex minimum probability of error (MPE) precoding problem. Numerical results confirm significant performance improvement. We then develop a low-complexity user selection algorithm for MPE precoding. Based on line packing principles in Grassmannian manifolds, the number of supported users is able to exceed the number of transmit antennas, and hence the proposed approach is able to support more simultaneous users compared with existing multiuser transmit precoding methods."
"We propose Turing Learning, a novel system identification method for inferring the behavior of natural or artificial systems. Turing Learning simultaneously optimizes two populations of computer programs, one representing models of the behavior of the system under investigation, and the other representing classifiers. By observing the behavior of the system as well as the behaviors produced by the models, two sets of data samples are obtained. The classifiers are rewarded for discriminating between these two sets, that is, for correctly categorizing data samples as either genuine or counterfeit. Conversely, the models are rewarded for 'tricking' the classifiers into categorizing their data samples as genuine. Unlike other methods for system identification, Turing Learning does not require predefined metrics to quantify the difference between the system and its models. We present two case studies with swarms of simulated robots and prove that the underlying behaviors cannot be inferred by a metric-based system identification method. By contrast, Turing Learning infers the behaviors with high accuracy. It also produces a useful by-product - the classifiers - that can be used to detect abnormal behavior in the swarm. Moreover, we show that Turing Learning also successfully infers the behavior of physical robot swarms. The results show that collective behaviors can be directly inferred from motion trajectories of individuals in the swarm, which may have significant implications for the study of animal collectives. Furthermore, Turing Learning could prove useful whenever a behavior is not easily characterizable using metrics, making it suitable for a wide range of applications."
"Control theory can provide useful insights into the properties of controlled, dynamic systems. One important property of nonlinear systems is the region of attraction (ROA), a safe subset of the state space in which a given controller renders an equilibrium point asymptotically stable. The ROA is typically estimated based on a model of the system. However, since models are only an approximation of the real world, the resulting estimated safe region can contain states outside the ROA of the real system. This is not acceptable in safety-critical applications. In this paper, we consider an approach that learns the ROA from experiments on a real system, without ever leaving the true ROA and, thus, without risking safety-critical failures. Based on regularity assumptions on the model errors in terms of a Gaussian process prior, we use an underlying Lyapunov function in order to determine a region in which an equilibrium point is asymptotically stable with high probability. Moreover, we provide an algorithm to actively and safely explore the state space in order to expand the ROA estimate. We demonstrate the effectiveness of this method in simulation."
"The paper looks at a scaled variant of the stochastic gradient descent algorithm for the matrix completion problem. Specifically, we propose a novel matrix-scaling of the partial derivatives that acts as an efficient preconditioning for the standard stochastic gradient descent algorithm. This proposed matrix-scaling provides a trade-off between local and global second order information. It also resolves the issue of scale invariance that exists in matrix factorization models. The overall computational complexity is linear with the number of known entries, thereby extending to a large-scale setup. Numerical comparisons show that the proposed algorithm competes favorably with state-of-the-art algorithms on various different benchmarks."
"Multi-agent differential games are important and useful tools for analyzing many practical problems. With the recent surge of interest in using UAVs for civil purposes, the importance and urgency of developing tractable multi-agent analysis techniques that provide safety and performance guarantees is at an all-time high. Hamilton-Jacobi (HJ) reachability has successfully provided safety guarantees to small-scale systems and is flexible in terms of system dynamics. However, the exponential complexity scaling of HJ reachability prevents its direct application to large scale problems when the number of vehicles is greater than two. In this paper, we overcome the scalability limitations of HJ reachability by using a mixed integer program that exploits the properties of HJ solutions to provide higher-level control logic. Our proposed method provides safety guarantee for three-vehicle systems -- a previously intractable task for HJ reachability -- without incurring significant additional computation cost. Furthermore, our method is scalable beyond three vehicles and performs significantly better by several metrics than an extension of pairwise collision avoidance to multi-vehicle collision avoidance. We demonstrate our proposed method in simulations."
"Multi-UAV systems are safety-critical, and guarantees must be made to ensure no undesirable configurations such as collisions occur. Hamilton-Jacobi (HJ) reachability is ideal for analyzing such safety-critical systems because it provides safety guarantees and is flexible in terms of system dynamics, however, its direct application is limited to small-scale systems typically of no more than two vehicles because of the exponentially-scaling computation complexity. Previously, the sequential path planning (SPP) method, which assigns strict priorities to vehicles, was proposed, SPP allows multi-vehicle path planning to be done with a linearly-scaling computation complexity. However, the previous SPP formulation assumed that there are no disturbances in the vehicle dynamics, and that every vehicle has perfect knowledge of the position of higher-priority vehicles. In this paper, we make SPP more practical by providing three different methods to account for disturbances in dynamics and imperfect knowledge of higher-priority vehicles' states. Each method has different assumptions about information sharing. We demonstrate our proposed methods in simulations."
"This paper presents a semi-parametric algorithm for online learning of a robot inverse dynamics model. It combines the strength of the parametric and non-parametric modeling. The former exploits the rigid body dynamics equa- tion, while the latter exploits a suitable kernel function. We provide an extensive comparison with other methods from the literature using real data from the iCub humanoid robot. In doing so we also compare two different techniques, namely cross validation and marginal likelihood optimization, for estimating the hyperparameters of the kernel function."
"This paper deals with the problem of multi-degree reduction of a composite B\'ezier curve with the parametric continuity constraints at the endpoints of the segments. We present a novel method which is based on the idea of using constrained dual Bernstein polynomials to compute the control points of the reduced composite curve. In contrast to other methods, ours minimizes the $L_2$-error for the whole composite curve instead of minimizing the $L_2$-errors for each segment separately. As a result, an additional optimization is possible. Examples show that the new method gives much better results than multiple application of the degree reduction of a single B\'ezier curve."
"Explicit constructions of polar codes and polar lattices for both lossless and lossy Gray-Wyner problems are studied. Polar codes are employed to extract Wyner's common information of doubly symmetric binary source; polar lattices are then extended to extract that of a pair of Gaussian sources or multiple Gaussian sources. With regard to the discrete sources, the entire best-known region of the lossless Gray-Wyner problem are achieved by specifying the test channels to construct polar codes without time-sharing. As a result, we are able to give an interpretation that the Wyner's common information remains the same to the lossy case when the distortion is small [1]. Finally, the entire best-known lossy Gray-Wyner region for discrete sources can also be achieved using polar codes. With regard to the Gaussian sources, the best-known lossy Gray-Wyner region for bivariate Gaussian sources with a specific covariance matrix [1] can be achieved by using polar lattices. Moreover, we prove that extracting Wyner's common information of a pair of Gaussian sources is equivalent to implementing the lossy compression for a single Gaussian source, which implies that the common information can be extracted by a polar lattice for quantization. Furthermore, we extend this result to the case of multiple Gaussian sources."
"Understanding how people move within a geographic area, e.g. a city, a country or the whole world, is fundamental in several applications, from predicting the spatio-temporal evolution of an epidemics to inferring migration patterns. Mobile phone records provide an excellent proxy of human mobility, showing that movements exhibit a high level of memory. However, the precise role of memory in widely adopted proxies of mobility, as mobile phone records, is unknown. Here we use 560 millions of call detail records from Senegal to show that standard Markovian approaches, including higher-order ones, fail in capturing real mobility patterns and introduce spurious movements never observed in reality. We introduce an adaptive memory-driven approach to overcome such issues. At variance with Markovian models, it is able to realistically model conditional waiting times, i.e. the probability to stay in a specific area depending on individual's historical movements. Our results demonstrate that in standard mobility models the individuals tend to diffuse faster than what observed in reality, whereas the predictions of the adaptive memory approach significantly agree with observations. We show that, as a consequence, the incidence and the geographic spread of a disease could be inadequately estimated when standard approaches are used, with crucial implications on resources deployment and policy making during an epidemic outbreak."
"Transfer learning is aimed to make use of valuable knowledge in a source domain to help model performance in a target domain. It is particularly important to neural networks, which are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct systematic case studies and provide an illuminating picture on the transferability of neural networks in NLP."
"Probabilistic inference algorithms such as Sequential Monte Carlo (SMC) provide powerful tools for constraining procedural models in computer graphics, but they require many samples to produce desirable results. In this paper, we show how to create procedural models which learn how to satisfy constraints. We augment procedural models with neural networks which control how the model makes random choices based on the output it has generated thus far. We call such models neurally-guided procedural models. As a pre-computation, we train these models to maximize the likelihood of example outputs generated via SMC. They are then used as efficient SMC importance samplers, generating high-quality results with very few samples. We evaluate our method on L-system-like models with image-based constraints. Given a desired quality threshold, neurally-guided models can generate satisfactory results up to 10x faster than unguided models."
"We give and prove an optimal exact quantum query algorithm with complexity $k+1$ for computing the promise problem (i.e., symmetric and partial Boolean function) $DJ_n^k$ defined as: $DJ_n^k(x)=1$ for $|x|=n/2$, $DJ_n^k(x)=0$ for $|x|$ in the set $\{0, 1,\ldots, k, n-k, n-k+1,\ldots,n\}$, and it is undefined for the rest cases, where $n$ is even, $|x|$ is the Hamming weight of $x$. The case of $k=0$ is the well-known Deutsch-Jozsa problem. We outline all symmetric (and partial) Boolean functions with degrees 1 and 2, and prove their exact quantum query complexity. Then we prove that any symmetrical (and partial) Boolean function $f$ has exact quantum 1-query complexity if and only if $f$ can be computed by the Deutsch-Jozsa algorithm. We also discover the optimal exact quantum 2-query complexity for distinguishing between inputs of Hamming weight $\{ \lfloor n/2\rfloor, \lceil n/2\rceil \}$ and Hamming weight in the set $\{ 0, n\}$ for all odd $n$. In addition, a method is provided to determine the degree of any symmetrical (and partial) Boolean function."
"Compressed bitmap indexes are used in databases and search engines. Many bitmap compression techniques have been proposed, almost all relying primarily on run-length encoding (RLE). However, on unsorted data, we can get superior performance with a hybrid compression technique that uses both uncompressed bitmaps and packed arrays inside a two-level tree. An instance of this technique, Roaring, has recently been proposed. Due to its good performance, it has been adopted by several production platforms (e.g., Apache Lucene, Apache Spark, Apache Kylin and Druid).   Yet there are cases where run-length encoded bitmaps are smaller than the original Roaring bitmaps---typically when the data is sorted so that the bitmaps contain long compressible runs. To better handle these cases, we build a new Roaring hybrid that combines uncompressed bitmaps, packed arrays and RLE compressed segments. The result is a new Roaring format that compresses better.   Overall, our new implementation of Roaring can be several times faster (up to two orders of magnitude) than the implementations of traditional RLE-based alternatives (WAH, Concise, EWAH) while compressing better. We review the design choices and optimizations that make these good results possible."
"We introduce the localized Lasso, which is suited for learning models that are both interpretable and have a high predictive power in problems with high dimensionality $d$ and small sample size $n$. More specifically, we consider a function defined by local sparse models, one at each data point. We introduce sample-wise network regularization to borrow strength across the models, and sample-wise exclusive group sparsity (a.k.a., $\ell_{1,2}$ norm) to introduce diversity into the choice of feature sets in the local models. The local models are interpretable in terms of similarity of their sparsity patterns. The cost function is convex, and thus has a globally optimal solution. Moreover, we propose a simple yet efficient iterative least-squares based optimization procedure for the localized Lasso, which does not need a tuning parameter, and is guaranteed to converge to a globally optimal solution. The solution is empirically shown to outperform alternatives for both simulated and genomic personalized medicine data."
"The micompr R package implements a procedure for assessing if two or more multivariate samples are drawn from the same distribution. The procedure uses principal component analysis to convert multivariate observations into a set of linearly uncorrelated statistical measures, which are then compared using a number of statistical methods. This technique is independent of the distributional properties of samples and automatically selects features that best explain their differences. The procedure is appropriate for comparing samples of time series, images, spectrometric measures or similar high-dimension multivariate observations."
"SimOutUtils is a suite of MATLAB/Octave functions for studying and analyzing time series-like output from stochastic simulation models. More specifically, SimOutUtils allows modelers to study and visualize simulation output dynamics, perform distributional analysis of output statistical summaries, as well as compare these summaries in order to assert the statistical equivalence of two or more model implementations. Additionally, the provided functions are able to produce publication quality figures and tables showcasing results from the specified simulation output studies."
"Attribute recognition, particularly facial, extracts many labels for each image. While some multi-task vision problems can be decomposed into separate tasks and stages, e.g., training independent models for each task, for a growing set of problems joint optimization across all tasks has been shown to improve performance. We show that for deep convolutional neural network (DCNN) facial attribute extraction, multi-task optimization is better. Unfortunately, it can be difficult to apply joint optimization to DCNNs when training data is imbalanced, and re-balancing multi-label data directly is structurally infeasible, since adding/removing data to balance one label will change the sampling of the other labels. This paper addresses the multi-label imbalance problem by introducing a novel mixed objective optimization network (MOON) with a loss function that mixes multiple task objectives with domain adaptive re-weighting of propagated loss. Experiments demonstrate that not only does MOON advance the state of the art in facial attribute recognition, but it also outperforms independently trained DCNNs using the same data. When using facial attributes for the LFW face recognition task, we show that our balanced (domain adapted) network outperforms the unbalanced trained network."
"We propose a new method to accelerate the convergence of optimization algorithms. This method simply adds a power coefficient $\gamma\in[0,1)$ to the gradient during optimization. We call this the Powerball method and analyze the convergence rate for the Powerball method for strongly convex functions and show that it has a faster convergence rate than gradient descent and Newton's method in the initial iterations. We also demonstrate that the Powerball method provides a $10$-fold speed up of the convergence of both gradient descent and L-BFGS on multiple real datasets."
"Graphlets are induced subgraph patterns and have been frequently applied to characterize the local topology structures of graphs across various domains, e.g., online social networks (OSNs) and biological networks. Discovering and computing graphlet statistics are highly challenging. First, the massive size of real-world graphs makes the exact computation of graphlets extremely expensive. Secondly, the graph topology may not be readily available so one has to resort to web crawling using the available application programming interfaces (APIs). In this work, we propose a general and novel framework to estimate graphlet statistics of ""any size"". Our framework is based on collecting samples through consecutive steps of random walks. We derive an analytical bound on the sample size (via the Chernoff-Hoeffding technique) to guarantee the convergence of our unbiased estimator. To further improve the accuracy, we introduce two novel optimization techniques to reduce the lower bound on the sample size. Experimental evaluations demonstrate that our methods outperform the state-of-the-art method up to an order of magnitude both in terms of accuracy and time cost."
"We study the problem of compilation of quantum algorithms into optimized physical-level circuits executable in a quantum information processing (QIP) experiment based on trapped atomic ions. We report a complete strategy: starting with an algorithm in the form of a quantum computer program, we compile it into a high-level logical circuit that goes through multiple stages of decomposition into progressively lower-level circuits until we reach the physical execution-level specification. We skip the fault-tolerance layer, as it is not within the scope of this work. The different stages are structured so as to best assist with the overall optimization while taking into account numerous optimization criteria, including minimizing the number of expensive two-qubit gates, minimizing the number of less expensive single-qubit gates, optimizing the runtime, minimizing the overall circuit error, and optimizing classical control sequences. Our approach allows a trade-off between circuit runtime and quantum error, as well as to accommodate future changes in the optimization criteria that may likely arise as a result of the anticipated improvements in the physical-level control of the experiment."
"We describe here the package {\tt subdivision\\_solver} for the mathematical software {\tt SageMath}. It provides a solver on real numbers for square systems of large dense polynomials. By large polynomials we mean multivariate polynomials with large degrees, which coefficients have large bit-size. While staying robust, symbolic approaches to solve systems of polynomials see their performances dramatically affected by high degree and bit-size of input polynomials.Available numeric approaches suffer from the cost of the evaluation of large polynomials and their derivatives.Our solver is based on interval analysis and bisections of an initial compact domain of $\R^n$ where solutions are sought. Evaluations on intervals with Horner scheme is performed by the package {\tt fast\\_polynomial} for {\tt SageMath}.The non-existence of a solution within a box is certified by an evaluation scheme that uses a Taylor expansion at order 2, and existence and uniqueness of a solution within a box is certified with krawczyk operator.The precision of the working arithmetic is adapted on the fly during the subdivision process and we present a new heuristic criterion to decide if the arithmetic precision has to be increased."
"Large medical image data sets with high dimensionality require substantial amount of computation time for data creation and data processing. This paper presents a novel generalized method that finds optimal image-based feature sets that reduce computational time complexity while maximizing overall classification accuracy for detection of diabetic retinopathy (DR). First, region-based and pixel-based features are extracted from fundus images for classification of DR lesions and vessel-like structures. Next, feature ranking strategies are used to distinguish the optimal classification feature sets. DR lesion and vessel classification accuracies are computed using the boosted decision tree and decision forest classifiers in the Microsoft Azure Machine Learning Studio platform, respectively. For images from the DIARETDB1 data set, 40 of its highest-ranked features are used to classify four DR lesion types with an average classification accuracy of 90.1% in 792 seconds. Also, for classification of red lesion regions and hemorrhages from microaneurysms, accuracies of 85% and 72% are observed, respectively. For images from STARE data set, 40 high-ranked features can classify minor blood vessels with an accuracy of 83.5% in 326 seconds. Such cloud-based fundus image analysis systems can significantly enhance the borderline classification performances in automated screening systems."
"With the development of e-commerce, many products are now being sold worldwide, and manufacturers are eager to obtain a better understanding of customer behavior in various regions. To achieve this goal, most previous efforts have focused mainly on questionnaires, which are time-consuming and costly. The tremendous volume of product reviews on e-commerce websites has seen a new trend emerge, whereby manufacturers attempt to understand user preferences by analyzing online reviews. Following this trend, this paper addresses the problem of studying customer behavior by exploiting recently developed opinion mining techniques. This work is novel for three reasons. First, questionnaire-based investigation is automatically enabled by employing algorithms for template-based question generation and opinion mining-based answer extraction. Using this system, manufacturers are able to obtain reports of customer behavior featuring a much larger sample size, more direct information, a higher degree of automation, and a lower cost. Second, international customer behavior study is made easier by integrating tools for multilingual opinion mining. Third, this is the first time an automatic questionnaire investigation has been conducted to compare customer behavior in China and America, where product reviews are written and read in Chinese and English, respectively. Our study on digital cameras, smartphones, and tablet computers yields three findings. First, Chinese customers follow the Doctrine of the Mean, and often use euphemistic expressions, while American customers express their opinions more directly. Second, Chinese customers care more about general feelings, while American customers pay more attention to product details. Third, Chinese customers focus on external features, while American customers care more about the internal features of products."
"Deep networks are now able to achieve human-level performance on a broad spectrum of recognition tasks. Independently, neuromorphic computing has now demonstrated unprecedented energy-efficiency through a new chip architecture based on spiking neurons, low precision synapses, and a scalable communication network. Here, we demonstrate that neuromorphic computing, despite its novel architectural primitives, can implement deep convolution networks that i) approach state-of-the-art classification accuracy across 8 standard datasets, encompassing vision and speech, ii) perform inference while preserving the hardware's underlying energy-efficiency and high throughput, running on the aforementioned datasets at between 1200 and 2600 frames per second and using between 25 and 275 mW (effectively > 6000 frames / sec / W) and iii) can be specified and trained using backpropagation with the same ease-of-use as contemporary deep learning. For the first time, the algorithmic power of deep learning can be merged with the efficiency of neuromorphic processors, bringing the promise of embedded, intelligent, brain-inspired computing one step closer."
"Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a ""colorization Turing test,"" asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32% of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks."
"The information content of a node in information networks is influenced by its neighbors in the networks. Recently there have been many works on modeling information diffusion, but few have integrated these models for online decision making. A framework for guiding information diffusion is critically important for understanding the vulnerabilities of these networks and designing good policies to suppress rumors and misinformation. Here, we propose a unified stochastic differential equation framework for modeling information diffusion over networks and designing the control policy to guide such diffusion. Our framework can handle noisy data, networks with time-varying edges and node birth processes. Using both synthetic and real world networks, we showed that our framework is robust, able to steer both stable and unstable information diffusion systems to desired states with faster convergence, less variance and lower control cost than alternatives."
"In this paper, a part-based technique for real time detection of users' faces on mobile devices is proposed. This method is specifically designed for detecting partially cropped and occluded faces captured using a smartphone's front-facing camera for continuous authentication. The key idea is to detect facial segments in the frame and cluster the results to obtain the region which is most likely to contain a face. Extensive experimentation on a mobile dataset of 50 users shows that our method performs better than many state-of-the-art face detection methods in terms of accuracy and processing speed."
"In this work, connected cubic planar bipartite graphs and related binary self-dual codes are studied. Binary self-dual codes of length 16 are obtained by face-vertex incidence matrices of these graphs. By considering their lifts to the ring R_2 new extremal binary self-dual codes of lengths 64 are constructed as Gray images. More precisely, we construct 15 new codes of length 64. Moreover, 10 new codes of length 66 were obtained by applying a building-up construction to the binary codes. Codes with these weight enumerators are constructed for the first time in the literature. The results are tabulated."
"This paper considers a string of vehicles where the local control law uses the states of the vehicle's immediate predecessor and follower. The coupling towards the preceding vehicle can be chosen different to the coupling towards the following vehicle, which is often referred to as an asymmetric bidirectional string. Further, the asymmetry for the velocity coupling can be chosen differently to the asymmetry in the position coupling. It is investigated how the effect of disturbance on the control errors in the string depends on the string length. It is shown, that in case of symmetric position coupling and asymmetric velocity coupling, linear scaling can be achieved. For symmetric interaction in both states, i.e., in symmetric bidirectional strings, the errors scale quadratically in the number of vehicles. When the coupling in position is asymmetric, exponential scaling may occur or the system might even become unstable. The paper thus gives a comprehensive overview of the achievable performance in linear, asymmetric, bidirectional platoons. The results reveal that symmetry in the position coupling and asymmetry in velocity coupling qualitatively improves the performance of the string. Extensive numerical results illustrate the theoretical findings."
"Event retrieval and recognition in a large corpus of videos necessitates a holistic fixed-size visual representation at the video clip level that is comprehensive, compact, and yet discriminative. It shall comprehensively aggregate information across relevant video frames, while suppress redundant information, leading to a compact representation that can effectively differentiate among different visual events. In search for such a representation, we propose to build a spatially consistent counting grid model to aggregate together deep features extracted from different video frames. The spatial consistency of the counting grid model is achieved by introducing a prior model estimated from a large corpus of video data. The counting grid model produces an intermediate tensor representation for each video, which automatically identifies and removes the feature redundancy across the different frames. The tensor representation is subsequently reduced to a fixed-size vector representation by averaging over the counting grid. When compared to existing methods on both event retrieval and event classification benchmarks, we achieve significantly better accuracy with much more compact representation."
"This paper revisits the Gaussian degraded relay channel, where the link that carries information from the source to the destination is a physically degraded version of the link that carries information from the source to the relay. The source and the relay are subject to expected power constraints. The $\varepsilon$-capacity of the channel is characterized and it is strictly larger than the capacity for $\varepsilon>0$, which implies that the channel does not possess the strong converse property. The proof of the achievability part is based on several key ideas: block Markov coding which is used in the classical decode-forward strategy, power control for Gaussian channels under expected power constraints, and a careful scaling between the block size and the total number of block uses. The converse part is proved by first establishing two non-asymptotic lower bounds on the error probability, which are derived from the type-II errors of some binary hypothesis tests. Subsequently, each lower bound is simplified by conditioning on an event related to the power of some linear combination of the codewords transmitted by the source and the relay. Lower and upper bounds on the second-order term of the optimal coding rate in terms of blocklength and error probability are also obtained."
"Recently, neural models have been proposed for headline generation by learning to map documents to headlines with recurrent neural networks. Nevertheless, as traditional neural network utilizes maximum likelihood estimation for parameter optimization, it essentially constrains the expected training objective within word level rather than sentence level. Moreover, the performance of model prediction significantly relies on training data distribution. To overcome these drawbacks, we employ minimum risk training strategy in this paper, which directly optimizes model parameters in sentence level with respect to evaluation metrics and leads to significant improvements for headline generation. Experiment results show that our models outperforms state-of-the-art systems on both English and Chinese headline generation tasks."
"Convex relaxations of nonconvex multilabel problems have been demonstrated to produce superior (provably optimal or near-optimal) solutions to a variety of classical computer vision problems. Yet, they are of limited practical use as they require a fine discretization of the label space, entailing a huge demand in memory and runtime. In this work, we propose the first sublabel accurate convex relaxation for vectorial multilabel problems. The key idea is that we approximate the dataterm of the vectorial labeling problem in a piecewise convex (rather than piecewise linear) manner. As a result we have a more faithful approximation of the original cost function that provides a meaningful interpretation for the fractional solutions of the relaxed convex problem. In numerous experiments on large-displacement optical flow estimation and on color image denoising we demonstrate that the computed solutions have superior quality while requiring much lower memory and runtime."
"The study of networks plays a crucial role in investigating the structure, dynamics, and function of a wide variety of complex systems in myriad disciplines. Despite the success of traditional network analysis, standard networks provide a limited representation of these systems, which often includes different types of relationships (i.e., ""multiplexity"") among their constituent components and/or multiple interacting subsystems. Such structural complexity has a significant effect on both dynamics and function. Throwing away or aggregating available structural information can generate misleading results and provide a major obstacle towards attempts to understand the system under analysis. The recent ""multilayer' approach for modeling networked systems explicitly allows the incorporation of multiplexity and other features of realistic networked systems. On one hand, it allows one to couple different structural relationships by encoding them in a convenient mathematical object. On the other hand, it also allows one to couple different dynamical processes on top of such interconnected structures. The resulting framework plays a crucial role in helping to achieve a thorough, accurate understanding of complex systems. The study of multilayer networks has also revealed new physical phenomena that remained hidden when using the traditional network representation of graphs. Here we survey progress towards a deeper understanding of dynamical processes on multilayer networks, and we highlight some of the physical phenomena that emerge from multilayer structure and dynamics."
"We initiate the study of dynamic algorithms for graph sparsification problems and obtain fully dynamic algorithms, allowing both edge insertions and edge deletions, that take polylogarithmic time after each update in the graph. Our three main results are as follows. First, we give a fully dynamic algorithm for maintaining a $ (1 \pm \epsilon) $-spectral sparsifier with amortized update time $poly(\log{n}, \epsilon^{-1})$. Second, we give a fully dynamic algorithm for maintaining a $ (1 \pm \epsilon) $-cut sparsifier with \emph{worst-case} update time $poly(\log{n}, \epsilon^{-1})$. Both sparsifiers have size $ n \cdot poly(\log{n}, \epsilon^{-1})$. Third, we apply our dynamic sparsifier algorithm to obtain a fully dynamic algorithm for maintaining a $(1 + \epsilon)$-approximation to the value of the maximum flow in an unweighted, undirected, bipartite graph with amortized update time $poly(\log{n}, \epsilon^{-1})$."
"This paper considers online convex optimization over a complicated constraint set, which typically consists of multiple functional constraints and a set constraint. The conventional projection based online projection algorithm (Zinkevich, 2003) can be difficult to implement due to the potentially high computation complexity of the projection operation. In this paper, we relax the functional constraints by allowing them to be violated at each round but still requiring them to be satisfied in the long term. This type of relaxed online convex optimization (with long term constraints) was first considered in Mahdavi et al. (2012). That prior work proposes an algorithm to achieve $O(\sqrt{T})$ regret and $O(T^{3/4})$ constraint violations for general problems and another algorithm to achieve an $O(T^{2/3})$ bound for both regret and constraint violations when the constraint set can be described by a finite number of linear constraints. A recent extension in Jenatton et al. (2016) can achieve $O(T^{\max\{\beta,1-\beta\}})$ regret and $O(T^{1-\beta/2})$ constraint violations where $\beta\in (0,1)$. The current paper proposes a new simple algorithm that yields improved performance in comparison to prior works. The new algorithm achieves an $O(\sqrt{T})$ regret bound with finite constraint violations."
"Almost all real-world networks are subject to constant evolution, and plenty of evolving networks have been investigated to uncover the underlying mechanisms for a deeper understanding of the organization and development of them. Compared with the rapid expansion of the empirical studies about evolution mechanisms exploration, the future links prediction methods corresponding to the evolution mechanisms are deficient. Real-world information always contain hints of what would happen next, which is also the case in the observed evolving networks. In this paper, we firstly propose a structured-dependent index to strengthen the robustness of link prediction methods. Then we treat the observed links and their timestamps in evolving networks as known information. We envision evolving networks as dynamic systems and model the evolutionary dynamics of nodes similarity. Based on the iterative updating of nodes' network position, the potential trend of evolving networks is uncovered, which improves the accuracy of future links prediction. Experiments on various real-world networks show that the proposed index performs better than baseline methods and the spatial-temporal position drift model performs well in real-world evolving networks."
"In this paper, we propose a data structure, a quadruple neighbor list (QN-list, for short), to support real time queries of all longest increasing subsequence (LIS) and LIS with constraints over sequential data streams. The QN-List built by our algorithm requires $O(w)$ space, where $w$ is the time window size. The running time for building the initial QN-List takes $O(w\log w)$ time. Applying the QN-List, insertion of the new item takes $O(\log w)$ time and deletion of the first item takes $O(w)$ time. To the best of our knowledge, this is the first work to support both LIS enumeration and LIS with constraints computation by using a single uniform data structure for real time sequential data streams. Our method outperforms the state-of-the-art methods in both time and space cost, not only theoretically, but also empirically."
"We propose a unified and systematic framework for performing online nonnegative matrix factorization in the presence of outliers. Our framework is particularly suited to large-scale data. We propose two solvers based on projected gradient descent and the alternating direction method of multipliers. We prove that the sequence of objective values converges almost surely by appealing to the quasi-martingale convergence theorem. We also show the sequence of learned dictionaries converges to the set of stationary points of the expected loss function almost surely. In addition, we extend our basic problem formulation to various settings with different constraints and regularizers. We also adapt the solvers and analyses to each setting. We perform extensive experiments on both synthetic and real datasets. These experiments demonstrate the computational efficiency and efficacy of our algorithms on tasks such as (parts-based) basis learning, image denoising, shadow removal and foreground-background separation."
"Recognising human activities from streaming videos poses unique challenges to learning algorithms: predictive models need to be scalable, incrementally trainable, and must remain bounded in size even when the data stream is arbitrarily long. Furthermore, as parameter tuning is problematic in a streaming setting, suitable approaches should be parameterless, and make no assumptions on what class labels may occur in the stream. We present here an approach to the recognition of human actions from streaming data which meets all these requirements by: (1) incrementally learning a model which adaptively covers the feature space with simple local classifiers; (2) employing an active learning strategy to reduce annotation requests; (3) achieving promising accuracy within a fixed model size. Extensive experiments on standard benchmarks show that our approach is competitive with state-of-the-art non-incremental methods, and outperforms the existing active incremental baselines."
"The efficacy of learning in a classroom depends on how engaged students are with the learning material. Is it possible to assess students engagement directly from their brain activity without distracting them from the material at hand? We performed simultaneous recordings of electroencephalography (EEG) from multiple students in a classroom, and measured the inter-subject correlation (ISC) of activity evoked by a common video stimulus. Many aspects of attentional modulation of this ISC, which were previously established in a laboratory setting, are reliably reproduced here with portable low-cost equipment. The present data suggests that stimulus-evoked neural responses, known to be modulated by attention, can be used to assess the engagement of a group of students in real-time."
"High computational complexity hinders the widespread usage of Convolutional Neural Networks (CNNs), especially in mobile devices. Hardware accelerators are arguably the most promising approach for reducing both execution time and power consumption. One of the most important steps in accelerator development is hardware-oriented model approximation. In this paper we present Ristretto, a model approximation framework that analyzes a given CNN with respect to numerical resolution used in representing weights and outputs of convolutional and fully connected layers. Ristretto can condense models by using fixed point arithmetic and representation instead of floating point. Moreover, Ristretto fine-tunes the resulting fixed point network. Given a maximum error tolerance of 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available."
"This tutorial is intended as an accessible but rigorous first reference for someone interested in learning how to model and analyze cellular network performance using stochastic geometry. In particular, we focus on computing the signal-to-interference-plus-noise ratio (SINR) distribution, which can be characterized by the coverage probability (the SINR CCDF) or the outage probability (its CDF). We model base stations (BSs) in the network as a realization of a homogeneous Poisson point process of density $\lambda$, and compute the SINR for three main cases: the downlink, uplink, and finally the multi-tier downlink, which is characterized by having $k$ tiers of BSs each with a unique density $\lambda_i$ and transmit power $p_i$. These three baseline results have been extensively extended to many different scenarios, and we conclude with a brief summary of some of those extensions."
"In this paper, we study several microphone channel selection and weighting methods for robust automatic speech recognition (ASR) in noisy conditions. For channel selection, we investigate two methods based on the maximum likelihood (ML) criterion and minimum autoencoder reconstruction criterion, respectively. For channel weighting, we produce enhanced log Mel filterbank coefficients as a weighted sum of the coefficients of all channels. The weights of the channels are estimated by using the ML criterion with constraints. We evaluate the proposed methods on the CHiME-3 noisy ASR task. Experiments show that channel weighting significantly outperforms channel selection due to its higher flexibility. Furthermore, on real test data in which different channels have different gains of the target signal, the channel weighting method performs equally well or better than the MVDR beamforming, despite the fact that the channel weighting does not make use of the phase delay information which is normally used in beamforming."
"Discriminative methods often generate hand poses kinematically implausible, then generative methods are used to correct (or verify) these results in a hybrid method. Estimating 3D hand pose in a hierarchy, where the high-dimensional output space is decomposed into smaller ones, has been shown effective. Existing hierarchical methods mainly focus on the decomposition of the output space while the input space remains almost the same along the hierarchy. In this paper, a hybrid hand pose estimation method is proposed by applying the kinematic hierarchy strategy to the input space (as well as the output space) of the discriminative method by a spatial attention mechanism and to the optimization of the generative method by hierarchical Particle Swarm Optimization (PSO). The spatial attention mechanism integrates cascaded and hierarchical regression into a CNN framework by transforming both the input(and feature space) and the output space, which greatly reduces the viewpoint and articulation variations. Between the levels in the hierarchy, the hierarchical PSO forces the kinematic constraints to the results of the CNNs. The experimental results show that our method significantly outperforms four state-of-the-art methods and three baselines on three public benchmarks."
"In this paper, we describe a novel deep convolutional neural networks (CNN) based approach called contextual deep CNN that can jointly exploit spatial and spectral features for hyperspectral image classification. The contextual deep CNN first concurrently applies multiple 3-dimensional local convolutional filters with different sizes jointly exploiting spatial and spectral features of a hyperspectral image. The initial spatial and spectral feature maps obtained from applying the variable size convolutional filters are then combined together to form a joint spatio-spectral feature map. The joint feature map representing rich spectral and spatial properties of the hyperspectral image is then fed through fully convolutional layers that eventually predict the corresponding label of each pixel vector. The proposed approach is tested on the Indian Pines data and performance comparison shows enhanced classification performance of the proposed approach over the current state of the art."
"We consider the well-studied cake cutting problem in which the goal is to find an envy-free allocation based on queries from $n$ agents. The problem has received attention in computer science, mathematics, and economics. It has been a major open problem whether there exists a discrete and bounded envy-free protocol. We resolve the problem by proposing a discrete and bounded envy-free protocol for any number of agents. The maximum number of queries required by the protocol is $n^{n^{n^{n^{n^n}}}}$. We additionally show that even if we do not run our protocol to completion, it can find in at most $n^{n+1}$ queries a partial allocation of the cake that achieves proportionality (each agent gets at least $1/n$ of the value of the whole cake) and envy-freeness. Finally we show that an envy-free partial allocation can be computed in $n^{n+1}$ queries such that each agent gets a connected piece that gives the agent at least $1/(3n)$ of the value of the whole cake."
"In the face of serious infectious diseases, governments endeavour to implement containment measures such as public vaccination at a macroscopic level. Meanwhile, individuals tend to protect themselves by avoiding contacts with infections at a microscopic level. However, a comprehensive understanding of how such combined strategy influences epidemic dynamics is still lacking. We study a susceptible-infected-susceptible epidemic model with imperfect vaccination on dynamic contact networks, where the macroscopic intervention is represented by random vaccination of the population and the microscopic protection is characterised by susceptible individuals rewiring contacts from infective neighbours. In particular, the model is formulated both in populations without and then with demographic effects. Using the pairwise approximation and the probability generating function approach, we investigate both dynamics of the epidemic and the underlying network. For populations without demography, the emerging degree correlations, bistable states, and oscillations demonstrate the combined effects of the public vaccination program and individual protective behavior. Compared to either strategy in isolation, the combination of public vaccination and individual protection is more effective in preventing and controlling the spread of infectious diseases by increasing both the invasion threshold and the persistence threshold. For populations with additional demographic factors, the integration between vaccination intervention and individual rewiring may promote epidemic spreading due to the birth effect. Moreover, the degree distributions of both networks in the steady state is closely related to the degree distribution of newborns, which leads to uncorrelated connectivity. All the results demonstrate the importance of both local protection and global intervention, as well as the demographic effects."
"We prove that P != NP by proving the existence of a class of functions we call Tau, each of whose members satisfies the conditions of one-way functions. Each member of Tau is a function computable in polynomial time, with negligible probability of finding its inverse by any polynomial probabilistic algorithm. We also prove that no polynomial-time algorithm exists to compute the inverse of members of Tau, and that the problem of computing the inverse of Tau cannot be reduced to FSAT in polynomial time."
"Very deep convolutional neural networks introduced new problems like vanishing gradient and degradation. The recent successful contributions towards solving these problems are Residual and Highway Networks. These networks introduce skip connections that allow the information (from the input or those learned in earlier layers) to flow more into the deeper layers. These very deep models have lead to a considerable decrease in test errors, on benchmarks like ImageNet and COCO. In this paper, we propose the use of exponential linear unit instead of the combination of ReLU and Batch Normalization in Residual Networks. We show that this not only speeds up learning in Residual Networks but also improves the accuracy as the depth increases. It improves the test error on almost all data sets, like CIFAR-10 and CIFAR-100"
"Robotic-assisted Minimally Invasive Surgery (RMIS) can benefit from the automation of common, repetitive or well-defined but ergonomically difficult tasks. One such task is the scanning of a pick-up endomicroscopy probe over a complex, undulating tissue surface in order to enhance the effective field-of-view through video mosaicing. In this paper, the da Vinci surgical robot, through the dVRK framework, is used for autonomous scanning and 2D mosaicing over a user-defined region of interest. To achieve the level of precision required for high quality large-area mosaic generation, which relies on sufficient overlap between consecutive image frames, visual servoing is performed using a tracking marker attached to the probe. The resulting sub-millimetre accuracy of the probe motion allows for the generation of large endomicroscopy mo- saics with minimal intervention from the surgeon. It also allows the probe to be maintained in an orientation perpendicular to the local tissue surface, providing optimal imaging results. Images are streamed from the endomicroscope and overlaid live onto the surgeons view, while 2D mosaics are generated in real-time, and fused into a 3D stereo reconstruction of the surgical scene, thus providing intuitive visualisation and fusion of the multi-scale images. The system therefore offers significant potential to enhance surgical procedures, by providing the operator with cellular-scale information over a larger area than could typically be achieved by manual scanning."
"Deep Convolutional Neural Networks (CNN) have exhibited superior performance in many visual recognition tasks including image classification, object detection, and scene label- ing, due to their large learning capacity and resistance to overfit. For the image classification task, most of the current deep CNN- based approaches take the whole size-normalized image as input and have achieved quite promising results. Compared with the previously dominating approaches based on feature extraction, pooling, and classification, the deep CNN-based approaches mainly rely on the learning capability of deep CNN to achieve superior results: the burden of minimizing intra-class variation while maximizing inter-class difference is entirely dependent on the implicit feature learning component of deep CNN; we rely upon the implicitly learned filters and pooling component to select the discriminative regions, which correspond to the activated neurons. However, if the irrelevant regions constitute a large portion of the image of interest, the classification performance of the deep CNN, which takes the whole image as input, can be heavily affected. To solve this issue, we propose a novel latent CNN framework, which treats the most discriminate region as a latent variable. We can jointly learn the global CNN with the latent CNN to avoid the aforementioned big irrelevant region issue, and our experimental results show the evident advantage of the proposed latent CNN over traditional deep CNN: latent CNN outperforms the state-of-the-art performance of deep CNN on standard benchmark datasets including the CIFAR-10, CIFAR- 100, MNIST and PASCAL VOC 2007 Classification dataset."
"Thanks to the efforts of the robotics and autonomous systems community, robots are becoming ever more capable. There is also an increasing demand from end-users for autonomous service robots that can operate in real environments for extended periods. In the STRANDS project we are tackling this demand head-on by integrating state-of-the-art artificial intelligence and robotics research into mobile service robots, and deploying these systems for long-term installations in security and care environments. Over four deployments, our robots have been operational for a combined duration of 104 days autonomously performing end-user defined tasks, covering 116km in the process. In this article we describe the approach we have used to enable long-term autonomous operation in everyday environments, and how our robots are able to use their long run times to improve their own performance."
"This paper presents an analytical comparison of active and passive receiver models in diffusive molecular communication. In the active model, molecules are absorbed when they collide with the receiver surface. In the passive model, the receiver is a virtual boundary that does not affect molecule behavior. Two approaches are presented to derive transforms between the receiver signals. As an example, two models for an unbounded diffusion-only molecular communication system with a spherical receiver are unified. As time increases in the three-dimensional system, the transform functions have constant scaling factors, such that the receiver models are effectively equivalent. Methods are presented to enable the transformation of stochastic simulations, which are used to verify the transforms and demonstrate that transforming the simulation of a passive receiver can be more efficient and more accurate than the direct simulation of an absorbing receiver."
"Hybrid IP networks that use both control paradigms - distributed and centralized - promise the best of two worlds: programmability and agility of SDN, and reliability and fault tolerance of distributed routing protocols like OSPF. The common approaches follow a division of labor concept, where SDN controls prioritized traffic and OSPF assures care-free operation of best effort traffic. We propose SDN Partitioning, which establishes centralized control over the distributed routing protocol by partitioning the topology into sub-domains with SDN-enabled border nodes, such that OSPF's routing updates have to traverse SDN border nodes to reach neighboring sub-domains. This allows the central controller to modify how sub-domains view one another, which in turn allows to steer inter-sub-domain traffic. The degree of dynamic control against simplicity of OSPF can be trade off by adjusting the size of the sub-domains. This paper explains the technical requirements, presents a novel scheme for balanced topology partitioning, and provides the models for common network management tasks. Our performance evaluation shows that - already in its minimum configuration with two sub-domains - SDN Partitioning provides significant improvements in all respects compared to legacy routing protocols, whereas smaller sub-domains provide network control capabilities comparable to full SDN deployment."
"In order to properly assess the function and computational properties of simulated neural systems, it is necessary to account for the nature of the stimuli that drive the system. However, providing stimuli that are rich and yet both reproducible and amenable to experimental manipulations is technically challenging, and even more so if a closed-loop scenario is required. In this work, we present a novel approach to solve this problem, connecting robotics and neural network simulators. We implement a middleware solution that bridges the Robotic Operating System (ROS) to the Multi-Simulator Coordinator (MUSIC). This enables any robotic and neural simulators that implement the corresponding interfaces to be efficiently coupled, allowing real-time performance for a wide range of configurations. This work extends the toolset available for researchers in both neurorobotics and computational neuroscience, and creates the opportunity to perform closed-loop experiments of arbitrary complexity to address questions in multiple areas, including embodiment, agency, and reinforcement learning."
"Knowledge representation is an important, long-history topic in AI, and there have been a large amount of work for knowledge graph embedding which projects symbolic entities and relations into low-dimensional, real-valued vector space. However, most embedding methods merely concentrate on data fitting and ignore the explicit semantic expression, leading to uninterpretable representations. Thus, traditional embedding methods have limited potentials for many applications such as question answering, and entity classification. To this end, this paper proposes a semantic representation method for knowledge graph \textbf{(KSR)}, which imposes a two-level hierarchical generative process that globally extracts many aspects and then locally assigns a specific category in each aspect for every triple. Since both aspects and categories are semantics-relevant, the collection of categories in each aspect is treated as the semantic representation of this triple. Extensive experiments justify our model outperforms other state-of-the-art baselines substantially."
"Learning new representations in machine learning is often tackled using a factorization of the data. For many such problems, including sparse coding and matrix completion, learning these factorizations can be difficult, in terms of efficiency and to guarantee that the solution is a global minimum. Recently, a general class of objectives have been introduced, called induced regularized factor models (RFMs), which have an induced convex form that enables global optimization. Though attractive theoretically, this induced form is impractical, particularly for large or growing datasets. In this work, we investigate the use of a practical alternating minimization algorithms for induced RFMs, that ensure convergence to global optima. We characterize the stationary points of these models, and, using these insights, highlight practical choices for the objectives. We then provide theoretical and empirical evidence that alternating minimization, from a random initialization, converges to global minima for a large subclass of induced RFMs. In particular, we prove that induced RFMs do not have degenerate saddlepoints and that local minima are actually global minima. Finally, we provide an extensive investigation into practical optimization choices for using alternating minimization for induced RFMs, for both batch and stochastic gradient descent."
"Human beings often assess the aesthetic quality of an image coupled with the identification of the image's semantic content. This paper addresses the correlation issue between automatic aesthetic quality assessment and semantic recognition. We cast the assessment problem as the main task among a multi-task deep model, and argue that semantic recognition task offers the key to address this problem. Based on convolutional neural networks, we employ a single and simple multi-task framework to efficiently utilize the supervision of aesthetic and semantic labels. A correlation item between these two tasks is further introduced to the framework by incorporating the inter-task relationship learning. This item not only provides some useful insight about the correlation but also improves assessment accuracy of the aesthetic task. Particularly, an effective strategy is developed to keep a balance between the two tasks, which facilitates to optimize the parameters of the framework. Extensive experiments on the challenging AVA dataset and Photo.net dataset validate the importance of semantic recognition in aesthetic quality assessment, and demonstrate that multi-task deep models can discover an effective aesthetic representation to achieve state-of-the-art results."
"We present results from a new approach to learning and plasticity in neuromorphic hardware systems: to enable flexibility in implementable learning mechanisms while keeping high efficiency associated with neuromorphic implementations, we combine a general-purpose processor with full-custom analog elements.   This processor is operating in parallel with a fully parallel neuromorphic system consisting of an array of synapses connected to analog, continuous time neuron circuits. Novel analog correlation sensor circuits process spike events for each synapse in parallel and in real-time.   The processor uses this pre-processing to compute new weights possibly using additional information following its program.   Therefore, learning rules can be defined in software giving a large degree of flexibility.   Synapses realize correlation detection geared towards Spike-Timing Dependent Plasticity (STDP) as central computational primitive in the analog domain.   Operating at a speed-up factor of 1000 compared to biological time-scale, we measure time-constants from tens to hundreds of micro-seconds.   We analyze variability across multiple chips and demonstrate learning using a multiplicative STDP rule.   We conclude, that the presented approach will enable flexible and efficient learning as a platform for neuroscientific research and technological applications."
"While probability theory is normally applied to external environments, there has been some recent interest in probabilistic modeling of the outputs of computations that are too expensive to run. Since mathematical logic is a powerful tool for reasoning about computer programs, we consider this problem from the perspective of integrating probability and logic. Recent work on assigning probabilities to mathematical statements has used the concept of coherent distributions, which satisfy logical constraints such as the probability of a sentence and its negation summing to one. Although there are algorithms which converge to a coherent probability distribution in the limit, this yields only weak guarantees about finite approximations of these distributions. In our setting, this is a significant limitation: Coherent distributions assign probability one to all statements provable in a specific logical theory, such as Peano Arithmetic, which can prove what the output of any terminating computation is; thus, a coherent distribution must assign probability one to the output of any terminating computation. To model uncertainty about computations, we propose to work with approximations to coherent distributions. We introduce inductive coherence, a strengthening of coherence that provides appropriate constraints on finite approximations, and propose an algorithm which satisfies this criterion."
"In this paper, we investigate detectability and identifiability of attacks on linear dynamical systems that are subjected to external disturbances. We generalize a concept for a security index, which was previously introduced for static systems. The generalized index exactly quantifies the resources necessary for targeted attacks to be undetectable and unidentifiable in the presence of disturbances. This information is useful for both risk assessment and for the design of anomaly detectors. Finally, we show how techniques from the fault detection literature can be used to decouple disturbances and to identify attacks, under certain sparsity constraints."
"A long-term goal of machine learning research is to build an intelligent dialog agent. Most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). This kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. In this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. We study this setup in two domains: the bAbI dataset of (Weston et al., 2015) and large-scale question answering from (Dodge et al., 2015). We evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. In particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all."
"Generalized Probabilistic Logic (GPL) is a temporal logic, based on the modal mu-calculus, for specifying properties of reactive probabilistic systems. We explore XPL, an extension to GPL allowing the semantics of nondeterminism present in Markov decision processes (MDPs). XPL is expressive enough that a number of independently studied problems--- such as termination of Recursive MDPs (RMDPs), PCTL* model checking of MDPs, and reachability for Branching MDPs--- can all be cast as model checking over XPL. Termination of multi-exit RMDPs is undecidable; thus, model checking in XPL is undecidable in general. We define a subclass, called separable XPL, for which model checking is decidable. Decidable problems such as termination of 1-exit RMDPs, PCTL* model checking of MDPs, and reachability for Branching MDPs can be reduced to model checking separable XPL. Thus, XPL forms a uniform framework for studying problems involving systems with non-deterministic and probabilistic behaviors, while separable XPL provides a way to solve decidable fragments of these problems."
"We propose a cellular architecture that combines multiuser MIMO (MU-MIMO) downlink with opportunistic use of unlicensed ISM bands to establish device-to-device (D2D) cooperation. The architecture consists of a physical-layer cooperation scheme based on forming downlink virtual MIMO channels through D2D relaying, and a novel resource allocation strategy for such D2D-enabled networks. We prove the approximate optimality of the physical-layer scheme, and demonstrate that such cooperation boosts the effective SNR of the weakest user in the system, especially in the many-user regime, due to multiuser diversity. To harness this physical-layer scheme, we formulate the cooperative user scheduling and relay selection problem using the network utility maximization framework. For such a cooperative network, we propose a novel utility metric that jointly captures fairness in throughput and the cost of relaying in the system. We propose a joint user scheduling and relay selection algorithm, which we prove to be asymptotically optimal. We study the architecture through system-level simulations over a wide range of scenarios. The highlight of these simulations is an approximately $6\times$ improvement in data rate for cell-edge (bottom fifth-percentile) users (over the state-of-the-art) while still improving the overall throughput, and taking into account various system constraints."
"There has been intensive work on the parameterized complexity of the typically NP-hard task to edit undirected graphs into graphs fulfilling certain given vertex degree constraints. In this work, we lift the investigations to the case of directed graphs; herein, we focus on arc insertions. To this end, our general two-stage framework consists of efficiently solving a problem-specific number problem transferring its solution to a solution for the graph problem by applying flow computations. In this way, we obtain fixed-parameter tractability and polynomial kernelizability results, with the central parameter being the maximum vertex in- or outdegree of the output digraph. Although there are certain similarities with the much better studied undirected case, the flow computation used in the directed case seems not to work for the undirected case while f-factor computations as used in the undirected case seem not to work for the directed case."
"We propose behavioral specification theories for most equivalences in the linear-time--branching-time spectrum. Almost all previous work on specification theories focuses on bisimilarity, but there is a clear interest in specification theories for other preorders and equivalences. We show that specification theories for preorders cannot exist and develop a general scheme which allows us to define behavioral specification theories, based on disjunctive modal transition systems, for most equivalences in the linear-time--branching-time spectrum."
"Random resolution, defined by Buss, Kolodziejczyk and Thapen (JSL, 2014), is a sound propositional proof system that extends the resolution proof system by the possibility to augment any set of initial clauses by a set of randomly chosen clauses (modulo a technical condition). We show how to apply the general feasible interpolation theorem for semantic derivations of Krajicek (JSL, 1997) to random resolution. As a consequence we get a lower bound for random resolution refutations of the clique-coloring formulas."
"Variation graphs, which represent genetic variation within a population, are replacing sequences as reference genomes. Path indexes are one of the most important tools for working with variation graphs. They generalize text indexes to graphs, allowing one to find the paths matching the query string. We propose using de Bruijn graphs as path indexes, compressing them by merging redundant subgraphs, and encoding them with the Burrows-Wheeler transform. The resulting fast, space-efficient, and versatile index is used in the variation graph toolkit vg."
"In biomedical imaging reliable segmentation of objects (e.g. from small cells up to large organs) is of fundamental importance for automated medical diagnosis. New approaches for multi-scale segmentation can considerably improve performance in case of natural variations in intensity, size and shape. This paper aims at segmenting objects of interest based on shape contours and automatically finding multiple objects with different scales. The overall strategy of this work is to combine nonlinear segmentation with scales spaces and spectral decompositions recently introduced in literature. For this we generalize a variational segmentation model based on total variation using Bregman distances to construct an inverse scale space. This offers the new model to be accomplished by a scale analysis approach based on a spectral decomposition of the total variation. As a result we obtain a very efficient, (nearly) parameter-free multiscale segmentation method that comes with an adaptive regularization parameter choice. The added benefit of our method is demonstrated by systematic synthetic tests and its usage in a new biomedical toolbox for identifying and classifying circulating tumor cells. Due to the nature of nonlinear diffusion underlying, the mathematical concepts in this work offer promising extensions to nonlocal classification problems."
"We present a sparse multidimensional FFT randomized algorithm (sMFFT) for positive real vectors. The algorithm works in any fixed dimension, requires an (almost)-optimal number of samples ($\mathcal{O}\left( R \log\left( \frac{N}{R} \right ) \right )$) and runs in $\mathcal{O}\left( R \log(R) \log\left( \frac{N}{R} \right ) \right )$ complexity (where $N$ is the size of the vector and $R$ the number of nonzeros). It is stable to noise and exhibits an exponentially small probability of failure."
"Verification and validation of agentic behavior have been suggested as important research priorities in efforts to reduce risks associated with the creation of general artificial intelligence (Russell et al 2015). In this paper we question the appropriateness of using language of certainty with respect to efforts to manage that risk. We begin by establishing a very general formalism to characterize agentic behavior and to describe standards of acceptable behavior. We show that determination of whether an agent meets any particular standard is not computable. We discuss the extent of the burden associated with verification by manual proof and by automated behavioral governance. We show that to ensure decidability of the behavioral standard itself, one must further limit the capabilities of the agent. We then demonstrate that if our concerns relate to outcomes in the physical world, attempts at validation are futile. Finally, we show that layered architectures aimed at making these challenges tractable mistakenly equate intentions with actions or outcomes, thereby failing to provide any guarantees. We conclude with a discussion of why language of certainty should be eradicated from the conversation about the safety of general artificial intelligence."
"The paper studies recoverability of missing values for sequences in a pathwise setting without probabilistic assumptions. This setting is oriented on a situation where the underlying sequence is considered as a sole sequence rather than a member of an ensemble with known statistical properties. Sufficient conditions of recoverability are obtained; it is shown that sequences are recoverable if there is a certain degree of degeneracy of the Z-transforms. We found that, in some cases, this degree can be measured as the number of the derivatives of Z-transform vanishing at a point. For processes with non-degenerate Z-transform, an optimal recovering based on the projection on a set of recoverable sequences is suggested. Some robustness of the solution with respect to noise contamination and truncation is established."
"The alternating direction method of multipliers (ADMM) is a powerful optimization solver in machine learning. Recently, stochastic ADMM has been integrated with variance reduction methods for stochastic gradient, leading to SAG-ADMM and SDCA-ADMM that have fast convergence rates and low iteration complexities. However, their space requirements can still be high. In this paper, we propose an integration of ADMM with the method of stochastic variance reduced gradient (SVRG). Unlike another recent integration attempt called SCAS-ADMM, the proposed algorithm retains the fast convergence benefits of SAG-ADMM and SDCA-ADMM, but is more advantageous in that its storage requirement is very low, even independent of the sample size $n$. We also extend the proposed method for nonconvex problems, and obtain a convergence rate of $O(1/T)$. Experimental results demonstrate that it is as fast as SAG-ADMM and SDCA-ADMM, much faster than SCAS-ADMM, and can be used on much bigger data sets."
"How can we optimally trade extra computing power to reduce the communication load in distributed computing? We answer this question by characterizing a fundamental tradeoff relationship between computation and communication in distributed computing, i.e., the two are inverse-linearly proportional to each other.   More specifically, a general distributed computing framework, motivated by commonly used structures like MapReduce, is considered, where the goal is to compute $Q$ arbitrary output functions from $N$ input files, by decomposing the overall computation into computing a set of ""Map"" and ""Reduce"" functions distributedly across $K$ computing nodes. A coded scheme, named ""Coded Distributed Computing"" (CDC), is proposed to demonstrate that increasing the computation load of the Map phase by a factor of $r$ (i.e., evaluating each Map function at $r$ carefully chosen nodes) can create novel coding opportunities in the data shuffling phase that reduce the communication load by the same factor.   An information-theoretic lower bound on the communication load is also provided, which matches the communication load achieved by the CDC scheme. As a result, the optimal computation-communication tradeoff in distributed computing is exactly characterized."
"In this paper, we propose a Double Thompson Sampling (D-TS) algorithm for dueling bandit problems. As indicated by its name, D-TS selects both the first and the second candidates according to Thompson Sampling. Specifically, D-TS maintains a posterior distribution for the preference matrix, and chooses the pair of arms for comparison by sampling twice from the posterior distribution. This simple algorithm applies to general Copeland dueling bandits, including Condorcet dueling bandits as its special case. For general Copeland dueling bandits, we show that D-TS achieves $O(K^2 \log T)$ regret. For Condorcet dueling bandits, we further simplify the D-TS algorithm and show that the simplified D-TS algorithm achieves $O(K \log T + K^2 \log \log T)$ regret. Simulation results based on both synthetic and real-world data demonstrate the efficiency of the proposed D-TS algorithm."
"The proliferation of Social Network Sites (SNSs) has greatly reformed the way of information dissemination, but also provided a new venue for hosts with impure motivations to disseminate malicious information. Social trust is the basis for information dissemination in SNSs. Malicious hosts judiciously and dynamically make the balance between maintaining its social trust and selfishly maximizing its malicious gain over a long time-span. Studying the optimal response strategies for each malicious host could assist to design the best system maneuver so as to achieve the targeted level of overall malicious activities. In this paper, we propose an interaction-based social trust model, and formulate the maximization of long-term malicious gains of multiple competing hosts as a non-cooperative differential game. Through rigorous analysis, optimal response strategies are identified and the best system maneuver mechanism is presented. Extensive numerical studies further verify the analytical results."
"The ability to reuse or transfer knowledge from one task to another in lifelong learning problems, such as Minecraft, is one of the major challenges faced in AI. Reusing knowledge across tasks is crucial to solving tasks efficiently with lower sample complexity. We provide a Reinforcement Learning agent with the ability to transfer knowledge by learning reusable skills, a type of temporally extended action (also known as Options (Sutton et. al. 1999)). The agent learns reusable skills to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem. These reusable skills, which we refer to as Deep Skill Networks (DSNs), are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture. The H-DRLN, a hierarchical extension of Deep Q-Networks, learns to efficiently solve tasks by reusing knowledge from previously learned DSNs. The DSNs are incorporated into the H-DRLN using two techniques: (1) a DSN array and (2) skill distillation, our novel variation of policy distillation (Rusu et. al. 2015) for learning skills. Skill distillation enables the H-DRLN to scale in lifelong learning, by accumulating knowledge and encapsulating multiple reusable skills into a single distilled network. The H-DRLN exhibits superior performance and lower learning sample complexity (by taking advantage of temporally extended actions) compared to the regular Deep Q Network (Mnih et. al. 2015) in sub-domains of Minecraft. We also show the potential to transfer knowledge between related Minecraft tasks without any additional learning."
"For any sufficiently strong theory of arithmetic, the set of Diophantine equations provably unsolvable in the theory is algorithmically undecidable, as a consequence of the MRDP theorem. In contrast, we show decidability of Diophantine equations provably unsolvable in Robinson's arithmetic Q. The argument hinges on an analysis of a particular class of equations, hitherto unexplored in Diophantine literature. We also axiomatize the universal fragment of Q in the process."
"This paper describes an efficient approach to constructing a resultant polyline with a minimum number of segments and arcs. While fitting an arc can be done with complexity O(1) (see [1] and [2]), the main complexity is in checking that the resultant arc is within the specified tolerance. There are additional tests to check for the ends and for changes in direction (see [3, section 3] and [4, sections II.C and II.D]). However, the most important part in reducing complexity is the ability to subdivide the polyline in order to limit the number of arc fittings [2]. The approach described in this paper finds a compressed polyline with a minimum number of segments and arcs."
"Condorcet's Jury Theorem has been invoked for ensemble classifiers to indicate that the combination of many classifiers can have better predictive performance than a single classifier. Such a theoretical underpinning is unknown for consensus clustering. This article extends Condorcet's Jury Theorem to the mean partition approach under the additional assumptions that a unique ground-truth partition exists and sample partitions are drawn from a sufficiently small ball containing the ground-truth. As an implication of practical relevance, we question the claim that the quality of consensus clustering depends on the diversity of the sample partitions. Instead, we conjecture that limiting the diversity of the mean partitions is necessary for controlling the quality."
The capacity of classical channels is convex. This is not the case for the quantum capacity of a channel: the capacity of a mixture of different quantum channels exceeds the mixture of the individual capacities and thus is non-convex. Here we show that this effect goes beyond the quantum capacity and holds for the private and classical environment-assisted capacities of quantum channels.
"Localizing a radiant source is a widespread problem to many scientific and technological research areas. E.g. localization based on range measurements stays at the core of technologies like radar, sonar and wireless sensors networks. In this manuscript we study in depth the model for source localization based on range measurements obtained from the source signal, from the point of view of algebraic geometry. In the case of three receivers, we find unexpected connections between this problem and the geometry of Kummer's and Cayley's surfaces. Our work gives new insights also on the localization based on range differences."
"Arguments about linearizability of a concurrent data structure are typically carried out by specifying the linearization points of the data structure's procedures. Proofs that use such specifications are often cumbersome as the linearization points' position in time can be dynamic, non-local and non-regional: it can depend on the interference, run-time values and events from the past, or even future, appear in procedures other than the one considered, and might be only determined after the considered procedure has terminated. In this paper we propose a new method, based on a Hoare-style logic, for reasoning about concurrent objects with such linearization points.   We embrace the dynamic nature of linearization points, and encode it as part of the data structure's auxiliary state, so that it can be dynamically modified in place by auxiliary code, as needed when some appropriate run-time event occurs. We name the idea linking-in-time, because it reduces temporal reasoning to spatial reasoning. For example, modifying a temporal position of a linearization point can be modeled similarly to a pointer update in a heap. We illustrate the method by verifying an intricate optimal snapshot algorithm due to Jayanti."
"The accuracy of information retrieval systems is often measured using complex non-decomposable loss functions such as the average precision (AP) or the normalized discounted cumulative gain (NDCG). Given a set of positive (relevant) and negative (non-relevant) samples, the parameters of a retrieval system can be estimated using a rank SVM framework, which minimizes a regularized convex upper bound on the empirical loss. However, the high computational complexity of loss-augmented inference, which is required to learn a rank SVM, prohibits its use in large training datasets. To alleviate this defficiency, we present a novel quicksort avored algorithm for a large class of nondecomposable loss functions. We provide a complete characterization of the loss functions that are amenable to our algorithm. Furthermore, we prove that no comparison based algorithm can improve upon the computational complexity of our approach asymptotically. We demonstrate that it is possible to reduce the constant factors of the complexity by exploiting the special structure of the AP loss. Using the PASCAL VOC action recognition and object detection datasets, we show that our approach provides significantly better results than baseline methods that use a simpler decomposable loss in comparable runtime."
"In the private values single object auction model, we construct a satisfactory mechanism - a symmetric, dominant strategy incentive compatible, and budget-balanced mechanism. Our mechanism allocates the object to the highest valued agent with more than 99% probability provided there are at least 14 agents. It is also ex-post individually rational. We show that our mechanism is optimal in a restricted class of satisfactory ranking mechanisms. Since achieving efficiency through a dominant strategy incentive compatible and budget-balanced mechanism is impossible in this model, our results illustrate the limits of this impossibility."
"Direct load control of a heterogeneous cluster of residential demand flexibility sources is a high-dimensional control problem with partial observability. This work proposes a novel approach that uses a convolutional neural network to extract hidden state-time features to mitigate the curse of partial observability. More specific, a convolutional neural network is used as a function approximator to estimate the state-action value function or Q-function in the supervised learning step of fitted Q-iteration. The approach is evaluated in a qualitative simulation, comprising a cluster of thermostatically controlled loads that only share their air temperature, whilst their envelope temperature remains hidden. The simulation results show that the presented approach is able to capture the underlying hidden features and successfully reduce the electricity cost the cluster."
"Large music content libraries often comprise multiple versions of a piece of music. To establish a link between different versions, automatic music alignment methods map each position in one version to a corresponding position in another version. Due to the leeway in interpreting a piece, any two versions can differ significantly, for example, in terms of local tempo, articulation, or playing style. For a given pair of versions, these differences can be significant such that even state-of-the-art methods fail to identify a correct alignment. In this paper, we present a novel method that increases the robustness for difficult to align cases. Instead of aligning only pairs of versions as done in previous methods, our method aligns multiple versions in a joint manner. This way, the alignment can be computed by comparing each version not only with one but with several versions, which stabilizes the comparison and leads to an increase in alignment robustness. Using recordings from the Mazurka Project, the alignment error for our proposed method was 14% lower on average compared to a state-of-the-art method, with significantly less outliers (standard deviation 53% lower)."
"In the past, manually re-drawing an image in a certain artistic style required a professional artist and a long time. Doing this for a video sequence single-handed was beyond imagination. Nowadays computers provide new possibilities. We present an approach that transfers the style from one image (for example, a painting) to a whole video sequence. We make use of recent advances in style transfer in still images and propose new initializations and loss functions applicable to videos. This allows us to generate consistent and stable stylized video sequences, even in cases with large motion and strong occlusion. We show that the proposed method clearly outperforms simpler baselines both qualitatively and quantitatively."
"Parkinson's disease (PD) is a slowly progressing neurodegenerative disease with early manifestation of motor signs. Objective measurements of motor signs are of vital importance for diagnosing, monitoring and developing disease modifying therapies, particularly for the early stages of the disease when putative neuroprotective treatments could stop neurodegeneration. Current medical practice has limited tools to routinely monitor PD motor signs with enough frequency and without undue burden for patients and the healthcare system. In this paper, we present data indicating that the routine interaction with computer keyboards can be used to detect motor signs in the early stages of PD. We explore a solution that measures the key hold times (the time required to press and release a key) during the normal use of a computer without any change in hardware and converts it to a PD motor index. This is achieved by the automatic discovery of patterns in the time series of key hold times using an ensemble regression algorithm. This new approach discriminated early PD groups from controls with an AUC = 0.81 (n = 42/43; mean age = 59.0/60.1; women = 43%/60%;PD/controls). The performance was comparable or better than two other quantitative motor performance tests used clinically: alternating finger tapping (AUC = 0.75) and single key tapping (AUC = 0.61)."
"Understanding 3D object structure from a single image is an important but difficult task in computer vision, mostly due to the lack of 3D object annotations in real images. Previous work tackles this problem by either solving an optimization task given 2D keypoint positions, or training on synthetic data with ground truth 3D information. In this work, we propose 3D INterpreter Network (3D-INN), an end-to-end framework which sequentially estimates 2D keypoint heatmaps and 3D object structure, trained on both real 2D-annotated images and synthetic 3D data. This is made possible mainly by two technical innovations. First, we propose a Projection Layer, which projects estimated 3D structure to 2D space, so that 3D-INN can be trained to predict 3D structural parameters supervised by 2D annotations on real images. Second, heatmaps of keypoints serve as an intermediate representation connecting real and synthetic data, enabling 3D-INN to benefit from the variation and abundance of synthetic 3D objects, without suffering from the difference between the statistics of real and synthesized images due to imperfect rendering. The network achieves state-of-the-art performance on both 2D keypoint estimation and 3D structure recovery. We also show that the recovered 3D information can be used in other vision applications, such as 3D rendering and image retrieval."
We establish unconditionally that for every integer $k \geq 1$ there is a language $L \in \mbox{P}$ such that it is consistent with Cook's theory PV that $L \notin Size(n^k)$. Our argument is non-constructive and does not provide an explicit description of this language.
"Causal inference concerns the identification of cause-effect relationships between variables. However, often only linear combinations of variables constitute meaningful causal variables. For example, recovering the signal of a cortical source from electroencephalography requires a well-tuned combination of signals recorded at multiple electrodes. We recently introduced the MERLiN (Mixture Effect Recovery in Linear Networks) algorithm that is able to recover, from an observed linear mixture, a causal variable that is a linear effect of another given variable. Here we relax the assumption of this cause-effect relationship being linear and present an extended algorithm that can pick up non-linear cause-effect relationships. Thus, the main contribution is an algorithm (and ready to use code) that has broader applicability and allows for a richer model class. Furthermore, a comparative analysis indicates that the assumption of linear cause-effect relationships is not restrictive in analysing electroencephalographic data."
"Proximity based applications are becoming fast growing markets suggesting that Device-to-Device (D2D) communications is becoming an essential part of future mobile data networks. We propose scalable admission and power control methods for D2D communications underlay cellular networks to increase the reuse of frequency resources and thus network capacity while maintaining QoS to all users. The aim of the proposed methods is to maximize the number of D2D links under QoS constraints, therefore maximizing network frequency reuse, while considering different levels of complexity and available channel state information (CSI) in a multi-cell environment. Numerical results show that by using D2D and the proposed multi-cell interference coordination and low power transmission method, the network spectral efficiency can be increased by as much as ten times, while low outage probability can be assured to provided QoS for all users."
"The implementation of efficient multigrid preconditioners for elliptic partial differential equations (PDEs) is a challenge due to the complexity of the resulting algorithms and corresponding computer code. For sophisticated finite element discretisations on unstructured grids an efficient implementation can be very time consuming and requires the programmer to have in-depth knowledge of the mathematical theory, parallel computing and optimisation techniques on manycore CPUs. In this paper we show how the development of bespoke multigrid preconditioners can be simplified significantly by using a framework which allows the expression of the each component of the algorithm at the correct abstraction level. Our approach (1) allows the expression of the finite element problem in a language which is close to the mathematical formulation of the problem, (2) guarantees the automatic generation and efficient execution of parallel optimised low-level computer code and (3) is flexible enough to support different abstraction levels and give the programmer control over details of the preconditioner. We use the composable abstractions of the Firedrake/PyOP2 package to demonstrate the efficiency of this approach for the solution of strongly anisotropic PDEs in atmospheric modelling. The weak formulation of the PDE is expressed in Unified Form Language (UFL) and the lower PyOP2 abstraction layer allows the manual design of computational kernels for a bespoke geometric multigrid preconditioner. We compare the performance of this preconditioner to a single-level method and hypre's BoomerAMG algorithm. The Firedrake/PyOP2 code is inherently parallel and we present a detailed performance analysis for a single node (24 cores) on the ARCHER supercomputer. Our implementation utilises a significant fraction of the available memory bandwidth and shows very good weak scaling on up to 6,144 compute cores."
"In this paper, we introduce several new schemes for calculation of discrete wavelet transforms of images. These schemes reduce the number of steps and, as a consequence, allow to reduce the number of synchronizations on parallel architectures. As an additional useful property, the proposed schemes can reduce also the number of arithmetic operations. The schemes are primarily demonstrated on CDF 5/3 and CDF 9/7 wavelets employed in JPEG 2000 image compression standard. However, the presented method is general, and it can be applied on any wavelet transform. As a result, our scheme requires only two memory barriers for 2-D CDF 5/3 transform compared to four barriers in the original separable form or three barriers in the non-separable scheme recently published. Our reasoning is supported by exhaustive experiments on high-end graphics cards."
"Multi-agent networked linear dynamic systems have attracted attention of researchers in power systems, intelligent transportation, and industrial automation. The agents might cooperatively optimize a global performance objective, resulting in social optimization, or try to satisfy their own selfish objectives using a noncooperative differential game. However, these solutions usually require significant feedback of data from system states to possibly distant control inputs, thus resulting in high cost of the underlying communication network. To enable economically-viable communication, a game-theoretic framework is proposed under the \textit{communication cost}, or \textit{sparsity}, constraint, given by the number of communicating state/control input pairs. As this constraint tightens, the system transitions from dense to sparse communication, providing the trade-off between dynamic system performance and information exchange. Moreover, using the proposed sparsity-constrained distributed social optimization and noncooperative game algorithms, we develop a method to allocate the costs of the communication infrastructure fairly and according to the agents' diverse needs for feedback and cooperation. Numerical results illustrate utilization of the proposed algorithms to enable and ensure economic fairness of wide-area control among power companies."
"Distributed Stream Processing Engines (DSPEs) target applications related to continuous computation, online machine learning and real-time query processing. DSPEs operate on high volume of data by applying lightweight operations on real-time and continuous streams. Such systems require clusters of hundreds of machine for their deployment. Streaming applications come with various requirements, i.e., low-latency, high throughput, scalability and high availability. In this survey, we study the fault tolerance problem for DSPEs. We discuss fault tolerance techniques that are used in modern stream processing engines that are Storm, S4, Samza, SparkStreaming and MillWheel. Further, we give insight on fault tolerance approaches that we categorize as active replication, passive replication and upstream backup. Finally, we discuss implications of the fault tolerance techniques for different streaming application requirements."
"The famous shortest path tree lemma states that, for any node $s$ in a graph $G = (V, E)$, there is a subgraph on $O(n)$ edges that preserves all distances between node pairs in the set $\{s\} \times V$. A very basic question in distance sketching research, with applications to other problems in the field, is to categorize when else graphs admit sparse subgraphs that preserve distances between a set $P$ of $p$ node pairs, where $P$ has some different structure than $\{s\} \times V$ or possibly no guaranteed structure at all. Trivial lower bounds of a path or a clique show that such a subgraph will need $\Omega(n+p)$ edges in the worst case. The question is then to determine when these trivial lower bounds are sharp; that is, when do graphs have \emph{linear size distance preservers} on $O(n + p)$ edges?   In this paper, we show:   (1) All $G, P$ has a distance preserver on $O(n)$ edges whenever $p = O(n^{1/3})$, even if $G$ is directed and/or weighted. These are the first nontrivial preservers of size $O(n)$ known for directed graphs.   (2) All $G, P$ has a distance preserver on $O(p)$ edges whenever $p = \Omega\left(\frac{n^2}{rs(n)}\right)$, and $G$ is undirected and unweighted. Here, $rs(n)$ is the Ruzsa-Szemer{\' e}di function from combinatoric graph theory. These are the first nontrivial preservers of size $O(p)$ known in any setting.   (3) To preserve distances within a subset of $s$ nodes in a graph, $\omega(s^2)$ edges are sometimes needed when $s = n^{2/3 - \epsilon}$ even if $G$ is undirected and unweighted. For weighted graphs, the range of this lower bound improves to $s = o(n^{2/3})$.   An interesting technical contribution in this paper is a new method for ""lazily"" breaking ties between equally short paths in a graph, which allows us to draw our new connections between distance sketching and the Ruzsa-Szemer{\' e}di problem."
"Humans demonstrate remarkable abilities to predict physical events in complex scenes. Two classes of models for physical scene understanding have recently been proposed: ""Intuitive Physics Engines"", or IPEs, which posit that people make predictions by running approximate probabilistic simulations in causal mental models similar in nature to video-game physics engines, and memory-based models, which make judgments based on analogies to stored experiences of previously encountered scenes and physical outcomes. Versions of the latter have recently been instantiated in convolutional neural network (CNN) architectures. Here we report four experiments that, to our knowledge, are the first rigorous comparisons of simulation-based and CNN-based models, where both approaches are concretely instantiated in algorithms that can run on raw image inputs and produce as outputs physical judgments such as whether a stack of blocks will fall. Both approaches can achieve super-human accuracy levels and can quantitatively predict human judgments to a similar degree, but only the simulation-based models generalize to novel situations in ways that people do, and are qualitatively consistent with systematic perceptual illusions and judgment asymmetries that people show."
"We formulate an info-clustering paradigm based on a multivariate mutual information measure that naturally extends Shannon's mutual information between two random variables to the multivariate case involving more than two random variables. With proper model reductions, we show that the paradigm can be applied to study the human genome and connectome in a more meaningful way than the conventional algorithmic approach. Not only can it provide justifications and refinements to some existing techniques, but it also inspires new computationally feasible solutions."
"The Kalman filter is an established tool for the analysis of dynamic systems with normally distributed noise, and it has been successfully applied in numerous application areas. It provides sequentially calculated estimates of the system states along with a corresponding covariance matrix. For nonlinear systems, the extended Kalman filter is often used which is derived from the Kalman filter by linearization around the current estimate. A key issue in metrology is the evaluation of the uncertainty associated with the Kalman filter state estimates. The ""Guide to the Expression of Uncertainty in Measurements"" (GUM) and its supplements serve as the de facto standard for uncertainty evaluation in metrology. We explore the relationship between the covariance matrix produced by the Kalman filter and a GUM-compliant uncertainty analysis. In addition, also the results of a Bayesian analysis are considered. For the case of linear systems with known system matrices, we show that all three approaches are compatible. When the system matrices are not precisely known, however, or when the system is nonlinear, this equivalence breaks down and different results can be reached then. Though for precisely known nonlinear systems the result of the extended Kalman filter still corresponds to the linearized uncertainty propagation of GUM. The extended Kalman filter can suffer from linearization and convergence errors. These disadvantages can be avoided to some extent by applying Monte Carlo procedures, and we propose such a method which is GUM-compliant and can also be applied online during the estimation. We illustrate all procedures in terms of a two-dimensional dynamic system and compare the results with those obtained by particle filtering, which has been proposed for the approximate calculation of a Bayesian solution. Finally, we give some recommendations based on our findings."
"We present an algorithm for the statistical learning setting with a bounded exp-concave loss in $d$ dimensions that obtains excess risk $O(d \log(1/\delta)/n)$ with probability at least $1 - \delta$. The core technique is to boost the confidence of recent in-expectation $O(d/n)$ excess risk bounds for empirical risk minimization (ERM), without sacrificing the rate, by leveraging a Bernstein condition which holds due to exp-concavity. We also show that with probability $1 - \delta$ the standard ERM method obtains excess risk $O(d (\log(n) + \log(1/\delta))/n)$. We further show that a regret bound for any online learner in this setting translates to a high probability excess risk bound for the corresponding online-to-batch conversion of the online learner. Lastly, we present two high probability bounds for the exp-concave model selection aggregation problem that are quantile-adaptive in a certain sense. The first bound is a purely exponential weights type algorithm, obtains a nearly optimal rate, and has no explicit dependence on the Lipschitz continuity of the loss. The second bound requires Lipschitz continuity but obtains the optimal rate."
"We give an overview of the diverse electoral systems used in local, national, or super-national elections around the world. We discuss existing methods for selecting single and multiple winners and give real-world examples for some more elaborate systems. Eventually, we elaborate on some of the better known strengths and weaknesses of various methods from both the theoretical and practical points of view."
"MapReduce is a popular programming model for data parallel computation. In MapReduce, the reducer produces an output from a list of inputs. Due to the scheduling policy of the platform, the inputs may arrive at the reducers in different order. The commutativity problem of reducers asks if the output of a reducer is independent of the order of its inputs. Although the problem is undecidable in general, the MapReduce programs in practice are usually used for data analytics and thus require very simple control flow. By exploiting the simplicity, we propose a programming language for reducers where the commutativity problem is decidable. The main idea of the reducer language is to separate the control and data flow of programs and disallow arithmetic operations in the control flow. The decision procedure for the commutativity problem is obtained through a reduction to the equivalence problem of streaming numerical transducers (SNTs), a novel automata model over infinite alphabets introduced in this paper. The design of SNTs is inspired by streaming transducers (Alur and Cerny, POPL 2011). Nevertheless, the two models are intrinsically different since the outputs of SNTs are integers while those of streaming transducers are data words. The decidability of the equivalence of SNTs is achieved with an involved combinatorial analysis of the evolvement of the values of the integer variables during the runs of SNTs."
"In this paper, we explore reduced-connectivity radio frequency (RF) switching networks for reducing the analog hardware complexity and switching power losses in antenna selection (AS) systems. In particular, we analyze different hardware architectures for implementing the RF switching matrices required in AS designs with a reduced number of RF chains. We explicitly show that fully-flexible switching matrices, which facilitate the selection of any possible subset of antennas and attain the maximum theoretical sum rates of AS, present numerous drawbacks such as the introduction of significant insertion losses, particularly pronounced in massive multiple-input multiple-output (MIMO) systems. Since these disadvantages make fully-flexible switching suboptimal in the energy efficiency sense, we further consider partially-connected switching networks as an alternative switching architecture with reduced hardware complexity, which we characterize in this work. In this context, we also analyze the impact of reduced switching connectivity on the analog hardware and digital signal processing of AS schemes that rely on channel power information. Overall, the analytical and simulation results shown in this paper demonstrate that partially-connected switching maximizes the energy efficiency of massive MIMO systems for a reduced number of RF chains, while fully-flexible switching offers sub-optimal energy efficiency benefits due to its significant switching power losses."
"We study the Bipartite Boolean Quadratic Programming Problem (BBQP) which is an extension of the well known Boolean Quadratic Programming Problem (BQP). Applications of the BBQP include mining discrete patterns from binary data, approximating matrices by rank-one binary matrices, computing the cut-norm of a matrix, and solving optimisation problems such as maximum weight biclique, bipartite maximum weight cut, maximum weight induced sub-graph of a bipartite graph, etc. For the BBQP, we first present several algorithmic components, specifically, hill climbers and mutations, and then show how to combine them in a high-performance metaheuristic. Instead of hand-tuning a standard metaheuristic to test the efficiency of the hybrid of the components, we chose to use an automated generation of a multi-component metaheuristic to save human time, and also improve objectivity in the analysis and comparisons of components. For this we designed a new metaheuristic schema which we call Conditional Markov Chain Search (CMCS). We show that CMCS is flexible enough to model several standard metaheuristics; this flexibility is controlled by multiple numeric parameters, and so is convenient for automated generation. We study the configurations revealed by our approach and show that the best of them outperforms the previous state-of-the-art BBQP algorithm by several orders of magnitude. In our experiments we use benchmark instances introduced in the preliminary version of this paper and described here, which have already become the de facto standard in the BBQP literature."
"Modeling data sharing in GPU programs is a challenging task because of the massive parallelism and complex data sharing patterns provided by GPU architectures. Better GPU caching efficiency can be achieved through careful task scheduling among different threads. Traditionally, in the field of parallel computing, graph partition models are used to model data communication and guide task scheduling. However, we discover that the previous methods are either inaccurate or expensive when applied to GPU programs. In this paper, we propose a novel task partition model that is accurate and gives rise to the development of fast and high quality task/data reorganization algorithms. We demonstrate the effectiveness of the proposed model by rigorous theoretical analysis of the algorithm bounds and extensive experimental analysis. The experimental results show that it achieves significant performance improvement across a representative set of GPU applications."
"Slow mixing is the central hurdle when working with Markov chains, especially those used for Monte Carlo approximations (MCMC). In many applications, it is only of interest to estimate the stationary expectations of a small set of functions, and so the usual definition of mixing based on total variation convergence may be too conservative. Accordingly, we introduce function-specific analogs of mixing times and spectral gaps, and use them to prove Hoeffding-like function-specific concentration inequalities. These results show that it is possible for empirical expectations of functions to concentrate long before the underlying chain has mixed in the classical sense, and we show that the concentration rates we achieve are optimal up to constants. We use our techniques to derive confidence intervals that are sharper than those implied by both classical Markov chain Hoeffding bounds and Berry-Esseen-corrected CLT bounds. For applications that require testing, rather than point estimation, we show similar improvements over recent sequential testing results for MCMC. We conclude by applying our framework to real data examples of MCMC, providing evidence that our theory is both accurate and relevant to practice."
"We study a Stackelberg game between a base station and a multi-antenna power beacon for wireless energy harvesting in a multiple sensor node scenario. Assuming imperfect CSI between the sensor nodes and the power beacon, we propose a utility function that is based on throughput non-outage probability at the base station. We provide an analytical solution for the equilibrium in case of a single sensor node. For the general case consisting of multiple sensor nodes, we provide upper and lower bounds on the power and price (players' strategies). We compare the bounds with solutions resulting from an exhaustive search and a relaxed semidefinite program, and find the upper bound to be tight."
"In this paper, we present an adaptation of the sequence-to-sequence model for structured output prediction in vision tasks. In this model the output variables for a given input are predicted sequentially using neural networks. The prediction for each output variable depends not only on the input but also on the previously predicted output variables. The model is applied to spatial localization tasks and uses convolutional neural networks (CNNs) for processing input images and a multi-scale deconvolutional architecture for making spatial predictions at each time step. We explore the impact of weight sharing with a recurrent connection matrix between consecutive predictions, and compare it to a formulation where these weights are not tied. Untied weights are particularly suited for problems with a fixed sized structure, where different classes of output are predicted in different steps. We show that chained predictions achieve top performing results on human pose estimation from single images and videos."
"Parallel Kinetic Monte Carlo (KMC) is a potent tool to simulate stochastic particle systems efficiently. However, despite literature on quantifying domain decomposition errors of the particle system for this class of algorithms in the short and in the long time regime, no study yet explores and quantifies the loss of time-reversibility in Parallel KMC. Inspired by concepts from non-equilibrium statistical mechanics, we propose the entropy production per unit time, or entropy production rate, given in terms of an observable and a corresponding estimator, as a metric that quantifies the loss of reversibility. Typically, this is a quantity that cannot be computed explicitly for Parallel KMC, which is why we develop a posteriori estimators that have good scaling properties with respect to the size of the system. Through these estimators, we can connect the different parameters of the scheme, such as the communication time step of the parallelization, the choice of the domain decomposition, and the computational schedule, with its performance in controlling the loss of reversibility. From this point of view, the entropy production rate can be seen both as an information criterion to compare the reversibility of different parallel schemes and as a tool to diagnose reversibility issues with a particular scheme. As a demonstration, we use Sandia Lab's SPPARKS software to compare different parallelization schemes and different domain (lattice) decompositions."
"The classical result of Vandermonde decomposition of positive semidefinite Toeplitz matrices can date back to the early twentieth century. It forms the basis of modern subspace and recent atomic norm methods for frequency estimation. In this paper, we study the Vandermonde decomposition in which the frequencies are restricted to lie in a given interval, referred to as frequency-selective Vandermonde decomposition. The existence and uniqueness of the decomposition are studied under explicit conditions on the Toeplitz matrix. The new result is connected by duality to the positive real lemma for trigonometric polynomials nonnegative on the same frequency interval. Its applications in the theory of moments and line spectral estimation are illustrated. In particular, it provides a solution to the truncated trigonometric $K$-moment problem. It is used to derive a primal semidefinite program formulation of the frequency-selective atomic norm in which the frequencies are known a priori to lie in a certain frequency band. Numerical examples are also provided."
"Cylindrical algebraic decomposition (CAD) is an important tool for working with polynomial systems, particularly quantifier elimination. However, it has complexity doubly exponential in the number of variables. The base algorithm can be improved by adapting to take advantage of any equational constraints (ECs): equations logically implied by the input. Intuitively, we expect the double exponent in the complexity to decrease by one for each EC. In ISSAC 2015 the present authors proved this for the factor in the complexity bound dependent on the number of polynomials in the input. However, the other term, that dependent on the degree of the input polynomials, remained unchanged.   In the present paper the authors investigate how CAD in the presence of ECs could be further refined using the technology of Groebner Bases to move towards the intuitive bound for polynomial degree."
"The rule-based OWL reasoning is to compute the deductive closure of an ontology by applying RDF/RDFS and OWL entailment rules. The performance of the rule-based OWL reasoning is often sensitive to the rule execution order. In this paper, we present an approach to enhancing the performance of the rule-based OWL reasoning on Spark based on a locally optimal executable strategy. Firstly, we divide all rules (27 in total) into four main classes, namely, SPO rules (5 rules), type rules (7 rules), sameAs rules (7 rules), and schema rules (8 rules) since, as we investigated, those triples corresponding to the first three classes of rules are overwhelming (e.g., over 99% in the LUBM dataset) in our practical world. Secondly, based on the interdependence among those entailment rules in each class, we pick out an optimal rule executable order of each class and then combine them into a new rule execution order of all rules. Finally, we implement the new rule execution order on Spark in a prototype called RORS. The experimental results show that the running time of RORS is improved by about 30% as compared to Kim & Park's algorithm (2015) using the LUBM200 (27.6 million triples)."
In this work we present a quantum algorithm for multiobjective combinatorial optimization. We show that the adiabatic algorithm of Farhi et al. [arXiv:quant-ph/0001106] can be used by mapping a multiobjective combinatorial optimization problem onto a Hamiltonian using a convex combination among objectives. We present mathematical properties of the eigenspectrum of the associated Hamiltonian and prove that the quantum adiabatic algorithm can find Pareto-optimal solutions provided certain convex combinations of objectives are used and the underlying multiobjective problem meets certain restrictions.
"We provide a numerical refutation of the developments of Fiorini et al. (2015)* for models with disjoint sets of descriptive variables. We also provide an insight into the meaning of the existence of a one-to-one linear map between solutions of such models.   *: Fiorini, S., S. Massar, S. Pokutta, H.R. Tiwary, and R. de Wolf (2015). Exponential Lower Bounds for Polytopes in Combinatorial Optimization. Journal of the ACM 62:2, Article No. 17."
"Identifying context-specific entity networks from aggregated data is an important task, arising often in bioinformatics and neuroimaging. Computationally, this task can be formulated as jointly estimating multiple different, but related, sparse Undirected Graphical Models (UGM) from aggregated samples across several contexts. Previous joint-UGM studies have mostly focused on sparse Gaussian Graphical Models (sGGMs) and can't identify context-specific edge patterns directly. We, therefore, propose a novel approach, SIMULE (detecting Shared and Individual parts of MULtiple graphs Explicitly) to learn multi-UGM via a constrained L1 minimization. SIMULE automatically infers both specific edge patterns that are unique to each context and shared interactions preserved among all the contexts. Through the L1 constrained formulation, this problem is cast as multiple independent subtasks of linear programming that can be solved efficiently in parallel. In addition to Gaussian data, SIMULE can also handle multivariate Nonparanormal data that greatly relaxes the normality assumption that many real-world applications do not follow. We provide a novel theoretical proof showing that SIMULE achieves a consistent result at the rate O(log(Kp)/n_{tot}). On multiple synthetic datasets and two biomedical datasets, SIMULE shows significant improvement over state-of-the-art multi-sGGM and single-UGM baselines."
"Suppose that a train is running along a railway network, starting from a designated origin, with the goal of reaching a designated destination. The network, however, is of a special nature: every time the train traverses a switch, the switch will change its position immediately afterwards. Hence, the next time the train traverses the same switch, the other direction will be taken, so that directions alternate with each traversal of the switch. Given a network with origin and destination, what is the complexity of deciding whether the train, starting at the origin, will eventually reach the destination? It is easy to see that this problem can be solved in exponential time, but we are not aware of any polynomial-time method. In this short paper, we prove that the problem is in NP $\cap$ coNP. This raises the question whether we have just failed to find a (simple) polynomial-time solution, or whether the complexity status is more subtle, as for some other well-known (two-player) graph games."
"The downlink coverage probability of a cellular network, when the base station locations are modelled by a Poisson point process (PPP), is known when the desired channel is Nakagami distributed with an integer shape parameter. However, for many interesting fading distributions such as Rician, Rician shadowing, $\kappa$-$\mu$, $\eta$-$\mu$, etc., the coverage probability is unknown. $\kappa$-$\mu$ shadowed fading is a generic fading distribution whose special cases are many of these popular distributions known so far. In this letter, we derive the coverage probability when the desired channel experiences $\kappa$-$\mu$ shadowed fading. Using numerical simulations, we verify our analytical expressions."
"Live multimedia streaming from mobile devices is rapidly gaining popularity but little is known about the QoE they provide. In this paper, we examine the Periscope service. We first crawl the service in order to understand its usage patterns. Then, we study the protocols used, the typical quality of experience indicators, such as playback smoothness and latency, video quality, and the energy consumption of the Android application."
"Concurrency control (CC) algorithms must trade off strictness for performance. In particular, serializable CC schemes generally pay higher cost to prevent anomalies, both in runtime overhead such as the maintenance of lock tables, and in efforts wasted by aborting transactions. We propose the serial safety net (SSN), a serializability-enforcing certifier which can be applied with minimal overhead on top of various CC schemes that offer higher performance but admit anomalies, such as snapshot isolation and read committed. The underlying CC mechanism retains control of scheduling and transactional accesses, while SSN tracks the resulting dependencies. At commit time, SSN performs an efficient validation test by examining only direct dependencies of the committing transaction to determine whether it can commit safely or must abort to avoid a potential dependency cycle.   SSN performs robustly for a variety of workloads. It maintains the characteristics of the underlying CC without biasing toward a certain type of transactions, though the underlying CC scheme might. Besides traditional OLTP workloads, SSN also efficiently handles emerging heterogeneous workloads which include a significant portion of long, read-mostly transactions. SSN can avoid tracking the vast majority of reads (thus reducing the overhead of serializability certification) and still produce serializable executions with little overhead. The dependency tracking and validation tests can be made fully parallel and latch-free on modern hardware with substantial core count and large main memory.   We demonstrate the efficiency, accuracy and robustness of SSN using extensive simulations and an implementation that overlays snapshot isolation in ERMIA, a memory-optimized OLTP engine. Evaluation results confirm that SSN is a promising approach to serializability with low abort rates and robust performance for various workloads."
"A new approach to computation of optimal policies for MDP (Markov decision process) models is introduced. The main idea is to solve not one, but an entire family of MDPs, parameterized by a weighting factor $\zeta$ that appears in the one-step reward function. For an MDP with $d$ states, the family of value functions $\{ h^*_\zeta : \zeta\in\Re\}$ is the solution to an ODE, $$ \frac{d}{d\zeta} h^*_\zeta = {\cal V}(h^*_\zeta) $$ where the vector field ${\cal V}\colon\Re^d\to\Re^d$ has a simple form, based on a matrix inverse.   This general methodology is applied to a family of average-cost optimal control models in which the one-step reward function is defined by Kullback-Leibler divergence. The motivation for this reward function in prior work is computation: The solution to the MDP can be expressed in terms of the Perron-Frobenius eigenvector for an associated positive matrix. The drawback with this approach is that no hard constraints on the control are permitted. It is shown here that it is possible to extend this framework to model randomness from nature that cannot be modified by the controller. Perron-Frobenius theory is no longer applicable -- the resulting dynamic programming equations appear as complex as a completely unstructured MDP model. Despite this apparent complexity, it is shown that this class of MDPs admits a solution via this new ODE technique. This approach is new and practical even for the simpler problem in which randomness from nature is absent."
"Neural correlates of movement planning onset and direction may be present in human electrocorticography in the signal dynamics of both motor and non-motor cortical regions. We use a three-stage model of jPCA reduced-rank hidden Markov model (jPCA-RR-HMM), regularized shrunken-centroid discriminant analysis (RDA), and LASSO regression to extract direction-sensitive planning information and movement onset in an upper-limb 3D isometric force task in a human subject. This mode achieves a relatively high true positive force-onset prediction rate of 60% within 250ms, and an above-chance 36% accuracy (17% chance) in predicting one of six planned 3D directions of isometric force using pre-movement signals. We also find direction-distinguishing information up to 400ms before force onset in the pre-movement signals, captured by electrodes placed over the limb-ipsilateral dorsal premotor regions. This approach can contribute to more accurate decoding of higher-level movement goals, at earlier timescales, and inform sensor placement. Our results also contribute to further understanding of the spatiotemporal features of human motor planning."
"This paper investigates the extent to which spectrum sharing in mmWave networks with multiple cellular operators is a viable alternative to traditional dedicated spectrum allocation. Specifically, we develop a general mathematical framework by which to characterize the performance gain that can be obtained when spectrum sharing is used, as a function of the underlying beamforming, operator coordination, bandwidth, and infrastructure sharing scenarios. The framework is based on joint beamforming and cell association optimization, with the objective of maximizing the long-term throughput of the users. Our asymptotic and non-asymptotic performance analyses reveal five key points: (1) spectrum sharing with light on-demand intra- and inter-operator coordination is feasible, especially at higher mmWave frequencies (for example, 73 GHz), (2) directional communications at the user equipment substantially alleviate the potential disadvantages of spectrum sharing (such as higher multiuser interference), (3) large numbers of antenna elements can reduce the need for coordination and simplify the implementation of spectrum sharing, (4) while inter-operator coordination can be neglected in the large-antenna regime, intra-operator coordination can still bring gains by balancing the network load, and (5) critical control signals among base stations, operators, and user equipment should be protected from the adverse effects of spectrum sharing, for example by means of exclusive resource allocation. The results of this paper, and their extensions obtained by relaxing some ideal assumptions, can provide important insights for future standardization and spectrum policy."
"Given a dataset of points in a metric space and an integer $k$, a diversity maximization problem requires determining a subset of $k$ points maximizing some diversity objective measure, e.g., the minimum or the average distance between two points in the subset. Diversity maximization is computationally hard, hence only approximate solutions can be hoped for. Although its applications are mainly in massive data analysis, most of the past research on diversity maximization focused on the sequential setting. In this work we present space and pass/round-efficient diversity maximization algorithms for the Streaming and MapReduce models and analyze their approximation guarantees for the relevant class of metric spaces of bounded doubling dimension. Like other approaches in the literature, our algorithms rely on the determination of high-quality core-sets, i.e., (much) smaller subsets of the input which contain good approximations to the optimal solution for the whole input. For a variety of diversity objective functions, our algorithms attain an $(\alpha+\epsilon)$-approximation ratio, for any constant $\epsilon>0$, where $\alpha$ is the best approximation ratio achieved by a polynomial-time, linear-space sequential algorithm for the same diversity objective. This improves substantially over the approximation ratios attainable in Streaming and MapReduce by state-of-the-art algorithms for general metric spaces. We provide extensive experimental evidence of the effectiveness of our algorithms on both real world and synthetic datasets, scaling up to over a billion points."
"A fruitful way of obtaining meaningful, possibly concrete, algorithmically random numbers is to consider a potential behaviour of a Turing machine and its probability with respect to a measure (or semi-measure) on the input space of binary codes. For example, Chaitin's Omega is a well known Martin-Loef random number that is obtained by considering the halting probability of a universal prefix-free machine. In the last decade, similar examples have been obtained for higher forms of randomness, i.e. randomness relative to strong oracles. In this work we obtain characterizations of the algorithmically random reals in higher randomness classes, as probabilities of certain events that can happen when an oracle universal machine runs probabilistically on a random oracle. Moreover we apply our analysis to different machine models, including oracle Turing machines, prefix-free machines, and models for infinite online computation. We find that in many cases the arithmetical complexity of a property is directly reflected in the strength of the algorithmic randomness of the probability with which it occurs, on any given universal machine. On the other hand, we point to many examples where this does not happen and the probability is a number whose algorithmic randomness is not the maximum possible (with respect to its arithmetical complexity). Finally we find that, unlike the halting probability of a universal machine, the probabilities of more complex properties like totality, cofinality, computability or completeness do not necessarily have the same Turing degree when they are defined with respect to different universal machines."
"We analyze the effects of relative increments of mutual information among the geographical, technological, and organizational distributions of firms on the relative augmentation of regional summary turnover in terms of synergies. How do increases in synergy in international cooperation affect regional turnover? The methodological contribution of this study is that we translate the synergy (abstractly measured in bits of information) into more familiar economic terms, such as turnover for the special case of domestic-foreign collaborations. The analysis is based on Norwegian data, as Norway is a small country with an open and export-oriented economy. Data for Norway is publicly available in great detail."
"The minimum mean-squared error (MMSE) is one of the most popular criteria for Bayesian estimation. Conversely, the signal-to-noise ratio (SNR) is a typical performance criterion in communications, radar, and generally detection theory. In this paper we first formalize an SNR criterion to design an estimator, and then we prove that there exists an equivalence between MMSE and maximum-SNR estimators, for any statistics. We also extend this equivalence to specific classes of suboptimal estimators, which are expressed by a basis expansion model (BEM). Then, by exploiting an orthogonal BEM for the estimator, we derive the MMSE estimator constrained to a given quantization resolution of the noisy observations, and we prove that this suboptimal MMSE estimator tends to the optimal MMSE estimator that uses an infinite resolution of the observation. Besides, we derive closed-form expressions for the mean-squared error (MSE) and for the SNR of the proposed suboptimal estimators, and we show that these expressions constitute tight, asymptotically exact, bounds for the optimal MMSE and maximum SNR."
"The question of how to parallelize the stochastic gradient descent (SGD) method has received much attention in the literature. In this paper, we focus instead on batch methods that use a sizeable fraction of the training set at each iteration to facilitate parallelism, and that employ second-order information. In order to improve the learning process, we follow a multi-batch approach in which the batch changes at each iteration. This can cause difficulties because L-BFGS employs gradient differences to update the Hessian approximations, and when these gradients are computed using different data points the process can be unstable. This paper shows how to perform stable quasi-Newton updating in the multi-batch setting, illustrates the behavior of the algorithm in a distributed computing platform, and studies its convergence properties for both the convex and nonconvex cases."
"In this paper, we address the distributed filtering and prediction of time-varying random fields represented by linear time-invariant (LTI) dynamical systems. The field is observed by a sparsely connected network of agents/sensors collaborating among themselves. We develop a Kalman filter type consensus+innovations distributed linear estimator of the dynamic field termed as Consensus+Innovations Kalman Filter. We analyze the convergence properties of this distributed estimator. We prove that the mean-squared error of the estimator asymptotically converges if the degree of instability of the field dynamics is within a pre-specified threshold defined as tracking capacity of the estimator. The tracking capacity is a function of the local observation models and the agent communication network. We design the optimal consensus and innovation gain matrices yielding distributed estimates with minimized mean-squared error. Through numerical evaluations, we show that, the distributed estimator with optimal gains converges faster and with approximately 3dB better mean-squared error performance than previous distributed estimators."
"Building discriminative representations for 3D data has been an important task in computer graphics and computer vision research. Convolutional Neural Networks (CNNs) have shown to operate on 2D images with great success for a variety of tasks. Lifting convolution operators to 3D (3DCNNs) seems like a plausible and promising next step. Unfortunately, the computational complexity of 3D CNNs grows cubically with respect to voxel resolution. Moreover, since most 3D geometry representations are boundary based, occupied regions do not increase proportionately with the size of the discretization, resulting in wasted computation. In this work, we represent 3D spaces as volumetric fields, and propose a novel design that employs field probing filters to efficiently extract features from them. Each field probing filter is a set of probing points --- sensors that perceive the space. Our learning algorithm optimizes not only the weights associated with the probing points, but also their locations, which deforms the shape of the probing filters and adaptively distributes them in 3D space. The optimized probing points sense the 3D space ""intelligently"", rather than operating blindly over the entire domain. We show that field probing is significantly more efficient than 3DCNNs, while providing state-of-the-art performance, on classification tasks for 3D object recognition benchmark datasets."
"In this paper, we introduce a new image representation based on a multilayer kernel machine. Unlike traditional kernel methods where data representation is decoupled from the prediction task, we learn how to shape the kernel with supervision. We proceed by first proposing improvements of the recently-introduced convolutional kernel networks (CKNs) in the context of unsupervised learning; then, we derive backpropagation rules to take advantage of labeled training data. The resulting model is a new type of convolutional neural network, where optimizing the filters at each layer is equivalent to learning a linear subspace in a reproducing kernel Hilbert space (RKHS). We show that our method achieves reasonably competitive performance for image classification on some standard ""deep learning"" datasets such as CIFAR-10 and SVHN, and also for image super-resolution, demonstrating the applicability of our approach to a large variety of image-related tasks."
"In this work, we study parameter tuning towards the M^2 metric, the standard metric for automatic grammar error correction (GEC) tasks. After implementing M^2 as a scorer in the Moses tuning framework, we investigate interactions of dense and sparse features, different optimizers, and tuning strategies for the CoNLL-2014 shared task. We notice erratic behavior when optimizing sparse feature weights with M^2 and offer partial solutions. We find that a bare-bones phrase-based SMT setup with task-specific parameter-tuning outperforms all previously published results for the CoNLL-2014 test set by a large margin (46.37% M^2 over previously 41.75%, by an SMT system with neural features) while being trained on the same, publicly available data. Our newly introduced dense and sparse features widen that gap, and we improve the state-of-the-art to 49.49% M^2."
"Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an $\epsilon$-ball around the observed data, which is only correct in the limit $\epsilon\!\rightarrow\!0$. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as $\epsilon\!\rightarrow\!0$, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior."
"We consider the problem of clustering partially labeled data from a minimal number of randomly chosen pairwise comparisons between the items. We introduce an efficient local algorithm based on a power iteration of the non-backtracking operator and study its performance on a simple model. For the case of two clusters, we give bounds on the classification error and show that a small error can be achieved from $O(n)$ randomly chosen measurements, where $n$ is the number of items in the dataset. Our algorithm is therefore efficient both in terms of time and space complexities. We also investigate numerically the performance of the algorithm on synthetic and real world data."
"The use of Bayesian methods in large-scale data settings is attractive because of the rich hierarchical models, uncertainty quantification, and prior specification they provide. Standard Bayesian inference algorithms are computationally expensive, however, making their direct application to large datasets difficult or infeasible. Recent work on scaling Bayesian inference has focused on modifying the underlying algorithms to, for example, use only a random data subsample at each iteration. We leverage the insight that data is often redundant to instead obtain a weighted subset of the data (called a coreset) that is much smaller than the original dataset. We can then use this small coreset in any number of existing posterior inference algorithms without modification. In this paper, we develop an efficient coreset construction algorithm for Bayesian logistic regression models. We provide theoretical guarantees on the size and approximation quality of the coreset -- both for fixed, known datasets, and in expectation for a wide class of data generative models. Crucially, the proposed approach also permits efficient construction of the coreset in both streaming and parallel settings, with minimal additional effort. We demonstrate the efficacy of our approach on a number of synthetic and real-world datasets, and find that, in practice, the size of the coreset is independent of the original dataset size. Furthermore, constructing the coreset takes a negligible amount of time compared to that required to run MCMC on it."
"In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks."
"Spectrum sharing mechanisms need to be carefully designed to enable inter-technology coexistence in the unlicensed bands, as these bands are an instance of a spectrum commons where heterogeneous technologies and deployments must coexist. Unlike in licensed bands, where multiple technologies could coexist only in a primary-secondary DSA mode, a spectrum commons offers competition opportunities between multiple dominant technologies, such as Wi-Fi and the recently proposed LTE in the 5 GHz unlicensed band. In this paper we systematically study the performance of different spectrum sharing schemes for inter-technology coexistence in a spectrum commons. Our contributions are threefold. First, we propose a general framework for comparative analysis of spectrum sharing mechanisms in time and frequency, by studying the effect of key constituent parameters. Second, we propose a novel throughput and interference model for inter-technology coexistence, integrating different distributed MAC sharing mechanisms at the same level of abstraction. Finally, we present a case study of IEEE 802.11n Wi-Fi and LTE in the 5 GHz unlicensed band, in order to obtain generalizable insight into coexistence in a spectrum commons. Our extensive simulation results show that LTE/Wi-Fi coexistence in the 5 GHz band can be ensured simply through channel selection schemes, such that time-sharing MAC mechanisms are irrelevant. We also show that, in the general co-channel case, the coexistence performance of MAC sharing mechanisms strongly depends on the interference coupling in the network, as determined by building shielding. We thus identify two regimes: (i) low interference coupling, e.g. residential indoor scenarios, where duty cycle mechanisms outperform sensing-based LBT mechanisms; and (ii) high interference coupling, e.g. open-plan indoor or outdoor hotspot scenarios, where LBT outperforms duty cycle mechanisms."
"In artificial neural networks, learning from data is a computationally demanding task in which a large number of connection weights are iteratively tuned through stochastic-gradient-based heuristic processes over a cost-function. It is not well understood how learning occurs in these systems, in particular how they avoid getting trapped in configurations with poor computational performance. Here we study the difficult case of networks with discrete weights, where the optimization landscape is very rough even for simple architectures, and provide theoretical and numerical evidence of the existence of rare - but extremely dense and accessible - regions of configurations in the network weight space. We define a novel measure, which we call the ""robust ensemble"" (RE), which suppresses trapping by isolated configurations and amplifies the role of these dense regions. We analytically compute the RE in some exactly solvable models, and also provide a general algorithmic scheme which is straightforward to implement: define a cost-function given by a sum of a finite number of replicas of the original cost-function, with a constraint centering the replicas around a driving assignment. To illustrate this, we derive several powerful new algorithms, ranging from Markov Chains to message passing to gradient descent processes, where the algorithms target the robust dense states, resulting in substantial improvements in performance. The weak dependence on the number of precision bits of the weights leads us to conjecture that very similar reasoning applies to more conventional neural networks. Analogous algorithmic schemes can also be applied to other optimization problems."
"We propose a pool-based non-parametric active learning algorithm for general metric spaces, called MArgin Regularized Metric Active Nearest Neighbor (MARMANN), which outputs a nearest-neighbor classifier. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of MARMANN is significantly lower than that of any passive learner with similar error guarantees. MARMANN is based on a generalized sample compression scheme, and a new label-efficient active model-selection procedure."
"Most of the semi-supervised learning methods developed so far use unlabeled data for regularization purposes under particular distributional assumptions such as the manifold assumption. On the other hand, recently developed methods of learning from positive and unlabeled data (PU learning) use unlabeled data for loss evaluation, i.e., label information is directly extracted from unlabeled data. In this paper, we extend PU learning to also incorporate negative data and propose a novel semi-supervised learning approach. We establish a generalization error bound for our novel method and show that the bound decreases with respect to the number of unlabeled data without the distributional assumptions that are required in existing semi-supervised learning methods. Through experiments, we demonstrate the usefulness of the proposed method."
"We introduce the Tucker Gaussian Process (TGP), a model for regression that regularises a Gaussian Process (GP) towards simpler regression functions for enhanced generalisation performance. We derive it using a novel approach to scalable GP learning, and show that our model is particularly well-suited to grid-structured data and problems where the dependence on covariates is close to being separable. A prime example is collaborative filtering, for which our model provides an effective GP based method that has a low-rank matrix factorisation at its core. We show that TGP generalises classical Bayesian matrix factorisation models, and goes beyond them to give a natural and elegant method for incorporating side information."
"A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 59,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a ""visual imagination"" of different futures based on different courses of action. Our experiments show that our proposed method produces more accurate video predictions both quantitatively and qualitatively, when compared to prior methods."
"A single human can be involved in a limited number of social relationships, and the distribution of strengths of such relationships shows significant skew. This skewness suggests that the costs and benefits of the social interactions required to bond with others (social grooming) depend on the social relationship strength: if they involved uniform costs and benefits, the distribution would not be skew. Here, we show that the cost of social grooming increases with the social relationship strength, and its gradient determines the structures of these relationships in society as evident from an analysis of data from six communication systems. This may be due to an increase in communication volumes, such as number of characters and duration of calls, along with an increase in the social relationship strength. We tested this hypothesis using an individual-based simulation where social grooming costs were assumed to increase linearly with the social relationship strength; this is the simplest assumption. The results indicated that this model fitted all data sets, i.e., it showed an explanation capacity for the phenomenon. Additionally, an analysis of this simulation suggested that the gradient of social grooming costs increases the width and shallowness of social relationships in its communication system."
"We study robust $H_\infty$ coherent-classical estimation for a class of physically realizable linear quantum systems with parameter uncertainties. Such a robust coherent-classical estimator, with or without coherent feedback, can yield better disturbance-to-error performance than the corresponding robust purely-classical estimator for an uncertain plant. Moreover, coherent feedback allows for such a robust coherent-classical estimator to be more robust to uncertainty in comparison to the robust classical-only estimator."
"This paper addresses the problem of natural image segmentation by extracting information from a multi-layer array which is constructed based on color, gradient, and statistical properties of the local neighborhoods in an image. A Gaussian Mixture Model (GMM) is used to improve the effectiveness of local spectral histogram features. Grouping these features leads to forming a rough initial over-segmented layer which contains coherent regions of pixels. The regions are merged by using two proposed functions for calculating the distance between two neighboring regions and making decisions about their merging. Extensive experiments are performed on the Berkeley Segmentation Dataset to evaluate the performance of our proposed method and compare the results with the recent state-of-the-art methods. The experimental results indicate that our method achieves higher level of accuracy for natural images compared to recent methods."
"CT protocol design and quality control would benefit from automated tools to estimate the quality of generated CT images. These tools could be used to identify erroneous CT acquisitions or refine protocols to achieve certain signal to noise characteristics. This paper investigates blind estimation methods to determine global signal strength and noise levels in chest CT images. Methods: We propose novel performance metrics corresponding to the accuracy of noise and signal estimation. We implement and evaluate the noise estimation performance of six spatial- and frequency- based methods, derived from conventional image filtering algorithms. Algorithms were tested on patient data sets from whole-body repeat CT acquisitions performed with a higher and lower dose technique over the same scan region. Results: The proposed performance metrics can evaluate the relative tradeoff of filter parameters and noise estimation performance. The proposed automated methods tend to underestimate CT image noise at low-flux levels. Initial application of methodology suggests that anisotropic diffusion and Wavelet-transform based filters provide optimal estimates of noise. Furthermore, methodology does not provide accurate estimates of absolute noise levels, but can provide estimates of relative change and/or trends in noise levels."
"Conditional random fields (CRFs) provide a powerful tool for structured prediction, but cast significant challenges in both the learning and inference steps. Approximation techniques are widely used in both steps, which should be considered jointly to guarantee good performance (a.k.a. ""inferning""). Perturb-and-MAP models provide a promising alternative to CRFs, but require global combinatorial optimization and hence they are usable only on specific models. In this work, we present a new Local Perturb-and-MAP (locPMAP) framework that replaces the global optimization with a local optimization by exploiting our observed connection between locPMAP and the pseudolikelihood of the original CRF model. We test our approach on three different vision tasks and show that our method achieves consistently improved performance over other approximate inference techniques optimized to a pseudolikelihood objective. Additionally, we demonstrate that we can integrate our method in the fully convolutional network framework to increase our model's complexity. Finally, our observed connection between locPMAP and the pseudolikelihood leads to a novel perspective for understanding and using pseudolikelihood."
"We study the phase retrieval problem, which solves quadratic system of equations, i.e., recovers a vector $\boldsymbol{x}\in \mathbb{R}^n$ from its magnitude measurements $y_i=|\langle \boldsymbol{a}_i, \boldsymbol{x}\rangle|, i=1,..., m$. We develop a gradient-like algorithm (referred to as RWF representing reshaped Wirtinger flow) by minimizing a nonconvex nonsmooth loss function. In comparison with existing nonconvex Wirtinger flow (WF) algorithm \cite{candes2015phase}, although the loss function becomes nonsmooth, it involves only the second power of variable and hence reduces the complexity. We show that for random Gaussian measurements, RWF enjoys geometric convergence to a global optimal point as long as the number $m$ of measurements is on the order of $n$, the dimension of the unknown $\boldsymbol{x}$. This improves the sample complexity of WF, and achieves the same sample complexity as truncated Wirtinger flow (TWF) \cite{chen2015solving}, but without truncation in gradient loop. Furthermore, RWF costs less computationally than WF, and runs faster numerically than both WF and TWF. We further develop the incremental (stochastic) reshaped Wirtinger flow (IRWF) and show that IRWF converges linearly to the true signal. We further establish performance guarantee of an existing Kaczmarz method for the phase retrieval problem based on its connection to IRWF. We also empirically demonstrate that IRWF outperforms existing ITWF algorithm (stochastic version of TWF) as well as other batch algorithms."
"Coordinated multi-display environments from the desktop, second-screen to gigapixel display walls are increasingly common. Personal and intimate mobile and wearable devices such as head-mounted displays, smartwatches, smartphones and tablets are rarely part of such multi-device ecosystems. With this paper, we contribute to a better understanding about factors that impede the creation and use of such mobile multi-device ecosystems. We base our findings on literature research and an expert survey. Specifically, we present grounded challenges relevant for the design, development and use of mobile multi-device environments."
"Different types of dynamics have been studied in repeated game play, and one of them which has received much attention recently consists of those based on ""no-regret"" algorithms from the area of machine learning. It is known that dynamics based on generic no-regret algorithms may not converge to Nash equilibria in general, but to a larger set of outcomes, namely coarse correlated equilibria. Moreover, convergence results based on generic no-regret algorithms typically use a weaker notion of convergence: the convergence of the average plays instead of the actual plays. Some work has been done showing that when using a specific no-regret algorithm, the well-known multiplicative updates algorithm, convergence of actual plays to equilibria can be shown and better quality of outcomes in terms of the price of anarchy can be reached for atomic congestion games and load balancing games. Are there more cases of natural no-regret dynamics that perform well in suitable classes of games in terms of convergence and quality of outcomes that the dynamics converge to?   We answer this question positively in the bulletin-board model by showing that when employing the mirror-descent algorithm, a well-known generic no-regret algorithm, the actual plays converge quickly to equilibria in nonatomic congestion games. Furthermore, the bandit model considers a probably more realistic and prevalent setting with only partial information, in which at each time step each player only knows the cost of her own currently played strategy, but not any costs of unplayed strategies. For the class of atomic congestion games, we propose a family of bandit algorithms based on the mirror-descent algorithms previously presented, and show that when each player individually adopts such a bandit algorithm, their joint (mixed) strategy profile quickly converges with implications."
"Using top-ranked documents in response to a query has been shown to be an effective approach to improve the quality of query translation in dictionary-based cross-language information retrieval. In this paper, we propose a new method for dictionary-based query translation based on dimension projection of embedded vectors from the pseudo-relevant documents in the source language to their equivalents in the target language. To this end, first we learn low-dimensional vectors of the words in the pseudo-relevant collections separately and then aim to find a query-dependent transformation matrix between the vectors of translation pairs appeared in the collections. At the next step, representation of each query term is projected to the target language and then, after using a softmax function, a query-dependent translation model is built. Finally, the model is used for query translation. Our experiments on four CLEF collections in French, Spanish, German, and Italian demonstrate that the proposed method outperforms a word embedding baseline based on bilingual shuffling and a further number of competitive baselines. The proposed method reaches up to 87% performance of machine translation (MT) in short queries and considerable improvements in verbose queries."
"We propose a novel extension of the encoder-decoder framework, called a review network. The review network is generic and can enhance any existing encoder- decoder model: in this paper, we consider RNN decoders with both CNN and RNN encoders. The review network performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a thought vector after each review step; the thought vectors are used as the input of the attention mechanism in the decoder. We show that conventional encoder-decoders are a special case of our framework. Empirically, we show that our framework improves over state-of- the-art encoder-decoder systems on the tasks of image captioning and source code captioning."
"We provide tight upper and lower bounds on the complexity of minimizing the average of $m$ convex functions using gradient and prox oracles of the component functions. We show a significant gap between the complexity of deterministic vs randomized optimization. For smooth functions, we show that accelerated gradient descent (AGD) and an accelerated variant of SVRG are optimal in the deterministic and randomized settings respectively, and that a gradient oracle is sufficient for the optimal rate. For non-smooth functions, having access to prox oracles reduces the complexity and we present optimal methods based on smoothing that improve over methods using just gradient accesses."
"Like time complexity models that have significantly contributed to the analysis and development of fast algorithms, energy complexity models for parallel algorithms are desired as crucial means to develop energy efficient algorithms for ubiquitous multicore platforms. Ideal energy complexity models should be validated on real multicore platforms and applicable to a wide range of parallel algorithms. However, existing energy complexity models for parallel algorithms are either theoretical without model validation or algorithm-specific without ability to analyze energy complexity for a wide-range of parallel algorithms.   This paper presents a new general validated energy complexity model for parallel (multithreaded) algorithms. The new model abstracts away possible multicore platforms by their static and dynamic energy of computational operations and data access, and derives the energy complexity of a given algorithm from its work, span and I/O complexity. The new model is validated by different sparse matrix vector multiplication (SpMV) algorithms and dense matrix multiplication (matmul) algorithms running on high performance computing (HPC) platforms (e.g., Intel Xeon and Xeon Phi). The new energy complexity model is able to characterize and compare the energy consumption of SpMV and matmul kernels according to three aspects: different algorithms, different input matrix types and different platforms. The prediction of the new model regarding which algorithm consumes more energy with different inputs on different platforms, is confirmed by the experimental results. In order to improve the usability and accuracy of the new model for a wide range of platforms, the platform parameters of ICE model are provided for eleven platforms including HPC, accelerator and embedded platforms."
"The generalization error of deep neural networks via their classification margin is studied in this work. Our approach is based on the Jacobian matrix of a deep neural network and can be applied to networks with arbitrary non-linearities and pooling layers, and to networks with different architectures such as feed forward networks and residual networks. Our analysis leads to the conclusion that a bounded spectral norm of the network's Jacobian matrix in the neighbourhood of the training samples is crucial for a deep neural network of arbitrary depth and width to generalize well. This is a significant improvement over the current bounds in the literature, which imply that the generalization error grows with either the width or the depth of the network. Moreover, it shows that the the recently proposed batch normalization and weight normalization re-parametrizations enjoy good generalization properties, and leads to a novel network regularizer based on the network's Jacobian matrix. The analysis is supported with experimental results on the MNIST and CIFAR-10 datasets."
"Many automated system analysis techniques (e.g., model checking, model-based testing) rely on first obtaining a model of the system under analysis. System modeling is often done manually, which is often considered as a hindrance to adopt model-based system analysis and development techniques. To overcome this problem, researchers have proposed to automatically ""learn"" models based on sample system executions and shown that the learned models can be useful sometimes. There are however many questions to be answered. For instance, how much shall we generalize from the observed samples and how fast would learning converge? Or, would the analysis result based on the learned model be more accurate than the estimation we could have obtained by sampling many system executions within the same amount of time? In this work, we investigate existing algorithms for learning probabilistic models for model checking, propose an evolution-based approach for better controlling the degree of generalization and conduct an empirical study in order to answer the questions. One of our findings is that the effectiveness of learning may sometimes be limited."
"This paper puts forth a new algorithm, termed \emph{truncated amplitude flow} (TAF), to recover an unknown $n$-dimensional real-/complex-valued vector $\bm{x}$ from $m$ quadratic equations of the form $y_i=|\langle\bm{a}_i,\bm{x}\rangle|^2$. This problem is known to be \emph{NP-hard} in general. We prove that as soon as the number of equations $m$ is on the order of the number of unknowns $n$, TAF recovers the solution exactly (up to a global unimodular constant) with high probability and complexity growing linearly with the time required to read the data. Our method adopts the \emph{amplitude-based} cost function and proceeds in two stages: In stage one, we introduce an \emph{orthogonality-promoting} initialization that is obtained with a few simple power iterations. Stage two refines the initial estimate by successive updates of scalable \emph{truncated generalized gradient iterations}. The former is in sharp contrast to existing spectral initializations, while the latter handles the rather challenging nonconvex and nonsmooth amplitude-based cost function. In particular for real-valued vectors, our gradient truncation rule provably eliminates the erroneously estimated signs with high probability to markedly improve upon its untruncated version. Numerical tests demonstrate that our initialization method returns more accurate and robust estimates relative to its spectral counterparts. Furthermore, even under the same initialization, our amplitude-based refinement outperforms Wirtinger-based alternatives, corroborating the superior performance of TAF over state-of-the-art algorithms."
"We provide a counterexample to a conjecture by Thiagarajan (1996 and 2002) that regular prime event structures correspond exactly to finite 1-safe Petri nets. The same counterexample is used to disprove a closely related conjecture by Badouel, Darondeau, and Raoult (1999) that domains of regular event structures with bounded $\natural$-cliques are recognizable by finite trace automata. Event structures, trace automata, and Petri nets are fundamental models in concurrency theory. There exist nice interpretations of these structures as combinatorial and geometric objects and both conjectures can be reformulated in this framework. Namely, from a graph theoretical point of view, the domains of prime event structures correspond exactly to median graphs; from a geometric point of view, these domains are in bijection with CAT(0) cube complexes.   A necessary condition for both conjectures to be true is that domains of respective regular event structures admit a regular nice labeling (which corresponds to a special coloring of the hyperplanes of the associated CAT(0) cube complex). To disprove these conjectures, we describe a regular event domain (with bounded $\natural$-cliques) that does not admit a regular nice labeling. Our counterexample is derived from an example by Wise (1996 and 2007) of a nonpositively curved square complex ${\bf X}$ with six squares, whose edges are colored in five colors, and whose universal cover $\widetilde{\bf X}$ is a CAT(0) square complex containing a particular plane with an aperiodic tiling."
"In this paper we propose an energy-efficient camera-based gesture recognition system powered by light energy for ""always on"" applications. Low energy consumption is achieved by directly extracting gesture features from the compressed measurements, which are the block averages and the linear combinations of the image sensor's pixel values. The gestures are recognized using a nearest-neighbour (NN) classifier followed by Dynamic Time Warping (DTW). The system has been implemented on an Analog Devices Black Fin ULP vision processor and powered by PV cells whose output is regulated by TI's DC-DC buck converter with Maximum Power Point Tracking (MPPT). Measured data reveals that with only 400 compressed measurements (768x compression ratio) per frame, the system is able to recognize key wake-up gestures with greater than 80% accuracy and only 95mJ of energy per frame. Owing to its fully self-powered operation, the proposed system can find wide applications in ""always-on"" vision systems such as in surveillance, robotics and consumer electronics with touch-less operation."
"Obtaining labels can be costly and time-consuming. Active learning allows a learning algorithm to intelligently query samples to be labeled for efficient learning. Fisher information ratio (FIR) has been used as an objective for selecting queries in active learning. However, little is known about the theory behind the use of FIR for active learning. There is a gap between the underlying theory and the motivation of its usage in practice. In this paper, we attempt to fill this gap and provide a rigorous framework for analyzing existing FIR-based active learning methods. In particular, we show that FIR can be asymptotically viewed as an upper bound of the expected variance of the log-likelihood ratio. Additionally, our analysis suggests a unifying framework that not only enables us to make theoretical comparisons among the existing querying methods based on FIR, but also allows us to give insight into the development of new active learning approaches based on this objective."
"The concept of Shannon entropy of random variables was generalized to measurable functions in general, and to simple functions with finite values in particular. It is shown that the information measure of a function is related to the time complexity of search problems concerning the functions in question. Formally, given a Turing reduction from a search problem f(x)=y to another function g(x), the amount of information about f(x)=y provided by querying g(x) is exactly equal to the average mutual information I(f;g). As a result, the average number of queries is I(f=y)/I(f;g), where I(f) is amount of self-information about the event {f=y}. In the idea case, if I(f=y)/I(f;g) is polynomial in the size of input and the function g(x) can be computed in polynomial time, then the problem f(x)=y also has polynomial-time algorithm. As it turns out, our information-based complexity estimation is a natural setting in which to study the power of randomized or probabilistic algorithms. Applying to decision problems, our result provides a strong evidence that P=RP=BPP. Further, Applying an argument similar to Carnot's work on measuring the efficiency of an ideal heat engine in thermodynamics, it is shown that I(f=y)/H(f) is a lower bound on query complexity of solving f(x)=y. In the worst case, if I(=y)/H(f) is exponential in the size of input, then the search problem f(x)=y is less likely to have polynomial-time algorithm. Applying to the famous Boolean Satisfiability problem, our result supports that P!=NP.Using brute-force search as a benchmark for efficiency, our results also support that P!=PP."
"We present a Bayesian sequential decision-making formulation of the information filtering problem, in which an algorithm presents items (news articles, scientific papers, tweets) arriving in a stream, and learns relevance from user feedback on presented items. We model user preferences using a Bayesian linear model, similar in spirit to a Bayesian linear bandit. We compute a computational upper bound on the value of the optimal policy, which allows computing an optimality gap for implementable policies. We then use this analysis as motivation in introducing a pair of new Decompose-Then-Decide (DTD) heuristic policies, DTD-Dynamic-Programming (DTD-DP) and DTD-Upper-Confidence-Bound (DTD-UCB). We compare DTD-DP and DTD-UCB against several benchmarks on real and simulated data, demonstrating significant improvement, and show that the achieved performance is close to the upper bound."
"This paper proposes a fountain coding system which has lower space decoding complexity and lower decoding erasure rate than the Raptor coding systems. The main idea of the proposed fountain code is employing shift and exclusive OR to generate the output packets. This technique is known as the zigzag decodable code, which is efficiently decoded by the zigzag decoder. In other words, we propose a fountain code based on the zigzag decodable code in this paper. Moreover, we analyze the overhead for the received packets, decoding erasure rate, decoding complexity, and asymptotic overhead of the proposed fountain code. As the result, we show that the proposed fountain code outperforms the Raptor codes in terms of the overhead and decoding erasure rate. Simulation results show that the proposed fountain coding system outperforms Raptor coding system in terms of the overhead and the space decoding complexity."
"In recent years there has been considerable interest in theories over string equations, length function, and string-number conversion predicate within the formal verification, software engineering, and security communities. SMT solvers for these theories, such as Z3str2, CVC4, and S3, are of immense practical value in exposing security vulnerabilities in string-intensive programs. Additionally, there are many open decidability and complexity-theoretic questions in the context of theories over strings that are of great interest to mathematicians. Motivated by the above-mentioned applications and open questions, we study a first-order, many-sorted, quantifier-free theory $T_{s,n}$ of string equations, linear arithmetic over string length, and string-number conversion predicate and prove three theorems. First, we prove that the satisfiability problem for the theory $T_{s,n}$ is undecidable via a reduction from a theory of linear arithmetic over natural numbers with power predicate, we call power arithmetic. Second, we show that the string-numeric conversion predicate is expressible in terms of the power predicate, string equations, and length function. This second theorem, in conjunction with the reduction we propose for the undecidability theorem, suggests that the power predicate is expressible in terms of word equations and length function if and only if the string-numeric conversion predicate is also expressible in the same fragment. Such results are very useful tools in comparing the expressive power of different theories, and for establishing decidability and complexity results. Third, we provide a consistent axiomatization ${\Gamma}$ for the functions and predicates of $T_{s,n}$. Additionally, we prove that the theory $T_{\Gamma}$ , obtained via logical closure of ${\Gamma}$, is not a complete theory."
"In this paper, we present a general framework for the construction of quantum tensor product codes (QTPC). In a classical tensor product code (TPC), its parity check matrix is constructed via the tensor product of parity check matrices of the two component codes. We show that by adding some constraints on the component codes, we can get several classes of dual-containing TPCs. By selecting different component codes, the proposed method enables the construction of a large family of QTPCs and they can provide the similar quantum error control abilities as the corresponding classical TPCs. In particular, if one of the component codes is selected as a burst error-correction code, then QTPCs have multiple quantum burst error-correction abilities, provided these bursts fall in distinct subblocks. Compared with concatenated quantum codes (CQC), the component code selections of QTPCs are much more exible than those of CQCs since only one of the component codes of QTPCs needs to satisfy the dual-containing restriction. Furthermore, we show that it is possible to construct QTPCs with parameters better than other classes of quantum error-correction codes (QECC), e.g., CQCs and quantum BCH codes. It is known that classical TPCs cannot have parameters better than classical BCH codes. However, we show that QTPCs can have better parameters than quantum BCH codes. Many QTPCs are obtained with parameters better than previously known QECCs available in the literature."
"In this paper, the impact of in-band full-duplex (IBFD) wireless communications on secret key generation via physical layer channel state information is investigated. A key generation strategy for IBFD wireless devices to increase the rate of generated secret keys over multipath fading channels is proposed. Conventionally, due to the half-duplex (HD) constraint on wireless transmissions, sensing simultaneous reciprocal channel measurements is not possible, which leads to a degraded key generation rate. However, with the advent of IBFD wireless devices, the legitimate nodes can sense the shared wireless link simultaneously at the possible cost of a self-interference (SI) channel estimation and some residual self-interference (RSI). As we demonstrate, with HD correlated observations the key rate is upper bounded by a constant, while with IBFD the key rate is only limited by the SI cancellation performance and is in general greater than that of its HD counterpart. Our analysis shows that with reasonable levels of SI cancellation, in the high SNR regime the key rate of IBFD is much higher, while in low SNRs, the HD system performs better. Finally, the key rate loss due to the overhead imposed by the SI channel estimation phase is discussed."
"A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling ""where to look"" or visual attention, it is equally important to model ""what words to listen to"" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA."
"Motivated by online recommendation and advertising systems, we consider a causal model for stochastic contextual bandits with a latent low-dimensional confounder. In our model, there are $L$ observed contexts and $K$ arms of the bandit. The observed context influences the reward obtained through a latent confounder variable with cardinality $m$ ($m \ll L,K$). The arm choice and the latent confounder causally determines the reward while the observed context is correlated with the confounder. Under this model, the $L \times K$ mean reward matrix $\mathbf{U}$ (for each context in $[L]$ and each arm in $[K]$) factorizes into non-negative factors $\mathbf{A}$ ($L \times m$) and $\mathbf{W}$ ($m \times K$). This insight enables us to propose an $\epsilon$-greedy NMF-Bandit algorithm that designs a sequence of interventions (selecting specific arms), that achieves a balance between learning this low-dimensional structure and selecting the best arm to minimize regret. Our algorithm achieves a regret of $\mathcal{O}\left(L\mathrm{poly}(m, \log K) \log T \right)$ at time $T$, as compared to $\mathcal{O}(LK\log T)$ for conventional contextual bandits, assuming a constant gap between the best arm and the rest for each context. These guarantees are obtained under mild sufficiency conditions on the factors that are weaker versions of the well-known Statistical RIP condition. We further propose a class of generative models that satisfy our sufficient conditions, and derive a lower bound of $\mathcal{O}\left(Km\log T\right)$. These are the first regret guarantees for online matrix completion with bandit feedback, when the rank is greater than one. We further compare the performance of our algorithm with the state of the art, on synthetic and real world data-sets."
"A basic setup of a two-tier network, where two mobile users exchange messages with a multi-antenna macrocell basestation, is studied from a rate perspective subject to beamforming and power constraints. The communication is facilitated by two femtocell basestations which act as relays as there is no direct link between the macrocell basestation and the mobile users. We propose a scheme based on physical-layer network coding and compute-and-forward combined with a novel approach that solves the problem of beamformer design and power allocation. We also show that the optimal beamformers are always a convex combination of the channels between the macro- and femtocell basestations. We then establish the cut-set bound of the setup to show that the presented scheme almost achieves the capacity of the setup numerically."
"In the problem of edge sign prediction, we are given a directed graph (representing a social network), and our task is to predict the binary labels of the edges (i.e., the positive or negative nature of the social relationships). Many successful heuristics for this problem are based on the troll-trust features, estimating at each node the fraction of outgoing and incoming positive/negative edges. We show that these heuristics can be understood, and rigorously analyzed, as approximators to the Bayes optimal classifier for a simple probabilistic model of the edge labels. We then show that the maximum likelihood estimator for this model approximately corresponds to the predictions of a Label Propagation algorithm run on a transformed version of the original social graph. Extensive experiments on a number of real-world datasets show that this algorithm is competitive against state-of-the-art classifiers in terms of both accuracy and scalability. Finally, we show that troll-trust features can also be used to derive online learning algorithms which have theoretical guarantees even when edges are adversarially labeled."
"Graphs have been widely used in many applications such as social networks, collaboration networks, and biological networks. One important graph analytics is to explore cohesive subgraphs in a large graph. Among several cohesive subgraphs studied, k-core is one that can be computed in linear time for a static graph. Since graphs are evolving in real applications, in this paper, we study core maintenance which is to reduce the computational cost to compute k-cores for a graph when graphs are updated from time to time dynamically. We identify drawbacks of the existing efficient algorithm, which needs a large search space to find the vertices that need to be updated, and has high overhead to maintain the index built, when a graph is updated. We propose a new order-based approach to maintain an order, called k-order, among vertices, while a graph is updated. Our new algorithm can significantly outperform the state-of-the-art algorithm up to 3 orders of magnitude for the 11 large real graphs tested. We report our findings in this paper."
"A parallel reservoir simulator has been developed, which is designed for large-scale black oil simulations. It handles three phases, including water, oil and gas, and three components, including water, oil and gas. This simulator can calculate traditional reservoir models and naturally fractured models. Various well operations are supported, such as water flooding, gas flooding and polymer flooding. The operation constraints can be fixed bottom-hole pressure, a fixed fluid rate, and combinations of them. The simulator is based on our in-house platform, which provides grids, cell-centred data, linear solvers, preconditioners and well modeling. The simulator and the platform use MPI for communications among computation nodes. Our simulator is capable of simulating giant reservoir models with hundreds of millions of grid cells. Numerical simulations show that our simulator matches with commercial simulators and it has excellent scalability."
"The structure of communication networks is an important determinant of the capacity of teams, organizations and societies to solve policy, business and science problems. Yet, previous studies reached contradictory results about the relationship between network structure and performance, finding support for the superiority of both well-connected efficient and poorly connected inefficient network structures. Here we argue that understanding how communication networks affect group performance requires taking into consideration the social learning strategies of individual team members. We show that efficient networks outperform inefficient networks when individuals rely on conformity by copying the most frequent solution among their contacts. However, inefficient networks are superior when individuals follow the best member by copying the group member with the highest payoff. In addition, groups relying on conformity based on a small sample of others excel at complex tasks, while groups following the best member achieve greatest performance for simple tasks. Our findings reconcile contradictory results in the literature and have broad implications for the study of social learning across disciplines."
"In this paper, we compare different metrics to predict the error rate of optical systems based on nonbinary forward error correction (FEC). It is shown that the correct metric to predict the performance of coded modulation based on nonbinary FEC is the mutual information. The accuracy of the prediction is verified in a detailed example with multiple constellation formats, FEC overheads in both simulations and optical transmission experiments over a recirculating loop. It is shown that the employed FEC codes must be universal if performance prediction based on thresholds is used. A tutorial introduction into the computation of the threshold from optical transmission measurements is also given."
"Spiking neural networks (SNNs) with adaptive synapses reflect core properties of biological neural networks. Speech recognition, as an application involving audio coding and dynamic learning, provides a good test problem to study SNN functionality. We present a simple, novel, and efficient nonrecurrent SNN that learns to convert a speech signal into a spike train signature. The signature is distinguishable from signatures for other speech signals representing different words, thereby enabling digit recognition and discrimination in devices that use only spiking neurons. The method uses a small, nonrecurrent SNN consisting of Izhikevich neurons equipped with spike timing dependent plasticity (STDP) and biologically realistic synapses. This approach introduces an efficient and fast network without error-feedback training, although it does require supervised training. The new simulation results produce discriminative spike train patterns for spoken digits in which highly correlated spike trains belong to the same category and low correlated patterns belong to different categories. The proposed SNN is evaluated using a spoken digit recognition task where a subset of the Aurora speech dataset is used. The experimental results show that the network performs well in terms of accuracy rate and complexity."
"Previous research has shown that neural networks can model survival data in situations in which some patients' death times are unknown, e.g. right-censored. However, neural networks have rarely been shown to outperform their linear counterparts such as the Cox proportional hazards model. In this paper, we run simulated experiments and use real survival data to build upon the risk-regression architecture proposed by Faraggi and Simon. We demonstrate that our model, DeepSurv, not only works as well as other survival models but actually outperforms in predictive ability on survival data with linear and nonlinear risk functions. We then show that the neural network can also serve as a recommender system by including a categorical variable representing a treatment group. This can be used to provide personalized treatment recommendations based on an individual's calculated risk. We provide an open source Python module that implements these methods in order to advance research on deep learning and survival analysis."
"Content-based analysis and retrieval of digital images found in scientific articles is often hindered by images consisting of multiple subfigures (compound figures). We address this problem by proposing a method to automatically classify and separate compound figures, which consists of two main steps: (i) a supervised compound figure classifier (CFC) discriminates between compound and non-compound figures using task-specific image features; and (ii) an image processing algorithm is applied to predicted compound images to perform compound figure separation (CFS). Our CFC approach is shown to achieve state-of-the-art classification performance on a published dataset. Our CFS algorithm shows superior separation accuracy on two different datasets compared to other known automatic approaches. Finally, we propose a method to evaluate the effectiveness of the CFC-CFS process chain and use it to optimize the misclassification loss of CFC for maximal effectiveness in the process chain."
"In this paper we consider methods for sharing free text Twitter data, with the goal of protecting the privacy of individuals in the data while still releasing data that carries research value, i.e. minimizes risk and maximizes utility. We propose three protection methods: simple redaction of hashtags and twitter handles, an epsilon-differentially private Multinomial-Dirichlet synthesizer, and novel synthesis models based on a neural generative model. We evaluate these three methods using empirical measures of risk and utility. We define risk based on possible identification of users in the Twitter data, and we define utility based on two general language measures and two model-based tasks. We find that redaction maintains high utility for simple tasks but at the cost of high risk, while some neural synthesis models are able to produce higher levels of utility, even for more complicated tasks, while maintaining lower levels of risk. In practice, utility and risk present a trade-off, with some methods offering lower risk or higher utility. This work presents possible methods to approach the problem of privacy for free text and which methods could be used to meet different utility and risk thresholds."
"Given an image, we wish to produce an image of larger size with significantly more pixels and higher image quality. This is generally known as the Single Image Super-Resolution (SISR) problem. The idea is that with sufficient training data (corresponding pairs of low and high resolution images) we can learn set of filters (i.e. a mapping) that when applied to given image that is not in the training set, will produce a higher resolution version of it, where the learning is preferably low complexity. In our proposed approach, the run-time is more than one to two orders of magnitude faster than the best competing methods currently available, while producing results comparable or better than state-of-the-art.   A closely related topic is image sharpening and contrast enhancement, i.e., improving the visual quality of a blurry image by amplifying the underlying details (a wide range of frequencies). Our approach additionally includes an extremely efficient way to produce an image that is significantly sharper than the input blurry one, without introducing artifacts such as halos and noise amplification. We illustrate how this effective sharpening algorithm, in addition to being of independent interest, can be used as a pre-processing step to induce the learning of more effective upscaling filters with built-in sharpening and contrast enhancement effect."
"We study the projected gradient descent method on low-rank matrix problems with a strongly convex objective. We use the Burer-Monteiro factorization approach to implicitly enforce low-rankness; such factorization introduces non-convexity in the objective. We focus on constraint sets that include both positive semi-definite (PSD) constraints and specific matrix norm-constraints. Such criteria appear in quantum state tomography and phase retrieval applications.   We show that non-convex projected gradient descent favors local linear convergence in the factored space. We build our theory on a novel descent lemma, that non-trivially extends recent results on the unconstrained problem. The resulting algorithm is Projected Factored Gradient Descent, abbreviated as ProjFGD, and shows superior performance compared to state of the art on quantum state tomography and sparse phase retrieval applications."
"Projective measurement can increase the entropy of a state $\rho$, the increased entropy is not only up to the basis of projective measurement, but also has something to do with the properties of the state itself. In this paper we define this increased entropy as basis entropy. And then we discuss the usefulness of this new concept by showing its application in deciding whether a state is pure or not and detecting the existence of quantum discord. And as shown in the paper, this new concept can also be used to describe decoherence."
"We extend Generative Adversarial Networks (GANs) to the semi-supervised context by forcing the discriminator network to output class labels. We train a generative model G and a discriminator D on a dataset with inputs belonging to one of N classes. At training time, D is made to predict which of N+1 classes the input belongs to, where an extra class is added to correspond to the outputs of G. We show that this method can be used to create a more data-efficient classifier and that it allows for generating higher quality samples than a regular GAN."
"In recent years deep neural networks have achieved great success in sentiment classification for English, thanks in part to the availability of copious annotated resources. Unfortunately, most other languages do not enjoy such an abundance of annotated data for sentiment analysis. To combat this problem, we propose the Adversarial Deep Averaging Network (ADAN) to transfer sentiment knowledge learned from labeled English data to low-resource languages where only unlabeled data exists. ADAN is a ""Y-shaped"" network with two discriminative branches: a sentiment classifier and an adversarial language predictor. Both branches take input from a feature extractor that aims to learn hidden representations that capture the underlying sentiment of the text and are invariant across languages by competing with the adversary. Experiments on Chinese sentiment classification demonstrate that ADAN significantly outperforms several baselines, including a strong pipeline approach that relies on Google Translate, the state-of-the-art commercial machine translation system."
"A comprehensive study of the coded performance of long-haul spectrally-efficient WDM optical fiber transmission systems with different coded modulation decoding structures is presented. Achievable information rates are derived for three different square QAM formats and the optimal format is identified as a function of distance and specific decoder implementation. The four cases analyzed combine hard-decision (HD) or soft-decision (SD) decoding together with either a bit-wise or a symbol-wise demapper, the last two suitable for binary and nonbinary codes, respectively. The information rates achievable for each scheme are calculated based on the mismatched decoder principle. These quantities represent true indicators of the coded performance of the system for specific decoder implementations and when the modulation format and its input distribution are fixed. In combination with the structure of the decoder, two different receiver-side equalization strategies are also analyzed: electronic dispersion compensation and digital backpropagation. We show that, somewhat unexpectedly, schemes based on nonbinary HD codes can achieve information rates comparable to SD decoders and that, when SD is used, switching from a symbol-wise to a bit-wise decoder results in a negligible penalty. Conversely, from an information-theoretic standpoint, HD binary decoders are shown to be unsuitable for spectrally-efficient, long-haul systems."
We introduce a recurrent neural network language model (RNN-LM) with long short-term memory (LSTM) units that utilizes both character-level and word-level inputs. Our model has a gate that adaptively finds the optimal mixture of the character-level and word-level inputs. The gate creates the final vector representation of a word by combining two distinct representations of the word. The character-level inputs are converted into vector representations of words using a bidirectional LSTM. The word-level inputs are projected into another high-dimensional space by a word lookup table. The final vector representations of words are used in the LSTM language model which predicts the next word given all the preceding words. Our model with the gating mechanism effectively utilizes the character-level inputs for rare and out-of-vocabulary words and outperforms word-level language models on several English corpora.
"Cooperation can be supported by indirect reciprocity via reputation.Thanks to gossip, reputations are built and circulated and humans can identify defectors and ostracise them. However, the evolutionary stability of gossip is allegedly undermined by the fact that it is more error-prone that direct observation, whereas ostracism could be ineffective if the partner selection mechanism is not robust. The aim of this work is to investigate the conditions under which the combination of gossip and ostracism might support cooperation in groups of different sizes. We are also interested in exploring the extent to which errors in transmission might undermine the reliability of gossip as a mechanism for identifying defectors. Our results show that a large quantity of gossip is necessary to support cooperation, and that group structure can mitigate the effects of errors in transmission."
"Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time."
"We discuss Bayesian inference (BI) for the probabilistic identification of material parameters. This contribution aims to shed light on the use of BI for the identification of elastoplastic material parameters. For this purpose a single spring is considered, for which the stress-strain curves are artificially created. Besides offering a didactic introduction to BI, this paper proposes an approach to incorporate statistical errors both in the measured stresses, and in the measured strains. It is assumed that the uncertainty is only due to measurement errors and the material is homogeneous. Furthermore, a number of possible misconceptions on BI are highlighted based on the purely elastic case."
"A Boltzmann machine is a stochastic neural network that has been extensively used in the layers of deep architectures for modern machine learning applications. In this paper, we develop a Boltzmann machine that is capable of modelling thermodynamic observables for physical systems in thermal equilibrium. Through unsupervised learning, we train the Boltzmann machine on data sets constructed with spin configurations importance-sampled from the partition function of an Ising Hamiltonian at different temperatures using Monte Carlo (MC) methods. The trained Boltzmann machine is then used to generate spin states, for which we compare thermodynamic observables to those computed by direct MC sampling. We demonstrate that the Boltzmann machine can faithfully reproduce the observables of the physical system. Further, we observe that the number of neurons required to obtain accurate results increases as the system is brought close to criticality."
"We study nearest-neighbors walks on the two-dimensional square lattice, that is, models of walks on $\mathbb{Z}^2$ defined by a fixed step set that is a subset of the non-zero vectors with coordinates 0, 1 or $-1$. We concern ourselves with the enumeration of such walks starting at the origin and constrained to remain in the quarter plane $\mathbb{N}^2$, counted by their length and by the position of their ending point. Bousquet-M\'elou and Mishna [Contemp. Math., pp. 1--39, Amer. Math. Soc., 2010] identified 19 models of walks that possess a D-finite generating function; linear differential equations have then been guessed in these cases by Bostan and Kauers [FPSAC 2009, Discrete Math. Theor. Comput. Sci. Proc., pp. 201--215, 2009]. We give here the first proof that these equations are indeed satisfied by the corresponding generating functions. As a first corollary, we prove that all these 19 generating functions can be expressed in terms of Gauss' hypergeometric functions that are intimately related to elliptic integrals. As a second corollary, we show that all the 19 generating functions are transcendental, and that among their $19 \times 4$ combinatorially meaningful specializations only four are algebraic functions."
"In this paper, we use deep neural networks for inverting face sketches to synthesize photorealistic face images. We first construct a semi-simulated dataset containing a very large number of computer-generated face sketches with different styles and corresponding face images by expanding existing unconstrained face data sets. We then train models achieving state-of-the-art results on both computer-generated sketches and hand-drawn sketches by leveraging recent advances in deep learning such as batch normalization, deep residual learning, perceptual losses and stochastic optimization in combination with our new dataset. We finally demonstrate potential applications of our models in fine arts and forensic arts. In contrast to existing patch-based approaches, our deep-neural-network-based approach can be used for synthesizing photorealistic face images by inverting face sketches in the wild."
"Directly reading documents and being able to answer questions from them is an unsolved challenge. To avoid its inherent difficulty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, WikiMovies, a QA dataset that contains raw text alongside a preprocessed KB, in the domain of movies. Our method reduces the gap between all three settings. It also achieves state-of-the-art results on the existing WikiQA benchmark."
"Here, we present IDNet, a user authentication framework from smartphone-acquired motion signals. Its goal is to recognize a target user from their way of walking, using the accelerometer and gyroscope (inertial) signals provided by a commercial smartphone worn in the front pocket of the user's trousers. IDNet features several innovations including: i) a robust and smartphone-orientation-independent walking cycle extraction block, ii) a novel feature extractor based on convolutional neural networks, iii) a one-class support vector machine to classify walking cycles, and the coherent integration of these into iv) a multi-stage authentication technique. IDNet is the first system that exploits a deep learning approach as universal feature extractors for gait recognition, and that combines classification results from subsequent walking cycles into a multi-stage decision making framework. Experimental results show the superiority of our approach against state-of-the-art techniques, leading to misclassification rates (either false negatives or positives) smaller than 0.15% with fewer than five walking cycles. Design choices are discussed and motivated throughout, assessing their impact on the user authentication performance."
"This work focuses on answering single-relation factoid questions over Freebase. Each question can acquire the answer from a single fact of form (subject, predicate, object) in Freebase. This task, simple question answering (SimpleQA), can be addressed via a two-step pipeline: entity linking and fact selection. In fact selection, we match the subject entity in a fact candidate with the entity mention in the question by a character-level convolutional neural network (char-CNN), and match the predicate in that fact with the question by a word-level CNN (word-CNN). This work makes two main contributions. (i) A simple and effective entity linker over Freebase is proposed. Our entity linker outperforms the state-of-the-art entity linker over SimpleQA task. (ii) A novel attentive maxpooling is stacked over word-CNN, so that the predicate representation can be matched with the predicate-focused question representation more effectively. Experiments show that our system sets new state-of-the-art in this task."
"In this article we introduce a new model to study stability in multi-hop wireless networks in the framework of adversarial queueing. In such a model, a routing protocol consists of three components: a transmission policy, a scheduling policy to select the packet to transmit form a set of packets parked at a node, and a hearing control mechanism to coordinate transmissions with scheduling. For such a setting, we propose a definition of universal stability that takes into account not only the scheduling policies (as in the standard wireline adversarial model), but also the transmission policies. First, we show that any scheduling policy that is unstable in the classical wireline adversarial model remains unstable in the multi-hop radio network model, even in scenarios free of inter- ferences. Then, we show that both SIS and LIS (two well-known universally stable scheduling policies in the wireline adversarial model) remain stable in the multi-hop radio network model, provided a proactive hearing control is used. In contrast, such scheduling policies turn out to be unstable when using a reactive hearing control. However, the scheduling policy LIS can be enforced to be universally stable provided ties are resolved in a permanent manner. Such a situation doesn't hold in the case of SIS, which remains unstable regardless of how ties are resolved. Furthermore, for some transmission policies which we call regular, we also show that all scheduling policies that are universally stable when using proactive hearing control (which include SIS and LIS) remain universally stable when using reactive hearing control."
"We propose a novel algorithm for visual question answering based on a recurrent deep neural network, where every module in the network corresponds to a complete answering unit with attention mechanism by itself. The network is optimized by minimizing loss aggregated from all the units, which share model parameters while receiving different information to compute attention probability. For training, our model attends to a region within image feature map, updates its memory based on the question and attended image feature, and answers the question based on its memory state. This procedure is performed to compute loss in each step. The motivation of this approach is our observation that multi-step inferences are often required to answer questions while each problem may have a unique desirable number of steps, which is difficult to identify in practice. Hence, we always make the first unit in the network solve problems, but allow it to learn the knowledge from the rest of units by backpropagation unless it degrades the model. To implement this idea, we early-stop training each unit as soon as it starts to overfit. Note that, since more complex models tend to overfit on easier questions quickly, the last answering unit in the unfolded recurrent neural network is typically killed first while the first one remains last. We make a single-step prediction for a new question using the shared model. This strategy works better than the other options within our framework since the selected model is trained effectively from all units without overfitting. The proposed algorithm outperforms other multi-step attention based approaches using a single step prediction in VQA dataset."
"The production of color language is essential for grounded language generation. Color descriptions have many challenging properties: they can be vague, compositionally complex, and denotationally rich. We present an effective approach to generating color descriptions using recurrent neural networks and a Fourier-transformed color representation. Our model outperforms previous work on a conditional language modeling task over a large corpus of naturalistic color descriptions. In addition, probing the model's output reveals that it can accurately produce not only basic color terms but also descriptors with non-convex denotations (""greenish""), bare modifiers (""bright"", ""dull""), and compositional phrases (""faded teal"") not seen in training."
"The use of convex regularizers allow for easy optimization, though they often produce biased estimation and inferior prediction performance. Recently, nonconvex regularizers have attracted a lot of attention and outperformed convex ones. However, the resultant optimization problem is much harder. In this paper, for a large class of nonconvex regularizers, we propose to move the nonconvexity from the regularizer to the loss. The nonconvex regularizer is then transformed to a familiar convex regularizer, while the resultant loss function can still be guaranteed to be smooth. Learning with the convexified regularizer can be performed by existing efficient algorithms originally designed for convex regularizers (such as the standard proximal algorithm and Frank-Wolfe algorithm). Moreover, it can be shown that critical points of the transformed problem are also critical points of the original problem. Extensive experiments on a number of nonconvex regularization problems show that the proposed procedure is much faster than the state-of-the-art nonconvex solvers."
"With the help of information and communication technologies, studies on the overall social networks have been extensively reported recently. However, investigations on the directed Ego Communication Networks (ECNs) remain insufficient, where an ECN stands for a sub network composed of a centralized individual and his/her direct contacts. In this paper, the directed ECNs are built on the Call Detail Records (CDRs), which cover more than 7 million people of a provincial capital city in China for half a year. Results show that there is a critical size for ECN at about 150, above which the average emotional closeness between ego and alters drops, the balanced relationship between ego and network collapses, and the proportion of strong ties decreases. This paper not only demonstrate the significance of ECN size in affecting its properties, but also shows accordance with the ""Dunbar's Number"". These results can be viewed as a cross-culture supportive evidence to the well-known Social Brain Hypothesis (SBH)."
"In combinatorics on words, a word $w$ over an alphabet $\Sigma$ is said to avoid a pattern $p$ over an alphabet $\Delta$ of variables if there is no factor $f$ of $w$ such that $f=h(p)$ where $h:\Delta^*\to\Sigma^*$ is a non-erasing morphism. A pattern $p$ is said to be $k$-avoidable if there exists an infinite word over a $k$-letter alphabet that avoids $p$. We consider the patterns such that at most two variables appear at least twice, or equivalently, the formulas with at most two variables. For each such formula, we determine whether it is $2$-avoidable, and if it is $2$-avoidable, we determine whether it is avoided by exponentially many binary words."
"We study the following problem: with the power of postselection (classically or quantumly), what is your ability to answer adaptive queries to certain languages? More specifically, for what kind of computational classes $\mathcal{C}$, we have $\mathsf{P}^{\mathcal{C}}$ belongs to $\mathsf{PostBPP}$ and $\mathsf{PostBQP}$? While a complete answer to the above question seems impossible given the development of present computational complexity theory. We study the analogous question in query complexity, which sheds light on the limitation of {\em relativized} methods (the relativization barrier) to the above question.   Informally, we show that, for a partial function $f$, if there is no efficient (In the world of query complexity, being efficient means using $O(\operatorname*{polylog}(n))$ time.) {\em small bounded-error} algorithm for $f$ classically or quantumly, then there is no efficient postselection bounded-error algorithm to answer adaptive queries to $f$ classically or quantumly. Our results imply a new proof for the classical oracle separation $\mathsf{P}^{\mathsf{NP}^{\mathcal{O}}} \not\subset \mathsf{PP}^{\mathcal{O}}$. They also lead to a new oracle separation $\mathsf{P}^{\mathsf{SZK}^{\mathcal{O}}} \not\subset \mathsf{PP}^{\mathcal{O}}$.   Our result also implies a hardness amplification construction for polynomial approximation: given a function $f$ on $n$ bits, we construct an adaptive-version of $f$, denoted by $F$, on $O(m \cdot n)$ bits, such that if $f$ requires large degree to approximate to error $2/3$ in a certain one-sided sense, then $F$ requires large degree to approximate even to error $1/2 - 2^{-m}$. Our construction achieves the same amplification in the work of Thaler (ICALP, 2016), by composing a function with $O(\log n)$ {\em deterministic query complexity}."
"The ""r-trie"", a new kind of self-balancing ternary search trie, is presented. It generalizes the simple balancing strategy used in Aragon and Seidel's randomized binary search trees (better known as ""treaps""). This means that a reasonably implemented r-trie is shaped like a ternary search trie built by inserting the strings in random order. As a result, the time cost of searching, inserting, or deleting a string of length k in an r-trie for n strings is in O(k + log n) with high probability."
"In an end-to-end dialog system, the aim of dialog state tracking is to accurately estimate a compact representation of the current dialog status from a sequence of noisy observations produced by the speech recognition and the natural language understanding modules. This paper introduces a novel method of dialog state tracking based on the general paradigm of machine reading and proposes to solve it using an End-to-End Memory Network, MemN2N, a memory-enhanced neural network architecture. We evaluate the proposed approach on the second Dialog State Tracking Challenge (DSTC-2) dataset. The corpus has been converted for the occasion in order to frame the hidden state variable inference as a question-answering task based on a sequence of utterances extracted from a dialog. We show that the proposed tracker gives encouraging results. Then, we propose to extend the DSTC-2 dataset with specific reasoning capabilities requirement like counting, list maintenance, yes-no question answering and indefinite knowledge management. Finally, we present encouraging results using our proposed MemN2N based tracking model."
"In many scheduling applications, minimizing delays is of high importance. One adverse effect of such delays is that the reward for completion of a job may decay over time. Indeed in healthcare settings, delays in access to care can result in worse outcomes, such as an increase in mortality risk. Motivated by managing hospital operations in disaster scenarios, as well as other applications in perishable inventory control and information services, we consider non-preemptive scheduling of jobs whose internal value decays over time. Because solving for the optimal scheduling policy is computationally intractable, we focus our attention on the performance of three intuitive heuristics: (1) a policy which maximizes the expected immediate reward, (2) a policy which maximizes the expected immediate reward rate, and (3) a policy which prioritizes jobs with imminent deadlines. We provide performance guarantees for all three policies and show that many of these performance bounds are tight. In addition, we provide numerical experiments and simulations to compare how the policies perform in a variety of scenarios. Our theoretical and numerical results allow us to establish rules-of-thumb for applying these heuristics in a variety of situations, including patient scheduling scenarios."
"In this paper, we describe a case study in a big metropolis, in which from data collected by digital sensors, we tried to understand mobility patterns of persons using buses and how this can generate knowledge to suggest interventions that are applied incrementally into the transportation network in use. We have first estimated an Origin-Destination matrix of buses users from datasets about the ticket validation and GPS positioning of buses. Then we represent the supply of buses with their routes through bus stops as a complex network, which allowed us to understand the bottlenecks of the current scenario and, in particular, applying community discovery techniques, to identify clusters that the service supply infrastructure has. Finally, from the superimposing of the flow of people represented in the OriginDestination matrix in the supply network, we exemplify how micro-interventions can be prospected by means of an example of the introduction of express routes."
"Exposing latent knowledge in geospatial trajectories has the potential to provide a better understanding of the movements of individuals and groups. Motivated by such a desire, this work presents the context tree, a new hierarchical data structure that summarises the context behind user actions in a single model. We propose a method for context tree construction that augments geospatial trajectories with land usage data to identify such contexts. Through evaluation of the construction method and analysis of the properties of generated context trees, we demonstrate the foundation for understanding and modelling behaviour afforded. Summarising user contexts into a single data structure gives easy access to information that would otherwise remain latent, providing the basis for better understanding and predicting the actions and behaviours of individuals and groups. Finally, we also present a method for pruning context trees, for use in applications where it is desirable to reduce the size of the tree while retaining useful information."
"We study the factors affecting training time in multi-device deep learning systems. Given a specification of a convolutional neural network, our goal is to minimize the time to train this model on a cluster of commodity CPUs and GPUs. We first focus on the single-node setting and show that by using standard batching and data-parallel techniques, throughput can be improved by at least 5.5x over state-of-the-art systems on CPUs. This ensures an end-to-end training speed directly proportional to the throughput of a device regardless of its underlying hardware, allowing each node in the cluster to be treated as a black box. Our second contribution is a theoretical and empirical study of the tradeoffs affecting end-to-end training time in a multiple-device setting. We identify the degree of asynchronous parallelization as a key factor affecting both hardware and statistical efficiency. We see that asynchrony can be viewed as introducing a momentum term. Our results imply that tuning momentum is critical in asynchronous parallel configurations, and suggest that published results that have not been fully tuned might report suboptimal performance for some configurations. For our third contribution, we use our novel understanding of the interaction between system and optimization dynamics to provide an efficient hyperparameter optimizer. Our optimizer involves a predictive model for the total time to convergence and selects an allocation of resources to minimize that time. We demonstrate that the most popular distributed deep learning systems fall within our tradeoff space, but do not optimize within the space. By doing this optimization, our prototype runs 1.9x to 12x faster than the fastest state-of-the-art systems."
"In this paper, we study sparse spike deconvolution over the space of complex-valued measures when the input measure is a finite sum of Dirac masses. We introduce a modified version of the Beurling Lasso (BLasso), a semi-definite program that we refer to as the Concomitant Beurling Lasso (CBLasso). This new procedure estimates the target measure and the unknown noise level simultaneously. Contrary to previous estimators in the literature, theory holds for a tuning parameter that depends only on the sample size, so that it can be used for unknown noise level problems. Consistent noise level estimation is standardly proved. As for Radon measure estimation, theoretical guarantees match the previous state-of-the-art results in Super-Resolution regarding minimax prediction and localization. The proofs are based on a bound on the noise level given by a new tail estimate of the supremum of a stationary non-Gaussian process through the Rice method."
"Word embeddings play a significant role in many modern NLP systems. Since learning one representation per word is problematic for polysemous words and homonymous words, researchers propose to use one embedding per word sense. Their approaches mainly train word sense embeddings on a corpus. In this paper, we propose to use word sense definitions to learn one embedding per word sense. Experimental results on word similarity tasks and a word sense disambiguation task show that word sense embeddings produced by our approach are of high quality."
"We give a necessary and sufficient condition for a function $E(t)$ being of minimum phase, and hence for its phase being univocally determined by its intensity $|E(t)|^2$. This condition is based on the knowledge of $E(t)$ alone and not of its analytic continuation in the complex plane, thus greatly simplifying its practical applicability. We apply these results to find the class of all band-limited signals that correspond to distinct receiver states when the detector is sensitive to the field intensity only and insensitive to the field phase, and discuss the performance of a recently proposed transmission scheme able to linearly detect all distinguishable states."
"We use tropical geometry to analyze finite valued mechanisms. We geometrically characterize all mechanisms on arbitrary type spaces and derive geometric as well as algebraic criteria for weak monotonicity, incentive compatibility and revenue equivalence. As corollaries we obtain various results known in the literature and shed new light on their nature. In particular we give conditions for the sufficiency of weak monotonicity that generalize previous results involving convexity."
"Semidefinite programs (SDP's) can be solved in polynomial time by interior point methods, but scalability can be an issue. To address this shortcoming, over a decade ago, Burer and Monteiro proposed to solve SDP's with few equality constraints via rank-restricted, non-convex surrogates. Remarkably, for some applications, local optimization methods seem to converge to global optima of these non-convex surrogates reliably. Although some theory supports this empirical success, a complete explanation of it remains an open question. In this paper, we consider a class of SDP's which includes applications such as max-cut, community detection in the stochastic block model, robust PCA, phase retrieval and synchronization of rotations. We show that the low-rank Burer-Monteiro formulation of SDP's in that class almost never has any spurious local optima."
"The incomplete moment generating function (IMGF) has paramount relevance in communication theory, since it appears in a plethora of scenarios when analyzing the performance of communication systems. We here present a general method for calculating the IMGF of any arbitrary fading distribution. Then, we provide exact closed-form expressions for the IMGF of the very general {\kappa}-{\mu} shadowed fading model, which includes the popular {\kappa}-{\mu}, {\eta}-{\mu}, Rician shadowed and other classical models as particular cases. We illustrate the practical applicability of this result by analyzing several scenarios of interest in wireless communications: (1) Physical layer security in the presence of an eavesdropper, (2) Outage probability analysis with interference and background noise, (3) Channel capacity with side information at the transmitter and the receiver, and (4) Average bit-error rate with adaptive modulation, when the fading on the desired link can be modeled by any of the aforementioned distributions."
"Good OCR results for historical printings rely on the availability of recognition models trained on diplomatic transcriptions as ground truth, which is both a scarce resource and time-consuming to generate. Instead of having to train a separate model for each historical typeface, we propose a strategy to start from models trained on a combined set of available transcriptions in a variety of fonts. These \emph{mixed models} result in character accuracy rates over 90\% on a test set of printings from the same period of time, but without any representation in the training data, demonstrating the possibility to overcome the typography barrier by generalizing from a few typefaces to a larger set of (similar) fonts in use over a period of time. The output of these mixed models is then used as a baseline to be further improved by both fully automatic methods and semi-automatic methods involving a minimal amount of manual transcriptions. In order to evaluate the recognition quality of each model in a series of models generated during the training process in the absence of any ground truth, we introduce two readily observable quantities that correlate well with true accuracy. These quantities are \emph{mean character confidence C} (as given by the OCR engine OCRopus) and \emph{mean token lexicality L} (a distance measure of OCR tokens from modern wordforms taking historical spelling patterns into account, which can be calculated for any OCR engine). Whereas the fully automatic method is able to improve upon the result of a mixed model by only 1-2 percentage points, already 100-200 hand-corrected lines lead to much better OCR results with character error rates of only a few percent. This procedure minimizes the amount of ground truth production and does not depend on the previous construction of a specific typographic model."
"Multivariate pattern analyses approaches in neuroimaging are fundamentally concerned with investigating the quantity and type of information processed by various regions of the human brain; typically, estimates of classification accuracy are used to quantify information. While a extensive and powerful library of methods can be applied to train and assess classifiers, it is not always clear how to use the resulting measures of classification performance to draw scientific conclusions: e.g. for the purpose of evaluating redundancy between brain regions. An additional confound for interpreting classification performance is the dependence of the error rate on the number and choice of distinct classes obtained for the classification task. In contrast, mutual information is a quantity defined independently of the experimental design, and has ideal properties for comparative analyses. Unfortunately, estimating the mutual information based on observations becomes statistically infeasible in high dimensions without some kind of assumption or prior.   In this paper, we construct a novel classification-based estimator of mutual information based on high-dimensional asymptotics. We show that in a particular limiting regime, the mutual information is an invertible function of the expected $k$-class Bayes error. While the theory is based on a large-sample, high-dimensional limit, we demonstrate through simulations that our proposed estimator has superior performance to the alternatives in problems of moderate dimensionality."
"We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.   The dataset is freely available at https://stanford-qa.com"
"As deep neural networks continue to revolutionize various application domains, there is increasing interest in making these powerful models more understandable and interpretable, and narrowing down the causes of good and bad predictions. We focus on recurrent neural networks (RNNs), state of the art models in speech recognition and translation. Our approach to increasing interpretability is by combining an RNN with a hidden Markov model (HMM), a simpler and more transparent model. We explore various combinations of RNNs and HMMs: an HMM trained on LSTM states; a hybrid model where an HMM is trained first, then a small LSTM is given HMM state distributions and trained to fill in gaps in the HMM's performance; and a jointly trained hybrid model. We find that the LSTM and HMM learn complementary information about the features in the text."
"We study the expressive power of deep neural networks before and after training. Considering neural nets after random initialization, we show that three natural measures of expressivity all display an exponential dependence on the depth of the network. We prove, theoretically and experimentally, that all of these measures are in fact related to a fourth quantity, trajectory length. This quantity grows exponentially in the depth of the network, and is responsible for the depth sensitivity observed. These results translate to consequences for networks during and after training. The connection of all expressivity measures to trajectory length suggests that parameters earlier in the network have greater influence on the expressive power -- in particular, given a layer, its influence on expressivity is determined by the remaining depth of the network after that layer. This is verified with experiments on MNIST and CIFAR-10. We also find that the training process decreases depth sensitivity for real and synthetic data, but at different rates."
"In applications involving matching of image sets, the information from multiple images must be effectively exploited to represent each set. State-of-the-art methods use probabilistic distribution or subspace to model a set and use specific distance measure to compare two sets. These methods are slow to compute and not compact to use in a large scale scenario. Learning-based hashing is often used in large scale image retrieval as they provide a compact representation of each sample and the Hamming distance can be used to efficiently compare two samples. However, most hashing methods encode each image separately and discard knowledge that multiple images in the same set represent the same object or person. We investigate the set hashing problem by combining both set representation and hashing in a single deep neural network. An image set is first passed to a CNN module to extract image features, then these features are aggregated using two types of set feature to capture both set specific and database-wide distribution information. The computed set feature is then fed into a multilayer perceptron to learn a compact binary embedding. Triplet loss is used to train the network by forming set similarity relations using class labels. We extensively evaluate our approach on datasets used for image matching and show highly competitive performance compared to state-of-the-art methods."
"Visual Question Answering (VQA) has attracted a lot of attention in both Computer Vision and Natural Language Processing communities, not least because it offers insight into the relationships between two important sources of information. Current datasets, and the models built upon them, have focused on questions which are answerable by direct analysis of the question and image alone. The set of such questions that require no external information to answer is interesting, but very limited. It excludes questions which require common sense, or basic factual knowledge to answer, for example. Here we introduce FVQA, a VQA dataset which requires, and supports, much deeper reasoning. FVQA only contains questions which require external information to answer.   We thus extend a conventional visual question answering dataset, which contains image-question-answerg triplets, through additional image-question-answer-supporting fact tuples. The supporting fact is represented as a structural triplet, such as <Cat,CapableOf,ClimbingTrees>.   We evaluate several baseline models on the FVQA dataset, and describe a novel model which is capable of reasoning about an image on the basis of supporting facts."
"Fifth generation (5G) wireless networks are expected to utilize wide bandwidths available at millimeter wave (mmWave) frequencies for enhancing system throughput. However, the unfavorable channel conditions of mmWave links, e.g., higher path loss and attenuation due to atmospheric gases or water vapor, hinder reliable communications. To compensate for these severe losses, it is essential to have a multitude of antennas to generate sharp and strong beams for directional transmission. In this paper, we consider mmWave systems using uniform planar array (UPA) antennas, which effectively place more antennas on a two-dimensional grid. A hybrid beamforming setup is also considered to generate beams by combining a multitude of antennas using only a few radio frequency chains. We focus on designing a set of transmit beamformers generating beams adapted to the directional characteristics of mmWave links assuming a UPA and hybrid beamforming. We first define ideal beam patterns for UPA structures. Each beamformer is constructed to minimize the mean squared error from the corresponding ideal beam pattern. Simulation results verify that the proposed codebooks enhance beamforming reliability and data rate in mmWave systems."
"Tensor contractions constitute a key computational ingredient of numerical multi-linear algebra. However, as the order and dimension of tensors grow, the time and space complexities of tensor-based computations grow quickly. Existing approaches for tensor contractions typically involves explicit copy and transpose operations. In this paper, we propose and evaluate a new BLAS-like primitive STRIDEDBATCHEDGEMM that is capable of performing a wide range of tensor contractions on CPU and GPU efficiently. Through systematic benchmarking, we demonstrate the advantages of our approach over conventional approaches. Concretely, we implement the Tucker decomposition and show that using our kernels yields 100x speedup as compared to the implementation using existing state-of-the-art libraries."
"We consider the point process of signal strengths emitted from transmitters in a wireless network and observed at a fixed position. In our model, transmitters are placed deterministically or randomly according to a hard core or Poisson point process and signals are subjected to power law path loss and random propagation effects that may be correlated between transmitters.   We provide bounds on the distance between the point process of signal strengths and a Poisson process with the same mean measure, assuming correlated log-normal shadowing. For ""strong shadowing"" and moderate correlations, we find that the signal strengths are close to a Poisson process, generalizing a recently shown analogous result for independent shadowing."
"Several important tasks in medical image analysis can be stated in the form of an optimization problem whose feasible solutions are connected subgraphs. Examples include the reconstruction of neural or vascular structures under connectedness constraints. We discuss the minimum cost connected subgraph (MCCS) problem and its approximations from the perspective of medical applications. We propose a) objective-dependent constraints and b) novel constraint generation schemes to solve this optimization problem exactly by means of a branch-and-cut algorithm. These are shown to improve scalability and allow us to solve instances of two medical benchmark datasets to optimality for the first time. This enables us to perform a quantitative comparison between exact and approximative algorithms, where we identify the geodesic tree algorithm as an excellent alternative to exact inference on the examined datasets."
"For any integer $n\geq 1$ a middle levels Gray code is a cyclic listing of all $n$-element and $(n+1)$-element subsets of $\{1,2,\ldots,2n+1\}$ such that any two consecutive subsets differ in adding or removing a single element. The question whether such a Gray code exists for any $n\geq 1$ has been the subject of intensive research during the last 30 years, and has been answered affirmatively only recently [T. M\""utze. Proof of the middle levels conjecture. To appear in Proc. London Math. Soc., 2014]. In a follow-up paper [T. M\""utze and J. Nummenpalo. An efficient algorithm for computing a middle levels Gray code. Proc. ESA, 2015] this existence proof was turned into an algorithm that computes each new set in the Gray code in time $\mathcal{O}(n)$ on average. In this work we complete this line of research by presenting an algorithm for computing a middle levels Gray code in optimal time and space: Each new set is generated in time $\mathcal{O}(1)$, and the required space is $\mathcal{O}(n)$."
"In this work, we propose explicit state-space based fault detection, isolation and estimation filters that are data-driven and are directly identified and constructed from only the system input-output (I/O) measurements and through estimating the system Markov parameters. The proposed methodology does not involve a reduction step and does not require identification of the system extended observability matrix or its left null space. The performance of our proposed filters is directly connected to and linearly dependent on the errors in the Markov parameters identification process. The estimation filters operate with a subset of the system I/O data that is selected by the designer. It is shown that the proposed filters provide asymptotically unbiased estimates by invoking low order filters as long as the selected subsystem has a stable inverse. We have derived the estimation error dynamics in terms of the Markov parameters identification errors and have shown that they can be directly synthesized from the healthy system I/O data. Consequently, the estimation errors can be effectively compensated for. Finally, we have provided several illustrative case study simulations that demonstrate and confirm the merits of our proposed schemes as compared to methodologies that are available in the literature."
"Optokinetic nystagmus (OKN) is an involuntary eye movement responsible for stabilizing retinal images in the presence of relative motion between an observer and the environment. Fully understanding the development of optokinetic nystagmus requires a neurally plausible computational model that accounts for the neural development and the behavior. To date, work in this area has been limited. We propose a neurally plausible framework for the joint development of disparity and motion tuning in the visual cortex, the optokinetic and vergence eye movements. This framework models the joint emergence of both perception and behavior, and accounts for the importance of the development of normal vergence control and binocular vision in achieving normal monocular OKN (mOKN) behaviors. Because the model includes behavior, we can simulate the same perturbations as performed in past experiments, such as artificially induced strabismus. The proposed model agrees both qualitatively and quantitatively with a number of findings from the literature on both binocular vision as well as the optokinetic reflex. Finally, our model also makes quantitative predictions about the OKN behavior using the same methods used to characterize the OKN in the experimental literature."
"Understanding fluctuation of users help stakeholders to provide a better support to communities. Below we present an experiment where we detect communities, their evolution and based on the data characterize users that stay, leave or join a community. Using a resulted feature set and logistic regression we operate with models of users that are joining and users that are staying in a community. In the related work we emphasize a number of features we will include in our future experiments to enhance train accuracy. This work represents a first from a series of experiments devoted to user fluctuation in communities."
"Since the shooting of Black teenager Michael Brown by White police officer Darren Wilson in Ferguson, Missouri, the protest hashtag #BlackLivesMatter has amplified critiques of extrajudicial killings of Black Americans. In response to #BlackLivesMatter, other Twitter users have adopted #AllLivesMatter, a counter-protest hashtag whose content argues that equal attention should be given to all lives regardless of race. Through a multi-level analysis, we study how these protests and counter-protests diverge by quantifying aspects of their discourse. In particular, our methodology facilitates additional qualitative analysis by measuring whether these divergences result from widespread discussion or a few popular retweets within these groups. We find that #BlackLivesMatter exhibits many informationally rich conversations, while those within #AllLivesMatter are more muted and susceptible to hijacking. We also show that the discussion within #BlackLivesMatter is more likely to center around the deaths of Black Americans, while that of #AllLivesMatter is more likely to sympathize with the lives of police officers and express politically conservative views."
"In recent years, what has become known as collaborative consumption has undergone rapid expansion through peer-to-peer (P2P) platforms. In the field of tourism, a particularly notable example is that of Airbnb. This article analyses the spatial patterns of Airbnb in Barcelona and compares them with hotels and sightseeing spots. New sources of data, such as Airbnb listings and geolocated photographs are used. Analysis of bivariate spatial autocorrelation reveals a close spatial relationship between Airbnb and hotels, with a marked centre-periphery pattern, although Airbnb predominates around the main hotel axis and hotels predominate in some peripheral areas of the city. Another interesting finding is that Airbnb capitalises more on the advantages of proximity to the main tourist attractions of the city than does the hotel sector. Finally, it was possible to detect those parts of the city that have seen the greatest increase in pressure from tourism related to Airbnb's recent expansion."
"Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze two models, one each from two major classes of VQA models -- with-attention and without-attention and show the similarities and differences in the behavior of these models. We also analyze the winning entry of the VQA Challenge 2016.   Our behavior analysis reveals that despite recent progress, today's VQA models are ""myopic"" (tend to fail on sufficiently novel instances), often ""jump to conclusions"" (converge on a predicted answer after 'listening' to just half the question), and are ""stubborn"" (do not change their answers across images)."
"Localization in Wireless Sensor Networks (WSNs) has been a challenging problem in the last decade. The most explored approaches for this purpose are based on multidimensional scaling (MDS) technique. The first algorithm that introduced MDS for nodes localization in sensor networks is well known as MDS-MAP. Since its appearance in 2003, many variations of MDS-MAP have been proposed in the literature. This paper aims to provide a comprehensive survey of the localization techniques that are based on MDS. We classify MDS-based algorithms according to different taxonomy features and different evaluation metrics."
"This paper addresses the nonlinear elliptic curl-curl equation with uncertainties in the material law. It is frequently employed in the numerical evaluation of magnetostatic fields, where the uncertainty is ascribed to the so-called B-H curve. A truncated Karhunen-Lo\`eve approximation of the stochastic B-H curve is presented and analyzed with regard to monotonicity constraints. A stochastic non-linear curl-curl formulation is introduced and numerically approximated by a finite element and collocation method in the deterministic and stochastic variable, respectively. The stochastic regularity is analyzed by a higher order sensitivity analysis. It is shown that, unlike to linear and several nonlinear elliptic problems, the solution is not analytic with respect to the random variables and an algebraic decay of the stochastic error is obtained. Numerical results for both the Karhunen-Lo\`eve expansion and the stochastic curl-curl equation are given for illustration."
"This paper studies the problem of receiver modeling in molecular communication systems. We consider the diffusive molecular communication channel between a transmitter nano-machine and a receiver nano-machine in a fluid environment. The information molecules released by the transmitter nano-machine into the environment can degrade in the channel via a first-order degradation reaction and those that reach the receiver nano-machine can participate in a reversible bimolecular reaction with receiver receptor proteins. Thereby, we distinguish between two scenarios. In the first scenario, we assume that the entire surface of the receiver is covered by receptor molecules. We derive a closed-form analytical expression for the expected received signal at the receiver, i.e., the expected number of activated receptors on the surface of the receiver. Then, in the second scenario, we consider the case where the number of receptor molecules is finite and the uniformly distributed receptor molecules cover the receiver surface only partially. We show that the expected received signal for this scenario can be accurately approximated by the expected received signal for the first scenario after appropriately modifying the forward reaction rate constant. The accuracy of the derived analytical results is verified by Brownian motion particle-based simulations of the considered environment, where we also show the impact of the effect of receptor occupancy on the derived analytical results."
"Many practical perception systems exist within larger processes that include interactions with users or additional components capable of evaluating the quality of predicted solutions. In these contexts, it is beneficial to provide these oracle mechanisms with multiple highly likely hypotheses rather than a single prediction. In this work, we pose the task of producing multiple outputs as a learning problem over an ensemble of deep networks -- introducing a novel stochastic gradient descent based approach to minimize the loss with respect to an oracle. Our method is simple to implement, agnostic to both architecture and loss function, and parameter-free. Our approach achieves lower oracle error compared to existing methods on a wide range of tasks and deep architectures. We also show qualitatively that the diverse solutions produced often provide interpretable representations of task ambiguity."
"We prove the Ramsey property of classes of ordered structures with closures and given local properties. This generalises earlier results: the Ne\v{s}et\v{r}il-R\""odl Theorem, the Ramsey property of partial orders and metric spaces as well as the author's Ramsey lift of bowtie-free graphs. We use this framework to give new examples of Ramsey classes. Among others, we show (and characterise) the Ramsey property of convexly ordered $S$-metric spaces and prove the Ramsey Theorem for Finite Models (i.e. structures with both functions and relations) thus providing the ultimate generalisation of Structural the Ramsey Theorem. We also show the Ramsey Theorem for structures with linear ordering on relations (""totally ordered structures""). All of these results are natural, easy to state, yet proofs involve most of the theory developed here.   We characterise classes of structures defined by forbidden homomorphisms having a Ramsey lift and extend this to special cases of classes with closures. We apply this to prove the Ramsey property of many Cherlin-Shelah-Shi classes.   In several instances our results are the best possible and confirm the meta-conjecture that Ramsey classes are ""close"" to lifted universal classes."
"In this paper, we study the uniform hard capacitated knapsack median problem, a common generalization of the $k$-median problem and facility location problem. Natural LP of the problem is known to have an unbounded integrality gap. We give first constant factor approximation for the capacitated knapsack median problem violating the budget only by an additive $f_{max}$ where $f_{max}$ is the maximum cost of a facility opened by the optimal and capacities by $5$ factor. To the best of our knowledge no constant factor approximation is known for the problem. We also give a constant factor ($O(1/\epsilon)$) approximation for the capacitated facility location problem with uniform capacities by rounding the solution of the natural LP, violating the capacities a little ($1 + \epsilon$). The LP is known to have an unbounded integrality gap without violating the capacities, even when the capacities are uniform. Thus, we achieve the best possible from the natural LP for the problem. The result shows that standard LP is not too bad for capacitated facility location problem."
"We provide an overview of the recent trends towards digitalization and large scale data analytics in healthcare. It is expected that these trends are instrumental in the dramatic changes in the way healthcare will be organized in the future. We discuss the recent political initiatives designed to shift care delivery processes from paper to electronic, with the goals of more effective treatments with better outcomes; cost pressure is a major driver of innovation. We describe newly developed networks of healthcare providers, research organizations and commercial vendors to jointly analyze data for the development of decision support systems. We address the trend towards continuous healthcare where health is monitored by wearable and stationary devices; a related development is that patients increasingly assume responsibility for their own health data. Finally we discuss recent initiatives towards a personalized medicine, based on advances in molecular medicine, data management, and data analytics."
"We analyse some enumerative and asymptotic properties of lattice paths below a line of rational slope. We illustrate our approach with Dyck paths under a line of slope $2/5$. This answers Knuth's problem #4 from his ""Flajolet lecture"" during the conference ""Analysis of Algorithms"" (AofA'2014) in Paris in June 2014. Our approach extends the work of Banderier and Flajolet for asymptotics and enumeration of directed lattice paths to the case of generating functions involving several dominant singularities, and has applications to a full class of problems involving some ""periodicities"". A key ingredient in the proof is the generalization of an old trick by Knuth himself (for enumerating permutations sortable by a stack), promoted by Flajolet and others as the ""kernel method"". All the corresponding generating functions are algebraic, and they offer some new combinatorial identities, which can also be tackled in the A=B spirit of Wilf-Zeilberger-Petkovsek. We show how to obtain similar results for any rational slope. An interesting case is e.g. Dyck paths below the slope $2/3$ (this corresponds to the so-called Duchon's club model), for which we solve a conjecture related to the asymptotics of the area below such lattice paths. Our work also gives access to lattice paths below an irrational slope (e.g. Dyck paths below $y=x/\sqrt{2}$), a problem that we study in a companion article."
"The problem of accurately predicting relative reading difficulty across a set of sentences arises in a number of important natural language applications, such as finding and curating effective usage examples for intelligent language tutoring systems. Yet while significant research has explored document- and passage-level reading difficulty, the special challenges involved in assessing aspects of readability for single sentences have received much less attention, particularly when considering the role of surrounding passages. We introduce and evaluate a novel approach for estimating the relative reading difficulty of a set of sentences, with and without surrounding context. Using different sets of lexical and grammatical features, we explore models for predicting pairwise relative difficulty using logistic regression, and examine rankings generated by aggregating pairwise difficulty labels using a Bayesian rating system to form a final ranking. We also compare rankings derived for sentences assessed with and without context, and find that contextual features can help predict differences in relative difficulty judgments across these two conditions."
"We provide a simple and novel algorithmic design technique, for which we call iterative partial rounding, that gives a tight rounding-based approximation for vertex cover with hard capacities (VC-HC). In particular, we obtain an $f$-approximation for VC-HC on hypergraphs, improving over a previous results of Cheung et al (SODA 2014) to the tight extent. This also closes the gap of approximation since it was posted by Chuzhoy and Naor in (FOCS 2002). We believe that our rounding technique is of independence interests when hard constraints are considered.   Our main technical tool for establishing the approximation guarantee is a separation lemma that certifies the existence of a strong partition for solutions that are basic feasible in an extended version of the natural LP."
"The Internet not only has changed the dynamics of our collective attention, but also through the transactional log of online activities, provides us with the opportunity to study attention dynamics at scale. In this paper, we particularly study attention to aircraft incidents and accidents using Wikipedia transactional data in two different language editions, English and Spanish. We study both the editorial activities on and the viewership of the articles about airline crashes. We analyse how the level of attention is influenced by different parameters such as number of deaths, airline region, and event locale and date. We find evidence that the attention given by Wikipedia editors to pre-Wikipedia aircraft incidents and accidents depends on the region of the airline for both English and Spanish editions. North American airline companies receive more prompt coverage in English Wikipedia. We also observe that the attention given by Wikipedia visitors is influenced by the airline region but only for events with high number of deaths. Finally we show that the rate and time span of the decay of attention is independent of the number of deaths and a fast decay within about a week seems to be universal. We discuss the implications of these findings in the context of attention bias."
"We propose an input/output conformance testing theory utilizing Modal Interface Automata with Input Refusals (IR-MIA) as novel behavioral formalism for both the specification and the implementation under test. A modal refinement relation on IR-MIA allows distinguishing between obligatory and allowed output behaviors, as well as between implicitly underspecified and explicitly forbidden input behaviors. The theory therefore supports positive and negative conformance testing with optimistic and pessimistic environmental assumptions. We further show that the resulting conformance relation on IR-MIA, called modal-irioco, enjoys many desirable properties concerning component-based behaviors. First, modal-irioco is preserved under modal refinement and constitutes a preorder under certain restrictions which can be ensured by a canonical input completion for IR-MIA. Second, under the same restrictions, modal-irioco is compositional with respect to parallel composition of IR-MIA with multi-cast and hiding. Finally, the quotient operator on IR-MIA, as the inverse to parallel composition, facilitates decompositionality in conformance testing to solve the unknown-component problem."
"Fronthaul limitations in terms of capacity and latency motivate the growing interest in the wireless industry for the study of alternative functional splits between cloud and edge nodes in Cloud Radio Access Network (C-RAN). This work contributes to this line of work by investigating the optimal functional split of control and data plane functionalities at the edge nodes and at the Remote Cloud Center (RCC) as a function of the fronthaul latency. The model under study consists of a two-user time-varying uplink channel in which the RCC has global but delayed channel state information (CSI) due to fronthaul latency, while edge nodes have local but timely CSI. Adopting the adaptive sum-rate as the performance criterion, functional splits whereby the control functionality of rate selection and the decoding of the data-plane frames are carried out at either the edge nodes or at the RCC are compared through analysis and numerical results."
"This is the second in a series of papers extending Martin-L\""{o}f's meaning explanation of dependent type theory to account for higher-dimensional types. We build on the cubical realizability framework for simple types developed in Part I, and extend it to a meaning explanation of dependent higher-dimensional type theory. This extension requires generalizing the computational Kan condition given in Part I, and considering the action of type families on paths. We define identification types, which classify identifications (paths) in a type, and dependent function and product types. The main result is a canonicity theorem, which states that a closed term of boolean type evaluates to either true or false. This result establishes the first computational interpretation of higher dependent type theory by giving a deterministic operational semantics for its programs, including operations that realize the Kan condition."
"This article introduces two Tensor Network-based iterative algorithms for the identification of high-order discrete-time nonlinear multiple-input multiple-output (MIMO) Volterra systems. The system identification problem is rewritten in terms of a Volterra tensor, which is never explicitly constructed, thus avoiding the curse of dimensionality. It is shown how each iteration of the two identification algorithms involves solving a linear system of low computational complexity. The proposed algorithms are guaranteed to monotonically converge and numerical stability is ensured through the use of orthogonal matrix factorizations. The performance and accuracy of the two identification algorithms are illustrated by numerical experiments, where accurate degree-10 MIMO Volterra models are identified in about 1 second in Matlab on a standard desktop pc."
"Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality."
"We propose new methods for the prediction of 5-year mortality in elderly individuals using chest computed tomography (CT). The methods consist of a classifier that performs this prediction using a set of features extracted from the CT image and segmentation maps of multiple anatomic structures. We explore two approaches: 1) a unified framework based on deep learning, where features and classifier are automatically learned in a single optimisation process; and 2) a multi-stage framework based on the design and selection/extraction of hand-crafted radiomics features, followed by the classifier learning process. Experimental results, based on a dataset of 48 annotated chest CTs, show that the deep learning model produces a mean 5-year mortality prediction accuracy of 68.5%, while radiomics produces a mean accuracy that varies between 56% to 66% (depending on the feature selection/extraction method and classifier). The successful development of the proposed models has the potential to make a profound impact in preventive and personalised healthcare."
"A recent work shows how we can optimize a tree based mode of operation for a hash function where the sizes of input message blocks and digest are the same, subject to the constraint that the involved tree structure has all its leaves at the same depth. In this note, we show that we can further optimize the running time of such a mode by using a tree having leaves at all its levels. We make the assumption that the input message block has a size a multiple of that of the digest and denote by $d$ the ratio block size over digest size. The running time is evaluated in terms of number of operations performed by the hash function, i.e. the number of calls to its underlying function. It turns out that a digest can be computed in $\lceil \log_{d+1} (l/2) \rceil+2$ evaluations of the underlying function using $\lceil l/2 \rceil$ processors, where $l$ is the number of blocks of the message. Other results of interest are discussed, such as the optimization of the parallel running time for a tree of restricted height."
"One of the main obstacles to broad application of reinforcement learning methods is the parameter sensitivity of our core learning algorithms. In many large-scale applications, online computation and function approximation represent key strategies in scaling up reinforcement learning algorithms. In this setting, we have effective and reasonably well understood algorithms for adapting the learning-rate parameter, online during learning. Such meta-learning approaches can improve robustness of learning and enable specialization to current task, improving learning speed. For temporal-difference learning algorithms which we study here, there is yet another parameter, $\lambda$, that similarly impacts learning speed and stability in practice. Unfortunately, unlike the learning-rate parameter, $\lambda$ parametrizes the objective function that temporal-difference methods optimize. Different choices of $\lambda$ produce different fixed-point solutions, and thus adapting $\lambda$ online and characterizing the optimization is substantially more complex than adapting the learning-rate parameter. There are no meta-learning method for $\lambda$ that can achieve (1) incremental updating, (2) compatibility with function approximation, and (3) maintain stability of learning under both on and off-policy sampling. In this paper we contribute a novel objective function for optimizing $\lambda$ as a function of state rather than time. We derive a new incremental, linear complexity $\lambda$-adaption algorithm that does not require offline batch updating or access to a model of the world, and present a suite of experiments illustrating the practicality of our new algorithm in three different settings. Taken together, our contributions represent a concrete step towards black-box application of temporal-difference learning methods in real world problems."
"In recent years, there is an increasing demand of big memory systems so to perform large scale data analytics. Since DRAM memories are expensive, some researchers are suggesting to use other memory systems such as non-volatile memory (NVM) technology to build large-memory computing systems. However, whether the NVM technology can be a viable alternative (either economically and technically) to DRAM remains an open question. To answer this question, it is important to consider how to design a memory system from a ""system perspective"", that is, incorporating different performance characteristics and price ratios from hybrid memory devices.   This paper presents an analytical model of a ""hybrid page cache system"" so to understand the diverse design space and performance impact of a hybrid cache system. We consider (1) various architectural choices, (2) design strategies, and (3) configuration of different memory devices. Using this model, we provide guidelines on how to design hybrid page cache to reach a good trade-off between high system throughput (in I/O per sec or IOPS) and fast cache reactivity which is defined by the time to fill the cache. We also show how one can configure the DRAM capacity and NVM capacity under a fixed budget. We pick PCM as an example for NVM and conduct numerical analysis. Our analysis indicates that incorporating PCM in a page cache system significantly improves the system performance, and it also shows larger benefit to allocate more PCM in page cache in some cases. Besides, for the common setting of performance-price ratio of PCM, ""flat architecture"" offers as a better choice, but ""layered architecture"" outperforms if PCM write performance can be significantly improved in the future."
"Many different deletion operations are investigated applied to languages accepted by one-way and two-way deterministic reversal-bounded multicounter machines, deterministic pushdown automata, and finite automata. Operations studied include the prefix, suffix, infix and outfix operations, as well as left and right quotient with languages from different families. It is often expected that language families defined from deterministic machines will not be closed under deletion operations. However, here, it is shown that one-way deterministic reversal-bounded multicounter languages are closed under right quotient with languages from many different language families; even those defined by nondeterministic machines such as the context-free languages. Also, it is shown that when starting with one-way deterministic machines with one counter that makes only one reversal, taking the left quotient with languages from many different language families -- again including those defined by nondeterministic machines such as the context-free languages -- yields only one-way deterministic reversal-bounded multicounter languages (by increasing the number of counters). However, if there are two more reversals on the counter, or a second 1-reversal-bounded counter, taking the left quotient (or even just the suffix operation) yields languages that can neither be accepted by deterministic reversal-bounded multicounter machines, nor by 2-way nondeterministic machines with one reversal-bounded counter."
"We consider the amount of dissipated work occurring in a given nonequilibrium process as a function of the initial distribution over microstates. We show that the extra dissipated work for any distribution, above the minimum achievable, has a simple information-theoretic form that depends only on the initial and final microstate distributions, and reflects the logical irreversibility of the process. We then consider the case where the process induces dynamics over a coarse-grained space that implement some specified computation. We show that our analysis of dissipated work still applies, though now stated in terms of distributions over the coarse-grained states. This extra dissipated work is a novel thermodynamic cost of computation, in addition to the well-known Landauer's cost."
"Using neural networks to generate replies in human-computer dialogue systems is attracting increasing attention over the past few years. However, the performance is not satisfactory: the neural network tends to generate safe, universally relevant replies which carry little meaning. In this paper, we propose a content-introducing approach to neural network-based generative dialogue systems. We first use pointwise mutual information (PMI) to predict a noun as a keyword, reflecting the main gist of the reply. We then propose seq2BF, a ""sequence to backward and forward sequences"" model, which generates a reply containing the given keyword. Experimental results show that our approach significantly outperforms traditional sequence-to-sequence models in terms of human evaluation and the entropy measure, and that the predicted keyword can appear at an appropriate position in the reply."
"This paper proposes a rough set theoretic approach to Domain theory, thereby establishing a direct connection between Rough Set Theory and Domain Theory. With a rough set theoretic mind-set, we tailor-made new approximation operators specially suited for Domain Theory. Our proposed approach not only offers a fresh perspective to existing concepts and results in Domain Theory through the rough set theoretic lens, but also reveals ways to establishing novel domain-theoretic results. For instance, (1) the well-known interpolation property of the way-below relation on a continuous poset is equivalent to the idempotence of a certain set-operator; (2) the continuity of a poset can be characterized by the coincidence of the Scott closure operator and the upper approximation operator induced by the way below relation; and as a result, (3) a new characterization of Scott closure is obtained. Additionally, we show how, to each approximating relation, an associated order-compatible topology can be defined in such a way that for the case of a continuous poset the topology associated to the way-below relation is exactly the Scott topology. A preliminary investigation is carried out on this new topology."
"In this paper we show a new way of constructing deterministic polynomial-time approximation algorithms for computing complex-valued evaluations of a large class of graph polynomials on bounded degree graphs. In particular, our approach works for the Tutte polynomial and independence polynomial, as well as partition functions of complex-valued spin and edge-coloring models.   More specifically, we define a large class of graph polynomials $\mathcal C$ and show that if $p\in \cal C$ and there is a disk $D$ centered at zero in the complex plane such that $p(G)$ does not vanish on $D$ for all bounded degree graphs graphs $G$, then for each $z$ in the interior of $D$ there exists a deterministic polynomial-time approximation algorithm for evaluating $p(G)$ at $z$. This gives an explicit connection between absence of zeros of graph polynomials and the existence of efficient approximation algorithms, allowing us to show new relationships between well-known conjectures.   Our work builds on a recent line of work initiated by. Barvinok, which provides a new algorithmic approach besides the existing Markov chain Monte Carlo method and the correlation decay method for these types of problems."
"Using the path-integral technique we examine the mutual information for the communication channel modelled by the nonlinear Schr\""odinger equation with additive Gaussian noise. The nonlinear Schr\""odinger equation is one of the fundamental models in nonlinear physics, and it has a broad range of applications, including fiber optical communications --- the backbone of the Internet. At large signal-to-noise ratio ($\mathrm{SNR}$) we present the mutual information through the path-integral which is convenient for the perturbative expansion in nonlinearity. In the limit of small noise and small nonlinearity we derive analytically the first nonzero nonlinear correction to the mutual information for the channel."
"Given a plane forest $F = (V, E)$ of $|V| = n$ points, we find the minimum set $S \subseteq E$ of edges such that the edge-constrained minimum spanning tree over the set $V$ of vertices and the set $S$ of constraints contains $F$. We present an $O(n \log n )$-time algorithm that solves this problem. We generalize this to other proximity graphs in the constraint setting, such as the relative neighbourhood graph, Gabriel graph, $\beta$-skeleton and Delaunay triangulation. We present an algorithm that identifies the minimum set $S\subseteq E$ of edges of a given plane graph $I=(V,E)$ such that $I \subseteq CG_\beta(V, S)$ for $1 \leq \beta \leq 2$, where $CG_\beta(V, S)$ is the constraint $\beta$-skeleton over the set $V$ of vertices and the set $S$ of constraints. The running time of our algorithm is $O(n)$, provided that the constrained Delaunay triangulation of $I$ is given."
"Several recent results in the field of quantum thermodynamics have been obtained using the tools of quantum information theory and resource theories. So far, the resource theories utilised to describe quantum thermodynamics have assumed the existence of an infinite thermal reservoir, by declaring that thermal states at some background temperature come for free. Here, we propose a resource theory of quantum thermodynamics without a background temperature, so that no states at all come for free. We apply this resource theory to the case of many non-interacting systems, and show that states are classified by their entropy and average energy. This implies that thermodynamics can be described in a two-dimensional convex set that we call the energy-entropy diagram. The answers to many resource-theoretic questions about thermodynamics can be read off from this diagram, such as the efficiency of a heat engine consisting of finite reservoirs or the rate of conversion between two states. This allows us to consider a resource theory which puts work and heat on an equal footing, and serves as a model for more general resource theories."
"Algorand is a truly decentralized, new, and secure way to manage a shared ledger. Unlike prior approaches based on {\em proof of work}, it requires a negligible amount of computation, and generates a transaction history that does not fork with overwhelmingly high probability. This approach cryptographically selects ---in a way that is provably immune from manipulations, unpredictable until the last minute, but ultimately universally clear--- a set of verifiers in charge of constructing a block of valid transactions. This approach applies to any way of implementing a shared ledger via a tamper-proof sequence of blocks, including traditional blockchains. This paper also presents more efficient alternatives to blockchains, which may be of independent interest.   Algorand significantly enhances all applications based on a public ledger: payments, smart contracts, stock settlement, etc. But, for concreteness, we shall describe it only as a money platform."
"Our goal is to combine the rich multistep inference of symbolic logical reasoning with the generalization capabilities of neural networks. We are particularly interested in complex reasoning about entities and relations in text and large-scale knowledge bases (KBs). Neelakantan et al. (2015) use RNNs to compose the distributed semantics of multi-hop paths in KBs; however for multiple reasons, the approach lacks accuracy and practicality. This paper proposes three significant modeling advances: (1) we learn to jointly reason about relations, entities, and entity-types; (2) we use neural attention modeling to incorporate multiple paths; (3) we learn to share strength in a single RNN that represents logical composition across all relations. On a largescale Freebase+ClueWeb prediction task, we achieve 25% error reduction, and a 53% error reduction on sparse relations due to shared strength. On chains of reasoning in WordNet we reduce error in mean quantile by 84% versus previous state-of-the-art. The code and data are available at https://rajarshd.github.io/ChainsofReasoning"
"By establishing a relation between information erasure and continuous phase transitions we generalise the Landauer bound to analog computing systems. The entropy production per degree of freedom during erasure of an analog variable (reset to standard value) is given by the logarithm of the configurational volume measured in units of its minimal quantum. As a consequence every computation has to be carried on with a finite number of bits and infinite precision is forbidden by the fundamental laws of physics, since it would require an infinite amount of energy."
"Ideas from Fourier analysis have been used in cryptography for three decades. Akavia, Goldwasser and Safra unified some of these ideas to give a complete algorithm that finds significant Fourier coefficients of functions on any finite abelian group. Their algorithm stimulated a lot of interest in the cryptography community, especially in the context of ""bit security"". This paper attempts to be a friendly and comprehensive guide to the tools and results in this field. The intended readership is cryptographers who have heard about these tools and seek an understanding of their mechanics, and their usefulness and limitations. A compact overview of the algorithm is presented with emphasis on the ideas behind it. We survey some applications of this algorithm, and explain that several results should be taken in the right context. We point out that some of the most important bit security problems are still open. Our original contributions include: an approach to the subject based on modulus switching; a discussion of the limitations on the usefulness of these tools; an answer to an open question about the modular inversion hidden number problem."
"In this article, we propose a sampling-based motion planning algorithm equipped with an information-theoretic convergence criterion for incremental informative motion planning. The proposed approach allows dense map representations and incorporates the full state uncertainty into the planning process. The problem is formulated as a maximization problem with a budget constraint. Our approach is built on rapidly-exploring information gathering algorithms and benefits from advantages of sampling-based optimal motion planning algorithms. We propose two information functions and their variants for fast and online computations. We prove an information-theoretic convergence for the entire exploration and information gathering mission based on the least upper bound of the average map entropy. The convergence criterion gives rise to a natural automatic stopping criterion for information-driven motion control. We demonstrate the performance of the proposed algorithms using three scenarios: comparison of the proposed information functions and sensor configuration selection, robotic exploration in unknown environments, and a wireless signal strength monitoring task in a lake from a publicly available dataset collected using an autonomous surface vehicle."
"We propose a cell-edge-aware (CEA) zero forcing (ZF) precoder that exploits the excess spatial degrees of freedom provided by a large number of base station (BS) antennas to suppress inter-cell interference at the most vulnerable user equipments (UEs). We evaluate the downlink performance of CEA-ZF, as well as that of a conventional cell-edge-unaware (CEU) ZF precoder in a network with random base station topology. Our analysis and simulations show that the proposed CEA-ZF precoder outperforms CEU-ZF precoding in terms of (i) aggregate per-cell data rate, (ii) coverage probability, and (iii) 95%-likely, or edge user, rate. In particular, when both perfect channel state information and a large number of antennas N are available at the BSs, we demonstrate that the outage probability under CEA-ZF and CEU-ZF decay as 1/N^2 and 1/N, respectively. This result identifies CEA-ZF as a more effective precoding scheme for massive MIMO cellular networks. Our framework also reveals the importance of scheduling the optimal number of UEs per BS, and confirms the necessity to control the amount of pilot contamination received during the channel estimation phase."
"Csisz\'ar's channel coding theorem for multiple codebooks is generalized allowing the codeword lenghts differ across codebooks. Also in this case, for each codebook an error exponent can be achieved that equals the random coding exponent for this codebook alone, in addition, erasure detection failure probability tends to 0. This is proved even for sender and receiver not knowing the channel. As a corollary, a substantial improvement is obtained when the sender knows the channel."
"We consider the class of valuations on indivisible items called gross-substitute (GS). This class was introduced by Kelso and Crawford (1982) and is widely used in studies of markets with indivisibilities. GS is a condition on the demand-flow in a specific scenario: some items become more expensive while other items retain their price. We prove that GS implies a much stronger condition, describing the demand-flow in the general scenario in which all prices may change. We prove that the demand of GS agents always flows (weakly) downwards, i.e, from items with higher price-increase to items with lower price-increase. We show that this property is equivalent to GS and is not true when there are complementarities."
"Several studies on sentence processing suggest that the mental lexicon keeps track of the mutual expectations between words. Current DSMs, however, represent context words as separate features, thereby loosing important information for word expectations, such as word interrelations. In this paper, we present a DSM that addresses this issue by defining verb contexts as joint syntactic dependencies. We test our representation in a verb similarity task on two datasets, showing that joint contexts achieve performances comparable to single dependencies or even better. Moreover, they are able to overcome the data sparsity problem of joint feature spaces, in spite of the limited size of our training corpus."
"Zero-shot learning for visual recognition, e.g., object and action recognition, has recently attracted a lot of attention. However, it still remains challenging in bridging the semantic gap between visual features and their underlying semantics and transferring knowledge to semantic categories unseen during learning. Unlike most of the existing methods that learn either a direct mapping from visual features to their semantic representations or a common latent space by the joint use of visual features and their semantic representations, we propose a stagewise bidirectional latent embedding framework for zero-shot visual recognition. In the bottom-up stage, a latent embedding space is first created by exploring the topological and labeling information underlying training data of known classes via supervised locality preserving projection and the latent representations of training data are used to form landmarks that guide embedding semantics underlying unseen classes onto this latent space. In the top-down stage, semantic representations for unseen classes are then projected to the latent embedding space to preserve the semantic relatedness via the semi-supervised Sammon mapping with landmarks. As a result, the resultant latent embedding space allows for predicting the label of a test instance with a simple nearest neighbor algorithm. To evaluate the effectiveness of the proposed framework, we have conducted experiments on four benchmark datasets in object and action recognition, i.e., AwA, CUB-200-2011, UCF101 and HMDB51. The experimental results under comparative studies demonstrate that our proposed approach yields the state-of-the-art performance."
"We present a specialized network simplex algorithm for the budget-constrained minimum cost flow problem, which is an extension of the traditional minimum cost flow problem by a second kind of costs associated with each edge, whose total value in a feasible flow is constrained by a given budget B. We present a fully combinatorial description of the algorithm that is based on a novel incorporation of two kinds of integral node potentials and three kinds of reduced costs. We prove optimality criteria and combine two methods that are commonly used to avoid cycling in traditional network simplex algorithms into new techniques that are applicable to our problem. With these techniques and our definition of the reduced costs, we are able to prove a pseudo-polynomial running time of the overall procedure, which can be further improved by incorporating Dantzig's pivoting rule. Moreover, we present computational results that compare our procedure with Gurobi."
"In this work, we aim to contribute to the understanding of the human pro-social behavior by studying the influence that a particular form of social pressure ""being watched"" has on the evolution of cooperative behavior. We study how cooperation emerge in multiplex complex topologies by analyzing a particular bidirectionally-coupled dynamics on top of a two-layers multiplex network (duplex). The coupled dynamics appears between the Prisoner's Dilemma game in a network, and a threshold cascade model in the other. The threshold model is intended to abstract the behavior of a network of vigilant nodes, that impose pressure of being observed altering hence the temptation to defect of the dilemma. Cooperation or defection in the game also affects the state of a node of being vigilant. We analyze these processes on different duplex networks structures and assess the influence of the topology, average degree and correlated multiplexity, on the outcome of cooperation. Interestingly, we find that the social pressure of vigilance may impact cooperation positively or negatively, depending on the duplex structure, specifically the degree correlations between layers is determinant. Our results give further quantitative insights in the promotion of cooperation under social pressure."
"The analysis of bibliometric networks, such as co-authorship, bibliographic coupling, and co-citation networks, has received a considerable amount of attention. Much less attention has been paid to the construction of these networks. We point out that different approaches can be taken to construct a bibliometric network. Normally the full counting approach is used, but we propose an alternative fractional counting approach. The basic idea of the fractional counting approach is that each action, such as co-authoring or citing a publication, should have equal weight, regardless of for instance the number of authors, citations, or references of a publication. We present two empirical analyses in which the full and fractional counting approaches yield very different results. These analyses deal with co-authorship networks of universities and bibliographic coupling networks of journals. Based on theoretical considerations and on the empirical analyses, we conclude that for many purposes the fractional counting approach is preferable over the full counting one."
"We present a dataset for evaluating the tracking accuracy of monocular visual odometry and SLAM methods. It contains 50 real-world sequences comprising more than 100 minutes of video, recorded across dozens of different environments -- ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position. This allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated. We provide exposure times for each frame as reported by the sensor, the camera response function, and dense lens attenuation factors. We also propose a novel, simple approach to non-parametric vignette calibration, which requires minimal set-up and is easy to reproduce. Finally, we thoroughly evaluate two existing methods (ORB-SLAM and DSO) on the dataset, including an analysis of the effect of image resolution, camera field of view, and the camera motion direction."
"We propose a novel direct sparse visual odometry formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry -- represented as inverse depth in a reference frame -- and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on mostly white walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness."
"In this paper we study the problem of exact recovery of the pure-strategy Nash equilibria (PSNE) set of a graphical game from noisy observations of joint actions of the players alone. We consider sparse linear influence games --- a parametric class of graphical games with linear payoffs, and represented by directed graphs of n nodes (players) and in-degree of at most k. We present an $\ell_1$-regularized logistic regression based algorithm for recovering the PSNE set exactly, that is both computationally efficient --- i.e. runs in polynomial time --- and statistically efficient --- i.e. has logarithmic sample complexity. Specifically, we show that the sufficient number of samples required for exact PSNE recovery scales as $\mathcal{O}(\mathrm{poly}(k) \log n)$. We also validate our theoretical results using synthetic experiments."
"The following report introduces ideas augmenting standard Long Short Term Memory (LSTM) architecture with multiple memory cells per hidden unit in order to improve its generalization capabilities. It considers both deterministic and stochastic variants of memory operation. It is shown that the nondeterministic Array-LSTM approach improves state-of-the-art performance on character level text prediction achieving 1.402 BPC on enwik8 dataset. Furthermore, this report estabilishes baseline neural-based results of 1.12 BPC and 1.19 BPC for enwik9 and enwik10 datasets respectively."
"The density of a code is the fraction of the coding space covered by packing balls centered around the codewords. This paper investigates the density of codes in the complex Stiefel and Grassmann manifolds equipped with their commonly used chordal distance. The choice of distance enforces the treatment of the manifolds as subspaces of Euclidean hyperspheres. In this geometry, the densest packings are not necessarily equivalent to the maximum-minimum-distance codes. Computing a code's density follows from computing i) the normalized volume of a metric ball and ii) the kissing radius, the radius of the largest balls one can pack around the codewords without overlapping. First, the normalized volume of a metric ball is evaluated by asymptotic approximations. For a small ball, its volume can be well-approximated by the volume of a locally-equivalent tangential ball. In order to properly normalize this approximation, the precise volumes of the manifolds induced by their spherical embedding are computed. For larger balls, a hyperspherical cap approximation is used, which is justified by a volume comparison theorem showing that the normalized volume of a ball in the Stiefel or Grassmann manifold is asymptotically equal to the normalized volume of a ball in its embedding sphere as the dimension grows to infinity. Then, bounds on the kissing radius are derived alongside corresponding bounds on the density. Unlike for spherical codes or for codes in flat spaces, the kissing radius of a Grassmann or Stiefel code cannot be exactly determined from its minimum distance. It is nonetheless possible to derive bounds on density as functions of the minimum distance. Stiefel and Grassmann codes have larger density than their image spherical codes when dimensions tend to infinity. The bounds on density lead to refinements of the standard Hamming bounds for Stiefel and Grassmann codes."
"GPGPU applications exploit on-chip scratchpad memory available in the Graphics Processing Units (GPUs) to improve performance. The amount of thread level parallelism present in the GPU is limited by the number of resident threads, which in turn depends on the availability of scratchpad memory in its streaming multiprocessor (SM). Since the scratchpad memory is allocated at thread block granularity, part of the memory may remain unutilized. In this paper, we propose architectural and compiler optimizations to improve the scratchpad utilization. Our approach, Scratchpad Sharing, addresses scratchpad under-utilization by launching additional thread blocks in each SM. These thread blocks use unutilized scratchpad and also share scratchpad with other resident blocks. To improve the performance of scratchpad sharing, we propose Owner Warp First (OWF) scheduling that schedules warps from the additional thread blocks effectively. The performance of this approach, however, is limited by the availability of the shared part of scratchpad.   We propose compiler optimizations to improve the availability of shared scratchpad. We describe a scratchpad allocation scheme that helps in allocating scratchpad variables such that shared scratchpad is accessed for short duration. We introduce a new instruction, relssp, that when executed, releases the shared scratchpad. Finally, we describe an analysis for optimal placement of relssp instructions such that shared scratchpad is released as early as possible.   We implemented the hardware changes using the GPGPU-Sim simulator and implemented the compiler optimizations in Ocelot framework. We evaluated the effectiveness of our approach on 19 kernels from 3 benchmarks suites: CUDA-SDK, GPGPU-Sim, and Rodinia. The kernels that underutilize scratchpad memory show an average improvement of 19% and maximum improvement of 92.17% compared to the baseline approach."
"In recent years, maritime safety and efficiency become more and more important across the world. Automatic Identification System (AIS) tracks vessel movement by onboard transceiver and terrestrial and/or satellite base station. The data collected by AIS contains broadcast kinematic information and static information. Both of them are useful for anomaly detection and route prediction which are key techniques in intelligent maritime research area. This paper is devoted to construct a standard AIS database for maritime trajectory learning, prediction and data mining. A path prediction algorithm is tested on this AIS database and the testing results show this database can be used as a standardized training resource for different trajectory prediction algorithms and other AIS data mining algorithms."
"Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with 'deep' transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which are deep not only in time but also in space, extending the LSTM architecture to larger step-to-step transition depths. Experiments demonstrate that the proposed architecture results in powerful and efficient models benefiting from up to 10 layers in the recurrent transition. On the Penn Treebank language modeling corpus, a single network outperforms all previous ensemble results with a perplexity of 66.0 on the test set. On the larger Hutter Prize Wikipedia dataset, a single network again significantly outperforms all previous results with an entropy of 1.32 bits per character on the test set."
"Characterizing driving styles of human drivers using vehicle sensor data, e.g., GPS, is an interesting research problem and an important real-world requirement from automotive industries. A good representation of driving features can be highly valuable for autonomous driving, auto insurance, and many other application scenarios. However, traditional methods mainly rely on handcrafted features, which limit machine learning algorithms to achieve a better performance. In this paper, we propose a novel deep learning solution to this problem, which could be the first attempt of extending deep learning to driving behavior analysis based on GPS data. The proposed approach can effectively extract high level and interpretable features describing complex driving patterns. It also requires significantly less human experience and work. The power of the learned driving style representations are validated through the driver identification problem using a large real dataset."
"To obtain the highest confidence on the correction of numerical simulation programs implementing the finite element method, one has to formalize the mathematical notions and results that allow to establish the soundness of the method. The Lax-Milgram theorem may be seen as one of those theoretical cornerstones: under some completeness and coercivity assumptions, it states existence and uniqueness of the solution to the weak formulation of some boundary value problems. The purpose of this document is to provide the formal proof community with a very detailed pen-and-paper proof of the Lax-Milgram theorem."
"Semantic object parts can be useful for several visual recognition tasks. Lately, these tasks have been addressed using Convolutional Neural Networks (CNN), achieving outstanding results. In this work we study whether CNNs learn semantic parts in their internal representation. We investigate the responses of convolutional filters and try to associate their stimuli with semantic parts. We perform two extensive quantitative analysis. First, we use ground-truth part bounding-boxes from the PASCAL-Part dataset to determine how many of those semantic parts emerge in the CNN. We explore this emergence for different layers, network depths, and supervision levels. Second, we collect human judgements in order to study what fraction of all filters systematically fire on any semantic part, even if not annotated in PASCAL-Part. Moreover, we explore several connections between discriminative power and semantics. We find out which are the most discriminative filters for object recognition, and analyze whether they respond to semantic parts or to other image patches. We also investigate the other direction: we determine which semantic parts are the most discriminative and whether they correspond to those parts emerging in the network. This enables to gain an even deeper understanding of the role of semantic parts in the network."
"An identifying code in a graph is a subset of vertices having a nonempty and distinct intersection with the closed neighborhood of every vertex. We prove that the infimum density of any identifying code in $S_k$ (an infinite strip of $k$ rows in the square grid) can always be achieved by a periodic identifying code with pattern length at most $2^{4k}$. Assisted by a compute program implementing Karp's algorithm for minimum cycle mean, we find a periodic identifying code in $S_4$ with the minimum density $11/28$, and a periodic identifying code in $S_5$ with the minimum density $19/50$."
"We study the problem of planning coordinated motions for two disc robots in an otherwise obstacle-free plane. We give a characterization of collision-avoiding motions that minimize the total trace length of the disc centres, for all initial and final configurations of the robots. The individual traces are composed of at most six (straight or circular-arc) segments, and their total length can be expressed as a simple integral with a closed form solution depending only on the initial and final configuration of the robots."
"A non-empty subset of a topological space is irreducible if whenever it is covered by the union of two closed sets, then already it is covered by one of them. Irreducible sets occur in proliferation: (1) every singleton set is irreducible, (2) directed subsets (which of fundamental status in domain theory) of a poset are exactly its Alexandroff irreducible sets, (3) directed subsets (with respect to the specialization order) of a $T_0$ space are always irreducible, and (4) the topological closure of every irreducible set is again irreducible. In recent years, the usefulness of irreducible sets in domain theory and non-Hausdorff topology has expanded. Notably, Zhao and Ho (2009) developed the core of domain theory directly in the context of $T_0$ spaces by choosing the irreducible sets as the topological substitute for directed sets. Just as the existence of suprema of directed subsets is featured prominently in domain theory (and hence the notion of a dcpo -- a poset in which all directed suprema exist), so too is that of irreducible subsets in the topological domain theory developed by Zhao and Ho (2009). The topological counterpart of a dcpo is thus this: A $T_0$ space is said to be strongly complete if the suprema of all irreducible subsets exist. In this paper, we show that the category, $\mathbf{scTop^+}$, of strongly complete $T_0$ spaces forms are reflective subcategory of a certain lluf subcategory, $\mathbf{Top^+}$, of $T_0$ spaces."
"Structure editors allow programmers to edit the tree structure of a program directly. This can have cognitive benefits, particularly for novice and end-user programmers (as evidenced by the popularity of structure editors like Scratch.) It also simplifies matters for tool designers, because they do not need to contend with malformed program text.   This paper defines Hazelnut, a structure editor based on a small bidirectionally typed lambda calculus extended with holes and a cursor (a la Huet's zipper.) Hazelnut goes one step beyond syntactic well-formedness: it's edit actions operate over statically meaningful (i.e. well-typed) terms. Naively, this prohibition on ill-typed edit states would force the programmer to construct terms in a rigid ""outside-in"" manner. To avoid this problem, the action semantics automatically places terms assigned a type that is inconsistent with the expected type inside a hole. This safely defers the type consistency check until the term inside the hole is finished.   Hazelnut is a foundational type-theoretic account of typed structure editing, rather than an end-user tool itself. To that end, we describe how Hazelnut's rich metatheory, which we have mechanized in Agda, guides the definition of an extension to the calculus. We also discuss various plausible evaluation strategies for terms with holes, and in so doing reveal connections with gradual typing and contextual modal type theory (the Curry-Howard interpretation of contextual modal logic.) Finally, we discuss how Hazelnut's semantics lends itself to implementation as a functional reactive program. Our reference implementation is written using js_of_ocaml."
"In this paper, we discuss the accuracy and complexity of various numerical techniques to solve the stochastic Landau-Lifshitz-Gilbert-Slonczewski (s-LLGS) equation. The s-LLGS equation is widely used by researchers to study the temporal evolution of the macrospin subject to spin torque and thermal noise. The numerical simulation of the s-LLGS equation requires an appropriate choice of stochastic calculus and the numerical integration scheme. In this paper, we focus on implicit midpoint, Heun, and Euler-Heun methods that converge to the Stratonovich solution of the s-LLGS equation. We also demonstrate a new method intended to solve stochastic differential equations (SDEs) with small noise, and test its capability to handle the s-LLGS equation. The choice of specific stochastic calculus while solving SDEs determines which numerical integration scheme to use. In this sense, methods, such as Euler and Gear, which are typically used by SPICE-based circuit simulators do not yield the expected outcome when solving the Stratonovich s-LLGS equation. While the trapezoidal method in SPICE does solve for the Stratonovich solution, its accuracy is limited by the minimum time-step of integration in SPICE. Through several numerical tests, including path-wise error, preservation of the magnetization norm, and 50% magnetization reversal boundary of the macrospin, we clearly illustrate the accuracy of various numerical schemes for solving the s-LLGS equation. The results in this paper will serve as guidelines for researchers to understand the tradeoffs between accuracy and complexity of various numerical methods and the choice of appropriate calculus to handle SDEs."
"This paper adapts a popular image quality measure called structural similarity for high precision registration based tracking while also introducing a simpler and faster variant of the same. Further, these are evaluated comprehensively against existing measures using a unified approach to study registration based trackers that decomposes them into three constituent sub modules - appearance model, state space model and search method. Several popular trackers in literature are broken down using this method so that their contributions - as of this paper - are shown to be limited to only one or two of these submodules. An open source tracking framework is made available and is used to perform all experiments on four publicly available datasets so the results are easily reproducible. In addition, by following this decomposition closely through extensive use of generic programming, this framework provides a convenient interface to plug in a new method for any sub module and combine it with existing methods for the others. It can also serve as a fast and flexible solution for practical tracking applications due to its highly efficient C++ implementation."
"A novel deep learning method for improving the belief propagation algorithm is proposed. The method generalizes the standard belief propagation algorithm by assigning weights to the edges of the Tanner graph. These edges are then trained using deep learning techniques. A well-known property of the belief propagation algorithm is the independence of the performance on the transmitted codeword. A crucial property of our new method is that our decoder preserved this property. Furthermore, this property allows us to learn only a single codeword instead of exponential number of code-words. Improvements over the belief propagation algorithm are demonstrated for various high density parity check codes."
"A set of software metrics for the evaluation of power management systems (PMSs) is presented. Such systems for managing power need to be autonomous, scalable, low in complexity, and comprised of portable algorithms in order to be well applied across the varying implementations that utilize power systems. Although similar metrics exist for software in general, their definitions do not readily lend themselves to the unique characteristics of power management systems or systems of similar architecture."
"Several tasks in argumentation mining and debating, question-answering, and natural language inference involve classifying a sequence in the context of another sequence (referred as bi-sequence classification). For several single sequence classification tasks, the current state-of-the-art approaches are based on recurrent and convolutional neural networks. On the other hand, for bi-sequence classification problems, there is not much understanding as to the best deep learning architecture. In this paper, we attempt to get an understanding of this category of problems by extensive empirical evaluation of 19 different deep learning architectures (specifically on different ways of handling context) for various problems originating in natural language processing like debating, textual entailment and question-answering. Following the empirical evaluation, we offer our insights and conclusions regarding the architectures we have considered. We also establish the first deep learning baselines for three argumentation mining tasks."
"Locally-biased graph algorithms are algorithms that attempt to find local or small-scale structure in a large data graph. In some cases, this can be accomplished by adding some sort of locality constraint and calling a traditional graph algorithm; but more interesting are locally-biased graph algorithms that compute answers by running a procedure that does not even look at most of the input graph. This corresponds more closely to what practitioners from various data science domains do, but it does not correspond well with the way that algorithmic and statistical theory is typically formulated. Recent work from several research communities has focused on developing locally-biased graph algorithms that come with strong complementary algorithmic and statistical theory and that are useful in practice in downstream data science applications. We provide a review and overview of this work, highlighting commonalities between seemingly-different approaches, and highlighting promising directions for future work."
"In this work, we analyze the failing sets of the interval-passing algorithm (IPA) for compressed sensing. The IPA is an efficient iterative algorithm for reconstructing a k-sparse nonnegative n-dimensional real signal x from a small number of linear measurements y. In particular, we show that the IPA fails to recover x from y if and only if it fails to recover a corresponding binary vector of the same support, and also that only positions of nonzero values in the measurement matrix are of importance for success of recovery. Based on this observation, we introduce termatiko sets and show that the IPA fails to fully recover x if and only if the support of x contains a nonempty termatiko set, thus giving a complete (graph-theoretic) description of the failing sets of the IPA. Finally, we present an extensive numerical study showing that in many cases there exist termatiko sets of size strictly smaller than the stopping distance of the binary measurement matrix; even as low as half the stopping distance in some cases."
"We introduce a general framework for end-to-end optimization of the rate--distortion performance of nonlinear transform codes assuming scalar quantization. The framework can be used to optimize any differentiable pair of analysis and synthesis transforms in combination with any differentiable perceptual metric. As an example, we consider a code built from a linear transform followed by a form of multi-dimensional local gain control. Distortion is measured with a state-of-the-art perceptual metric. When optimized over a large database of images, this representation offers substantial improvements in bitrate and perceptual appearance over fixed (DCT) codes, and over linear transform codes optimized for mean squared error."
"In this note the precise minimum number of key comparisons any dual-pivot quickselect algorithm (without sampling) needs on average is determined. The result is in the form of exact as well as asymptotic formul\ae{} of this number of a comparison-optimal algorithm. It turns out that the main terms of these asymptotic expansions coincide with the main terms of the corresponding analysis of the classical quickselect, but still---as this was shown for Yaroslavskiy quickselect---more comparisons are needed in the dual-pivot variant. The results are obtained by solving a second order differential equation for the generating function obtained from a recursive approach."
"We study the role of the second language in bilingual word embeddings in monolingual semantic evaluation tasks. We find strongly and weakly positive correlations between down-stream task performance and second language similarity to the target language. Additionally, we show how bilingual word embeddings can be employed for the task of semantic language classification and that joint semantic spaces vary in meaningful ways across second languages. Our results support the hypothesis that semantic language similarity is influenced by both structural similarity as well as geography/contact."
"The present manuscript deals with the realization of Processing In-memory (PIM) computing architecture using Quantum Dot Cellular Automata (QCA) and Akers array. The PIM computing architecture becomes popular due to its effective framework for storage and computation of data in a single unit. Here, we illustrate two input NAND and NOR gate with the help of QCA based Akers Array as a case study. The QCA flip-flop is used as a primitive cell to design PIM architecture. The results suggested that both the gate have minimum power dissipation. The polarization results of proposed architecture suggested that the signals are in good control. The foot print of the primitive cell equals to 0.04 micron^2, which is smaller than conventional CMOS primitive cell. The combination of QCA and Akers array provides many additional benefits over the conventional architecture like reduction in the power consumption and feature size, furthermore, it also improves the computational speed."
"We present a novel dynamic configuration technique for deep neural networks that permits step-wise energy-accuracy trade-offs during runtime. Our configuration technique adjusts the number of channels in the network dynamically depending on response time, power, and accuracy targets. To enable this dynamic configuration technique, we co-design a new training algorithm, where the network is incrementally trained such that the weights in channels trained in earlier steps are fixed. Our technique provides the flexibility of multiple networks while storing and utilizing one set of weights. We evaluate our techniques using both an ASIC-based hardware accelerator as well as a low-power embedded GPGPU and show that our approach leads to only a small or negligible loss in the final network accuracy. We analyze the performance of our proposed methodology using three well-known networks for MNIST, CIFAR-10, and SVHN datasets, and we show that we are able to achieve up to 95% energy reduction with less than 1% accuracy loss across the three benchmarks. In addition, compared to prior work on dynamic network reconfiguration, we show that our approach leads to approximately 50% savings in storage requirements, while achieving similar accuracy."
"This paper proposes the application of the waveform relaxation method to the homogenization of multiscale magnetoquasistatic problems. In the monolithic heterogeneous multiscale method, the nonlinear macroscale problem is solved using the Newton--Raphson scheme. The resolution of many mesoscale problems per Gauss point allows to compute the homogenized constitutive law and its derivative by finite differences. In the proposed approach, the macroscale problem and the mesoscale problems are weakly coupled and solved separately using the finite element method on time intervals for several waveform relaxation iterations. The exchange of information between both problems is still carried out using the heterogeneous multiscale method. However, the partial derivatives can now be evaluated exactly by solving only one mesoscale problem per Gauss point."
"Polycyclic groups are natural generalizations of cyclic groups but with more complicated algorithmic properties. They are finitely presented and the word, conjugacy, and isomorphism decision problems are all solvable in these groups. Moreover, the non-virtually nilpotent ones exhibit an exponential growth rate. These properties make them suitable for use in group-based cryptography, which was proposed in 2004 by Eick and Kahrobaei. Since then, many cryptosystems have been created that employ polycyclic groups. These include key exchanges such as non-commutative ElGamal, authentication schemes based on the twisted conjugacy problem, and secret sharing via the word problem. In response, heuristic and deterministic methods of cryptanalysis have been developed, including the length-based and linear decomposition attacks. Despite these efforts, there are classes of infinite polycyclic groups that remain suitable for cryptography. The analysis of algorithms for search and decision problems in polycyclic groups has also been developed. In addition to results for the aforementioned problems we present those concerning polycyclic representations, group morphisms, and orbit decidability. Though much progress has been made, many algorithmic and complexity problems remain unsolved, we conclude with a number of them. Of particular interest is to show that cryptosystems using infinite polycyclic groups are resistant to cryptanalysis on a quantum computer."
"Given a field $F$, a scalar $\lambda\in F$ and a matrix $A\in F^{n\times n}$, the twisted centralizer code $C_F(A,\lambda):=\{B\in F^{n\times n}\mid AB-\lambda BA=0\}$ is a linear code of length $n^2$. When $A$ is cyclic and $\lambda\ne0$ we prove that $\dim C_F(A,\lambda)=\mathrm{deg}(\gcd(c_A(t),\lambda^n c_A(\lambda^{-1}t)))$ where $c_A(t)$ denotes the characteristic polynomial of $A$. We also show how $C_F(A,\lambda)$ decomposes, and we estimate the probability that $C_F(A,\lambda)$ is nonzero when $|F|$ is finite. Finally, we prove $\dim C_F(A,\lambda)\leqslant n^2/2$ for $\lambda\not\in\{0,1\}$ and `almost all' matrices $A$."
"Aims. We present an innovative artificial neural network (ANN) architecture, called Generative ANN (GANN), that computes the forward model, that is it learns the function that relates the unknown outputs (stellar atmospheric parameters, in this case) to the given inputs (spectra). Such a model can be integrated in a Bayesian framework to estimate the posterior distribution of the outputs. Methods. The architecture of the GANN follows the same scheme as a normal ANN, but with the inputs and outputs inverted. We train the network with the set of atmospheric parameters (Teff, logg, [Fe/H] and [alpha/Fe]), obtaining the stellar spectra for such inputs. The residuals between the spectra in the grid and the estimated spectra are minimized using a validation dataset to keep solutions as general as possible. Results. The performance of both conventional ANNs and GANNs to estimate the stellar parameters as a function of the star brightness is presented and compared for different Galactic populations. GANNs provide significantly improved parameterizations for early and intermediate spectral types with rich and intermediate metallicities. The behaviour of both algorithms is very similar for our sample of late-type stars, obtaining residuals in the derivation of [Fe/H] and [alpha/Fe] below 0.1dex for stars with Gaia magnitude Grvs<12, which accounts for a number in the order of four million stars to be observed by the Radial Velocity Spectrograph of the Gaia satellite. Conclusions. Uncertainty estimation of computed astrophysical parameters is crucial for the validation of the parameterization itself and for the subsequent exploitation by the astronomical community. GANNs produce not only the parameters for a given spectrum, but a goodness-of-fit between the observed spectrum and the predicted one for a given set of parameters. Moreover, they allow us to obtain the full posterior distribution..."
"The Knowledge Gradient (KG) policy was originally proposed for online ranking and selection problems but has recently been adapted for use in online decision making in general and multi-armed bandit problems (MABs) in particular. We study its use in a class of exponential family MABs and identify weaknesses, including a propensity to take actions which are dominated with respect to both exploitation and exploration. We propose variants of KG which avoid such errors. These new policies include an index heuristic which deploys a KG approach to develop an approximation to the Gittins index. A numerical study shows this policy to perform well over a range of MABs including those for which index policies are not optimal. While KG does not make dominated actions when bandits are Gaussian, it fails to be index consistent and appears not to enjoy a performance advantage over competitor policies when arms are correlated to compensate for its greater computational demands."
"The quantity of small scale solar photovoltaic (PV) arrays in the United States has grown rapidly in recent years. As a result, there is substantial interest in high quality information about the quantity, power capacity, and energy generated by such arrays, including at a high spatial resolution (e.g., counties, cities, or even smaller regions). Unfortunately, existing methods for obtaining this information, such as surveys and utility interconnection filings, are limited in their completeness and spatial resolution. This work presents a computer algorithm that automatically detects PV panels using very high resolution color satellite imagery. The approach potentially offers a fast, scalable method for obtaining accurate information on PV array location and size, and at much higher spatial resolutions than are currently available. The method is validated using a very large (135 km^2) collection of publicly available [1] aerial imagery, with over 2,700 human annotated PV array locations. The results demonstrate the algorithm is highly effective on a per-pixel basis. It is likewise effective at object-level PV array detection, but with significant potential for improvement in estimating the precise shape/size of the PV arrays. These results are the first of their kind for the detection of solar PV in aerial imagery, demonstrating the feasibility of the approach and establishing a baseline performance for future investigations."
"We present a polynomial-space algorithm that computes the number independent sets of any input graph in time $O(1.1387^n)$ for graphs with maximum degree 3 and in time $O(1.2355^n)$ for general graphs, where n is the number of vertices. Together with the inclusion-exclusion approach of Bj\""orklund, Husfeldt, and Koivisto [SIAM J. Comput. 2009], this leads to a faster polynomial-space algorithm for the graph coloring problem with running time $O(2.2355^n)$. As a byproduct, we also obtain an exponential-space $O(1.2330^n)$ time algorithm for counting independent sets. Our main algorithm counts independent sets in graphs with maximum degree 3 and no vertex with three neighbors of degree 3. This polynomial-space algorithm is analyzed using the recently introduced Separate, Measure and Conquer approach [Gaspers & Sorkin, ICALP 2015]. Using Wahlstr\""om's compound measure approach, this improvement in running time for small degree graphs is then bootstrapped to larger degrees, giving the improvement for general graphs. Combining both approaches leads to some inflexibility in choosing vertices to branch on for the small-degree cases, which we counter by structural graph properties."
"Phylogenetic networks are increasingly used in evolutionary biology to represent the history of species that have undergone reticulate events such as horizontal gene transfer, hybrid speciation and recombination. One of the most fundamental questions that arise in this context is whether the evolution of a gene with one copy in all species can be explained by a given network. In mathematical terms, this is often translated in the following way: is a given phylogenetic tree contained in a given phylogenetic network? Recently this tree containment problem has been widely investigated from a computational perspective, but most studies have only focused on the topology of the phylo- genies, ignoring a piece of information that, in the case of phylogenetic trees, is routinely inferred by evolutionary analyses: branch lengths. These measure the amount of change (e.g., nucleotide substitutions) that has occurred along each branch of the phylogeny. Here, we study a number of versions of the tree containment problem that explicitly account for branch lengths. We show that, although length information has the potential to locate more precisely a tree within a network, the problem is computationally hard in its most general form. On a positive note, for a number of special cases of biological relevance, we provide algorithms that solve this problem efficiently. This includes the case of networks of limited complexity, for which it is possible to recover, among the trees contained by the network with the same topology as the input tree, the closest one in terms of branch lengths."
"We design a new nonparametric method that allows one to estimate the matrix of integrated kernels of a multivariate Hawkes process. This matrix not only encodes the mutual influences of each nodes of the process, but also disentangles the causality relationships between them. Our approach is the first that leads to an estimation of this matrix without any parametric modeling and estimation of the kernels themselves. A consequence is that it can give an estimation of causality relationships between nodes (or users), based on their activity timestamps (on a social network for instance), without knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment matching method that fits the third-order integrated cumulants of the process. We show on numerical experiments that our approach is indeed very robust to the shape of the kernels, and gives appealing results on the MemeTracker database."
"We propose a novel support vector regression approach called e-Distance Weighted Support Vector Regression (e-DWSVR).e-DWSVR specifically addresses two challenging issues in support vector regression: first, the process of noisy data; second, how to deal with the situation when the distribution of boundary data is different from that of the overall data. The proposed e-DWSVR optimizes the minimum margin and the mean of functional margin simultaneously to tackle these two issues. In addition, we use both dual coordinate descent (CD) and averaged stochastic gradient descent (ASGD) strategies to make e-DWSVR scalable to large scale problems. We report promising results obtained by e-DWSVR in comparison with existing methods on several benchmark datasets."
"This paper considers the problem of interpolating signals defined on graphs. A major presumption considered by many previous approaches to this problem has been low-pass/band-limitedness of the underlying graph signal. However, inspired by the findings on sparse signal reconstruction, we consider the graph signal to be rather sparse/compressible in the Graph Fourier Transform (GFT) domain and propose the Iterative Method with Adaptive Thresholding for Graph Interpolation (IMATGI) algorithm for sparsity promoting interpolation of the underlying graph signal. We provide a formal convergence proof for the proposed algorithm. We also demonstrate efficient performance of the proposed IMATGI algorithm in reconstructing randomly generated sparse graph signals. Finally, we consider the widely desirable application of recommendation systems and show by simulations that IMATGI outperforms state-of-the-art algorithms on the benchmark datasets in this application."
"Low level images representation in feature space performs poorly for classification with high accuracy since this level of representation is not able to project images into the discriminative feature space. In this work, we propose an efficient image representation model for classification. First we apply Hidden Markov Model (HMM) on ordered grids represented by different type of image descriptors in order to include causality of local properties existing in image for feature extraction and then we train up a separate classifier for each of these features sets. Finally we ensemble these classifiers efficiently in a way that they can cancel out each other errors for obtaining higher accuracy. This method is evaluated on 15 natural scene dataset. Experimental results show the superiority of the proposed method in comparison to some current existing methods"
"In this paper we proposed an ordered patch based method using Conditional Random Field (CRF) in order to encode local properties and their spatial relationship in images to address texture classification, face recognition, and scene classification problems. Typical image classification approaches work without considering spatial causality among distinctive properties of an image for image representation in feature space. In this method first, each image is encoded as a sequence of ordered patches, including local properties. Second, the sequence of these ordered patches is modeled as a probabilistic feature vector by CRF to model spatial relationship of these local properties. And finally, image classification is performed on such probabilistic image representation. Experimental results on several standard image datasets indicate that proposed method outperforms some of existing image classification methods."
"Preserving details in restoring images highly corrupted by impulse noise remains a challenging problem. We proposed an algorithm based on radial basis functions (RBF) interpolation which estimates the intensities of corrupted pixels by their neighbors. In this algorithm, first intensity values of noisy pixels in the corrupted image are estimated using RBFs. Next, the image is smoothed. The proposed algorithm can effectively remove the highly dense impulse noise. Experimental results show the superiority of the proposed algorithm in comparison to the recent similar methods both in noise suppression and detail preservation. Extensive simulations show better results in measure of peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM), especially when the image is corrupted by very highly dense impulse noise."
"Understanding visual reality involves acquiring common-sense knowledge about countless regularities in the visual world, e.g., how illumination alters the appearance of objects in a scene, and how motion changes their apparent spatial relationship. These regularities are hard to label for training supervised machine learning algorithms; consequently, algorithms need to learn these regularities from the real world in an unsupervised way. We present a novel network meta-architecture that can learn world dynamics from raw, continuous video. The components of this network can be implemented using any algorithm that possesses three key capabilities: prediction of a signal over time, reduction of signal dimensionality (compression), and the ability to use supplementary contextual information to inform the prediction. The presented architecture is highly-parallelized and scalable, and is implemented using localized connectivity, processing, and learning. We demonstrate an implementation of this architecture where the components are built from multi-layer perceptrons. We apply the implementation to create a system capable of stable and robust visual tracking of objects as seen by a moving camera. Results show performance on par with or exceeding state-of-the-art tracking algorithms. The tracker can be trained in either fully supervised or unsupervised-then-briefly-supervised regimes. Success of the briefly-supervised regime suggests that the unsupervised portion of the model extracts useful information about visual reality. The results suggest a new class of AI algorithms that uniquely combine prediction and scalability in a way that makes them suitable for learning from and --- and eventually acting within --- the real world."
"Factorization machines (FMs) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional. Unfortunately, despite increasing interest in FMs, there exists to date no efficient training algorithm for higher-order FMs (HOFMs). In this paper, we present the first generic yet efficient algorithms for training arbitrary-order HOFMs. We also present new variants of HOFMs with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy. We demonstrate the proposed approaches on four different link prediction tasks."
"We strive to use session type technology to prove behavioural properties of fault-tolerant distributed algorithms. Session types are designed to abstractly capture the structure of (even multi-party) communication protocols. The goal of session types is the analysis and verification of the protocols' behavioural properties. One important such property is progress, i.e., the absence of (unintended) deadlock. Distributed algorithms often resemble (compositions of) multi-party communication protocols. In contrast to protocols that are typically studied with session types, they are often designed to cope with system failures. An essential behavioural property is (successful) termination, despite failures, but it is often elaborate to prove for distributed algorithms.   We extend multi-party session types (and multi-party session types with nested sessions) by optional blocks that cover a limited class of link and crash failures. This allows us to automatically derive termination of distributed algorithms that come within these limits. To illustrate our approach, we prove termination for an implementation of the *rotating coordinator* Consensus algorithm."
"Large-scale annotated datasets allow AI systems to learn from and build upon the knowledge of the crowd. Many crowdsourcing techniques have been developed for collecting image annotations. These techniques often implicitly rely on the fact that a new input image takes a negligible amount of time to perceive. In contrast, we investigate and determine the most cost-effective way of obtaining high-quality multi-label annotations for temporal data such as videos. Watching even a short 30-second video clip requires a significant time investment from a crowd worker; thus, requesting multiple annotations following a single viewing is an important cost-saving strategy. But how many questions should we ask per video? We conclude that the optimal strategy is to ask as many questions as possible in a HIT (up to 52 binary questions after watching a 30-second video clip in our experiments). We demonstrate that while workers may not correctly answer all questions, the cost-benefit analysis nevertheless favors consensus from multiple such cheap-yet-imperfect iterations over more complex alternatives. When compared with a one-question-per-video baseline, our method is able to achieve a 10% improvement in recall 76.7% ours versus 66.7% baseline) at comparable precision (83.8% ours versus 83.0% baseline) in about half the annotation time (3.8 minutes ours compared to 7.1 minutes baseline). We demonstrate the effectiveness of our method by collecting multi-label annotations of 157 human activities on 1,815 videos."
"We present a novel approach for collision-free global navigation for continuous-time multi-agent systems with general linear dynamics. Our approach is general and can be used to perform collision-free navigation in 2D and 3D workspaces with narrow passages and crowded regions. As part of pre-computation, we compute multiple bridges in the narrow or tight regions in the workspace using kinodynamic RRT algorithms. Our bridge has certain geometric characteristics, that en- able us to calculate a collision-free trajectory for each agent using simple interpolation at runtime. Moreover, we combine interpolated bridge trajectories with local multi-agent navigation algorithms to compute global collision-free paths for each agent. The overall approach combines the performance benefits of coupled multi-agent algorithms with the pre- computed trajectories of the bridges to handle challenging scenarios. In practice, our approach can handle tens to hundreds of agents in real-time on a single CPU core in 2D and 3D workspaces."
"Spanners, emulators, and approximate distance oracles can be viewed as lossy compression schemes that represent an unweighted graph metric in small space, say $\tilde{O}(n^{1+\delta})$ bits. There is an inherent tradeoff between the sparsity parameter $\delta$ and the stretch function $f$ of the compression scheme, but the qualitative nature of this tradeoff has remained a persistent open problem.   In this paper we show that the recent additive spanner lower bound of Abboud and Bodwin is just the first step in a hierarchy of lower bounds that fully characterize the asymptotic behavior of the optimal stretch function $f$ as a function of $\delta \in (0,1/3)$. Specifically, for any integer $k\ge 2$, any compression scheme with size $O(n^{1+\frac{1}{2^k-1} - \epsilon})$ has a sublinear additive stretch function $f$: $$f(d) = d + \Omega(d^{1-\frac{1}{k}}).$$ This lower bound matches Thorup and Zwick's (2006) construction of sublinear additive emulators. It also shows that Elkin and Peleg's $(1+\epsilon,\beta)$-spanners have an essentially optimal tradeoff between $\delta,\epsilon,$ and $\beta$, and that the sublinear additive spanners of Pettie (2009) and Chechik (2013) are not too far from optimal.   To complement these lower bounds we present a new construction of $(1+\epsilon, O(k/\epsilon)^{k-1})$-spanners with size $O((k/\epsilon)^{h_k} kn^{1+\frac{1}{2^{k+1}-1}})$, where $h_k < 3/4$. This size bound improves on the spanners of Elkin and Peleg (2004), Thorup and Zwick (2006), and Pettie (2009). According to our lower bounds neither the size nor stretch function can be substantially improved."
"Effective SLAM using a single monocular camera is highly preferred due to its simplicity. However, when compared to trajectory planning methods using depth-based SLAM, Monocular SLAM in loop does need additional considerations. One main reason being that for the optimization, in the form of Bundle Adjustment (BA), to be robust, the SLAM system needs to scan the area for a reasonable duration. Most monocular SLAM systems do not tolerate large camera rotations between successive views and tend to breakdown. Other reasons for Monocular SLAM failure include ambiguities in decomposition of the Essential Matrix, feature-sparse scenes and more layers of non linear optimization apart from BA. This paper presents a novel formulation based on Reinforcement Learning (RL) that generates fail safe trajectories wherein the SLAM generated outputs (scene structure and camera motion) do not deviate largely from their true values. Quintessentially, the RL framework successfully learns the otherwise complex relation between motor actions and perceptual inputs that result in trajectories that do not cause failure of SLAM, which are almost intractable to capture in an obvious mathematical formulation. We show systematically in simulations how the quality of the SLAM map and trajectory dramatically improves when trajectories are computed by using RL."
"In the design of software and cyber-physical systems, security is often perceived as a qualitative need, but can only be attained quantitatively. Especially when distributed components are involved, it is hard to predict and confront all possible attacks. A main challenge in the development of complex systems is therefore to discover attacks, quantify them to comprehend their likelihood, and communicate them to non-experts for facilitating the decision process. To address this three-sided challenge we propose a protection analysis over the Quality Calculus that (i) computes all the sets of data required by an attacker to reach a given location in a system, (ii) determines the cheapest set of such attacks for a given notion of cost, and (iii) derives an attack tree that displays the attacks graphically. The protection analysis is first developed in a qualitative setting, and then extended to quantitative settings following an approach applicable to a great many contexts. The quantitative formulation is implemented as an optimisation problem encoded into Satisfiability Modulo Theories, allowing us to deal with complex cost structures. The usefulness of the framework is demonstrated on a national-scale authentication system, studied through a Java implementation of the framework."
"We introduce a framework for model learning and planning in stochastic domains with continuous state and action spaces and non-Gaussian transition models. It is efficient because (1) local models are estimated only when the planner requires them; (2) the planner focuses on the most relevant states to the current planning problem; and (3) the planner focuses on the most informative and/or high-value actions. Our theoretical analysis shows the validity and asymptotic optimality of the proposed approach. Empirically, we demonstrate the effectiveness of our algorithm on a simulated multi-modal pushing problem."
"In the real world datasets (e.g.,DBpedia query log), queries built on well-designed patterns containing only AND and OPT operators (for short, WDAO-patterns) account for a large proportion among all SPARQL queries. In this paper, we present a plugin-based framework for all SELECT queries built on WDAO-patterns, named PIWD. The framework is based on a parse tree called \emph{well-designed AND-OPT tree} (for short, WDAO-tree) whose leaves are basic graph patterns (BGP) and inner nodes are the OPT operators. We prove that for any WDAO-pattern, its parse tree can be equivalently transformed into a WDAO-tree. Based on the proposed framework, we can employ any query engine to evaluate BGP for evaluating queries built on WDAO-patterns in a convenient way. Theoretically, we can reduce the query evaluation of WDAO-patterns to subgraph homomorphism as well as BGP since the query evaluation of BGP is equivalent to subgraph homomorphism. Finally, our preliminary experiments on gStore and RDF-3X show that PIWD can answer all queries built on WDAO-patterns effectively and efficiently."
"Symbolic Computation and Satisfiability Checking are two research areas, both having their individual scientific focus but sharing also common interests in the development, implementation and application of decision procedures for arithmetic theories. Despite their commonalities, the two communities are rather weakly connected. The aim of our newly accepted SC-square project (H2020-FETOPEN-CSA) is to strengthen the connection between these communities by creating common platforms, initiating interaction and exchange, identifying common challenges, and developing a common roadmap from theory along the way to tools and (industrial) applications. In this paper we report on the aims and on the first activities of this project, and formalise some relevant challenges for the unified SC-square community."
"Convolutional neural networks (CNN) have led to many state-of-the-art results spanning through various fields. However, a clear and profound theoretical understanding of the forward pass, the core algorithm of CNN, is still lacking. In parallel, within the wide field of sparse approximation, Convolutional Sparse Coding (CSC) has gained increasing attention in recent years. A theoretical study of this model was recently conducted, establishing it as a reliable and stable alternative to the commonly practiced patch-based processing. Herein, we propose a novel multi-layer model, ML-CSC, in which signals are assumed to emerge from a cascade of CSC layers. This is shown to be tightly connected to CNN, so much so that the forward pass of the CNN is in fact the thresholding pursuit serving the ML-CSC model. This connection brings a fresh view to CNN, as we are able to attribute to this architecture theoretical claims such as uniqueness of the representations throughout the network, and their stable estimation, all guaranteed under simple local sparsity conditions. Lastly, identifying the weaknesses in the above pursuit scheme, we propose an alternative to the forward pass, which is connected to deconvolutional, recurrent and residual networks, and has better theoretical guarantees."
"The growing popularity and adoption of differential privacy in academic and industrial settings has resulted in the development of increasingly sophisticated algorithms for releasing information while preserving privacy. Accompanying this phenomenon is the natural rise in the development and publication of incorrect algorithms, thus demonstrating the necessity of formal verification tools. However, existing formal methods for differential privacy face a dilemma: methods based on customized logics can verify sophisticated algorithms but comes with a steep learning curve and significant annotation burden on the programmers; while existing programming platforms lack expressive power for some sophisticated algorithms.   In this paper, we present LightDP, a simple imperative language that strikes a better balance between expressive power and usefulness. The core of LightDP is a novel relational type system that separates relational reasoning from privacy budget calculations. With dependent types, the type system is powerful enough to verify sophisticated algorithms where the composition theorem falls short. In addition, the inference engine of LightDP infers most of the proof details, and even searches for the proof with minimal privacy cost when multiple proofs exist. We show that LightDP verifies sophisticated algorithms with little manual effort."
"An approach to the formal description of service contracts is presented in terms of automata. We focus on the basic property of guaranteeing that in the multi-party composition of principals each of them gets his requests satisfied, so that the overall composition reaches its goal. Depending on whether requests are satisfied synchronously or asynchronously, we construct an orchestrator that at static time either yields composed services enjoying the required properties or detects the principals responsible for possible violations. To do that in the asynchronous case we resort to Linear Programming techniques. We also relate our automata with two logically based methods for specifying contracts."
"The giant $k$-core --- maximal connected subgraph of a network where each node has at least $k$ neighbors --- is important in the study of phase transitions and in applications of network theory. Unlike Erd\H{o}s-R\'enyi graphs and other random networks where $k$-cores emerge discontinuously for $k\ge 3$, we show that transitive linking (or triadic closure) leads to 3-cores emerging through single or double phase transitions of both discontinuous and continuous nature. We also develop a $k$-core calculation that includes clustering and provides insights into how high-level connectivity emerges."
"We introduce Forman-Ricci curvature and its corresponding flow as characteristics for complex networks attempting to extend the common approach of node-based network analysis by edge-based characteristics. Following a theoretical introduction and mathematical motivation, we apply the proposed network-analytic methods to static and dynamic complex networks and compare the results with established node-based characteristics. Our work suggests a number of applications for data mining, including denoising and clustering of experimental data, as well as extrapolation of network evolution."
"Markerless motion capture algorithms require a 3D body with properly personalized skeleton dimension and/or body shape and appearance to successfully track a person. Unfortunately, many tracking methods consider model personalization a different problem and use manual or semi-automatic model initialization, which greatly reduces applicability. In this paper, we propose a fully automatic algorithm that jointly creates a rigged actor model commonly used for animation - skeleton, volumetric shape, appearance, and optionally a body surface - and estimates the actor's motion from multi-view video input only. The approach is rigorously designed to work on footage of general outdoor scenes recorded with very few cameras and without background subtraction. Our method uses a new image formation model with analytic visibility and analytically differentiable alignment energy. For reconstruction, 3D body shape is approximated as Gaussian density field. For pose and shape estimation, we minimize a new edge-based alignment energy inspired by volume raycasting in an absorbing medium. We further propose a new statistical human body model that represents the body surface, volumetric Gaussian density, as well as variability in skeleton shape. Given any multi-view sequence, our method jointly optimizes the pose and shape parameters of this model fully automatically in a spatiotemporal way."
"Limited angle problem is a challenging issue in x-ray computed tomography (CT) field. Iterative reconstruction methods that utilize the additional prior can suppress artifacts and improve image quality, but unfortunately require increased computation time. An interesting way is to restrain the artifacts in the images reconstructed from the practical filtered back projection (FBP) method. Frikel and Quinto have proved that the streak artifacts in FBP results could be characterized. It indicates that the artifacts created by FBP method have specific and similar characteristics in a stationary limited-angle scanning configuration. Based on this understanding, this work aims at developing a method to extract and suppress specific artifacts of FBP reconstructions for limited-angle tomography. A data-driven learning-based method is proposed based on a deep convolutional neural network. An end-to-end mapping between the FBP and artifact-free images is learned and the implicit features involving artifacts will be extracted and suppressed via nonlinear mapping. The qualitative and quantitative evaluations of experimental results indicate that the proposed method show a stable and prospective performance on artifacts reduction and detail recovery for limited angle tomography. The presented strategy provides a simple and efficient approach for improving image quality of the reconstruction results from limited projection data."
"We present a multi-contact walking pattern generator based on preview-control of the 3D acceleration of the center of mass (COM). A key point in the design of our algorithm is the calculation of contact-stability constraints. Thanks to a mathematical observation on the algebraic nature of the frictional wrench cone, we show that the 3D volume of feasible COM accelerations is a always a downward-pointing cone. We reduce its computation to a convex hull of (dual) 2D points, for which optimal O(n log n) algorithms are readily available. This reformulation brings a significant speedup compared to previous methods, which allows us to compute time-varying contact-stability criteria fast enough for the control loop. Next, we propose a conservative trajectory-wide contact-stability criterion, which can be derived from COM-acceleration volumes at marginal cost and directly applied in a model-predictive controller. We finally implement this pipeline and exemplify it with the HRP-4 humanoid model in multi-contact dynamically walking scenarios."
"We consider the algorithmic problem of generating each subset of $[n]:=\{1,2,\ldots,n\}$ whose size is in some interval $[k,l]$, $0\leq k\leq l\leq n$, exactly once (cyclically) by repeatedly adding or removing a single element, or by exchanging a single element. For $k=0$ and $l=n$ this is the classical problem of generating all $2^n$ subsets of $[n]$ by element additions/removals, and for $k=l$ this is the classical problem of generating all $\binom{n}{k}$ subsets of $[n]$ by element exchanges. We prove the existence of such cyclic minimum-change enumerations for a large range of values $n$, $k$, and $l$, improving upon and generalizing several previous results. For all these existential results we provide optimal algorithms to compute the corresponding Gray codes in constant time $\mathcal{O}(1)$ per generated set and space $\mathcal{O}(n)$. Rephrased in terms of graph theory, our results establish the existence of (almost) Hamilton cycles in the subgraph of the $n$-dimensional cube $Q_n$ induced by all levels $[k,l]$. We reduce all remaining open cases to a generalized version of the middle levels conjecture, which asserts that the subgraph of $Q_{2k+1}$ induced by all levels $[k-c,k+1+c]$, $c\in\{0,1,\ldots,k\}$, has a Hamilton cycle. We also prove an approximate version of this conjecture, showing that this graph has a cycle that visits a $(1-o(1))$-fraction of all vertices."
"Recently, implementing Radio Access Network (RAN) functionalities on cloud-based computing platform has become an emerging solution that leverages the many advantages of cloud infrastructure, such as shared computing resources and storage capacity, while lowering the operational cost. In this paper, we propose a novel caching framework aimed at fully exploiting the potential of such Cloud-based RAN (C-RAN) systems through cooperative hierarchical caching which minimizes the network costs of content delivery and improves users' Quality of Experience (QoE). In particular, we consider the cloud-cache in the cloud processing unit (CPU) as a new layer in the RAN cache hierarchy, bridging the capacity-performance gap between the traditional edge-based and core-based caching schemes. A delay cost model is introduced to characterize and formulate the cache placement optimization problem, which is shown to be NP-complete. As such, a low complexity, heuristic cache management strategy is proposed, constituting of a proactive cache distribution algorithm and a reactive cache replacement algorithm. Extensive numerical simulations are carried out using both real-world YouTube video requests and synthetic content requests. It is demonstrated that our proposed Octopus caching strategy significantly outperforms the traditional caching strategies in terms of cache hit ratio, average content access delay and backhaul traffic load."
"In this paper, we present a new approach to learning cascaded classifiers for use in computing environments that involve networks of heterogeneous and resource-constrained, low-power embedded compute and sensing nodes. We present a generalization of the classical linear detection cascade to the case of tree-structured cascades where different branches of the tree execute on different physical compute nodes in the network. Different nodes have access to different features, as well as access to potentially different computation and energy resources. We concentrate on the problem of jointly learning the parameters for all of the classifiers in the cascade given a fixed cascade architecture and a known set of costs required to carry out the computation at each node.To accomplish the objective of joint learning of all detectors, we propose a novel approach to combining classifier outputs during training that better matches the hard cascade setting in which the learned system will be deployed. This work is motivated by research in the area of mobile health where energy efficient real time detectors integrating information from multiple wireless on-body sensors and a smart phone are needed for real-time monitoring and delivering just-in-time adaptive interventions. We apply our framework to the problem of cigarette smoking detection from a combination of wrist-worn actigraphy data and respiration chest band data."
We present a non-linear dynamical system for modelling the effect of drug infusions on the vital signs of patients admitted in Intensive Care Units (ICUs). More specifically we are interested in modelling the effect of a widely used anaesthetic drug (Propofol) on a patient's monitored depth of anaesthesia and haemodynamics. We compare our approach with one from the Pharmacokinetics/Pharmacodynamics (PK/PD) literature and show that we can provide significant improvements in performance without requiring the incorporation of expert physiological knowledge in our system.
"In this paper we present a new coherence-based performance guarantee for the Orthogonal Matching Pursuit (OMP) algorithm. An upper bound for the probability of correctly identifying the support of a sparse signal with additive white Gaussian noise is derived. Compared to previous work, the new bound takes into account the signal parameters such as dynamic range, noise variance, and sparsity. Numerical simulations show significant improvements over previous work."
"The state-of-the-art CNN models give good performance on sentence classification tasks. The purpose of this work is to empirically study desirable properties such as semantic coherence, attention mechanism and reusability of CNNs in these tasks. Semantically coherent kernels are preferable as they are a lot more interpretable for explaining the decision of the learned CNN model. We observe that the learned kernels do not have semantic coherence. Motivated by this observation, we propose to learn kernels with semantic coherence using clustering scheme combined with Word2Vec representation and domain knowledge such as SentiWordNet. We suggest a technique to visualize attention mechanism of CNNs for decision explanation purpose. Reusable property enables kernels learned on one problem to be used in another problem. This helps in efficient learning as only a few additional domain specific filters may have to be learned. We demonstrate the efficacy of our core ideas of learning semantically coherent kernels and leveraging reusable kernels for efficient learning on several benchmark datasets. Experimental results show the usefulness of our approach by achieving performance close to the state-of-the-art methods but with semantic and reusable properties."
"Fine-grained classification is a relatively new field that has concentrated on using information from a single image, while ignoring the enormous potential of using video data to improve classification. In this work we present the novel task of video-based fine-grained object classification, propose a corresponding new video dataset, and perform a systematic study of several recent deep convolutional neural network (DCNN) based approaches, which we specifically adapt to the task. We evaluate three-dimensional DCNNs, two-stream DCNNs, and bilinear DCNNs. Two forms of the two-stream approach are used, where spatial and temporal data from two independent DCNNs are fused either via early fusion (combination of the fully-connected layers) and late fusion (concatenation of the softmax outputs of the DCNNs). For bilinear DCNNs, information from the convolutional layers of the spatial and temporal DCNNs is combined via local co-occurrences. We then fuse the bilinear DCNN and early fusion of the two-stream approach to combine the spatial and temporal information at the local and global level (Spatio-Temporal Co-occurrence). Using the new and challenging video dataset of birds, classification performance is improved from 23.1% (using single images) to 41.1% when using the Spatio-Temporal Co-occurrence system. Incorporating automatically detected bounding box location further improves the classification accuracy to 53.6%."
"This study presents a rapid multiple incremental and decremental mechanism based on Weight-Error Curves (WECs) for support-vector analysis. Recursion-free computation is proposed for predicting the Lagrangian multipliers of new samples. This study examines Ridge Support Vector Models, subsequently devising a recursion-free function derived from WECs. With the proposed function, all the new Lagrangian multipliers can be computed at once without using any gradual step sizes. Moreover, such a function relaxes a constraint, where the increment of new multiple Lagrangian multipliers should be the same in the previous work, thereby easily satisfying the requirement of KKT conditions. The proposed mechanism no longer requires typical bookkeeping strategies, which compute the step size by checking all the training samples in each incremental round."
"We investigate social networks of characters found in cultural works such as novels and films. These character networks exhibit many of the properties of complex networks such as skewed degree distribution and community structure, but may be of relatively small order with a high multiplicity of edges. Building on recent work of beveridge, we consider graph extraction, visualization, and network statistics for three novels: Twilight by Stephanie Meyer, Steven King's The Stand, and J.K. Rowling's Harry Potter and the Goblet of Fire. Coupling with 800 character networks from films found in the http://moviegalaxies.com/ database, we compare the data sets to simulations from various stochastic complex networks models including random graphs with given expected degrees (also known as the Chung-Lu model), the configuration model, and the preferential attachment model. Using machine learning techniques based on motif (or small subgraph) counts, we determine that the Chung-Lu model best fits character networks and we conjecture why this may be the case."
"Semantic labeling (or pixel-level land-cover classification) in ultra-high resolution imagery (< 10cm) requires statistical models able to learn high level concepts from spatial data, with large appearance variations. Convolutional Neural Networks (CNNs) achieve this goal by learning discriminatively a hierarchy of representations of increasing abstraction.   In this paper we present a CNN-based system relying on an downsample-then-upsample architecture. Specifically, it first learns a rough spatial map of high-level representations by means of convolutions and then learns to upsample them back to the original resolution by deconvolutions. By doing so, the CNN learns to densely label every pixel at the original resolution of the image. This results in many advantages, including i) state-of-the-art numerical accuracy, ii) improved geometric accuracy of predictions and iii) high efficiency at inference time.   We test the proposed system on the Vaihingen and Potsdam sub-decimeter resolution datasets, involving semantic labeling of aerial images of 9cm and 5cm resolution, respectively. These datasets are composed by many large and fully annotated tiles allowing an unbiased evaluation of models making use of spatial information. We do so by comparing two standard CNN architectures to the proposed one: standard patch classification, prediction of local label patches by employing only convolutions and full patch labeling by employing deconvolutions. All the systems compare favorably or outperform a state-of-the-art baseline relying on superpixels and powerful appearance descriptors. The proposed full patch labeling CNN outperforms these models by a large margin, also showing a very appealing inference time."
"A variety of statistical graphical models have been defined to represent the conditional independences underlying a random vector of interest. Similarly, many different graphs embedding various types of preferential independences, as for example conditional utility independence and generalized additive independence, have more recently started to appear. In this paper we define a new graphical model, called a directed expected utility network, whose edges depict both probabilistic and utility conditional independences. These embed a very flexible class of utility models, much larger than those usually conceived in standard influence diagrams. Our graphical representation, and various transformations of the original graph into a tree structure, are then used to guide fast routines for the computation of a decision problem's expected utilities. We show that our routines generalize those usually utilized in standard influence diagrams' evaluations under much more restrictive conditions. We then proceed with the construction of a directed expected utility network to support decision makers in the domain of household food security."
"In this paper, we study an extension of the stable model semantics for disjunctive logic programs where each true atom in a model is associated with an algebraic expression (in terms of rule labels) that represents its justifications. As in our previous work for non-disjunctive programs, these justifications are obtained in a purely semantic way, by algebraic operations (product, addition and application) on a lattice of causal values. Our new definition extends the concept of causal stable model to disjunctive logic programs and satisfies that each (standard) stable model corresponds to a disjoint class of causal stable models sharing the same truth assignments, but possibly varying the obtained explanations. We provide a pair of illustrative examples showing the behaviour of the new semantics and discuss the need of introducing a new type of rule, which we call causal-choice. This type of rule intuitively captures the idea of ""$A$ may cause $B$"" and, when causal information is disregarded, amounts to a usual choice rule under the standard stable model semantics. (Under consideration for publication in Theory and Practice of Logic Programming)"
"Soft noncoherent detection, which relies on calculating the \textit{a posteriori} probabilities (APPs) of the bits transmitted with no channel estimation, is imperative for achieving excellent detection performance in high-dimensional wireless communications. In this paper, a high-performance belief propagation (BP)-based soft multiple-symbol differential detection (MSDD) framework, dubbed BP-MSDD, is proposed with its illustrative application in differential space-time block-code (DSTBC)-aided ultra-wideband impulse radio (UWB-IR) systems. Firstly, we revisit the signal sampling with the aid of a trellis structure and decompose the trellis into multiple subtrellises. Furthermore, we derive an APP calculation algorithm, in which the forward-and-backward message passing mechanism of BP operates on the subtrellises. The proposed BP-MSDD is capable of significantly outperforming the conventional hard-decision MSDDs. However, the computational complexity of the BP-MSDD increases exponentially with the number of MSDD trellis states. To circumvent this excessive complexity for practical implementations, we reformulate the BP-MSDD, and additionally propose a Viterbi algorithm (VA)-based hard-decision MSDD (VA-HMSDD) and a VA-based soft-decision MSDD (VA-SMSDD). Moreover, both the proposed BP-MSDD and VA-SMSDD can be exploited in conjunction with soft channel decoding to obtain powerful iterative detection and decoding based receivers. Simulation results demonstrate the effectiveness of the proposed algorithms in DSTBC-aided UWB-IR systems."
"Imperfect recall games represent dynamic interactions where players forget previously known information, such as a history of played actions. The importance of imperfect recall games stems from allowing a concise representation of strategies compared to perfect recall games where players remember all information. However, most of the algorithmic results are negative for imperfect recall games -- a Nash equilibrium~(NE) does not have to exist and computing a best response or a maxmin strategy is NP-hard. We focus on a subclass of imperfect recall games, called A-loss recall games, where a best response can be found in polynomial time. We derive novel properties of A-loss recall games, including (1) a sufficient and necessary condition for the existence of NE in A-loss recall games, (2) example where both NE and maxmin require irrational numbers for rational input, and (3) NP-hardness of problems related to finding maxmin strategies and existence of a NE strategy."
"Internet of Things (IoT) domains generate large volumes of high velocity event streams from sensors, which need to be analyzed with low latency to drive intelligent decisions. Big Data platforms for Complex Event Processing (CEP) enable such analytics. Traditionally, limited analytics are performed on the gateway edge device, or comprehensive analytics performed on Cloud Virtual Machines (VM) across all sensor streams. Leveraging the growing prevalence of captive edge resources in combination with Cloud VMs can offer better performance, flexibility and monetary costs. Here, we propose an approach to schedule an event analytics application composed as a Directed Acyclic Graph (DAG) of CEP queries across a collection of edge and Cloud resources. The goal of this optimization problem is to map the queries to the resources such that the end-to-end latency for the DAG is minimized, while also ensuring that a resource's compute and energy capacity are not saturated. We propose a brute-force optimal algorithm (BF) and a Generic Algorithm (GA) meta-heuristic to solve this problem. We perform comprehensive real-world benchmarks on the compute, network and energy capacity of edge and Cloud resources for over 17 CEP query configurations. These results are used to define a realistic simulation study that validate the BF and solutions for a wide variety over 45 DAGs. Our results show that the GA approach comes within 99% of the optimal BF solution, maps DAGs with 4 - 50 queries within 0.001 - 25 secs (unlike BF which takes hours for > 12 queries), and in fewer than 10% of the experiments is unable to offer a feasible solution."
"A Winograd schema is a pair of sentences that differ in a single word and that contain an ambiguous pronoun whose referent is different in the two sentences and requires the use of commonsense knowledge or world knowledge to disambiguate. This paper discusses how Winograd schemas and other sentence pairs could be used as challenges for machine translation using distinctions between pronouns, such as gender, that appear in the target language but not in the source."
"The topological structure of complex networks has fascinated researchers for several decades, resulting in the discovery of many universal properties and reoccurring characteristics of different kinds of networks. However, much less is known today about the network dynamics: indeed, complex networks in reality are not static, but rather dynamically evolve over time.   Our paper is motivated by the empirical observation that network evolution patterns seem far from random, but exhibit structure. Moreover, the specific patterns appear to depend on the network type, contradicting the existence of a ""one fits it all"" model. However, we still lack observables to quantify these intuitions, as well as metrics to compare graph evolutions. Such observables and metrics are needed for extrapolating or predicting evolutions, as well as for interpolating graph evolutions.   To explore the many faces of graph dynamics and to quantify temporal changes, this paper suggests to build upon the concept of centrality, a measure of node importance in a network. In particular, we introduce the notion of centrality distance, a natural similarity measure for two graphs which depends on a given centrality, characterizing the graph type. Intuitively, centrality distances reflect the extent to which (non-anonymous) node roles are different or, in case of dynamic graphs, have changed over time, between two graphs.   We evaluate the centrality distance approach for five evolutionary models and seven real-world social and physical networks. Our results empirically show the usefulness of centrality distances for characterizing graph dynamics compared to a null-model of random evolution, and highlight the differences between the considered scenarios. Interestingly, our approach allows us to compare the dynamics of very different networks, in terms of scale and evolution speed."
"A (possibly denerate) drawing of a graph $G$ in the plane is approximable by an embedding if it can be turned into an embedding by an arbitrarily small perturbation. We show that testing, whether a straight-line drawing of a planar graph $G$ in the plane is approximable by an embedding, can be carried out in polynomial time, if a desired embedding of $G$ belongs to a fixed isotopy class. In other words, we show that c-planarity with pipes is tractable for graphs with fixed embeddings.   To the best of our knowledge an analogous result was previously known essentially only when $G$ is a cycle."
"SEAndroid enforcement is now mandatory for Android devices. In order to provide the desired level of security for their products, Android OEMs need to be able to minimize their mistakes in writing SEAndroid policies. However, existing SEAndroid and SELinux tools are not very useful for this purpose. It has been shown that SEAndroid policies found in commercially available devices for multiple manufacturers contain mistakes and redundancies [15]. In this paper we present a new tool, SELint, which aims to help OEMs to produce better SEAndroid policies. SELint is extensible and configurable to suit the needs of different OEMs. It is provided with a default configuration based on the AOSP SEAndroid policy, but can be customized by OEMs."
"Initial timing acquisition in narrow-band IoT (NB-IoT) devices is done by detecting a periodically transmitted known sequence. The detection has to be done at lowest possible latency, because the RF-transceiver, which dominates downlink power consumption of an NB-IoT modem, has to be turned on throughout this time. Auto-correlation detectors show low computational complexity from a signal processing point of view at the price of a higher detection latency. In contrast a maximum likelihood cross-correlation detector achieves low latency at a higher complexity as shown in this paper. We present a hardware implementation of the maximum likelihood cross-correlation detection. The detector achieves an average detection latency which is a factor of two below that of an auto-correlation method and is able to reduce the required energy per timing acquisition by up to 34%."
"Traditional static resource analyses estimate the total resource usage of a program, without executing it. In this paper we present a novel resource analysis whose aim is instead the static profiling of accumulated cost, i.e., to discover, for selected parts of the program, an estimate or bound of the resource usage accumulated in each of those parts. Traditional resource analyses are parametric in the sense that the results can be functions on input data sizes. Our static profiling is also parametric, i.e., our accumulated cost estimates are also parameterized by input data sizes. Our proposal is based on the concept of cost centers and a program transformation that allows the static inference of functions that return bounds on these accumulated costs depending on input data sizes, for each cost center of interest. Such information is much more useful to the software developer than the traditional resource usage functions, as it allows identifying the parts of a program that should be optimized, because of their greater impact on the total cost of program executions. We also report on our implementation of the proposed technique using the CiaoPP program analysis framework, and provide some experimental results. This paper is under consideration for acceptance in TPLP."
"Consider an on-line system in which a player is sequentially given independent and uniformly chosen points from $[0,1]$. After looking at each point the player is allowed to either keep it or replace it with a fresh uniformly chosen point. This choice may depend on the location of all previously kept points. However, only one retry is allowed at every step. The goal of the player is to reduce the discrepancy of the empirical distribution of points, that is -- to reduce the maximal deviation of the number of points in an interval from its expectation.   Our main result is a retry strategy which guarantees discrepancy of $O(\log^3 N/N)$ with high probability (compare with $\Theta(1/\sqrt{N})$ without the power of one-retry). This answers a question of Benjamini. We also show that even if retry is allowed only with probability $\eta$ at every step for arbitrary $0<\eta$, a similar result holds.   Finally, we show that the best maximum load obtainable by a one-retry strategy in the classical $N$ balls thrown into $N$ bins setting is $\Theta(\sqrt{\frac{\log N}{\log \log N}})$."
"We describe TweeTIME, a temporal tagger for recognizing and normalizing time expressions in Twitter. Most previous work in social media analysis has to rely on temporal resolvers that are designed for well-edited text, and therefore suffer from the reduced performance due to domain mismatch. We present a minimally supervised method that learns from large quantities of unlabeled data and requires no hand-engineered rules or hand-annotated training corpora. TweeTIME achieves 0.68 F1 score on the end-to-end task of resolving date expressions, outperforming a broad range of state-of-the-art systems."
"We propose stochastic rank-$1$ bandits, a class of online learning problems where at each step a learning agent chooses a pair of row and column arms, and receives the product of their values as a reward. The challenge is that the values of the row and column are unobserved. These values are stochastic and drawn independently of each other. We propose an efficient algorithm for solving our problem, Rank1Elim, and derive a $O((K + L) (1 / \Delta) \log n)$ upper bound on its $n$-step regret, where $K$ is the number of rows, $L$ is the number of columns, and $\Delta$ is the minimum of the row and column gaps. This is the first bandit algorithm for finding the maximum entry of a rank-$1$ matrix whose regret is linear in $K + L$, $1 / \Delta$, and $\log n$. We evaluate our proposed algorithm on both synthetic and real-world problems, and observe that it leverages the structure of our problems and can learn near-optimal solutions even when our modeling assumptions are mildly violated."
"This letter studies a wireless system consisting of distributed ground terminals (GTs) communicating with an unmanned aerial vehicle (UAV) that serves as a mobile base station (BS). The UAV flies cyclically above the GTs at a fixed altitude, which results in a cyclical pattern of the strength of the UAV-GT channels. To exploit such periodic channel variations, we propose a new cyclical multiple access (CMA) scheme to schedule the communications between the UAV and GTs in a cyclical time-division manner based on the flying UAV's position. The time allocations to different GTs are optimized to maximize their minimum throughput. It is revealed that there is a fundamental tradeoff between throughput and access delay in the proposed CMA. Simulation results show significant throughput gains over the case of a static UAV BS in delay-tolerant applications."
"Physics is one of the most successful endeavors in science. Being a prototypic big science it also reflects the growing tendency for scientific collaborations. Utilizing 250,000 papers from ArXiv.org a prepublishing platform prevalent in Physics we construct large coauthorship networks to investigate how individual network positions influence scientific success. In this context, success is seen as getting a paper published in high impact journals of physical subdisciplines as compared to not getting it published at all or in rather peripheral journals only. To control the nested levels of authors and papers, and to consider the time elapsing between working paper and prominent journal publication we employ multilevel eventhistory models with various network measures as covariates. Our results show that the maintenance of even a moderate number of persistent ties is crucial for scientific success. Also, even with low volumes of social capital Physicists who occupy brokerage positions enhance their chances of articles in high impact journals significantly. Surprisingly, inter(sub)disciplinary collaborations decrease the probability of getting a paper published in specialized journals for almost all positions."
"We study a fine hierarchy of Borel-piecewise continuous functions, especially, between closed-piecewise continuity and $G_\delta$-piecewise continuity. Our aim is to understand how a priority argument in computability theory is connected to the notion of $G_\delta$-piecewise continuity, and then we utilize this connection to obtain separation results on subclasses of $G_\delta$-piecewise continuous reductions for uniformization problems on set-valued functions with compact graphs. This method is also applicable for separating various non-constructive principles in the Weihrauch lattice."
"While initially devised for image categorization, convolutional neural networks (CNNs) are being increasingly used for the pixelwise semantic labeling of images. However, the proper nature of the most common CNN architectures makes them good at recognizing but poor at localizing objects precisely. This problem is magnified in the context of aerial and satellite image labeling, where a spatially fine object outlining is of paramount importance. Different iterative enhancement algorithms have been presented in the literature to progressively improve the coarse CNN outputs, seeking to sharpen object boundaries around real image edges. However, one must carefully design, choose and tune such algorithms. Instead, our goal is to directly learn the iterative process itself. For this, we formulate a generic iterative enhancement process inspired from partial differential equations, and observe that it can be expressed as a recurrent neural network (RNN). Consequently, we train such a network from manually labeled data for our enhancement task. In a series of experiments we show that our RNN effectively learns an iterative process that significantly improves the quality of satellite image classification maps."
"The ubiquitous presence of sequence data across fields such as the web, healthcare, bioinformatics, and text mining has made sequence mining a vital research area. However, sequence mining is particularly challenging because of the absence of an accurate and fast approach to find (dis)similarity between sequences. As a measure of (dis)similarity, mainstream data mining methods such as k-means, kNN and regression have proved distance between data points in a Euclidean space to be most effective. But a distance measure between sequences is not obvious due to their unstructuredness---arbitrary strings of arbitrary length. We, therefore, propose a new function, Sequence Graph Transform (SGT), that extracts sequence features and embeds them in a finite-dimensional Euclidean space. SGT is scalable due to a low computation and has a universal applicability to most sequence problems. We theoretically show that SGT can capture both short and long patterns in sequences and provides an accurate distance-based measure of (dis)similarity between them. This is also validated experimentally. Finally, we show its real world application for clustering, classification, search and visualization on different sequence problems."
"We study the problem of finding approximate Nash equilibria that satisfy certain conditions, such as providing good social welfare. In particular, we study the problem $\epsilon$-NE $\delta$-SW: find an $\epsilon$-approximate Nash equilibrium ($\epsilon$-NE) that is within $\delta$ of the best social welfare achievable by an $\epsilon$-NE. Our main result is that, if the randomized exponential-time hypothesis (RETH) is true, then solving $\left(\frac{1}{8} - \mathrm{O}(\delta)\right)$-NE $\mathrm{O}(\delta)$-SW for an $n\times n$ bimatrix game requires $n^{\mathrm{\widetilde \Omega}(\delta^{\Lambda} \log n)}$ time, where $\Lambda$ is a constant. Building on this result, we show similar conditional running time lower bounds on a number of decision problems for approximate Nash equilibria that do not involve social welfare, including maximizing or minimizing a certain player's payoff, or finding approximate equilibria contained in a given pair of supports. We show quasi-polynomial lower bounds for these problems assuming that RETH holds, and these lower bounds apply to $\epsilon$-Nash equilibria for all $\epsilon < \frac{1}{8}$. The hardness of these other decision problems has so far only been studied in the context of exact equilibria."
"Deep neural network (DNN) models have recently obtained state-of-the-art prediction accuracy for the transcription factor binding (TFBS) site classification task. However, it remains unclear how these approaches identify meaningful DNA sequence signals and give insights as to why TFs bind to certain locations. In this paper, we propose a toolkit called the Deep Motif Dashboard (DeMo Dashboard) which provides a suite of visualization strategies to extract motifs, or sequence patterns from deep neural network models for TFBS classification. We demonstrate how to visualize and understand three important DNN models: convolutional, recurrent, and convolutional-recurrent networks. Our first visualization method is finding a test sequence's saliency map which uses first-order derivatives to describe the importance of each nucleotide in making the final prediction. Second, considering recurrent models make predictions in a temporal manner (from one end of a TFBS sequence to the other), we introduce temporal output scores, indicating the prediction score of a model over time for a sequential input. Lastly, a class-specific visualization strategy finds the optimal input sequence for a given TFBS positive class via stochastic gradient optimization. Our experimental results indicate that a convolutional-recurrent architecture performs the best among the three architectures. The visualization techniques indicate that CNN-RNN makes predictions by modeling both motifs as well as dependencies among them."
"High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNNs evaluation. Experimental results show that SSL achieves on average 5.1x and 3.1x speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual Network (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%, which is still slightly higher than that of original ResNet with 32 layers. For AlexNet, structure regularization by SSL also reduces the error by around ~1%. Open source code is in https://github.com/wenwei202/caffe/tree/scnn"
"In this paper, we focus on the problem of inferring the underlying reward function of an expert given demonstrations, which is often referred to as inverse reinforcement learning (IRL). In particular, we propose a model-free density-based IRL algorithm, named density matching reward learning (DMRL), which does not require model dynamics. The performance of DMRL is analyzed theoretically and the sample complexity is derived. Furthermore, the proposed DMRL is extended to handle nonlinear IRL problems by assuming that the reward function is in the reproducing kernel Hilbert space (RKHS) and kernel DMRL (KDMRL) is proposed. The parameters for KDMRL can be computed analytically, which greatly reduces the computation time. The performance of KDMRL is extensively evaluated in two sets of experiments: grid world and track driving experiments. In grid world experiments, the proposed KDMRL method is compared with both model-based and model-free IRL methods and shows superior performance on a nonlinear reward setting and competitive performance on a linear reward setting in terms of expected value differences. Then we move on to more realistic experiments of learning different driving styles for autonomous navigation in complex and dynamic tracks using KDMRL and receding horizon control."
"Social discrimination is considered illegal and unethical in the modern world. Such discrimination is often implicit in observed decisions' datasets, and anti-discrimination organizations seek to discover cases of discrimination and to understand the reasons behind them. Previous work in this direction adopted simple observational data analysis; however, this can produce biased results due to the effect of confounding variables. In this paper, we propose a causal discrimination discovery and understanding approach based on propensity score analysis. The propensity score is an effective statistical tool for filtering out the effect of confounding variables. We employ propensity score weighting to balance the distribution of individuals from protected and unprotected groups w.r.t. the confounding variables. For each individual in the dataset, we quantify its causal discrimination or favoritism with a neighborhood-based measure calculated on the balanced distributions. Subsequently, the causal discrimination/favoritism patterns are understood by learning a regression tree. Our approach avoids common pitfalls in observational data analysis and make its results legally admissible. We demonstrate the results of our approach on two discrimination datasets."
"With the agreement of my coauthors, I Zhangyang Wang would like to withdraw the manuscript ""Stacked Approximated Regression Machine: A Simple Deep Learning Approach"". Some experimental procedures were not included in the manuscript, which makes a part of important claims not meaningful. In the relevant research, I was solely responsible for carrying out the experiments; the other coauthors joined in the discussions leading to the main algorithm.   Please see the updated text for more details."
"Training a neural network demands a large amount of labeled data. Keeping the data after the training may not be allowed because of hardware or power restrictions for on-device learning. In this study, we train a new RNN, called a student network, using a previously developed RNN, the teacher network, without using the original data. The teacher network is used for generating data to train the student network. The softmax output of the teacher RNN is used as for the soft target when training a student network. The performance evaluation is conducted using a character-level language model. The experimental results show that the proposed method yields good performance approaching that of the original data based training. This work not only gives the insight to connect between the learning and generation but also can be useful when the original training data is not available."
"Given an $n\times n$ matrix $A$ over a field $F$ and a scalar $a\in F$, we consider the linear codes $C(A,a):=\{B\in F^{n\times n}\mid \,AB=aBA\}$ of length $n^2$. We call $C(A,a)$ a twisted centralizer code. We investigate properties of these codes including their dimensions, minimum distances, parity-check matrices, syndromes, and automorphism groups. The minimal distance of a centralizer code (when $a=1$) is at most $n$, however for $a\ne 0,1$ the minimal distance can be much larger, as large as $n^2$."
"This paper investigates a wireless energy harvesting cooperative network (WEHCN) consisting of a source, a decode-and-forward (DF) relay and a destination. We consider the relay as an energy harvesting (EH) node equipped with EH circuit and a rechargeable battery. Moreover, the direct link between source and destination is assumed to exist. The relay can thus harvest and accumulate energy from radio-frequency signals ejected by the source and assist its information transmission opportunistically. We develop an incremental accumulate-then-forward (IATF) relaying protocol for the considered WEHCN. In the IATF protocol, the source sends its information to destination via the direct link and requests the relay to cooperate only when it is necessary such that the relay has more chances to accumulate the harvested energy. By modeling the charging/discharging behaviors of the relay battery as a finite-state Markov chain, we derive a closed-form expression for the outage probability of the proposed IATF. Numerical results validate our theoretical analysis and show that the IATF scheme can significantly outperform the direct transmission scheme without cooperation."
"In this paper, for the purpose of data centre energy consumption monitoring and analysis, we propose to detect the running programs in a server by classifying the observed power consumption series. Time series classification problem has been extensively studied with various distance measurements developed; also recently the deep learning based sequence models have been proved to be promising. In this paper, we propose a novel distance measurement and build a time series classification algorithm hybridizing nearest neighbour and long short term memory (LSTM) neural network. More specifically, first we propose a new distance measurement termed as Local Time Warping (LTW), which utilizes a user-specified set for local warping, and is designed to be non-commutative and non-dynamic programming. Second we hybridize the 1NN-LTW and LSTM together. In particular, we combine the prediction probability vector of 1NN-LTW and LSTM to determine the label of the test cases. Finally, using the power consumption data from a real data center, we show that the proposed LTW can improve the classification accuracy of DTW from about 84% to 90%. Our experimental results prove that the proposed LTW is competitive on our data set compared with existed DTW variants and its non-commutative feature is indeed beneficial. We also test a linear version of LTW and it can significantly outperform existed linear runtime lower bound methods like LB_Keogh. Furthermore, with the hybrid algorithm, for the power series classification task we achieve an accuracy up to about 93%. Our research can inspire more studies on time series distance measurement and the hybrid of the deep learning models with other traditional models."
"Large-scale simulations play a central role in science and the industry. Several challenges occur when building simulation software, because simulations require complex software developed in a dynamic construction process. That is why simulation software engineering (SSE) is emerging lately as a research focus. The dichotomous trade-off between scalability and efficiency (SE) on the one hand and maintainability and portability (MP) on the other hand is one of the core challenges. We report on the SE/MP trade-off in the context of an ongoing systematic literature review (SLR). After characterizing the issue of the SE/MP trade-off using two examples from our own research, we (1) review the 33 identified articles that assess the trade-off, (2) summarize the proposed solutions for the trade-off, and (3) discuss the findings for SSE and future work. Overall, we see evidence for the SE/MP trade-off and first solution approaches. However, a strong empirical foundation has yet to be established; general quantitative metrics and methods supporting software developers in addressing the trade-off have to be developed. We foresee considerable future work in SSE across scientific communities."
"Automatically analyzing information flow within Android applications that rely on cryptographic operations with their computational security guarantees imposes formidable challenges that existing approaches for understanding an app's behavior struggle to meet. These approaches do not distinguish cryptographic and non-cryptographic operations, and hence do not account for cryptographic protections: f(m) is considered sensitive for a sensitive message m irrespective of potential secrecy properties offered by a cryptographic operation f. These approaches consequently provide a safe approximation of the app's behavior, but they mistakenly classify a large fraction of apps as potentially insecure and consequently yield overly pessimistic results.   In this paper, we show how cryptographic operations can be faithfully included into existing approaches for automated app analysis. To this end, we first show how cryptographic operations can be expressed as symbolic abstractions within the comprehensive Dalvik bytecode language. These abstractions are accessible to automated analysis, and they can be conveniently added to existing app analysis tools using minor changes in their semantics. Second, we show that our abstractions are faithful by providing the first computational soundness result for Dalvik bytecode, i.e., the absence of attacks against our symbolically abstracted program entails the absence of any attacks against a suitable cryptographic program realization. We cast our computational soundness result in the CoSP framework, which makes the result modular and composable."
"Most existing star-galaxy classifiers use the reduced summary information from catalogs, requiring careful feature extraction and selection. The latest advances in machine learning that use deep convolutional neural networks allow a machine to automatically learn the features directly from data, minimizing the need for input from human experts. We present a star-galaxy classification framework that uses deep convolutional neural networks (ConvNets) directly on the reduced, calibrated pixel values. Using data from the Sloan Digital Sky Survey (SDSS) and the Canada-France-Hawaii Telescope Lensing Survey (CFHTLenS), we demonstrate that ConvNets are able to produce accurate and well-calibrated probabilistic classifications that are competitive with conventional machine learning techniques. Future advances in deep learning may bring more success with current and forthcoming photometric surveys, such as the Dark Energy Survey (DES) and the Large Synoptic Survey Telescope (LSST), because deep neural networks require very little, manual feature engineering."
"In this paper, a geometric framework for neural networks is proposed. This framework uses the inner product space structure underlying the parameter set to perform gradient descent not in a component-based form, but in a coordinate-free manner. Convolutional neural networks are described in this framework in a compact form, with the gradients of standard --- and higher-order --- loss functions calculated for each layer of the network. This approach can be applied to other network structures and provides a basis on which to create new networks."
"Within the field of Statistical Machine Translation (SMT), the neural approach (NMT) has recently emerged as the first technology able to challenge the long-standing dominance of phrase-based approaches (PBMT). In particular, at the IWSLT 2015 evaluation campaign, NMT outperformed well established state-of-the-art PBMT systems on English-German, a language pair known to be particularly hard because of morphology and syntactic differences. To understand in what respects NMT provides better translation quality than PBMT, we perform a detailed analysis of neural versus phrase-based SMT outputs, leveraging high quality post-edits performed by professional translators on the IWSLT data. For the first time, our analysis provides useful insights on what linguistic phenomena are best modeled by neural models -- such as the reordering of verbs -- while pointing out other aspects that remain to be improved."
"In 1963, Polyak proposed a simple condition that is sufficient to show a global linear convergence rate for gradient descent. This condition is a special case of the \L{}ojasiewicz inequality proposed in the same year, and it does not require strong convexity (or even convexity). In this work, we show that this much-older Polyak-\L{}ojasiewicz (PL) inequality is actually weaker than the main conditions that have been explored to show linear convergence rates without strong convexity over the last 25 years. We also use the PL inequality to give new analyses of randomized and greedy coordinate descent methods, sign-based gradient descent methods, and stochastic gradient methods in the classic setting (with decreasing or constant step-sizes) as well as the variance-reduced setting. We further propose a generalization that applies to proximal-gradient methods for non-smooth optimization, leading to simple proofs of linear convergence of these methods. Along the way, we give simple convergence results for a wide variety of problems in machine learning: least squares, logistic regression, boosting, resilient backpropagation, L1-regularization, support vector machines, stochastic dual coordinate ascent, and stochastic variance-reduced gradient methods."
"Memristor device modeling is currently a heavily researched topic and is becoming ever more important as memristor devices make their way into CMOS circuit designs, necessitating accurate and efficient memristor circuit simulations. In this paper, the Generalized Metastable Switch (MSS) memristor model is presented. The Generalized MSS model consists of a voltage-dependent stochastic component and a voltage-dependent exponential diode current component and is designed to be easy to implement, computationally efficient, and amenable to modeling a wide range of different memristor devices."
"(I) We prove that the (maximum) number of monotone paths in a geometric triangulation of $n$ points in the plane is $O(1.7864^n)$. This improves an earlier upper bound of $O(1.8393^n)$; the current best lower bound is $\Omega(1.7003^n)$.   (II) Given a planar geometric graph $G$ with $n$ vertices, we show that the number of monotone paths in $G$ can be computed in $O(n^2)$ time."
We present a motion planning algorithm to compute collision-free and smooth trajectories for robots cooperating with humans in a shared workspace. Our approach uses offline learning of human actions and their temporal coherence to predict the human actions at runtime. This data is used by an intention-aware motion planning algorithm that is used to compute a reliable trajectory based on these predicted actions. We highlight the performance of our planning algorithm in complex simulated scenarios and real world scenarios with 7-DOF robot arms operating in a workspace with a human performing complex tasks. We demonstrate the benefits of our intention-aware planner in terms of computing safe trajectories.
"We prove the Sedgewick-Bentley conjecture on median-of-$k$ Quicksort on equal keys: The average number of comparisons for Quicksort with fat-pivot (a.k.a. three-way) partitioning is asymptotically only a constant times worse than the information-theoretic lower bound for sorting $n$ i.i.d. elements, and that constant converges to 1 as $k \to \infty$. Hence, Quicksort with pivot sampling is an optimal distribution-sensitive algorithm for the i.i.d. sorting problem."
"We study the cohesion within and the coalitions between political groups in the Eighth European Parliament (2014--2019) by analyzing two entirely different aspects of the behavior of the Members of the European Parliament (MEPs) in the policy-making processes. On one hand, we analyze their co-voting patterns and, on the other, their retweeting behavior. We make use of two diverse datasets in the analysis. The first one is the roll-call vote dataset, where cohesion is regarded as the tendency to co-vote within a group, and a coalition is formed when the members of several groups exhibit a high degree of co-voting agreement on a subject. The second dataset comes from Twitter; it captures the retweeting (i.e., endorsing) behavior of the MEPs and implies cohesion (retweets within the same group) and coalitions (retweets between groups) from a completely different perspective.   We employ two different methodologies to analyze the cohesion and coalitions. The first one is based on Krippendorff's Alpha reliability, used to measure the agreement between raters in data-analysis scenarios, and the second one is based on Exponential Random Graph Models, often used in social-network analysis. We give general insights into the cohesion of political groups in the European Parliament, explore whether coalitions are formed in the same way for different policy areas, and examine to what degree the retweeting behavior of MEPs corresponds to their co-voting patterns. A novel and interesting aspect of our work is the relationship between the co-voting and retweeting patterns."
"Stacked Auto-Encoder (SAE) is a kind of deep learning algorithm for unsupervised learning. Which has multi layers that project the vector representation of input data into a lower vector space. These projection vectors are dense representations of the input data. As a result, SAE can be used for image compression. Using chaotic logistic map, the compression ones can further be encrypted. In this study, an application of image compression and encryption is suggested using SAE and chaotic logistic map. Experiments show that this application is feasible and effective. It can be used for image transmission and image protection on internet simultaneously."
"We leverage the Minimum Description Length (MDL) principle as a model selection technique for Bernoulli distributions and compare several types of MDL codes. We first present a simplistic crude two-part MDL code and a Normalized Maximum Likelihood (NML) code. We then focus on the enumerative two-part crude MDL code, suggest a Bayesian interpretation for finite size data samples, and exhibit a strong connection with the NML approach. We obtain surprising impacts on the estimation of the model complexity together with superior compression performance. This is then generalized to the case of the multinomial distributions. Both the theoretical analysis and the experimental comparisons suggest that one might use the enumerative code rather than NML in practice, for Bernoulli and multinomial distributions."
"In this paper, a scalable framework for wireless distributed computing is proposed, in which the data shuffling load does not increase with the number of users in the network. The key idea is to utilize a particular repetitive structure of computation assignments at the users, in order to provide coding opportunities at both the users and the access point, which reduce the shuffling load by a factor that grows linearly with the number of users. It is demonstrated that the proposed computation assignments and coded shuffling schemes are optimal (i.e., achieve the minimum shuffling load) for both a centralized setting and a decentralized setting, by developing tight information-theoretic lower bounds."
"This paper investigates the compress-and-forward scheme for an uplink cloud radio access network (C-RAN) model, where multi-antenna base-stations (BSs) are connected to a cloud-computing based central processor (CP) via capacity-limited fronthaul links. The BSs compress the received signals with Wyner-Ziv coding and send the representation bits to the CP; the CP performs the decoding of all the users' messages. Under this setup, this paper makes progress toward the optimal structure of the fronthaul compression and CP decoding strategies for the compress-and-forward scheme in C-RAN. On the CP decoding strategy design, this paper shows that under a sum fronthaul capacity constraint, a generalized successive decoding strategy of the quantization and user message codewords that allows arbitrary interleaved order at the CP achieves the same rate region as the optimal joint decoding. Further, it is shown that a practical strategy of successively decoding the quantization codewords first, then the user messages, achieves the same maximum sum rate as joint decoding under individual fronthaul constraints. On the joint optimization of user transmission and BS quantization strategies, this paper shows that if the input distributions are assumed to be Gaussian, then under joint decoding, the optimal quantization scheme for maximizing the achievable rate region is Gaussian. Moreover, Gaussian input and Gaussian quantization with joint decoding achieve to within a constant gap of the capacity region of the Gaussian multiple-input multiple-output (MIMO) uplink C-RAN model. Finally, this paper addresses the computational aspect of optimizing uplink MIMO C-RAN by showing that under fixed Gaussian input, the sum rate maximization problem over the Gaussian quantization noise covariance matrices can be formulated as convex optimization problems, thereby facilitating its efficient solution."
"A common practice in the estimation of the complexity of objects, in particular of graphs, is to rely on graph- and information-theoretic measures. Here, using integer sequences with properties such as Borel normality, we explain how these measures are not independent of the way in which the object, such a graph, can be described or observed. From observations that can reconstruct the same graph and are therefore essentially translations of the same description, we will see that not only is it necessary to pre-select a feature of interest where there is one when applying a computable measure such as Shannon Entropy, and to make an arbitrary selection where there is not, but that more general properties, such as the causal likeliness of a graph as a measure (opposed to randomness), can be largely misrepresented by computable measures such as Entropy and Entropy rate. We introduce recursive and non-recursive (uncomputable) graphs and graph constructions based on these integer sequences, whose different lossless descriptions have disparate Entropy values, thereby enabling the study and exploration of a measure's range of applications and demonstrating the weaknesses of computable measures of complexity."
"We develop an efficient alternating framework for learning a generalized version of Factorization Machine (gFM) on steaming data with provable guarantees. When the instances are sampled from $d$ dimensional random Gaussian vectors and the target second order coefficient matrix in gFM is of rank $k$, our algorithm converges linearly, achieves $O(\epsilon)$ recovery error after retrieving $O(k^{3}d\log(1/\epsilon))$ training instances, consumes $O(kd)$ memory in one-pass of dataset and only requires matrix-vector product operations in each iteration. The key ingredient of our framework is a construction of an estimation sequence endowed with a so-called Conditionally Independent RIP condition (CI-RIP). As special cases of gFM, our framework can be applied to symmetric or asymmetric rank-one matrix sensing problems, such as inductive matrix completion and phase retrieval."
"Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC on the test portion of the text."
"One fundamental goal in any learning algorithm is to mitigate its risk for overfitting. Mathematically, this requires that the learning algorithm enjoys a small generalization risk, which is defined either in expectation or in probability. Both types of generalization are commonly used in the literature. For instance, generalization in expectation has been used to analyze algorithms, such as ridge regression and SGD, whereas generalization in probability is used in the VC theory, among others. Recently, a third notion of generalization has been studied, called uniform generalization, which requires that the generalization risk vanishes uniformly in expectation across all bounded parametric losses. It has been shown that uniform generalization is, in fact, equivalent to an information-theoretic stability constraint, and that it recovers classical results in learning theory. It is achievable under various settings, such as sample compression schemes, finite hypothesis spaces, finite domains, and differential privacy. However, the relationship between uniform generalization and concentration remained unknown. In this paper, we answer this question by proving that, while a generalization in expectation does not imply a generalization in probability, a uniform generalization in expectation does imply concentration. We establish a chain rule for the uniform generalization risk of the composition of hypotheses and use it to derive a large deviation bound. Finally, we prove that the bound is tight."
"Abstract Meaning Representation (AMR) is a semantic representation for natural language that involves an easy annotation process, hence favoring the development of large datasets. It embeds traditional NLP tasks such as named entity recognition, semantic role labeling, word sense disambiguation and coreference resolution. We describe a transition system that parses the sentence left-to-right, in time linear in the size of the input. We further propose a test-suite that assesses specific subtasks that are helpful in comparing AMR parsers and show that our left-to-right parser is competitive with the state of the art on the LDC2015E86 dataset and it is superior in recovering reentrancies and handling polarity."
"In this paper, we investigate the quantization and the feedback of downlink spatial covariance matrix for massive multiple-input multiple-output (MIMO) systems with cascaded precoding. Massive MIMO has gained a lot of attention recently because of its ability to significantly improve the network performance. To reduce the overhead of downlink channel estimation and uplink feedback in frequency-division duplex massive MIMO systems, cascaded precoding has been proposed, where the outer precoder is implemented using traditional limited feedback while the inner precoder is determined by the spatial covariance matrix of the channels. In massive MIMO systems, it is difficult to quantize the spatial covariance matrix because of its large size caused by the huge number of antennas. In this paper, we propose a spatial spectrum based approach for the quantization and the feedback of the spatial covariance matrix. The proposed inner precoder can be viewed as modulated discrete prolate spheroidal sequences and thus achieve much smaller spatial leakage than the traditional discrete Fourier transform submatrix based precoding. Practical issues for the application of the proposed approach are also addressed in this paper."
"This paper emphasizes the significance to jointly exploit the problem structure and the parameter structure, in the context of deep modeling. As a specific and interesting example, we describe the deep double sparsity encoder (DDSE), which is inspired by the double sparsity model for dictionary learning. DDSE simultaneously sparsities the output features and the learned model parameters, under one unified framework. In addition to its intuitive model interpretation, DDSE also possesses compact model size and low complexity. Extensive simulations compare DDSE with several carefully-designed baselines, and verify the consistently superior performance of DDSE. We further apply DDSE to the novel application domain of brain encoding, with promising preliminary results achieved."
"Karp et al. (1988) described Deferred Data Structures for Multisets as ""lazy"" data structures which partially sort data to support online rank and select queries, with the minimum amount of work in the worst case over instances of size $n$ and number of queries $q$ fixed (i.e., the query size). Barbay et al. (2016) refined this approach to take advantage of the gaps between the positions hit by the queries (i.e., the structure in the queries). We develop new techniques in order to further refine this approach and to take advantage all at once of the structure (i.e., the multiplicities of the elements), the local order (i.e., the number and sizes of runs) and the global order (i.e., the number and positions of existing pivots) in the input; and of the structure and order in the sequence of queries. Our main result is a synergistic deferred data structure which performs much better on large classes of instances, while performing always asymptotically as good as previous solutions. As intermediate results, we describe two new synergistic sorting algorithms, which take advantage of the structure and order (local and global) in the input, improving upon previous results which take advantage only of the structure (Munro and Spira 1979) or of the local order (Takaoka 1997) in the input; and one new multiselection algorithm which takes advantage of not only the order and structure in the input, but also of the structure in the queries. We described two compressed data structures to represent a multiset taking advantage of both the local order and structure, while supporting the operators rank and select on the multiset."
"Positive unlabeled (PU) learning is useful in various practical situations, where there is a need to learn a classifier for a class of interest from an unlabeled data set, which may contain anomalies as well as samples from unknown classes. The learning task can be formulated as an optimization problem under the framework of statistical learning theory. Recent studies have theoretically analyzed its properties and generalization performance, nevertheless, little effort has been made to consider the problem of scalability, especially when large sets of unlabeled data are available. In this work we propose a novel scalable PU learning algorithm that is theoretically proven to provide the optimal solution, while showing superior computational and memory performance. Experimental evaluation confirms the theoretical evidence and shows that the proposed method can be successfully applied to a large variety of real-world problems involving PU learning."
"One of the simplest metalearning methods is the average ranking method. This method uses metadata in the form of test results of a given set of algorithms on given set of datasets and calculates an average rank for each algorithm. The ranks are used to construct the average ranking. We investigate the problem of how the process of generating the average ranking is affected by incomplete metadata including fewer test results. This issue is relevant, because if we could show that incomplete metadata does not affect the final results much, we could explore it in future design. We could simply conduct fewer tests and save thus computation time. In this paper we describe an upgraded average ranking method that is capable of dealing with incomplete metadata. Our results show that the proposed method is relatively robust to omission in test results in the meta datasets."
"We revisit the following problem (along with its higher dimensional variant): Given a set $S$ of $n$ points inside an axis-parallel rectangle $U$ in the plane, find a maximum-area axis-parallel sub-rectangle that is contained in $U$ but contains no points of $S$. (I) We present an algorithm that finds a large empty box amidst $n$ points in $[0,1]^d$: a box whose volume is at least $\frac{\log{d}}{4(n + \log{d})}$ can be computed in $O(n+d \log{d})$ time. (II) To better analyze the above approach, we introduce the concepts of perfect vector sets and properly overlapping partitions, in connection to the minimum volume of a maximum empty box amidst $n$ points in the unit hypercube $[0,1]^d$, and derive bounds on their sizes."
"As the practical use of answer set programming (ASP) has grown with the development of efficient solvers, we expect a growing interest in extensions of ASP as their semantics stabilize and solvers supporting them mature. Epistemic Specifications, which adds modal operators K and M to the language of ASP, is one such extension. We call a program in this language an epistemic logic program (ELP). Solvers have thus far been practical for only the simplest ELPs due to exponential growth of the search space. We describe a solver that is able to solve harder problems better (e.g., without exponentially-growing memory needs w.r.t. K and M occurrences) and faster than any other known ELP solver."
"With the rapid growth of Internet applications, sequential prediction in collaborative filtering has become an emerging and crucial task. Given the behavioral history of a specific user, predicting his or her next choice plays a key role in improving various online services. Meanwhile, there are more and more scenarios with multiple types of behaviors, while existing works mainly study sequences with a single type of behavior. As a widely used approach, Markov chain based models are based on a strong independence assumption. As two classical neural network methods for modeling sequences, recurrent neural networks can not well model short-term contexts, and the log-bilinear model is not suitable for long-term contexts. In this paper, we propose a Recurrent Log-BiLinear (RLBL) model. It can model multiple types of behaviors in historical sequences with behavior-specific transition matrices. RLBL applies a recurrent structure for modeling long-term contexts. It models several items in each hidden layer and employs position-specific transition matrices for modeling short-term contexts. Moreover, considering continuous time difference in behavioral history is a key factor for dynamic prediction, we further extend RLBL and replace position-specific transition matrices with time-specific transition matrices, and accordingly propose a Time-Aware Recurrent Log-BiLinear (TA-RLBL) model. Experimental results show that the proposed RLBL model and TA-RLBL model yield significant improvements over the competitive compared methods on three datasets, i.e., Movielens-1M dataset, Global Terrorism Database and Tmall dataset with different numbers of behavior types."
"In this article, we describe the design of two recommender systems (RS) designed to support the smoking cessation process through a mobile application. We plan to use a hybrid RS (content-based, utility-based, and demographic filtering) to tailor health recommendation messages, and a content-based RS to schedule a timely delivery of the message. We also define metrics that we will use to assess their performance, helping people quit smoking when we run the pilot."
"We present a type system and inference algorithm for a rich subset of JavaScript equipped with objects, structural subtyping, prototype inheritance, and first-class methods. The type system supports abstract and recursive objects, and is expressive enough to accommodate several standard benchmarks with only minor workarounds. The invariants enforced by the types enable an ahead-of-time compiler to carry out optimizations typically beyond the reach of static compilers for dynamic languages. Unlike previous inference techniques for prototype inheritance, our algorithm uses a combination of lower and upper bound propagation to infer types and discover type errors in all code, including uninvoked functions. The inference is expressed in a simple constraint language, designed to leverage off-the-shelf fixed point solvers. We prove soundness for both the type system and inference algorithm. An experimental evaluation showed that the inference is powerful, handling the aforementioned benchmarks with no manual type annotation, and that the inferred types enable effective static compilation."
"Ensuring the early detection of important social network users is a challenging task. Some peer ranking services are now well established, such as PeerIndex, Klout, or Kred. Their function is to rank users according to their influence. This notion of influence is however abstract, and the algorithms achieving this ranking are opaque. Following the rising demand for a more transparent web, we explore the problem of gaining knowledge by reverse engineering such peer ranking services, with regards to the social network topology they get as an input. Since these services exploit the online activity of users (and therefore their connectivity in social networks), we provide a precise evaluation of how topological metrics of the social network impact the final user ranking. Our approach is the following : we first model the ranking service as a black-box with which we interact by creating user profiles and by performing operations on them. Through those profiles, we trigger some slight topological modifications. By monitoring the impact of these modifications on the rankings of those profiles, we infer the weight of each topological metric in the black-box, thus reversing the service influence cookbook."
"Knowledge representation is an important, long-history topic in AI, and there have been a large amount of work for knowledge graph embedding which projects symbolic entities and relations into low-dimensional, real-valued vector space. However, most embedding methods merely concentrate on data fitting and ignore the explicit semantic expression, leading to uninterpretable representations. Thus, traditional embedding methods have limited potentials for many applications such as question answering, and entity classification. To this end, this paper proposes a semantic representation method for knowledge graph \textbf{(KSR)}, which imposes a two-level hierarchical generative process that globally extracts many aspects and then locally assigns a specific category in each aspect for every triple. Since both aspects and categories are semantics-relevant, the collection of categories in each aspect is treated as the semantic representation of this triple. Extensive experiments justify our model outperforms other state-of-the-art baselines substantially."
"We present new algorithms for learning Bayesian networks from data with missing values using a data augmentation approach. An exact Bayesian network learning algorithm is obtained by recasting the problem into a standard Bayesian network learning problem without missing data. To the best of our knowledge, this is the first exact algorithm for this problem. As expected, the exact algorithm does not scale to large domains. We build on the exact method to create an approximate algorithm using a hill-climbing technique. This algorithm scales to large domains so long as a suitable standard structure learning method for complete data is available. We perform a wide range of experiments to demonstrate the benefits of learning Bayesian networks with such new approach."
"In Distributional Semantic Models (DSMs), Vector Cosine is widely used to estimate similarity between word vectors, although this measure was noticed to suffer from several shortcomings. The recent literature has proposed other methods which attempt to mitigate such biases. In this paper, we intend to investigate APSyn, a measure that computes the extent of the intersection between the most associated contexts of two target words, weighting it by context relevance. We evaluated this metric in a similarity estimation task on several popular test sets, and our results show that APSyn is in fact highly competitive, even with respect to the results reported in the literature for word embeddings. On top of it, APSyn addresses some of the weaknesses of Vector Cosine, performing well also on genuine similarity estimation."
"A well-known problem of automatic defect repair is overfitting: the system tends to overfit to the test suite and has a low precision. In this paper we aim to produce precise patches, that is, any patch we produce has a relatively high probability to be correct. More concretely, we focus on condition synthesis, which was shown to be able to repair more than half of the defects in existing approaches. Our key insight is threefold. First, it is important to know what variables in a local context should be used in an ""if"" condition, and we propose a sorting method based on the dependency relations between variables. Second, we observe that the API document can be used to guide the repair process, and propose document analysis technique to further filter the variables. Third, it is important to know what predicates should be performed on the set of variables, and we propose to mine a set of frequently used predicates in similar contexts from existing projects.   Based on the insight, we develop a novel program repair system, ACS, that could generate precise conditions at faulty locations. Furthermore, given the generated conditions are very precise, we can perform a repair operation that is previously deemed to be too overfitting: directly returning the test oracle to repair the defect. Using our approach, we successfully repaired 18 defects on four projects of Defects4J, which is the largest number of fully automatically repaired defects reported on the dataset so far. More importantly, our approach achieves a precision of 78.3%, which is significantly higher than previous approaches, which are usually less than 40%."
"Multimedia or spoken content presents more attractive information than plain text content, but the former is more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It's therefore highly attractive to develop machines which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, a new task of machine comprehension of spoken content was proposed recently. The initial goal was defined as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native languages are not English. An Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture was also proposed for this task, which considered only the sequential relationship within the speech utterances. In this paper, we propose a new Hierarchical Attention Model (HAM), which constructs multi-hopped attention mechanism over tree-structured rather than sequential representations for the utterances. Improved comprehension performance robust with respect to ASR errors were obtained."
"Traditionally, network analysis is based on local properties of vertices, like their degree or clustering, and their statistical behavior across the network in question. This paper develops an approach which is different in two respects. We investigate edge-based properties, and we define global characteristics of networks directly. The latter will provide our affirmative answer to the question raised in the title. More concretely, we start with Forman's notion of the Ricci curvature of a graph, or more generally, a polyhedral complex. This will allow us to pass from a graph as representing a network to a polyhedral complex for instance by filling in triangles into connected triples of edges and to investigate the resulting effect on the curvature. This is insightful for two reasons: First, we can define a curvature flow in order to asymptotically simplify a network and reduce it to its essentials. Second, using a construction of Bloch, which yields a discrete Gauss-Bonnet theorem, we have the Euler characteristic of a network as a global characteristic. These two aspects beautifully merge in the sense that the asymptotic properties of the curvature flow are indicated by that Euler characteristic."
"Fine-grained user profile generation approaches have made it increasingly feasible to display on a profile page in which topics a user has expertise or interest. Earlier work on topical user profiling has been directed at enhancing search and personalization functionality, but making such profiles useful for human consumption presents new challenges. With this work, we have taken a first step toward a semantic layout mode for topical user profiles. We have developed a topical generalization approach which finds coherent groups of topics and and adds labels to them, based on their association with broader topics in the Wikipedia category graph. A nested layout mode, employing topical generalization, is compared with a simpler flat layout mode in our user study. The results indicate that users favor the nested structure over flat profiles, but tend to overlook the specific topics on the lower level. We propose a third layout mode to address this issue."
"This paper presents how we can achieve the state-of-the-art accuracy in multi-category object detection task while minimizing the computational cost by adapting and combining recent technical innovations. Following the common pipeline of ""CNN feature extraction + region proposal + RoI classification"", we mainly redesign the feature extraction part, since region proposal part is not computationally expensive and classification part can be efficiently compressed with common techniques like truncated SVD. Our design principle is ""less channels with more layers"" and adoption of some building blocks including concatenated ReLU, Inception, and HyperNet. The designed network is deep and thin and trained with the help of batch normalization, residual connections, and learning rate scheduling based on plateau detection. We obtained solid results on well-known object detection benchmarks: 83.8% mAP (mean average precision) on VOC2007 and 82.5% mAP on VOC2012 (2nd place), while taking only 750ms/image on Intel i7-6700K CPU with a single core and 46ms/image on NVIDIA Titan X GPU. Theoretically, our network requires only 12.3% of the computational cost compared to ResNet-101, the winner on VOC2012."
"Recommendation and collaborative filtering systems are important in modern information and e-commerce applications. As these systems are becoming increasingly popular in the industry, their outputs could affect business decision making, introducing incentives for an adversarial party to compromise the availability or integrity of such systems. We introduce a data poisoning attack on collaborative filtering systems. We demonstrate how a powerful attacker with full knowledge of the learner can generate malicious data so as to maximize his/her malicious objectives, while at the same time mimicking normal user behavior to avoid being detected. While the complete knowledge assumption seems extreme, it enables a robust assessment of the vulnerability of collaborative filtering schemes to highly motivated attacks. We present efficient solutions for two popular factorization-based collaborative filtering algorithms: the \emph{alternative minimization} formulation and the \emph{nuclear norm minimization} method. Finally, we test the effectiveness of our proposed algorithms on real-world data and discuss potential defensive strategies."
"The simultaneous orthogonal matching pursuit (SOMP) algorithm aims to find the joint support of a set of sparse signals acquired under a multiple measurement vector model. Critically, the analysis of SOMP depends on the maximal inner product of any atom of a suitable dictionary and the current signal residual, which is formed by the subtraction of previously selected atoms. This inner product, or correlation, is a key metric to determine the best atom to pick at each iteration. This paper provides, for each iteration of SOMP, a novel lower bound of the aforementioned metric for the atoms belonging to the correct and common joint support of the multiple signals. Although the bound is obtained for the noiseless case, its main purpose is to intervene in noisy analyses of SOMP. Finally, it is shown for specific signal patterns that the proposed bound outperforms state-of-the-art results for SOMP, and orthogonal matching pursuit (OMP) as a special case."
"Domain specific languages have successfully been used in a variety of fields to cleanly express scientific problems as well as to simplify implementation and performance opti- mization on different computer architectures. Although a large number of stencil languages are available, finite differ- ence domain specific languages have proved challenging to design because most practical use cases require additional features that fall outside the finite difference abstraction. Inspired by the complexity of real-world seismic imaging problems, we introduce Devito, a domain specific language in which high level equations are expressed using symbolic expressions from the SymPy package. Complex equations are automatically manipulated, optimized, and translated into highly optimized C code that aims to perform compa- rably or better than hand-tuned code. All this is transpar- ent to users, who only see concise symbolic mathematical expressions."
"Historically, infectious diseases caused considerable damage to human societies, and they continue to do so today. To help reduce their impact, mathematical models of disease transmission have been studied to help understand disease dynamics and inform prevention strategies. Vaccination - one of the most important preventive measures of modern times - is of great interest both theoretically and empirically. And in contrast to traditional approaches, recent research increasingly explores the pivotal implications of individual behavior and heterogeneous contact patterns in populations. Our report reviews the developmental arc of theoretical epidemiology with emphasis on vaccination, as it led from classical models assuming homogeneously mixing (mean-field) populations and ignoring human behavior, to recent models that account for behavioral feedback and/or population spatial/social structure. Many of the methods used originated in statistical physics, such as lattice and network models, and their associated analytical frameworks. Similarly, the feedback loop between vaccinating behavior and disease propagation forms a coupled nonlinear system with analogs in physics. We also review the new paradigm of digital epidemiology, wherein sources of digital data such as online social media are mined for high-resolution information on epidemiologically relevant individual behavior. Armed with the tools and concepts of statistical physics, and further assisted by new sources of digital data, models that capture nonlinear interactions between behavior and disease dynamics offer a novel way of modeling real-world phenomena, and can help improve health outcomes. We conclude the review by discussing open problems in the field and promising directions for future research."
"This paper explores the capabilities of convolutional neural networks to deal with a task that is easily manageable for humans: perceiving 3D pose of a human body from varying angles. However, in our approach, we are restricted to using a monocular vision system. For this purpose, we apply a convolutional neural network approach on RGB videos and extend it to three dimensional convolutions. This is done via encoding the time dimension in videos as the 3\ts{rd} dimension in convolutional space, and directly regressing to human body joint positions in 3D coordinate space. This research shows the ability of such a network to achieve state-of-the-art performance on the selected Human3.6M dataset, thus demonstrating the possibility of successfully representing temporal data with an additional dimension in the convolutional operation."
"We present a set of computing tools and techniques that every researcher can and should adopt. These recommendations synthesize inspiration from our own work, from the experiences of the thousands of people who have taken part in Software Carpentry and Data Carpentry workshops over the past six years, and from a variety of other guides. Unlike some other guides, our recommendations are aimed specifically at people who are new to research computing."
"This research started with an algebra for reasoning about rely/guarantee concurrency for a shared memory model. The approach taken led to a more abstract algebra of atomic steps, in which atomic steps synchronise (rather than interleave) when composed in parallel. The algebra of rely/guarantee concurrency then becomes an interpretation of the more abstract algebra. Many of the core properties needed for rely/guarantee reasoning can be shown to hold in the abstract algebra where their proofs are simpler and hence allow a higher degree of automation. Moreover, the realisation that the synchronisation mechanisms of standard process algebras, such as CSP and CCS/SCCS, can be interpreted in our abstract algebra gives evidence of its unifying power. The algebra has been encoded in Isabelle/HOL to provide a basis for tool support."
"In this paper we propose a tokenization algorithm of Reversible Hybrid type, as defined in PCI DSS guidelines for designing a tokenization solution, based on a block cipher with a secret key and (possibly public) additional input. We provide some formal proofs of security for it, which imply our algorithm satisfies the most significant security requirements described in PCI DSS tokenization guidelines. Finally, we give an instantiation with concrete cryptographic primitives and fixed length of the PAN, and we analyze its efficiency and security."
"Binary-input memoryless channels with a runlength constrained input are considered. Upper bounds to the capacity of such noisy runlength constrained channels are derived using the dual capacity method with Markov test distributions satisfying the Karush-Kuhn-Tucker (KKT) conditions for the capacity-achieving output distribution. Simplified algebraic characterizations of the bounds are presented for the binary erasure channel (BEC) and the binary symmetric channel (BSC). These upper bounds are very close to achievable rates, and improve upon previously known feedback-based bounds for a large range of channel parameters. For the binary-input Additive White Gaussian Noise (AWGN) channel, the upper bound is simplified to a small-scale numerical optimization problem. These results provide some of the simplest upper bounds for an open capacity problem that has theoretical and practical relevance."
"Despite significant developments in Proof Theory, surprisingly little attention has been devoted to the concept of proof verifier. In particular, the mathematical community may be interested in studying different types of proof verifiers (people, programs, oracles, communities, superintelligences) as mathematical objects. Such an effort could reveal their properties, their powers and limitations (particularly in human mathematicians), minimum and maximum complexity, as well as self-verification and self-reference issues. We propose an initial classification system for verifiers and provide some rudimentary analysis of solved and open problems in this important domain. Our main contribution is a formal introduction of the notion of unverifiability, for which the paper could serve as a general citation in domains of theorem proving, as well as software and AI verification."
"We present a method that uses ensemble learning to combine clinical and web-mined time-series data in order to predict future vaccination uptake. The clinical data is official vaccination registries, and the web data is query frequencies collected from Google Trends. Experiments with official vaccine records show that our method predicts vaccination uptake effectively (4.7 Root Mean Squared Error). Whereas performance is best when combining clinical and web data, using solely web data yields comparative performance. To our knowledge, this is the first study to predict vaccination uptake using web data (with and without clinical data)."
"Social networks and interactions in social media involve both positive and negative relationships. Signed graphs capture both types of relationships: positive edges correspond to pairs of ""friends"", and negative edges to pairs of ""foes"". The {\em edge sign prediction problem}, which aims to predict whether an interaction between a pair of nodes will be positive or negative, is an important graph mining task for which many heuristics have recently been proposed \cite{leskovec2010predicting,leskovec2010signed}.   Motivated by social balance theory, we model the edge sign prediction problem as a noisy correlation clustering problem with two clusters. We are allowed to query each pair of nodes whether they belong to the same cluster or not, but the answer to the query is corrupted with some probability $0<q<\frac{1}{2}$. Let $c=\frac{1}{2}-q$ be the gap. We provide an algorithm that recovers the clustering with high probability in the presence of noise for any constant gap $c$ with $O(n^{1+\tfrac{1}{\log\log{n}}}\log{n})$ queries. Our algorithm uses simple breadth first search as its main algorithmic primitive. Finally, we provide a novel generalization to $k \geq 3$ clusters and prove that our techniques can recover the clustering if the gap is constant in this generalized setting."
"Recent studies found that many channels are affected by additive noise that is impulsive in nature and is best explained by heavy-tailed symmetric alpha-stable distributions. Dealing with impulsive noise environments comes with an added complexity with respect to the standard Gaussian environment: the alpha-stable probability density functions have an infinite second moment and the ""nice"" Hilbert space structure of the space of random variables having a finite second moment is lost along with its tools and methodologies. This is indeed the case in estimation theory where classical tools to quantify performance of an estimator are tightly related to the assumption of finite variance variables. In alpha-stable environments, expressions such as the mean square error and the Cramer-Rao bound are hence problematic. In this work, we tackle the parameter estimation problem in impulsive noise environments and develop novel tools that are tailored to the alpha-stable and heavy-tailed noise environments, tools that coincide with the standard ones adopted in the Gaussian setup, namely a generalized ""power"" measure and a generalized Fisher information. We generalize known information inequalities commonly used in the Gaussian context: the de Bruijn's identity, the data processing inequality, the Fisher information inequality, the isoperimetric inequality for entropies and the Cramer-Rao bound. Additionally, we derive upper bounds on the differential entropy of independent sums having a stable component. Finally, the new ""power"" measure is used to shed some light on the additive alpha-stable noise channel capacity in a setup that generalizes the linear average power constrained AWGN channel. Our theoretical findings are paralleled with numerical evaluations of various quantities and bounds using developed {\em Matlab} packages."
"We develop an analytical framework for distribution of popular content in an Information Centric Network (ICN) that comprises of Access ICNs, a Transit ICN and a Content Provider. Using a generalized Zipf distribution to model content popularity, we devise a game theoretic approach to jointly determine caching and pricing strategies in such an ICN. Under the assumption that the caching cost of the access and transit ICNs is inversely proportional to popularity, we show that the Nash caching strategies in the ICN are 0-1 (all or nothing) strategies. Further, for the case of symmetric Access ICNs, we show that the Nash equilibrium is unique and the caching policy (0 or 1) is determined by a threshold on the popularity of the content (reflected by the Zipf probability metric), i.e., all content more popular than the threshold value is cached. We also show that the resulting threshold of the Access and Transit ICNs, as well as all prices can be obtained by a decomposition of the joint caching and pricing problem into two independent caching only and pricing only problems."
"Internal HITs on Mechanical Turk can be programmatically restrictive, and as a result, many requesters turn to using external HITs as a more flexible alternative. However, creating such HITs can be redundant and time-consuming. We present MmmTurkey, a framework that enables researchers to not only quickly create and manage external HITs, but more significantly also capture and record detailed worker behavioral data characterizing how each worker completes a given task."
"This note is an attempt to unconditionally prove the existence of weak one way functions (OWF). Starting from a provably intractable decision problem $L_D$ (whose existence is nonconstructively assured from the well-known discrete time-hierarchy theorem from complexity theory), we construct another intractable decision problem $L\subseteq \{0,1\}^*$ that has its words scattered across $\{0,1\}^\ell$ at a relative frequency $p(\ell)$, for which upper and lower bounds can be worked out. The value $p(\ell)$ is computed from the density of the language within $\{0,1\}^\ell$ divided by the total word count $2^\ell$. It corresponds to the probability of retrieving a yes-instance of a decision problem upon a uniformly random draw from $\{0,1\}^\ell$. The trick to find a language with known bounds on $p(\ell)$ relies on switching from $L_D$ to $L_0:=L_D\cap L'$, where $L'$ is an easy-to-decide language with a known density across $\{0,1\}^*$. In defining $L'$ properly (and upon a suitable G\""odel numbering), the hardness of deciding $L_D\cap L'$ is inherited from $L_D$, while its density is controlled by that of $L'$. The lower and upper approximation of $p(\ell)$ then let us construct an explicit threshold function (as in random graph theory) that can be used to efficiently and intentionally sample yes- or no-instances of the decision problem (language) $L_0$ (however, without any auxiliary information that could ease the decision like a polynomial witness). In turn, this allows to construct a weak OWF that encodes a bit string $w\in\{0,1\}^*$ by efficiently (in polynomial time) emitting a sequence of randomly constructed intractable decision problems, whose answers correspond to the preimage $w$."
"Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45% error on the permutation invariant MNIST task."
"We propose a unified coded framework for distributed computing with straggling servers, by introducing a tradeoff between ""latency of computation"" and ""load of communication"" for some linear computation tasks. We show that the coded scheme of [1]-[3] that repeats the intermediate computations to create coded multicasting opportunities to reduce communication load, and the coded scheme of [4], [5] that generates redundant intermediate computations to combat against straggling servers can be viewed as special instances of the proposed framework, by considering two extremes of this tradeoff: minimizing either the load of communication or the latency of computation individually. Furthermore, the latency-load tradeoff achieved by the proposed coded framework allows to systematically operate at any point on that tradeoff to perform distributed computing tasks. We also prove an information-theoretic lower bound on the latency-load tradeoff, which is shown to be within a constant multiplicative gap from the achieved tradeoff at the two end points."
"The read channel of a Flash memory cell degrades after repetitive program and erase (P/E) operations. This degradation is often modeled as a function of the number of P/E cycles. In contrast, this paper models the degradation as a function of the cumulative effect of the charge written and erased from the cell. Based on this modeling approach, this paper dynamically allocates voltage using lower-voltage write thresholds at the beginning of the device lifetime and increasing the thresholds as needed to maintain the mutual information of the read channel in the face of degradation. The paper introduces the technique in an idealized setting and then removes ideal assumptions about channel knowledge and available voltage resolution to conclude with a practical scheme with performance close to that of the idealized setting."
"We revisit the method of Kirschenhofer, Prodinger and Tichy to calculate the moments of comparisons used by the quick sort algorithm. We reemphasize that this approach helps in calculating these quantities with less computation. We also point out that as observed by Knuth this method also gives moments for total path length of a binary search tree built over a random set of n keys."
"This paper considers the problem of approximate nearest neighbor search in the compressed domain. We introduce polysemous codes, which offer both the distance estimation quality of product quantization and the efficient comparison of binary codes with Hamming distance. Their design is inspired by algorithms introduced in the 90's to construct channel-optimized vector quantizers. At search time, this dual interpretation accelerates the search. Most of the indexed vectors are filtered out with Hamming distance, letting only a fraction of the vectors to be ranked with an asymmetric distance estimator.   The method is complementary with a coarse partitioning of the feature space such as the inverted multi-index. This is shown by our experiments performed on several public benchmarks such as the BIGANN dataset comprising one billion vectors, for which we report state-of-the-art results for query times below 0.3\,millisecond per core. Last but not least, our approach allows the approximate computation of the k-NN graph associated with the Yahoo Flickr Creative Commons 100M, described by CNN image descriptors, in less than 8 hours on a single machine."
"Hebbian plasticity is a powerful principle that allows biological brains to learn from their lifetime experience. By contrast, artificial neural networks trained with backpropagation generally have fixed connection weights that do not change once training is complete. While recent methods can endow neural networks with long-term memories, Hebbian plasticity is currently not amenable to gradient descent. Here we derive analytical expressions for activity gradients in neural networks with Hebbian plastic connections. Using these expressions, we can use backpropagation to train not just the baseline weights of the connections, but also their plasticity. As a result, the networks ""learn how to learn"" in order to solve the problem at hand: the trained networks automatically perform fast learning of unpredictable environmental features during their lifetime, expanding the range of solvable problems. We test the algorithm on various on-line learning tasks, including pattern completion, one-shot learning, and reversal learning. The algorithm successfully learns how to learn the relevant associations from one-shot instruction, and fine-tunes the temporal dynamics of plasticity to allow for continual learning in response to changing environmental parameters. We conclude that backpropagation of Hebbian plasticity offers a powerful model for lifelong learning."
"We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation."
"In this paper, we present a geometric variational algorithm for optimizing the gaits of kinematic locomoting systems. The dynamics of this algorithm are analogous to the physics of a soap bubble, with the system's Lie bracket supplying an ""inflation pressure"" that is balanced by a ""surface tension"" term derived from a Riemannian metric on the system's shape space. We demonstrate this optimizer on a variety of system geometries (including Purcell's swimmer) and for optimization criteria that include maximizing displacement and efficiency of motion for both translation and turning motions."
"The assignment problem is one of the most well-studied settings in social choice, matching, and discrete allocation. We consider the problem with the additional feature that agents' preferences involve uncertainty. The setting with uncertainty leads to a number of interesting questions including the following ones. How to compute an assignment with the highest probability of being Pareto optimal? What is the complexity of computing the probability that a given assignment is Pareto optimal? Does there exist an assignment that is Pareto optimal with probability one? We consider these problems under two natural uncertainty models: (1) the lottery model in which each agent has an independent probability distribution over linear orders and (2) the joint probability model that involves a joint probability distribution over preference profiles. For both of the models, we present a number of algorithmic and complexity results."
"Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (""predictive analytics"") systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis.   The tension between model confidentiality and public access motivates our investigation of model extraction attacks. In such attacks, an adversary with black-box access, but no prior knowledge of an ML model's parameters or training data, aims to duplicate the functionality of (i.e., ""steal"") the model. Unlike in classical learning theory settings, ML-as-a-service offerings may accept partial feature vectors as inputs and include confidence values with predictions. Given these practices, we show simple, efficient attacks that extract target ML models with near-perfect fidelity for popular model classes including logistic regression, neural networks, and decision trees. We demonstrate these attacks against the online services of BigML and Amazon Machine Learning. We further show that the natural countermeasure of omitting confidence values from model outputs still admits potentially harmful model extraction attacks. Our results highlight the need for careful ML model deployment and new model extraction countermeasures."
"High-performance industrial control systems with tens to hundreds of sensors and actuators have stringent communication latency and reliability requirements. Current wireless technologies like WiFi, Bluetooth, LTE are unable to meet these requirements, forcing the use of wired communication in control systems. This paper introduces a wireless communication protocol framework, dubbed ""Occupy CoW,"" based on cooperative communication among nodes in the network to build the diversity necessary to deliver the target reliability. Simultaneous retransmission by many relays achieves this without significantly decreasing throughput or increasing latency. The protocol is analyzed using the communication theoretic delay-limited-capacity framework and compared to baseline schemes that primarily exploit frequency diversity. For a scenario inspired by an industrial printing application with thirty nodes in the control loop, total information throughput of 4.8 Mb/s, and cycle time under 2 ms, an idealized protocol can achieve a system probability of error lower than 10^{-9} with nominal SNR below 5 dB."
"We propose a heuristic method that generates a graph for order/degree problem. Target graphs of our heuristics have large order (> 4000) and diameter 3. We describe the ob- servation of smaller graphs and basic structure of our heuristics. We also explain an evaluation function of each edge for efficient 2-opt local search. Using them, we found the best solutions for several graphs."
"Super-resolution theory aims to estimate the discrete components lying in a continuous space that constitute a sparse signal with optimal precision. This work investigates the potential of recent super-resolution techniques for spectral estimation in multirate sampling systems. It is shown in the first part of this paper that, under the existence of a common supporting grid, the joint frequency estimation problem can be interpreted as a spectral recovery from partial uniform observations. Moreover, under a minimal separation constraint between the frequencies, the sparse spectrum of the observed signal can be exactly jointly recovered up to an aliasing factor, by solving a semidefinite program (SDP). It is proven that multirate sampling allows sub-Nyquist recovery of sparse spectra, and that the aliasing factor can be made considerably smaller than the classic Nyquist rate achieved by an estimation from a standard uniform sampling pattern.   The second part addresses the complexity issue arising from the high dimensionality of semidefinite inequalities induced by partial observation systems. An equivalent compact SDP of minimal dimension is derived by developing the Gram parametrization properties of sparse trigonometric polynomials. The atomic soft thresholding method for spectrum recovery in presence of noise is extended to fit in the partial measurements context, and a fast algorithm for sparse spectral estimation is provided to super-resolve noisy line spectra from the presented framework."
"This paper presents a novel system E3 for extracting keyphrases from news content for the purpose of offering the news audience a broad overview of news events, with especially high content volume. Given an input query, E3 extracts keyphrases and enrich them by tagging, ranking and finding role for frequently associated keyphrases. Also, E3 finds the novelty and activeness of keyphrases using news publication date, to identify the most interesting and informative keyphrases."
"The Fourier Transform is one of the most important linear transformations used in science and engineering. Cooley and Tukey's Fast Fourier Transform (FFT) from 1964 is a method for computing this transformation in time $O(n\log n)$. From a lower bound perspective, very little was known until recently. Ailon shows in 2013 an $\Omega(n\log n)$ for computing the normalized Fourier Transform assuming only unitary operations on two coordinates are allowed at each step, and no extra memory is allowed. In 2014, Ailon then improved the result to show that, in a $\kappa$-well conditioned computation, Fourier computation can be sped up by no more than $O(\kappa)$. The main conjecture is that [Ailon 2014] can be exponentially improved, in the sense that $\kappa$-well condition cannot admit $\omega(\log \kappa)$ speedup.   The main result here is that `algebraic' $\kappa$-well condition admits $O(\sqrt \kappa)$ speedup. The definition of algebraic condition number is obtained by formally viewing multiplication by constants, as performed by the algorithm, as multiplication by indeterminates, giving rise to computation of polynomial matrices. The algebraic condition number is related to the degree of these polynomials. Using the max modulus theorem from complex analysis, we show that algebraic condition number upper bounds standard condition number, and equals it in certain cases. Algebraic condition number is an interesting measure of numerical computation stability in its own right. Moreover, we believe that the approach of algebraic condition number has a good chance of establishing an algebraic version of the main conjecture."
"The adaptive gradient online learning method known as AdaGrad has seen widespread use in the machine learning community in stochastic and adversarial online learning problems and more recently in deep learning methods. The method's full-matrix incarnation offers much better theoretical guarantees and potentially better empirical performance than its diagonal version; however, this version is computationally prohibitive and so the simpler diagonal version often is used in practice. We introduce a new method, CompAdaGrad, that navigates the space between these two schemes and show that this method can yield results much better than diagonal AdaGrad while avoiding the (effectively intractable) $O(n^3)$ computational complexity of full-matrix AdaGrad for dimension $n$. CompAdaGrad essentially performs full-matrix regularization in a low-dimensional subspace while performing diagonal regularization in the complementary subspace. We derive CompAdaGrad's updates for composite mirror descent in case of the squared $\ell_2$ norm and the $\ell_1$ norm, demonstrate that its complexity per iteration is linear in the dimension, and establish guarantees for the method independent of the choice of composite regularizer. Finally, we show preliminary results on several datasets."
"Accurate estimation of spatial gait characteristics is critical to assess motor impairments resulting from neurological or musculoskeletal disease. Currently, however, methodological constraints limit clinical applicability of state-of-the-art double integration approaches to gait patterns with a clear zero-velocity phase. We describe a novel approach to stride length estimation that uses deep convolutional neural networks to map stride-specific inertial sensor data to the resulting stride length. The model is trained on a publicly available and clinically relevant benchmark dataset consisting of 1220 strides from 101 geriatric patients. Evaluation is done in a 10-fold cross validation and for three different stride definitions. Even though best results are achieved with strides defined from mid-stance to mid-stance with average accuracy and precision of 0.01 $\pm$ 5.37 cm, performance does not strongly depend on stride definition. The achieved precision outperforms state-of-the-art methods evaluated on this benchmark dataset by 3.0 cm (36%). Due to the independence of stride definition, the proposed method is not subject to the methodological constrains that limit applicability of state-of-the-art double integration methods. Furthermore, precision on the benchmark dataset could be improved. With more precise mobile stride length estimation, new insights to the progression of neurological disease or early indications might be gained. Due to the independence of stride definition, previously uncharted diseases in terms of mobile gait analysis can now be investigated by re-training and applying the proposed method."
"Measurement of stride-related, biomechanical parameters is the common rationale for objective gait impairment scoring. State-of-the-art double integration approaches to extract these parameters from inertial sensor data are, however, limited in their clinical applicability due to the underlying assumptions. To overcome this, we present a method to translate the abstract information provided by wearable sensors to context-related expert features based on deep convolutional neural networks. Regarding mobile gait analysis, this enables integration-free and data-driven extraction of a set of 8 spatio-temporal stride parameters. To this end, two modelling approaches are compared: A combined network estimating all parameters of interest and an ensemble approach that spawns less complex networks for each parameter individually. The ensemble approach is outperforming the combined modelling in the current application. On a clinically relevant and publicly available benchmark dataset, we estimate stride length, width and medio-lateral change in foot angle up to ${-0.15\pm6.09}$ cm, ${-0.09\pm4.22}$ cm and ${0.13 \pm 3.78^\circ}$ respectively. Stride, swing and stance time as well as heel and toe contact times are estimated up to ${\pm 0.07}$, ${\pm0.05}$, ${\pm 0.07}$, ${\pm0.07}$ and ${\pm0.12}$ s respectively. This is comparable to and in parts outperforming or defining state-of-the-art. Our results further indicate that the proposed change in methodology could substitute assumption-driven double-integration methods and enable mobile assessment of spatio-temporal stride parameters in clinically critical situations as e.g. in the case of spastic gait impairments."
"We present a computable algorithm that assigns probabilities to every logical statement in a given formal language, and refines those probabilities over time. For instance, if the language is Peano arithmetic, it assigns probabilities to all arithmetical statements, including claims about the twin prime conjecture, the outputs of long-running computations, and its own probabilities. We show that our algorithm, an instance of what we call a logical inductor, satisfies a number of intuitive desiderata, including: (1) it learns to predict patterns of truth and falsehood in logical statements, often long before having the resources to evaluate the statements, so long as the patterns can be written down in polynomial time; (2) it learns to use appropriate statistical summaries to predict sequences of statements whose truth values appear pseudorandom; and (3) it learns to have accurate beliefs about its own current beliefs, in a manner that avoids the standard paradoxes of self-reference. For example, if a given computer program only ever produces outputs in a certain range, a logical inductor learns this fact in a timely manner; and if late digits in the decimal expansion of $\pi$ are difficult to predict, then a logical inductor learns to assign $\approx 10\%$ probability to ""the $n$th digit of $\pi$ is a 7"" for large $n$. Logical inductors also learn to trust their future beliefs more than their current beliefs, and their beliefs are coherent in the limit (whenever $\phi \implies \psi$, $\mathbb{P}_\infty(\phi) \le \mathbb{P}_\infty(\psi)$, and so on); and logical inductors strictly dominate the universal semimeasure in the limit.   These properties and many others all follow from a single logical induction criterion, which is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence $\phi$ is associated with a stock that is worth \$1 per share if [...]"
"We propose a novel Connectionist Text Proposal Network (CTPN) that accurately localizes text lines in natural image. The CTPN detects a text line in a sequence of fine-scale text proposals directly in convolutional feature maps. We develop a vertical anchor mechanism that jointly predicts location and text/non-text score of each fixed-width proposal, considerably improving localization accuracy. The sequential proposals are naturally connected by a recurrent neural network, which is seamlessly incorporated into the convolutional network, resulting in an end-to-end trainable model. This allows the CTPN to explore rich context information of image, making it powerful to detect extremely ambiguous text. The CTPN works reliably on multi-scale and multi- language text without further post-processing, departing from previous bottom-up methods requiring multi-step post-processing. It achieves 0.88 and 0.61 F-measure on the ICDAR 2013 and 2015 benchmarks, surpass- ing recent results [8, 35] by a large margin. The CTPN is computationally efficient with 0:14s/image, by using the very deep VGG16 model [27]. Online demo is available at: http://textdet.com/."
"We consider the problem of object recognition in 3D using an ensemble of attribute-based classifiers. We propose two new concepts to improve classification in practical situations, and show their implementation in an approach implemented for recognition from point-cloud data. First, the viewing conditions can have a strong influence on classification performance. We study the impact of the distance between the camera and the object and propose an approach to fuse multiple attribute classifiers, which incorporates distance into the decision making. Second, lack of representative training samples often makes it difficult to learn the optimal threshold value for best positive and negative detection rate. We address this issue, by setting in our attribute classifiers instead of just one threshold value, two threshold values to distinguish a positive, a negative and an uncertainty class, and we prove the theoretical correctness of this approach. Empirical studies demonstrate the effectiveness and feasibility of the proposed concepts."
"In the relay based telecommunications with $K$ relays between the source and destination, $K+1$ time or frequency slots are required for a single frame transmission. However, without the relays, only one time or frequency slot is used for a single frame transmission. Therefore, despite the benefits of relaying systems, this type of communications is not efficient from the spectral efficiency viewpoint. One solution to reduce this issue might be the full-duplex (FD) relays. An old technique which is reconsidered recently to improve the spectral efficiency of telecommunication systems. However, FD relays have a certain complexity, so, some similar techniques such as successive relays with nearly the same performance but less complexity is taken into account now. In successive relaying systems, two relays between the source and destination are employed which receive the transmitted frames from the source and relay it to the destination successively. This structure generally acts like an FD relays. In this paper, the effective capacity performance of an amplify and forward (AF) successive relaying systems with power allocation strategy at the relays are studied perfectly. However, while the inter-rely interference (IRI) between two successive relays has to be managed well, the power allocation and the effective capacity is derived under different assumptions about the IRI. In this way, we assume weak or strong, short or long-term constraints on the IRI. Then we extract the optimal transmitted power at the relay to maximize the effective capacity under these constraints."
"The revelation principle is a fundamental theorem in many economics fields. In this paper, we construct a simple labor model to show that a social choice function which can be implemented costly in Bayesian Nash equilibrium may not be truthfully implementable. The key point is the strategy cost condition given in Section 4: In the direct mechanism, each agent only reports a type and will not pay the strategy cost which would be paid by himself when playing strategies in the original indirect mechanism. As a result, the revelation principle may not hold when agents' strategies are costly in the indirect mechanism."
"In this work, we provide a solution for pre-shaping a human-like robot hand for grasping based on visual information. Our approach uses convolutional neural networks (CNNs) to define a mapping between images and grasps. Applying CNNs to robotics applications is non-trivial for two reasons. First, collecting enough robot data to train a CNN at the same scale as the models trained in the vision community is extremely difficult. In this work, we demonstrate that by using a pre-trained CNN, a small set of grasping examples is sufficient for generalizing across different objects of similar shapes. Second, the final output of a CNN contains little location information of the observed object, which is essential for the robot to manipulate the object. We take advantage of the hierarchical nature of CNN layers and identify the 3D positions of features that capture the hierarchical support relations between filters in different CNN layers using an approach we call targeted backpropagation. Targeted backpropagation traces the activation of higher level features in a CNN backwards through the network to discover the locations in the observation that were responsible for making them fire, thus localizing important structures that are manipulable in the environment. We show that this approach outperforms approaches without targeted backpropagation in a cluttered scene. We further implement a hierarchical controller that controls fingers and palms based on features located in different layers of the CNN for pre-shaping the robot hand and demonstrate that this approach outperforms a point cloud based approach on a grasping task on Robonaut-2."
"The Internet of Things (IoT) systems are designed and developed either as standalone applications from the ground-up or with the help of IoT middleware platforms. They are designed to support different kinds of scenarios, such as smart homes and smart cities. Thus far, privacy concerns have not been explicitly considered by IoT applications and middleware platforms. This is partly due to the lack of systematic methods for designing privacy that can guide the software development process in IoT. In this paper, we propose a set of guidelines, a privacy-by-design framework, that can be used to assess privacy capabilities and gaps of existing IoT applications as well as middleware platforms. We have evaluated two open source IoT middleware platforms, namely OpenIoT and Eclipse SmartHome, to demonstrate how our framework can be used in this way."
"In this work we study a model of tax evasion. We considered a fixed population divided in three compartments, namely honest tax payers, tax evaders and a third class between the mentioned two, which we call \textit{susceptibles} to become evaders. The transitions among those compartments are ruled by probabilities, similarly to a model of epidemic spreading. These probabilities model social interactions among the individuals, as well as the government's fiscalization. We simulate the model on fully-connected graphs, as well as on scale-free and random complex networks. For the fully-connected and random graph cases we observe that the emergence of tax evaders in the population is associated with an active-absorbing nonequilibrium phase transition, that is absent in scale-free networks."
"We study the state complexity of binary operations on regular languages over different alphabets. It is known that if $L'_m$ and $L_n$ are languages of state complexities $m$ and $n$, respectively, and restricted to the same alphabet, the state complexity of any binary boolean operation on $L'_m$ and $L_n$ is $mn$, and that of product (concatenation) is $m 2^n - 2^{n-1}$. In contrast to this, we show that if $L'_m$ and $L_n$ are over different alphabets, the state complexity of union and symmetric difference is $(m+1)(n+1)$, that of difference is $mn+m$, that of intersection is $mn$, and that of product is $m2^n+2^{n-1}$. We also study unrestricted complexity of binary operations in the classes of regular right, left, and two-sided ideals, and derive tight upper bounds. The bounds for product of the unrestricted cases (with the bounds for the restricted cases in parentheses) are as follows: right ideals $m+2^{n-2}+2^{n-1}$ ($m+2^{n-2}$); left ideals $mn+m+n$ ($m+n-1$); two-sided ideals $m+2n$ ($m+n-1$). The state complexities of boolean operations on all three types of ideals are the same as those of arbitrary regular languages, whereas that is not the case if the alphabets of the arguments are the same. Finally, we update the known results about most complex regular, right-ideal, left-ideal, and two-sided-ideal languages to include the unrestricted cases."
"As the complexity of current data flow systems and according infrastructure networks increases, the security of data transition through such platforms becomes more important. Thus, different areas of steganography turn to one of the most challengeable topics of current researches. In this paper a novel method is presented to hide an image into the host image and Hardware/Software design is proposed to implement our stagenography system on FPGA- DE2 70 Altera board. The size of the secret image is quadrant of the host image. Host image works as a cipher key to completely distort and encrypt the secret image using XOR operand. Each pixel of the secret image is composed of 8 bits (4 bit-pair) in which each bit-pair is distorted by XORing it with two LSB bits of the host image and putting the results in the location of two LSB bits of host image. The experimental results show the effectiveness of the proposed method compared to the most recently proposed algorithms by considering that the obtained information entropy for encrypt image is approximately equal to 8."
"Preference orderings are orderings of a set of items according to the preferences (of judges). Such orderings arise in a variety of domains, including group decision making, consumer marketing, voting and machine learning. Measuring the mutual information and extracting the common patterns in a set of preference orderings are key to these areas. In this paper we deal with the representation of sets of preference orderings, the quantification of the degree to which judges agree on their ordering of the items (i.e. the concordance), and the efficient, meaningful description of such sets.   We propose to represent the orderings in a subsequence-based feature space and present a new algorithm to calculate the size of the set of all common subsequences - the basis of a quantification of concordance, not only for pairs of orderings but also for sets of orderings. The new algorithm is fast and storage efficient with a time complexity of only $O(Nn^2)$ for the orderings of $n$ items by $N$ judges and a space complexity of only $O(\min\{Nn,n^2\})$.   Also, we propose to represent the set of all $N$ orderings through a smallest set of covering preferences and present an algorithm to construct this smallest covering set.   The source code for the algorithms is available at https://github.com/zhiweiuu/secs"
"The growing demand for structured knowledge has led to great interest in relation extraction, especially in cases with limited supervision. However, existing distance supervision approaches only extract relations expressed in single sentences. In general, cross-sentence relation extraction is under-explored, even in the supervised-learning setting. In this paper, we propose the first approach for applying distant supervision to cross- sentence relation extraction. At the core of our approach is a graph representation that can incorporate both standard dependencies and discourse relations, thus providing a unifying way to model relations within and across sentences. We extract features from multiple paths in this graph, increasing accuracy and robustness when confronted with linguistic variation and analysis error. Experiments on an important extraction task for precision medicine show that our approach can learn an accurate cross-sentence extractor, using only a small existing knowledge base and unlabeled text from biomedical research articles. Compared to the existing distant supervision paradigm, our approach extracted twice as many relations at similar precision, thus demonstrating the prevalence of cross-sentence relations and the promise of our approach."
"An accurate approximate formula of the die-out probability in a SIS epidemic process on a network is proposed. The formula contains only three essential parameters: the largest eigenvalue of the adjacency matrix of the network, the effective infection rate of the virus, and the initial number of infected nodes in the network. The die-out probability formula is compared with the exact die-out probability in complete graphs, Erd\H{o}s-R\'enyi graphs, and a power-law graph. Furthermore, as an example, the formula is applied to the $N$-Intertwined Mean-Field Approximation, to explicitly incorporate the die-out."
An extension of the entropy power inequality to the form $N_r^\alpha(X+Y) \geq N_r^\alpha(X) + N_r^\alpha(Y)$ with arbitrary independent summands $X$ and $Y$ in $\mathbb{R}^n$ is obtained for the R\'enyi entropy and powers $\alpha \geq (r+1)/2$.
"The number of publications has been a fundamental merit in the competition for academic positions since the late 18th century. Today, the simple counting of publications has been supplemented with a whole range of bibliometric measures, which supposedly not only measures the volume of research but also its impact. In this study, we investigate how bibliometrics are used for evaluating the impact and quality of publications in two specific settings: biomedicine and economics. Our study exposes the extent and type of metrics used in external evaluations of candidates for academic positions at Swedish universities. Moreover, we show how different bibliometric indicators, both explicitly and implicitly, are employed to value and rank candidates. Our findings contribute to a further understanding of bibliometric indicators as judgment devices that are employed in evaluating individuals and their published works within specific fields. We also show how expertise in using bibliometrics for evaluative purposes is negotiated at the interface between domain knowledge and skills in using indicators. In line with these results we propose that the use of metrics in this context is best described as a form of citizen bibliometrics - an underspecified term which we build upon in the paper."
"When factorizing binary matrices, we often have to make a choice between using expensive combinatorial methods that retain the discrete nature of the data and using continuous methods that can be more efficient but destroy the discrete structure. Alternatively, we can first compute a continuous factorization and subsequently apply a rounding procedure to obtain a discrete representation. But what will we gain by rounding? Will this yield lower reconstruction errors? Is it easy to find a low-rank matrix that rounds to a given binary matrix? Does it matter which threshold we use for rounding? Does it matter if we allow for only non-negative factorizations? In this paper, we approach these and further questions by presenting and studying the concept of rounding rank. We show that rounding rank is related to linear classification, dimensionality reduction, and nested matrices. We also report on an extensive experimental study that compares different algorithms for finding good factorizations under the rounding rank model."
"We introduce a new scientometric index, inspired by the Lobby-Hirsch index from complex networks literature, that we call K index. The K index grows with the impact of the citing papers and can be thought of as a measure of scientific innovation and recognition. We show that the K-index can be easily computed from the Web of Science platform and presents several advantages over other bibliometric indexes. The K index is robust to self-citations, is not limited by the total number of papers published by a researcher and is able to distinguish in a consistent way researchers that have the same h index but different scientific impacts -- Einstein and Hirsch, for example. The K index successfully detects a known case of inflated numbers for papers, citations and h index due to scientific career fraud. Finally, we show that, in a sample of twenty-eight physics Nobel laureates and twenty-eight highly cited non-Nobel-laureate physicists, the K index correlates better to the achievement of scientific prizes than the number of papers, citations, citations per paper, citing articles and the h index. Clustering researchers in a K versus h plot reveals interesting patterns and outliers that suggest that these two indexes are complementary."
"Devices and sensors for identification of fallers can be used to implement actions to prevent these falls and to allow the elderly to live an independent life while reducing the long-term care costs. In previous studies the feature extraction is often dependent on step detection, the collected data sets are often unavailable, and a large number of features are attempted in order to sufficiently represent the faller and non-faller classes. In this study we present results of feature extraction on accelerometer data with the aim of identifying fallers among a group of 36 healthy older persons. The features are extracted from three different Timed Up and Go (TUG) tests combined with simultaneous manual and cognitive tasks. As a result, a set of features is described to discriminate fallers from non-fallers based on the segmentation of the TUG trials, in particular using TUG with cognitive task, while conventional functional tests fail to do so. In addition, we show that the fusion of features improve the discrimination power with the best result showing f1-Score = 0.83 and AUC = 0.84, demonstrating the relevance of the study. We concluded that features extracted from segmented TUG trials acquired with simultaneous tasks has potential to improve performance when identifying fallers via accelerometer sensors."
"Utilizing complex dynamics of chaotic maps and systems in encryption was studied comprehensively in the past two and a half decades. In 1989, Fridrich's chaotic image encryption scheme was designed by iterating chaotic position permutation and value substitution some rounds, which received intensive attention in the field of chaos-based cryptography. In 2010, Solak \textit{et al.} proposed a chosen-ciphertext attack on the Fridrich's scheme utilizing influence network between cipher-pixels and the corresponding plain-pixels. Based on their creative work, this paper scrutinized some properties of Fridrich's scheme with concise mathematical language. Then, some minor defects of the real performance of Solak's attack method were given. The work provides some bases for further optimizing attack on the Fridrich's scheme and its variants."
"As a new way of training generative models, Generative Adversarial Nets (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines."
"Multilinear Grammar (MLG) is an approach to integrating the many different syntagmatic structures of language into a coherent architecture, the Rank-Interpretation Architecture. The architecture defines ranks from discourse structure through utterances, phrasal structures, word structures to speech sounds. Each rank has its own specific kind of prosodic-phonetic interpretation and semantic-pragmatic interpretation. Common to models of all these subdomains are models based on regular languages, and processors with finite working memory."
"Deep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-of-concept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system -- though just a prototype -- learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game."
"In this paper, we consider a problem of formation control of large vehicle network and propose a systematic way to establish robust and efficient interaction between agents, referred as cascade formulation. The proposed formulation divides the network into smaller clusters and meta-cluster ensuring 2-rooted communication graph. We use complex Laplacian approach to provide control law for proposed formulation. Further, we provide sufficient conditions on proposed approach to ensure globally stable formation. We also demonstrate that the proposed formulation gives flexibility to constrain the eigenvalue spectra of overall closed-loop dynamics to ensure desired convergence rate and control input intensity. The paper also illustrates the robustness of the proposed formulation to some uncertainties such as loss in communication links and actuator failure. The effectiveness of the proposed approach is illustrated by simulating it for an example of vehicle network with thirty agents."
"Advances in deep reinforcement learning have allowed autonomous agents to perform well on Atari games, often outperforming humans, using only raw pixels to make their decisions. However, most of these games take place in 2D environments that are fully observable to the agent. In this paper, we present the first architecture to tackle 3D environments in first-person shooter games, that involve partially observable states. Typically, deep reinforcement learning methods only utilize visual input for training. We present a method to augment these models to exploit game feature information such as the presence of enemies or items, during the training phase. Our model is trained to simultaneously learn these features along with minimizing a Q-learning objective, which is shown to dramatically improve the training speed and performance of our agent. Our architecture is also modularized to allow different models to be independently trained for different phases of the game. We show that the proposed architecture substantially outperforms built-in AI agents of the game as well as humans in deathmatch scenarios."
"We give a quantum algorithm for solving semidefinite programs (SDPs). It has worst case running time n^{1/2}m^{1/2}s poly(log(n), log(m), R, 1/delta), with n and s the dimension and sparsity of the input matrices, respectively, m the number of constraints, delta the accuracy of the solution, and R an upper bound on the trace of the optimal solution. This gives a square-root unconditional speed-up over any classical method for solving SDPs both in n and m. We prove the algorithm cannot be substantially improved giving a Omega(n^{1/2} + m^{1/2}) quantum lower bound for solving semidefinite programs with constant s, R and delta.   We then argue that in some instances the algorithm offer even exponential speed-ups. This is the case whenever the quantum Gibbs states of Hamiltonians given by linear combinations of the input matrices of the SDP can be prepared efficiently on a quantum computer. An example are SDPs in which the input matrices have low-rank: For SDPs with the maximum rank of any input matrix bounded by r, we show the quantum algorithm runs in time poly(log(n), log(m), r, R, delta)m^{1/2}.   The quantum algorithm is constructed by a combination of quantum Gibbs sampling and the multiplicative weight method. In particular it is based on an classical algorithm of Arora and Kale for approximately solving SDPs. We present a modification of their algorithm to eliminate the need of solving an inner linear program which may be of independent interest."
"Using the matrix product state (MPS) representation of tensor train decompositions, in this paper we propose a tensor completion algorithm which alternates over the matrices (tensors) in the MPS representation. This development is motivated in part by the success of matrix completion algorithms which alternate over the (low-rank) factors. We comment on the computational complexity of the proposed algorithm and numerically compare it with existing methods employing low rank tensor train approximation for data completion as well as several other recently proposed methods. We show that our method is superior to existing ones for a variety of real settings."
"Unification in Description Logics has been introduced as a means to detect redundancies in ontologies. We try to extend the known decidability results for unification in the Description Logic $\mathcal{EL}$ to disunification since negative constraints can be used to avoid unwanted unifiers. While decidability of the solvability of general $\mathcal{EL}$-disunification problems remains an open problem, we obtain NP-completeness results for two interesting special cases: dismatching problems, where one side of each negative constraint must be ground, and local solvability of disunification problems, where we consider only solutions that are constructed from terms occurring in the input problem. More precisely, we first show that dismatching can be reduced to local disunification, and then provide two complementary NP-algorithms for finding local solutions of disunification problems."
"Linear complementary dual (LCD) codes is a class of linear codes introduced by Massey in 1964. LCD codes have been extensively studied in literature recently. In addition to their applications in data storage, communications systems, and consumer electronics, LCD codes have been employed in cryptography. More specifically, it has been shown that LCD codes can also help improve the security of the information processed by sensitive devices, especially against so-called side-channel attacks (SCA) and fault non-invasive attacks. In this paper, we are interested in the construction of particular algebraic geometry (AG) LCD codes which could be good candidates to be resistant against SCA. We firstly provide a construction scheme for obtaining LCD codes from elliptic curves. Then, some explicit LCD codes from elliptic curve are presented. MDS codes are of the most importance in coding theory due to their theoretical significance and practical interests. In this paper, all the constructed LCD codes from elliptic curves are MDS or almost MDS. Some infinite classes of LCD codes from elliptic curves are optimal due to the Griesmer bound. Finally, we introduce a construction mechanism for obtaining LCD codes from any algebraic curve and derive some explicit LCD codes from hyperelliptic curves and Hermitian curves."
"We design new sketching algorithms for unitarily invariant matrix norms, including the Schatten $p$-norms~$\|{\cdot}\|_{S_p}$, and obtain, as a by-product, streaming algorithms that approximate the norm of a matrix $A$ presented as a turnstile data stream. The primary advantage of our streaming algorithms is that they are simpler and faster than previous algorithms, while requiring the same or less storage. Our three main results are a faster sketch for estimating $\|{A}\|_{S_p}$, a smaller-space $O(1)$-pass sketch for $\|{A}\|_{S_p}$, and more general sketching technique that yields sublinear-space approximations for a wide class of matrix norms. These improvements are powered by dimensionality reduction techniques that are modern incarnations of the Johnson-Lindenstrauss Lemma~\cite{JL84}. When $p\geq 2$ is even or $A$ is PSD, our fast one-pass algorithm approximates $\|{A}\|_{S_p}$ in optimal, $n^{2-4/p}$, space with $O(1)$ update time and $o(n^{2.4(1-2/p)})$ time to extract the approximation from the sketch, while the $\lceil p/2\rceil$-pass algorithm is built on a smaller sketch of size $n^{1-1/(p-1)}$ with $O(1)$ update time and $n^{1-1/(p-1)}$ query time. Finally, for a PSD matrix $A$ and a unitarily invariant norm $l(\cdot)$, we prove that one can obtain an approximation to $l(A)$ from a sketch $GAH^T$ where $G$ and $H$ are independent Oblivious Subspace Embeddings and the dimension of the sketch is polynomial in the intrinsic dimension of $A$. The intrinsic dimension of a matrix is a robust version of the rank that is equal to the ratio $\sum_i\sigma_i/\sigma_1$. It is small, e.g., for models in machine learning which consist of a low rank matrix plus noise. Naturally, this leads to much smaller sketches for many norms."
"Kerr nonlinear cavities displaying optical thresholding have been proposed for the realization of ultra-low power photonic logic gates. In the ultra-low photon number regime, corresponding to energy levels in the attojoule scale, quantum input-output models become important to study the effect of unavoidable quantum fluctuations on the performance of such logic gates. However, being a quantum anharmonic oscillator, a Kerr-cavity has an infinite dimensional Hilbert space spanned by the Fock states of the oscillator. This poses a challenge to simulate and analyze photonic logic gates and circuits composed of multiple Kerr nonlinearities. For simulation, the Hilbert of the oscillator is typically truncated to the span of only a finite number of Fock states. This paper develops a quasi-principal components approach to identify important subspaces of a Kerr-cavity Hilbert space and exploits it to construct an approximate reduced model of the Kerr-cavity on a smaller Hilbert space. Using this approach, we find a reduced dimension model with a Hilbert space dimension of 15 that can closely match the magnitudes of the mean transmitted and reflected output fields of a conventional truncated Fock state model of dimension 75, when driven by an input coherent field that switches between two levels. For the same input, the reduced model also closely matches the magnitudes of the mean output fields of Kerr-cavity-based AND and NOT gates and a NAND latch obtained from simulation of the full 75 dimension model."
"This paper addresses automatic quality assessment of spoken language translation (SLT). This relatively new task is defined and formalized as a sequence labeling problem where each word in the SLT hypothesis is tagged as good or bad according to a large feature set. We propose several word confidence estimators (WCE) based on our automatic evaluation of transcription (ASR) quality, translation (MT) quality, or both (combined ASR+MT). This research work is possible because we built a specific corpus which contains 6.7k utterances for which a quintuplet containing: ASR output, verbatim transcript, text translation, speech translation and post-edition of translation is built. The conclusion of our multiple experiments using joint ASR and MT features for WCE is that MT features remain the most influent while ASR feature can bring interesting complementary information. Our robust quality estimators for SLT can be used for re-scoring speech translation graphs or for providing feedback to the user in interactive speech translation or computer-assisted speech-to-text scenarios."
"This paper discusses the automated visual identification of individual great white sharks from dorsal fin imagery. We propose a computer vision photo ID system and report recognition results over a database of thousands of unconstrained fin images. To the best of our knowledge this line of work establishes the first fully automated contour-based visual ID system in the field of animal biometrics. The approach put forward appreciates shark fins as textureless, flexible and partially occluded objects with an individually characteristic shape. In order to recover animal identities from an image we first introduce an open contour stroke model, which extends multi-scale region segmentation to achieve robust fin detection. Secondly, we show that combinatorial, scale-space selective fingerprinting can successfully encode fin individuality. We then measure the species-specific distribution of visual individuality along the fin contour via an embedding into a global `fin space'. Exploiting this domain, we finally propose a non-linear model for individual animal recognition and combine all approaches into a fine-grained multi-instance framework. We provide a system evaluation, compare results to prior work, and report performance and properties in detail."
"We investigated the possibility of using a machine-learning scheme in conjunction with commercial wearable EEG-devices for translating listener's subjective experience of music into scores that can be used for the automated annotation of music in popular on-demand streaming services. Based on the established -neuroscientifically sound- concepts of brainwave frequency bands, activation asymmetry index and cross-frequency-coupling (CFC), we introduce a Brain Computer Interface (BCI) system that automatically assigns a rating score to the listened song. Our research operated in two distinct stages: i) a generic feature engineering stage, in which features from signal-analytics were ranked and selected based on their ability to associate music induced perturbations in brainwaves with listener's appraisal of music. ii) a personalization stage, during which the efficiency of ex- treme learning machines (ELMs) is exploited so as to translate the derived pat- terns into a listener's score. Encouraging experimental results, from a pragmatic use of the system, are presented."
"We introduce expandable partial propensity direct method (EPDM) - a new exact stochastic simulation algorithm suitable for systems involving many interacting molecular species. The algorithm is especially efficient for sparsely populated systems, where the number of species that may potentially be generated is much greater than the number of species actually present in the system at any given time. The number of operations per reaction scales linearly with the number of species, but only those which have one or more molecules. To achieve this kind of performance we are employing a data structure which allows to add and remove species and their interactions on the fly. When a new specie is added, its interactions with every other specie are generated dynamically by a set of user-defined rules. By removing the records involving the species with zero molecules, we keep the number of species as low as possible. This enables simulations of systems for which listing all species is not practical. The algorithm is based on partial propensities direct method (PDM) by Ramaswamy et al. for sampling trajectories of the chemical master equation."
"It has been shown in the past that many real-world networks exhibit community structures and non-trivial clustering which comes with the occurrence of a notable number of triangular connections. Yet the influence of such connection patterns on the dynamics of disease transmission is not fully understood. In order to study their role in the context of Susceptible-Infected-Susceptible (SIS) epidemics we use the Triadic Random Graph (TRG) model to construct small networks (N=49) from distinct, closed, directed triadic subpatterns. We compare various global properties of TRGs and use the N-intertwined mean-field approximation (NIMFA) model to perform numerical simulations of SIS-dynamics on TRGs. The results show that the infection spread on undirected TRGs displays very similar behavior to TRGs with an abundance of (directed) feed-back-loops, while using (directed) feed-forward-loops as network-entities significantly slows down the epidemic and lowers the number of infected individuals in the endemic state. Moreover, we introduce a novel stochastic approach for modelling the SIS-epidemics on TRGs based on characterizing nodes according to their set of $\left(k_{in},k_{out}\right)$ within triads. Within this model, the topology of the network is given by the number and the local structure of directed triadic motifs and not by the adjacency matrix. Nevertheless, the outcome of simulations is qualitatively similar to the results of the NIMFA model."
"Spatial aspects of computation are becoming increasingly relevant in Computer Science, especially in the field of collective adaptive systems and when dealing with systems distributed in physical space. Traditional formal verification techniques are well suited to analyse the temporal evolution of programs; however, properties of space are typically not taken into account explicitly. We present a topology-based approach to formal verification of spatial properties depending upon physical space. We define an appropriate logic, stemming from the tradition of topological interpretations of modal logics, dating back to earlier logicians such as Tarski, where modalities describe neighbourhood. We lift the topological definitions to the more general setting of closure spaces, also encompassing discrete, graph-based structures. We extend the framework with a spatial surrounded operator, a propagation operator and with some collective operators. The latter are interpreted over arbitrary sets of points instead of individual points in space. We define efficient model checking procedures, both for the individual and the collective spatial fragments of the logic and provide a proof-of-concept tool."
"Feature selection methods are usually evaluated by wrapping specific classifiers and datasets in the evaluation process, resulting very often in unfair comparisons between methods. In this work, we develop a theoretical framework that allows obtaining the true feature ordering of two-dimensional sequential forward feature selection methods based on mutual information, which is independent of entropy or mutual information estimation methods, classifiers, or datasets, and leads to an undoubtful comparison of the methods. Moreover, the theoretical framework unveils problems intrinsic to some methods that are otherwise difficult to detect, namely inconsistencies in the construction of the objective function used to select the candidate features, due to various types of indeterminations and to the possibility of the entropy of continuous random variables taking null and negative values."
"Location data can be extremely useful to study commuting patterns and disruptions, as well as to predict real-time traffic volumes. At the same time, however, the fine-grained collection of user locations raises serious privacy concerns, as this can reveal sensitive information about the users, such as, life style, political and religious inclinations, or even identities. In this paper, we study the feasibility of crowd-sourced mobility analytics over aggregate location information: users periodically report their location, using a privacy-preserving aggregation protocol, so that the server can only recover aggregates -- i.e., how many, but not which, users are in a region at a given time. We experiment with real-world mobility datasets obtained from the Transport For London authority and the San Francisco Cabs network, and present a novel methodology based on time series modeling that is geared to forecast traffic volumes in regions of interest and to detect mobility anomalies in them. In the presence of anomalies, we also make enhanced traffic volume predictions by feeding our model with additional information from correlated regions. Finally, we present and evaluate a mobile app prototype, called Mobility Data Donors (MDD), in terms of computation, communication, and energy overhead, demonstrating the real-world deployability of our techniques."
"Given a fixed $k$ $\in$ $\mathbb{Z}^+$ and $\lambda$ $\in$ $\mathbb{Z}^+$, the objective of a $\lambda$-$L(k, k-1, \ldots, 2, 1)$-labeling of a graph $G$ is to assign non-negative integers (known as labels) from the set $\{0, \ldots, \lambda-1\}$ to the vertices of $G$ such that the adjacent vertices receive values which differ by at least $k$, vertices connected by a path of length two receive values which differ by at least $k-1$, and so on. The vertices which are at least $k+1$ distance apart can receive the same label. The smallest $\lambda$ for which there exists a $\lambda$-$L(k, k-1, \ldots, 2, 1)$-labeling of $G$ is known as the $L(k, k-1, \ldots, 2, 1)$-labeling number of $G$ and is denoted by $\lambda_k(G)$. The ratio between the upper bound and the lower bound of a $\lambda$-$L(k, k-1, \ldots, 2, 1)$-labeling is known as the approximation ratio. In this paper a lower bound on the value of the labeling number for square grid is computed and a formula is proposed which yields a $\lambda$-$L(k, k-1, \ldots, 2, 1)$-labeling of square grid, with approximation ratio at most $\frac{9}{8}$. The labeling presented is a no-hole one, i.e., it uses each label from $0$ to $\lambda-1$ at least once."
"This paper presents a theoretical analysis and practical evaluation of the main bottlenecks towards a scalable distributed solution for the training of Deep Neuronal Networks (DNNs). The presented results show, that the current state of the art approach, using data-parallelized Stochastic Gradient Descent (SGD), is quickly turning into a vastly communication bound problem. In addition, we present simple but fixed theoretic constraints, preventing effective scaling of DNN training beyond only a few dozen nodes. This leads to poor scalability of DNN training in most practical scenarios."
"Data association is one of the fundamental problems in multi-sensor systems. Most current techniques rely on pairwise data associations which can be spurious even after the employment of outlier rejection schemes. Considering multiple pairwise associations at once significantly increases accuracy and leads to consistency. In this work, we propose two fully decentralized methods for consistent global data association from pairwise data associations. The first method is a consensus algorithm on the set of doubly stochastic matrices. The second method is a decentralization of the spectral method proposed by Pachauri et al.. We demonstrate the effectiveness of both methods using theoretical analysis and experimental evaluation."
"This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with only minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases."
"Third-party services form an integral part of the mobile ecosystem: they allow app developers to add features such as performance analytics and social network integration, and to monetize their apps by enabling user tracking and targeted ad delivery. At present users, researchers, and regulators all have at best limited understanding of this third-party ecosystem. In this paper we seek to shrink this gap. Using data from users of our ICSI Haystack app we gain a rich view of the mobile ecosystem: we identify and characterize domains associated with mobile advertising and user tracking, thereby taking an important step towards greater transparency. We furthermore outline our steps towards a public catalog and census of analytics services, their behavior, their personal data collection processes, and their use across mobile apps."
"There are two well known systems formalizing total recursion beyond primitive recursion (\textbf{PR}), system \textbf{T} by G\""odel and system \textbf{F} by Girard and Reynolds. system \textbf{T} defines recursion on typed objects and can construct every function of Heyting arithmetic (\textbf{HA}). System \textbf{F} introduces type variables which can define the recursion of system \textbf{T}. The result is a system as expressive as second-order Heyting arithmetic (\textbf{HA}$_{2}$). Though, both are able to express unimaginably fast growing functions, in some applications a more flexible formalism is needed. One such application is CERES cut-elimination for schematic \textbf{LK}-proofs ($CERES_{s}$) where the shape of the recursion is important. In this paper we introduce a formalism for fast growing functions without a type theory foundation. The recursion is indexed by ordered sets of natural numbers. We highlight the relationship between our recursion and the Wainer hierarchy to provide an comparison to existing systems. We can show that our formalism expresses the functions expressible using system \textbf{T}. We leave comparison to system \textbf{F} and beyond to future work."
"Many objects in the real world are difficult to describe by a single numerical vector of a fixed length, whereas describing them by a set of vectors is more natural. Therefore, Multiple instance learning (MIL) techniques have been constantly gaining on importance throughout last years. MIL formalism represents each object (sample) by a set (bag) of feature vectors (instances) of fixed length where knowledge about objects (e.g., class label) is available on bag level but not necessarily on instance level. Many standard tools including supervised classifiers have been already adapted to MIL setting since the problem got formalized in late nineties. In this work we propose a neural network (NN) based formalism that intuitively bridges the gap between MIL problem definition and the vast existing knowledge-base of standard models and classifiers. We show that the proposed NN formalism is effectively optimizable by a modified back-propagation algorithm and can reveal unknown patterns inside bags. Comparison to eight types of classifiers from the prior art on a set of 14 publicly available benchmark datasets confirms the advantages and accuracy of the proposed solution."
"In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data."
Directed graphs are widely used in modelling of nonsymmetric relations in various sciences and engineering disciplines. We discuss invariants of strongly connected directed graphs - minimal number of vertices or edges necessary to remove to make remaining graphs not strongly connected. By analogy with undirected graphs these invariants are called strong vertex/edge connectivities. We review some properties of these invariants. Computational results for some publicly available connectome graphs used in neuroscience are described.
"Quasi-Monte Carlo methods are designed for integrands of bounded variation, and this excludes singular integrands. Several methods are known for integrands that become singular on the boundary of the unit cube $[0,1]^d$ or at isolated possibly unknown points within $[0,1]^d$. Here we consider functions on the square $[0,1]^2$ that may become singular as the point approaches the diagonal line $x_1=x_2$, and we study three quadrature methods. The first method splits the square into two triangles separated by a region around the line of singularity, and applies recently developed triangle QMC rules to the two triangular parts. For functions with a singularity `no worse than $|x_1-x_2|^{-A}$ for $0<A<1$ that method yields an error of $O( (\log(n)/n)^{(1-A)/2})$. We also consider methods extending the integrand into a region containing the singularity and show that method will not improve up on using two triangles. Finally, we consider transforming the integrand to have a more QMC-friendly singularity along the boundary of the square. This then leads to error rates of $O(n^{-1+\epsilon+A})$ when combined with some corner-avoiding Halton points or with randomized QMC."
"Diffusion in complex networks studied in the paper is a process of spreading information, ideas or substances with replication between nodes through edges from the regions of high to low concentration. Diffusion cascades are triggered by the activation of a small set of initial nodes - seeds - and later supported by the natural process. In this work, several novel approaches related to extension of the commonly used seeding strategies into a sequence of stages are proposed. Sequential seeding method is compared with a single stage approach using both real and artificial complex networks and applying various dynamic node ranking methods and diffusion parameters. The experimental results show that sequential seeding strategies deliver better results than single stage seeding in most cases. These strategies avoid seeding nodes that are activated through the natural diffusion process at the preceding stages of sequential seeding. The gain arises when a saved seed is allocated to a node difficult to reach via diffusion process. The performance is further improved by identifying non-activated nodes with high potential of activizing their not yet active neighbors. It is boosted by adaptive recomputing of network measures on the subgraph with non-activated nodes only. The higher length of seeding sequences tends to activate more nodes but extends the duration of diffusion process. The results demonstrate the trade-off between the coverage and speed of diffusion that can be balanced according to the importance assigned to coverage and time in specific applications."
"Linearizability and progress properties are key correctness notions for concurrent objects. However, model checking linearizability has suffered from the PSPACE-hardness of the trace inclusion problem. This paper proposes to exploit branching bisimulation, a fundamental semantic equivalence relation developed for process algebras which can be computed efficiently, in checking these properties. A quotient construction is provided which results in huge state space reductions. We confirm the advantages of the proposed approach on more than a dozen benchmark problems."
"We study the pricing problem faced by a firm that sells a large number of products, described via a wide range of features, to customers that arrive over time. This is motivated in part by the prevalence of online marketplaces that allow for real-time pricing. We propose a dynamic policy, called Regularized Maximum Likelihood Pricing (RMLP), that obtains asymptotically optimal revenue. Our policy leverages the structure (sparsity) of a high-dimensional demand space in order to obtain a logarithmic regret compared to the clairvoyant policy that knows the parameters of the demand in advance. More specifically, the regret of our algorithm is of $O(s_0 \log T (\log d + \log T))$, where $d$ and $s_0$ correspond to the dimension of the demand space and its sparsity. Furthermore, we show that no policy can obtain regret better than $O(s_0 (\log d + \log T))$."
"A common way of classifying network connectivity is the association of the nodal degree distribution to specific probability distribution models. During the last decades, researchers classified many networks using the Poisson or Pareto distributions. Urban infrastructures, like transportation (railways, roads, etc.) and distribution (gas, water, energy, etc.) systems, are peculiar networks strongly constrained by spatial characteristics of the environment where they are constructed. Consequently, the nodal degree of such networks spans very small ranges not allowing a reliable classification using the nodal degree distribution. In order to overcome this problem, we here (i) define the neighborhood degree, equal to the sum of the nodal degrees of the nearest topological neighbors, the adjacent nodes and (ii) propose to use neighborhood degree to classify infrastructure networks. Such neighborhood degree spans a wider range of degrees than the standard one allowing inferring the probabilistic model in a more reliable way, from a statistical standpoint. In order to test our proposal, we here analyze twenty-two real water distribution networks, built in different environments, demonstrating that the Poisson distribution generally models very well their neighborhood degree distributions. This result seems consistent with the less reliable classification achievable with the scarce information using the standard nodal degree distribution."
"We consider combinatorial aspects of $\lambda$-terms in the model based on de Bruijn indices where each building constructor is of size one. Surprisingly, the counting sequence for $\lambda$-terms corresponds also to two families of binary trees, namely black-white trees and zigzag-free ones. We provide a constructive proof of this fact by exhibiting appropriate bijections. Moreover, we identify the sequence of Motzkin numbers with the counting sequence for neutral $\lambda$-terms, giving a bijection which, in consequence, results in an exact-size sampler for the latter based on the exact-size sampler for Motzkin trees of Bodini et alli. Using the powerful theory of analytic combinatorics, we state several results concerning the asymptotic growth rate of $\lambda$-terms in neutral, normal, and head normal forms. Finally, we investigate the asymptotic density of $\lambda$-terms containing arbitrary fixed subterms showing that, inter alia, strongly normalising or typeable terms are asymptotically negligible in the set of all $\lambda$-terms."
"In this paper, we consider stochastic coverage of bounded domains by a diffusing swarm of robots that take local measurements of an underlying scalar field. We introduce three control methodologies with diffusion, advection, and reaction as independent control inputs. We analyze the diffusion-based control strategy using standard operator semigroup-theoretic arguments. We show that the diffusion coefficient can be chosen to be dependent only on the robots' local measurements to ensure that the swarm density converges to a function proportional to the scalar field. The boundedness of the domain precludes the need to impose assumptions on decaying properties of the scalar field at infinity. Moreover, exponential convergence of the swarm density to the equilibrium follows from properties of the spectrum of the semigroup generator. In addition, we use the proposed coverage method to construct a time-inhomogenous diffusion process and apply the observability of the heat equation to reconstruct the scalar field over the entire domain from observations of the robots' random motion over a small subset of the domain. We verify our results through simulations of the coverage scenario on a 2D domain and the field estimation scenario on a 1D domain."
"Conventional approaches to image de-fencing use multiple adjacent frames for segmentation of fences in the reference image and are limited to restoring images of static scenes only. In this paper, we propose a de-fencing algorithm for images of dynamic scenes using an occlusion-aware optical flow method. We divide the problem of image de-fencing into the tasks of automated fence segmentation from a single image, motion estimation under known occlusions and fusion of data from multiple frames of a captured video of the scene. Specifically, we use a pre-trained convolutional neural network to segment fence pixels from a single image. The knowledge of spatial locations of fences is used to subsequently estimate optical flow in the occluded frames of the video for the final data fusion step. We cast the fence removal problem in an optimization framework by modeling the formation of the degraded observations. The inverse problem is solved using fast iterative shrinkage thresholding algorithm (FISTA). Experimental results show the effectiveness of proposed algorithm."
"We present new quantum algorithms for Triangle Finding improving its best previously known quantum query complexities for both dense and spare instances.For dense graphs on $n$ vertices, we get a query complexity of $O(n^{5/4})$ without any of the extra logarithmic factors present in the previous algorithm of Le Gall [FOCS'14]. For sparse graphs with $m\geq n^{5/4}$ edges, we get a query complexity of $O(n^{11/12}m^{1/6}\sqrt{\log n})$, which is better than the one obtained by Le Gall and Nakajima [ISAAC'15] when $m \geq n^{3/2}$. We also obtain an algorithm with query complexity ${O}(n^{5/6}(m\log n)^{1/6}+d_2\sqrt{n})$ where $d_2$ is the variance of the degree distribution. Our algorithms are designed and analyzed in a new model of learning graphs that we call extended learning graphs. In addition, we present a framework in order to easily combine and analyze them. As a consequence we get much simpler algorithms and analyses than previous algorithms of Le Gall {\it et al} based on the MNRS quantum walk framework [SICOMP'11]."
"This paper introduces multiplicative LSTM, a novel hybrid recurrent neural network architecture for sequence modelling that combines the long short-term memory (LSTM) and multiplicative recurrent neural network architectures. Multiplicative LSTM is motivated by its flexibility to have very different recurrent transition functions for each possible input, which we argue helps make it more expressive in autoregressive density estimation. We show empirically that multiplicative LSTM outperforms standard LSTM and its deep variants for a range of character level modelling tasks. We also found that this improvement increases as the complexity of the task scales up. This model achieves a test error of 1.19 bits/character on the last 4 million characters of the Hutter prize dataset when combined with dynamic evaluation."
"The k-regret query aims to return a size-k subset of the entire database such that, for any query user that selects a data object in this size-k subset rather than in the entire database, her regret ratio is minimized. Here, the regret ratio is modeled by the level of difference in the optimality between the optimal object in the size-k subset returned and the optimal object in the entire database. The optimality of a data object in turn is usually modeled by a utility function of the query user. Compared with traditional top-k queries, k-regret queries have the advantage of not requiring users to specify their utility functions. They can discover a size-k subset that minimizes the regret ratio for a whole family of utility functions without knowing any particular of them. Previous studies have answered k-regret queries with additive utility functions such as the linear summation function. However, no existing result has been reported to answer k-regret queries with multiplicative utility functions, which are an important family of utility functions.   In this study, we break the barrier of multiplicative utility functions. We present an algorithm that can produce answers with a bounded regret ratio to k-regret queries with multiplicative utility functions. As a case study we apply this algorithm to process a special type of multiplicative utility functions, the Cobb-Douglas function, and a closely related function, the Constant Elasticity of Substitution function. We perform extensive experiments on the proposed algorithm. The results confirm that the proposed algorithm can answer k-regret queries with multiplicative utility functions efficiently with a constantly small regret ratio."
"Making cities smarter help improve city services and increase citizens' quality of life. Information and communication technologies (ICT) are fundamental for progressing towards smarter city environments. Smart City software platforms potentially support the development and integration of Smart City applications. However, the ICT community must overcome current significant technological and scientific challenges before these platforms can be widely used. This paper surveys the state-of-the-art in software platforms for Smart Cities. We analyzed 23 projects with respect to the most used enabling technologies, as well as functional and non-functional requirements, classifying them into four categories: Cyber-Physical Systems, Internet of Things, Big Data, and Cloud Computing. Based on these results, we derived a reference architecture to guide the development of next-generation software platforms for Smart Cities. Finally, we enumerated the most frequently cited open research challenges, and discussed future opportunities. This survey gives important references for helping application developers, city managers, system operators, end-users, and Smart City researchers to make project, investment, and research decisions."
"Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system."
"Previous attacks that link the sender and receiver of traffic in the Tor network (""correlation attacks"") have generally relied on analyzing traffic from TCP connections. The TCP connections of a typical client application, however, are often accompanied by DNS requests and responses. This additional traffic presents more opportunities for correlation attacks. This paper quantifies how DNS traffic can make Tor users more vulnerable to correlation attacks. We investigate how incorporating DNS traffic can make existing correlation attacks more powerful and how DNS lookups can leak information to third parties about anonymous communication. We (i) develop a method to identify the DNS resolvers of Tor exit relays; (ii) develop a new set of correlation attacks (DefecTor attacks) that incorporate DNS traffic to improve precision; (iii) analyze the Internet-scale effects of these new attacks on Tor users; and (iv) develop improved methods to evaluate correlation attacks. First, we find that there exist adversaries who can mount DefecTor attacks: for example, Google's DNS resolver observes almost 40% of all DNS requests exiting the Tor network. We also find that DNS requests often traverse ASes that the corresponding TCP connections do not transit, enabling additional ASes to gain information about Tor users' traffic. We then show that an adversary who can mount a DefecTor attack can often determine the website that a Tor user is visiting with perfect precision, particularly for less popular websites where the set of DNS names associated with that website may be unique to the site. We also use the Tor Path Simulator (TorPS) in combination with traceroute data from vantage points co-located with Tor exit relays to estimate the power of AS-level adversaries who might mount DefecTor attacks in practice."
"The feature-selection problem is formulated from an information-theoretic perspective. We show that the problem can be efficiently solved by an extension of the recently proposed info-clustering paradigm. This reveals the fundamental duality between feature selection and data clustering,which is a consequence of the more general duality between the principal partition and the principal lattice of partitions in combinatorial optimization."
"In the machine learning problems, the performance measure is used to evaluate the machine learning models. Recently, the number positive data points ranked at the top positions (Pos@Top) has been a popular performance measure in the machine learning community. In this paper, we propose to learn a convolutional neural network (CNN) model to maximize the Pos@Top performance measure. The CNN model is used to represent the multi-instance data point, and a classifier function is used to predict the label from the its CNN representation. We propose to minimize the loss function of Pos@Top over a training set to learn the filters of CNN and the classifier parameter. The classifier parameter vector is solved by the Lagrange multiplier method, and the filters are updated by the gradient descent method alternately in an iterative algorithm. Experiments over benchmark data sets show that the proposed method outperforms the state-of-the-art Pos@Top maximization methods."
"Frequent sequence mining methods often make use of constraints to control which subsequences should be mined. A variety of such subsequence constraints has been studied in the literature, including length, gap, span, regular-expression, and hierarchy constraints. In this paper, we show that many subsequence constraints---including and beyond those considered in the literature---can be unified in a single framework. A unified treatment allows researchers to study jointly many types of subsequence constraints (instead of each one individually) and helps to improve usability of pattern mining systems for practitioners. In more detail, we propose a set of simple and intuitive ""pattern expressions"" to describe subsequence constraints and explore algorithms for efficiently mining frequent subsequences under such general constraints. Our algorithms translate pattern expressions to compressed finite state transducers, which we use as computational model, and simulate these transducers in a way suitable for frequent sequence mining. Our experimental study on real-world datasets indicates that our algorithms---although more general---are competitive to existing state-of-the-art algorithms."
"The detection of road surface and free space remains challenging for non-flat plane, especially with the varying latitudinal and longitudinal slope or in the case of multi-ground plane. In this paper, we propose a framework of the road surface detection with stereo vision. The main contribution of this paper is a newly proposed descriptor which is implemented in disparity image to obtain a disparity feature image. The road regions can be distinguished from their surroundings effectively in the disparity feature image. Because the descriptor is implemented in the local area of the image, it can address well the problem of non-flat plane. And we also present a complete framework to detect the road surface regions base on the disparity feature image with a convolutional neural network architecture."
"We study the theoretical problem of scheduling a set of n independent moldable tasks on m processors, which arises in multiprocessor scheduling from large scale parallel computations. It is assumed that the speedup of executing a task is linear when it is assigned a small set of no more than h processors; its workload increases with the number of assigned processor but its execution time decreases in a linear or nonlinear way when this number is greater than h. The special case where h=1 has been extensively studied to minimize the makespan and the case where h>1 is recently verified by many applications. In this paper, a general procedure is proposed to address the problem in the general case. In particular, we propose an r(h)-approximation algorithm with a complexity of O(n) for the objective of maximizing the sum of values of tasks completed by a deadline, and an (1+\epsilon)/r(h)-approximation algorithm with a complexity of O(nlog(n/\epsilon)) for the objective of minimizing the makespan. For example, r(10)=4/5-max{k-1, 11}/m, and r(27)=7/8-max{k-1, 32.125}/m, where k is the upper bound on parallelized execution (in large scale parallel computations such as cluster computing, m >> k and k/m approaches 0)."
"Is there some absolute $\varepsilon > 0$ such that for any claw-free graph $G$, the chromatic number of the square of $G$ satisfies $\chi(G^2) \le (2-\varepsilon) \omega(G)^2$, where $\omega(G)$ is the clique number of $G$? Erd\H{o}s and Ne\v{s}et\v{r}il asked this question for the specific case of $G$ the line graph of a simple graph and this was answered in the affirmative by Molloy and Reed. We show that the answer to the more general question is also yes, and moreover that it essentially reduces to the original question of Erd\H{o}s and Ne\v{s}et\v{r}il."
"Let $G$ be a claw-free graph on $n$ vertices with clique number $\omega$. We prove the following for the chromatic number $\chi(G^2)$ of the square $G^2$ of $G$. If $\omega\le 3$, then $\chi(G^2)\le 10$. If $\omega \le 4$, then $\chi(G^2) \le 22$. This work is motivated by a strengthened form of a conjecture of Erd\H{o}s and Ne\v{s}et\v{r}il."
"Current large scale implementations of deep learning and data mining require thousands of processors, massive amounts of off-chip memory, and consume gigajoules of energy. Emerging memory technologies such as nanoscale two-terminal resistive switching memory devices offer a compact, scalable and low power alternative that permits on-chip co-located processing and memory in fine-grain distributed parallel architecture. Here we report first use of resistive switching memory devices for implementing and training a Restricted Boltzmann Machine (RBM), a generative probabilistic graphical model as a key component for unsupervised learning in deep networks. We experimentally demonstrate a 45-synapse RBM realized with 90 resistive switching phase change memory (PCM) elements trained with a bio-inspired variant of the Contrastive Divergence (CD) algorithm, implementing Hebbian and anti-Hebbian weight updates. The resistive PCM devices show a two-fold to ten-fold reduction in error rate in a missing pixel pattern completion task trained over 30 epochs, compared to untrained case. Measured programming energy consumption is 6.1 nJ per epoch with the resistive switching PCM devices, a factor of ~150 times lower than conventional processor-memory systems. We analyze and discuss the dependence of learning performance on cycle-to-cycle variations as well as number of gradual levels in the PCM analog memory devices."
"Given a linear system, we consider the expected energy to move from the origin to a uniformly random point on the unit sphere as a function of the set of actuated variables. We show this function is not necessarily supermodular, correcting some claims in the existing literature."
"While traditional multimedia applications such as games and videos are still popular, there has been a significant interest in the recent years towards new 3D media such as 3D immersion and Virtual Reality (VR) applications, especially 360 VR videos. 360 VR video is an immersive spherical video where the user can look around during playback. Unfortunately, 360 VR videos are extremely bandwidth intensive, and therefore are difficult to stream at acceptable quality levels. In this paper, we propose an adaptive bandwidth-efficient 360 VR video streaming system using a divide and conquer approach. In our approach, we propose a dynamic view-aware adaptation technique to tackle the huge streaming bandwidth demands of 360 VR videos. We spatially divide the videos into multiple tiles while encoding and packaging, use MPEG-DASH SRD to describe the spatial relationship of tiles in the 360-degree space, and prioritize the tiles in the Field of View (FoV). In order to describe such tiled representations, we extend MPEG-DASH SRD to the 3D space of 360 VR videos. We spatially partition the underlying 3D mesh, and construct an efficient 3D geometry mesh called hexaface sphere to optimally represent a tiled 360 VR video in the 3D space. Our initial evaluation results report up to 72% bandwidth savings on 360 VR video streaming with minor negative quality impacts compared to the baseline scenario when no adaptations is applied."
"We introduce the subset assignment problem in which items of varying sizes are placed in a set of bins with limited capacity. Items can be replicated and placed in any subset of the bins. Each (item, subset) pair has an associated cost. Not assigning an item to any of the bins is not free in general and can potentially be the most expensive option. The goal is to minimize the total cost of assigning items to subsets without exceeding the bin capacities. This problem is motivated by the design of caching systems composed of banks of memory with varying cost/performance specifications. The ability to replicate a data item in more than one memory bank can benefit the overall performance of the system with a faster recovery time in the event of a memory failure. For this setting, the number $n$ of data objects (items) is very large and the number $d$ of memory banks (bins) is a small constant (on the order of $3$ or $4$). Therefore, the goal is to determine an optimal assignment in time that minimizes dependence on $n$. The integral version of this problem is NP-hard since it is a generalization of the knapsack problem. We focus on an efficient solution to the LP relaxation as the number of fractionally assigned items will be at most $d$. If the data objects are small with respect to the size of the memory banks, the effect of excluding the fractionally assigned data items from the cache will be small. We give an algorithm that solves the LP relaxation and runs in time $O({3^d \choose d+1} \text{poly}(d) n \log(n) \log(nC) \log(Z))$, where $Z$ is the maximum item size and $C$ the maximum storage cost."
"We address the problem of integrating textual and visual information in vector space models for word meaning representation. We first present the Residual CCA (R-CCA) method, that complements the standard CCA method by representing, for each modality, the difference between the original signal and the signal projected to the shared, max correlation, space. We then show that constructing visual and textual representations and then post-processing them through composition of common modeling motifs such as PCA, CCA, R-CCA and linear interpolation (a.k.a sequential modeling) yields high quality models. On five standard semantic benchmarks our sequential models outperform recent multimodal representation learning alternatives, including ones that rely on joint representation learning. For two of these benchmarks our R-CCA method is part of the Best configuration our algorithm yields."
"Deep Learning is considered to be a quite young in the area of machine learning research, found its effectiveness in dealing complex yet high dimensional dataset that includes but limited to images, text and speech etc. with multiple levels of representation and abstraction. As there are a plethora of research on these datasets by various researchers , a win over them needs lots of attention. Careful setting of Deep learning parameters is of paramount importance in order to avoid the overfitting unlike conventional methods with limited parameter settings. Deep Convolutional neural network (DCNN) with multiple layers of compositions and appropriate settings might be is an efficient machine learning method that can outperform the conventional methods in a great way. However, due to its slow adoption in learning, there are also always a chance of overfitting during feature selection process, which can be addressed by employing a regularization method called dropout. Fast Random Forest (FRF) is a powerful ensemble classifier especially when the datasets are noisy and when the number of attributes is large in comparison to the number of instances, as is the case of Bioinformatics datasets. Several publicly available Bioinformatics dataset, Handwritten digits recognition and Image segmentation dataset are considered for evaluation of the proposed approach. The excellent performance obtained by the proposed DCNN based feature selection with FRF classifier on high dimensional datasets makes it a fast and accurate classifier in comparison the state-of-the-art."
We propose a new approach for the statistical comparison of algorithms which have been cross-validated on multiple data sets. It is a Bayesian hierarchical method; it draws inferences on single and on multiple datasets taking into account the mean and the variability of the cross-validation results. It is able to detect equivalent classifiers and to claim significances which have a practical impact. On each data sets it estimates more accurately than the existing methods the difference of accuracy between the two classifiers thanks to shrinkage. Such advantages are demonstrated by simulations on synthetic and real data.
"We present a structural clustering algorithm for large-scale datasets of small labeled graphs, utilizing a frequent subgraph sampling strategy. A set of representatives provides an intuitive description of each cluster, supports the clustering process, and helps to interpret the clustering results. The projection-based nature of the clustering approach allows us to bypass dimensionality and feature extraction problems that arise in the context of graph datasets reduced to pairwise distances or feature vectors. While achieving high quality and (human) interpretable clusterings, the runtime of the algorithm only grows linearly with the number of graphs. Furthermore, the approach is easy to parallelize and therefore suitable for very large datasets. Our extensive experimental evaluation on synthetic and real world datasets demonstrates the superiority of our approach over existing structural and subspace clustering algorithms, both, from a runtime and quality point of view."
"Rumour stance classification, the task that determines if each tweet in a collection discussing a rumour is supporting, denying, questioning or simply commenting on the rumour, has been attracting substantial interest. Here we introduce a novel approach that makes use of the sequence of transitions observed in tree-structured conversation threads in Twitter. The conversation threads are formed by harvesting users' replies to one another, which results in a nested tree-like structure. Previous work addressing the stance classification task has treated each tweet as a separate unit. Here we analyse tweets by virtue of their position in a sequence and test two sequential classifiers, Linear-Chain CRF and Tree CRF, each of which makes different assumptions about the conversational structure. We experiment with eight Twitter datasets, collected during breaking news, and show that exploiting the sequential structure of Twitter conversations achieves significant improvements over the non-sequential methods. Our work is the first to model Twitter conversations as a tree structure in this manner, introducing a novel way of tackling NLP tasks on Twitter conversations."
"We propose a novel online pricing mechanism for electric vehicle (EV) charging. The charging station decides prices for each arriving EV depending on the energy and the time within which the EV will be served. The user selects either one of the contracts by paying the prescribed price or rejects all of those depending on their surpluses. The charging station can serve users using renewable energy or conventional energy. Users may select longer deadlines as they may have to pay less because of the less amount of conventional energy, however, they have to wait a longer period. We consider a myopic charging station and show that there exists a pricing mechanism which jointly maximizes the social welfare and the profit of the charging station when the charging station knows the utilities of the users. However, when the charging station does not know the utilities of the users, the social welfare pricing strategy may not maximize the expected profit of the charging station and even the profit may be $0$. We propose a fixed profit pricing strategy which provides a guaranteed fixed profit to the charging station when it is unaware of the utilities of the users. Numerically, we show that how the charging station can select a profit margin to trade-off between profit and the users' surpluses. We also show empirically that since our proposed mechanism also controls the deadline of the vehicles compared to the existing pricing mechanisms, hence, the number of charging spots required can be lower and a greater efficiency can be achieved."
"In this paper, we consider the problem of attack identification in cyber-physical systems (CPS). Attack identification is often critical for the recovery and performance of a CPS that is targeted by malicious entities, allowing defenders to construct algorithms which bypass harmful nodes. Previous work has characterized limitations in the perfect identification of adversarial attacks on deterministic LTI systems. For instance, a system must remain observable after removing any 2q sensors to only identify q attacks. However, the ability for an attacker to create an unidentifiable attack requires knowledge of the system model. In this paper, we aim to limit the adversary's knowledge of the system model with the goal of accurately identifying all sensor attacks. Such a scheme will allow systems to withstand larger attacks or system operators to allocate fewer sensing devices to a control system while maintaining security. We explore how changing the dynamics of the system as a function of time allows us to actively identify malicious/faulty sensors in a control system. We discuss the design of time varying system matrices to meet this goal and evaluate performance in deterministic and stochastic systems."
"By Brook's Theorem, every n-vertex graph of maximum degree at most Delta >= 3 and clique number at most Delta is Delta-colorable, and thus it has an independent set of size at least n/Delta. We give an approximate characterization of graphs with independence number close to this bound, and use it to show that the problem of deciding whether such a graph has an indepdendent set of size at least n/Delta+k has a kernel of size O(k)."
"Computed tomography (CT) generates a stack of cross-sectional images covering a region of the body. The visual assessment of these images for the identification of potential abnormalities is a challenging and time consuming task due to the large amount of information that needs to be processed. In this article we propose a deep artificial neural network architecture, ReCTnet, for the fully-automated detection of pulmonary nodules in CT scans. The architecture learns to distinguish nodules and normal structures at the pixel level and generates three-dimensional probability maps highlighting areas that are likely to harbour the objects of interest. Convolutional and recurrent layers are combined to learn expressive image representations exploiting the spatial dependencies across axial slices. We demonstrate that leveraging intra-slice dependencies substantially increases the sensitivity to detect pulmonary nodules without inflating the false positive rate. On the publicly available LIDC/IDRI dataset consisting of 1,018 annotated CT scans, ReCTnet reaches a detection sensitivity of 90.5% with an average of 4.5 false positives per scan. Comparisons with a competing multi-channel convolutional neural network for multi-slice segmentation and other published methodologies using the same dataset provide evidence that ReCTnet offers significant performance gains."
"Recurrent Neural Networks have achieved state-of-the-art results for many problems in NLP and two most popular RNN architectures are Tail Model and Pooling Model. In this paper, a hybrid architecture is proposed and we present the first empirical study using LSTMs to compare performance of the three RNN structures on sentence classification task. Experimental results show that the Max Pooling Model or Hybrid Max Pooling Model achieves the best performance on most datasets, while Tail Model does not outperform other models."
"Web application providers have been migrating their applications to cloud data centers, attracted by the emerging cloud computing paradigm. One of the appealing features of cloud is elasticity. It allows cloud users to acquire or release computing resources on demand, which enables web application providers to auto-scale the resources provisioned to their applications under dynamic workload in order to minimize resource cost while satisfying Quality of Service (QoS) requirements. In this paper, we comprehensively analyze the challenges remain in auto-scaling web applications in clouds and review the developments in this field. We present a taxonomy of auto-scaling systems according to the identified challenges and key properties. We analyze the surveyed works and map them to the taxonomy to identify the weakness in this field. Moreover, based on the analysis, we propose new future directions."
"With the rapid growth of social media, rumors are also spreading widely on social media and bring harm to people's daily life. Nowadays, information credibility evaluation has drawn attention from academic and industrial communities. Current methods mainly focus on feature engineering and achieve some success. However, feature engineering based methods require a lot of labor and cannot fully reveal the underlying relations among data. In our viewpoint, the key elements of user behaviors for evaluating credibility are concluded as ""who"", ""what"", ""when"", and ""how"". These existing methods cannot model the correlation among different key elements during the spreading of microblogs. In this paper, we propose a novel representation learning method, Information Credibility Evaluation (ICE), to learn representations of information credibility on social media. In ICE, latent representations are learnt for modeling user credibility, behavior types, temporal properties, and comment attitudes. The aggregation of these factors in the microblog spreading process yields the representation of a user's behavior, and the aggregation of these dynamic representations generates the credibility representation of an event spreading on social media. Moreover, a pairwise learning method is applied to maximize the credibility difference between rumors and non-rumors. To evaluate the performance of ICE, we conduct experiments on a Sina Weibo data set, and the experimental results show that our ICE model outperforms the state-of-the-art methods."
"This paper presents a method of estimating the geometry of a room and the 3D pose of objects from a single 360-degree panorama image. Assuming Manhattan World geometry, we formulate the task as a Bayesian inference problem in which we estimate positions and orientations of walls and objects. The method combines surface normal estimation, 2D object detection and 3D object pose estimation. Quantitative results are presented on a dataset of synthetically generated 3D rooms containing objects, as well as on a subset of hand-labeled images from the public SUN360 dataset."
"We propose a ""formula slicing"" method for finding inductive invariants. It is based on the observation that many loops in the program affect only a small part of the memory, and many invariants which were valid before a loop are still valid after.   Given a precondition of the loop, obtained from the preceding program fragment, we weaken it until it becomes inductive. The weakening procedure is guided by counterexamples-to-induction given by an SMT solver. Our algorithm applies to programs with arbitrary loop structure, and it computes the strongest invariant in an abstract domain of weakenings of preconditions. We call this algorithm ""formula slicing"", as it effectively performs ""slicing"" on formulas derived from symbolic execution.   We evaluate our algorithm on the device driver benchmarks from the International Competition on Software Verification (SV-COMP), and we show that it is competitive with the state-of-the-art verification techniques."
"The matching problem is a notorious combinatorial optimization problem that has attracted for many years the attention of the statistical physics community. Here we analyze the Euclidean version of the problem, i.e. the optimal matching problem between points randomly distributed on a $d$-dimensional Euclidean space, where the cost to minimize depends on the points' pairwise distances. Using Mayer's cluster expansion we write a formal expression for the replicated action that is suitable for a saddle point computation. We give the diagrammatic rules for each term of the expansion, and we analyze in detail the one-loop diagrams. A characteristic feature of the theory, when diagrams are perturbatively computed around the mean field part of the action, is the vanishing of the mass at zero momentum. In the non-Euclidean case of uncorrelated costs instead, we predict and numerically verify an anomalous scaling for the sub-sub-leading correction to the asymptotic average cost."
"The decoding performance of polar codes strongly depends on the decoding algorithm used, while also the decoder throughput and its latency mainly depend on the decoding algorithm. In this work, we implement the powerful successive cancellation list (SCL) decoder on a GPU and identify the bottlenecks of this algorithm with respect to parallel computing and its difficulties. The inherent serial decoding property of the SCL algorithm naturally limits the achievable speed-up gains on GPUs when compared to CPU implementations. In order to increase the decoding throughput, we use a hybrid decoding scheme based on the belief propagation (BP) decoder, which can be intra and inter-frame parallelized. The proposed scheme combines excellent decoding performance and high throughput within the signal-to-noise ratio (SNR) region of interest."
"Statistical characterization of the signal-to-interference-plus-noise ratio (SINR) via its cumulative distribution function (CDF) is ubiquitous in a vast majority of technical contributions in the area of cellular networks since it boils down to averaging the Laplace transform of the aggregate interference, a benefit accorded at the expense of confinement to the simplistic Rayleigh fading. In this work, to capture diverse fading channels that appear in realistic outdoor/indoor wireless communication scenarios, we tackle the problem differently. By exploting the moment generating function (MGF) of the SINR, we succeed in analytically assessing cellular networks performance over the shadowed {\kappa}-{\mu}, {\kappa}-{\mu} and {\eta}-{\mu} fading models. The latter offer high flexibility by capturing diverse fading channels including Rayleigh, Nakagami-m, Rician, and Rician shadow fading distributions. These channel models have been recently praised for their capability to accurately model dense urban environments, future femtocells and device-to-device (D2D) shadowed channels. In addition to unifying the analysis for different channel models, this work integrates, for the first time, the coverage, the achievable rate, and the bit error probability (BEP), which are largely treated separately in the literature. The developed model and analysis are validated over a broad range of simulation setups and parameters."
"In social networks the {\sc Strong Triadic Closure} is an assignment of the edges with strong or weak labels such that any two vertices that have a common neighbor with a strong edge are adjacent. The problem of maximizing the number of strong edges that satisfy the strong triadic closure was recently shown to be NP-complete for general graphs. Here we initiate the study of graph classes for which the problem is solvable. We show that the problem admits a polynomial-time algorithm for two unrelated classes of graphs: proper interval graphs and trivially-perfect graphs. To complement our result, we show that the problem remains NP-complete on split graphs, and consequently also on chordal graphs. Thus we contribute to define the first border between graph classes on which the problem is polynomially solvable and on which it remains NP-complete."
"Robot warehouse automation has attracted significant interest in recent years, perhaps most visibly in the Amazon Picking Challenge (APC). A fully autonomous warehouse pick-and-place system requires robust vision that reliably recognizes and locates objects amid cluttered environments, self-occlusions, sensor noise, and a large variety of objects. In this paper we present an approach that leverages multi-view RGB-D data and self-supervised, data-driven learning to overcome those difficulties. The approach was part of the MIT-Princeton Team system that took 3rd- and 4th- place in the stowing and picking tasks, respectively at APC 2016. In the proposed approach, we segment and label multiple views of a scene with a fully convolutional neural network, and then fit pre-scanned 3D object models to the resulting segmentation to get the 6D object pose. Training a deep neural network for segmentation typically requires a large amount of training data. We propose a self-supervised method to generate a large labeled dataset without tedious manual segmentation. We demonstrate that our system can reliably estimate the 6D pose of objects under a variety of scenarios. All code, data, and benchmarks are available at http://www.andyzeng.com/apc2016"
The statistical leverage scores of a complex matrix $A\in\mathbb{C}^{n\times d}$ record the degree of alignment between col$(A)$ and the coordinate axes in $\mathbb{C}^n$. These score are used in random sampling algorithms for solving certain numerical linear algebra problems. In this paper we present a max-plus algebraic analogue for statistical leverage scores. We show that max-plus statistical leverage scores can be used to calculate the exact asymptotic behavior of the conventional statistical leverage scores of a generic matrices of Puiseux series and also provide a novel way to approximate the conventional statistical leverage scores of a fixed or complex matrix. The advantage of approximating a complex matrices scores with max-plus scores is that the max-plus scores can be computed very quickly. This approximation is typically accurate to within an order or magnitude and should be useful in practical problems where the true scores are known to vary widely.
"Recently, the problem of local minima in very high dimensional non-convex optimization has been challenged and the problem of saddle points has been introduced. This paper introduces a dynamic type of normalization that forces the system to escape saddle points. Unlike other saddle point escaping algorithms, second order information is not utilized, and the system can be trained with an arbitrary gradient descent learner. The system drastically improves learning in a range of deep neural networks on various data-sets in comparison to non-CPN neural networks."
"This paper addresses the structurally-constrained sparse decomposition of multi-dimensional signals onto overcomplete families of vectors, called dictionaries. The contribution of the paper is threefold. Firstly, a generic spatio-temporal regularization term is designed and used together with the standard $\ell_1$ regularization term to enforce a sparse decomposition preserving the spatio-temporal structure of the signal. Secondly, an optimization algorithm based on the split Bregman approach is proposed to handle the associated optimization problem, and its convergence is analyzed. Our well-founded approach yields same accuracy as the other algorithms at the state-of-the-art, with significant gains in terms of convergence speed. Thirdly, the empirical validation of the approach on artificial and real-world problems demonstrates the generality and effectiveness of the method. On artificial problems, the proposed regularization subsumes the Total Variation minimization and recovers the expected decomposition. On the real-world problem of electro-encephalography brainwave decomposition, the approach outperforms similar approaches in terms of P300 evoked potentials detection, using structured spatial priors to guide the decomposition."
"We consider model social networks in which information propagates directionally across layers of rational agents. Each agent makes a locally optimal estimate of the state of the world, and communicates this estimate to agents downstream. When agents receive information from the same source their estimates are correlated. We show that the resulting redundancy can lead to the loss of information about the state of the world across layers of the network, even when all agents have full knowledge of the network's structure. A simple algebraic condition identifies networks in which information loss occurs, and we show that all such networks must contain a particular network motif. We also study random networks asymptotically as the number of agents increases, and find a sharp transition in the probability of information loss at the point at which the number of agents in one layer exceeds the number in the previous layer."
"This paper aims to develop new and fast algorithms for recovering a sparse vector from a small number of measurements, which is a fundamental problem in the field of compressive sensing (CS). Currently, CS favors incoherent systems, in which any two measurements are as little correlated as possible. In reality, however, many problems are coherent, and conventional methods such as $L_1$ minimization do not work well. Recently, the difference of the $L_1$ and $L_2$ norms, denoted as $L_1$-$L_2$, is shown to have superior performance over the classic $L_1$ method, but it is computationally expensive. We derive an analytical solution for the proximal operator of the $L_1$-$L_2$ metric, and it makes some fast $L_1$ solvers such as forward-backward splitting (FBS) and alternative direction method of multipliers (ADMM) applicable for $L_1$-$L_2$. We describe in details how to incorporate the proximal operator into FBS and ADMM and show that the resulting algorithms are convergent under mild conditions. Both algorithms are shown to be much more efficient than the original implementation of $L_1$-$L_2$ based on a difference-of-convex approach in the numerical experiments."
First we study some properties of the modular group algebra $\mathbb{F}_{p^r}[G]$ where $G$ is the additive group of a Galois ring of characteristic $p^r$ and $\mathbb{F}_{p^r}$ is the field of $p^r$ elements. Secondly a description of the Generalized Reed-Muller codes over $\mathbb{F}_{p^r}$ in $\mathbb{F}_{p^r}[G]$ is presented.
"The advantages of using communication networks to interconnect controllers and physical plants motivate the increasing number of Networked Control Systems, in industrial and critical infrastructure facilities. However, this integration also exposes such control systems to new threats, typical of the cyber domain. In this context, studies have been conduced, aiming to explore vulnerabilities and propose security solutions for cyber-physical systems. In this paper, it is proposed a covert attack for service degradation, which is planned based on the intelligence gathered by another attack, herein proposed, referred as System Identification attack. The simulation results demonstrate that the joint operation of the two attacks is capable to affect, in a covert and accurate way, the physical behavior of a system."
"Knowledge Management (KM) is a relatively new phenomenon that appears in the field of Public Sector Organizations (PSO) bringing new paradigms of organizational management, challenges, risks and opportunities for its implementation, development and evaluation. KM can be seen as a systematic and deliberate effort to coordinate people, technology, organizational structures and its environment through knowledge reuse and innovation. This management approach has been established in parallel with the development and use of information and communications technologies (ICT). Nowadays more PSO are embodying KM practices in their core processes for support them, and as an advanced management strategy to create a new culture based on technology and resources efficiency. In this paper, we observed that KM can support organizational goals in PSO. The aim of this paper is to understand KM factors and its associated components, and propose KM metrics for measure KM programs in PSO. Through a critical literature review we analysed diverse studies related with KM performance indicators in PSO, then based on previous works we summarized the more convenient this purpose. We found that, in academic literature, studies about KM measurement in PSO are uncommon and emerging. As well, in the last section of this paper, we present a proposal of KM metrics for PSO, and some recommendations and practical implications for KM metrics development in PSO. This academic endeavour seeks to contribute to theoretical debate about KM measure development for KM initiatives in PSO."
"We present a new method for identifying the latent categorization of items based on their rankings. Complimenting a recent work that uses a Dirichlet prior on preference vectors and variational inference, we show that this problem can be effectively dealt with using existing community detection algorithms, with the communities corresponding to item categories. In particular we convert the bipartite ranking data to a unipartite graph of item affinities, and apply community detection algorithms. In this context we modify an existing algorithm - namely the label propagation algorithm to a variant that uses the distance between the nodes for weighting the label propagation - to identify the categories. We propose and analyze a synthetic ordinal ranking model and show its relation to the recently much studied stochastic block model. We test our algorithms on synthetic data and compare performance with several popular community detection algorithms. We also test the method on real data sets of movie categorization from the Movie Lens database. In all of the cases our algorithm is able to identify the categories for a suitable choice of tuning parameter."
"This paper describes our submission to the 1st 3D Face Alignment in the Wild (3DFAW) Challenge. Our method builds upon the idea of convolutional part heatmap regression [1], extending it for 3D face alignment. Our method decomposes the problem into two parts: (a) X,Y (2D) estimation and (b) Z (depth) estimation. At the first stage, our method estimates the X,Y coordinates of the facial landmarks by producing a set of 2D heatmaps, one for each landmark, using convolutional part heatmap regression. Then, these heatmaps, alongside the input RGB image, are used as input to a very deep subnetwork trained via residual learning for regressing the Z coordinate. Our method ranked 1st in the 3DFAW Challenge, surpassing the second best result by more than 22%."
"This paper proposes models of learning process in teams of individuals who collectively execute a sequence of tasks and whose actions are determined by individual skill levels and networks of interpersonal appraisals and influence. The closely-related proposed models have increasing complexity, starting with a centralized manager-based assignment and learning model, and finishing with a social model of interpersonal appraisal, assignments, learning, and influences. We show how rational optimal behavior arises along the task sequence for each model, and discuss conditions of suboptimality. Our models are grounded in replicator dynamics from evolutionary games, influence networks from mathematical sociology, and transactive memory systems from organization science."
"In this paper we propose a class of propagation models for multiple competing products over a social network. We consider two propagation mechanisms: social conversion and self conversion, corresponding, respectively, to endogenous and exogenous factors. A novel concept, the product-conversion graph, is proposed to characterize the interplay among competing products. According to the chronological order of social and self conversions, we develop two Markov-chain models and, based on the independence approximation, we approximate them with two respective difference equations systems.   Theoretical analysis on these two approximation models reveals the dependency of the systems' asymptotic behavior on the structures of both the product-conversion graph and the social network, as well as the initial condition. In addition to the theoretical work, accuracy of the independence approximation and the asymptotic behavior of the Markov-chain model are investigated via numerical analysis, for the case where social conversion occurs before self conversion.   Finally, we propose a class of multi-player and multi-stage competitive propagation games and discuss the seeding-quality trade-off, as well as the allocation of seeding resources among the individuals. We investigate the unique Nash equilibrium at each stage and analyze the system's behavior when every player is adopting the policy at the Nash equilibrium."
"Dasgupta recently introduced a cost function for the hierarchical clustering of a set of points given pairwise similarities between them. He showed that this function is NP-hard to optimize, but a top-down recursive partitioning heuristic based on an alpha_n-approximation algorithm for uniform sparsest cut gives an approximation of O(alpha_n log n) (the current best algorithm has alpha_n=O(sqrt{log n})). We show that the aforementioned sparsest cut heuristic in fact obtains an O(alpha_n)-approximation for hierarchical clustering. The algorithm also applies to a generalized cost function studied by Dasgupta. Moreover, we obtain a strong inapproximability result, showing that the hierarchical clustering objective is hard to approximate to within any constant factor assuming the Small-Set Expansion (SSE) Hypothesis. Finally, we discuss approximation algorithms based on convex relaxations. We present a spreading metric SDP relaxation for the problem and show that it has integrality gap at most O(sqrt{log n}). The advantage of the SDP relative to the sparsest cut heuristic is that it provides an explicit lower bound on the optimal solution and could potentially yield an even better approximation for hierarchical clustering. In fact our analysis of this SDP served as the inspiration for our improved analysis of the sparsest cut heuristic. We also show that a spreading metric LP relaxation gives an O(log n)-approximation."
"Neural encoder-decoder models have shown great success in many sequence generation tasks. However, previous work has not investigated situations in which we would like to control the length of encoder-decoder outputs. This capability is crucial for applications such as text summarization, in which we have to generate concise summaries with a desired length. In this paper, we propose methods for controlling the output sequence length for neural encoder-decoder models: two decoding-based methods and two learning-based methods. Results show that our learning-based methods have the capability to control length without degrading summary quality in a summarization task."
"Mean-squared-error (MSE) is one of the most widely used performance metrics for the designs and analysis of multi-input-multiple-output (MIMO) communications. Weighted MSE minimization, a more general formulation of MSE minimization, plays an important role in MIMO transceiver optimization. While this topic has a long history and has been extensively studied, existing treatments on the methods in solving the weighted MSE optimization are more or less sporadic and non-systematic. In this paper, we firstly review the two major methodologies, Lagrange multiplier method and majorization theory based method, and their common procedures in solving the weighted MSE minimization. Then some problems and limitations of the methods that were usually neglected or glossed over in existing literature are provided. These problems are fundamental and of critical importance for the corresponding MIMO transceiver optimizations. In addition, a new extended matrix-field weighted MSE model is proposed. Its solutions and applications are discussed in details. Compared with existing models, this new model has wider applications, e.g., nonlinear MIMO transceiver designs and capacity-maximization transceiver designs for general MIMO networks."
"In areas such as computer software and hardware, manufacturing systems, and transportation, engineers encounter networks with arbitrarily large numbers of isomorphic subprocesses. Parameterized systems provide a framework for modeling such networks. The analysis of parameterized systems is a challenge as some key properties such as nonblocking and deadlock-freedom are undecidable even for the case of a parameterized system with ring topology. In this paper, we introduce \textit{Parameterized-Chain Networks} (PCN) for modeling of networks containing several linear parameterized segments. Since deadlock analysis is undecidable, to achieve a tractable subproblem we limit the behavior of subprocesses of the network using our previously developed mathematical notion `weak invariant simulation.' We develop a dependency graph for analysis of PCN and show that partial and total deadlocks of the proposed PCN are characterized by full, consistent subgraphs of the dependency graph. We investigate deadlock in a traffic network as an illustrative example. This document contains all the details and proofs of the study."
"Distributed Denial of Service (DDoS) is a classic type of Cybercrime and can still strongly damage company reputation and increase costs. Attackers have continuously improved their strategies, and the amount of unleashed communication requests has doubled in volume, size and frequency. This has occurred through different isolated hosts, leading them to resource exhaustion. Previous studies have concentrated efforts in detecting or mitigating ongoing DDoS attacks. However, addressing DDoS when it is already in place may be too late. In this article, we attract the attention for the crucial role and importance of the early prediction of attack trends in order to support network resilience. We suggest the use of statistical and non-parametric leading indicators for identifying trends of volumetric DDoS attacks and we report promising results over real dataset from CAIDA."
"We present a proof of the conjecture $\mathcal{NP}$ = $\mathcal{PSPACE}$ by showing that arbitrary tautologies of Johansson's minimal propositional logic admit ""small"" polynomial-size dag-like natural deductions in Prawitz's system for minimal propositional logic. These ""small"" deductions arise from standard ""large""\ tree-like inputs by horizontal dag-like compression that is obtained by merging distinct nodes labeled with identical formulas occurring in horizontal sections of deductions involved. The underlying ""geometric"" idea: if the height, $h\left( \partial \right) $ , and the total number of distinct formulas, $\phi \left( \partial \right) $ , of a given tree-like deduction $\partial$ of a minimal tautology $\rho$ are both polynomial in the length of $\rho$, $\left| \rho \right|$, then the size of the horizontal dag-like compression is at most $h\left( \partial \right) \times \phi \left( \partial \right) $, and hence polynomial in $\left| \rho \right|$. The attached proof is due to the first author, but it was the second author who proposed an initial idea to attack a weaker conjecture $\mathcal{NP}= \mathcal{\mathit{co}NP}$ by reductions in diverse natural deduction formalisms for propositional logic. That idea included interactive use of minimal, intuitionistic and classical formalisms, so its practical implementation was too involved. The attached proof of $ \mathcal{NP}=\mathcal{PSPACE}$ runs inside the natural deduction interpretation of Hudelmaier's cutfree sequent calculus for minimal logic."
"Many real-world machine learning applications involve several learning tasks which are inter-related. For example, in healthcare domain, we need to learn a predictive model of a certain disease for many hospitals. The models for each hospital may be different because of the inherent differences in the distributions of the patient populations. However, the models are also closely related because of the nature of the learning tasks modeling the same disease. By simultaneously learning all the tasks, multi-task learning (MTL) paradigm performs inductive knowledge transfer among tasks to improve the generalization performance. When datasets for the learning tasks are stored at different locations, it may not always be feasible to transfer the data to provide a data-centralized computing environment due to various practical issues such as high data volume and privacy. In this paper, we propose a principled MTL framework for distributed and asynchronous optimization to address the aforementioned challenges. In our framework, gradient update does not wait for collecting the gradient information from all the tasks. Therefore, the proposed method is very efficient when the communication delay is too high for some task nodes. We show that many regularized MTL formulations can benefit from this framework, including the low-rank MTL for shared subspace learning. Empirical studies on both synthetic and real-world datasets demonstrate the efficiency and effectiveness of the proposed framework."
"We study the spread of discrete-time epidemics over arbitrary networks for well-known propagation models, namely SIS (susceptible-infected-susceptible), SIR (susceptible-infected-recovered), SIRS (susceptible-infected-recovered-susceptible) and SIV (susceptible-infected-vaccinated). Such epidemics are described by $2^n$- or $3^n$-state Markov chains. Ostensibly, because analyzing such Markov chains is too complicated, their $O(n)$-dimensional nonlinear ""mean-field"" approximation, and its linearization, are often studied instead. We provide a complete global analysis of the epidemic dynamics of the nonlinear mean-field approximation. In particular, we show that depending on the largest eigenvalue of the underlying graph adjacency matrix and the rates of infection, recovery, and vaccination, the global dynamics takes on one of two forms: either the epidemic dies out, or it converges to another unique fixed point (the so-called endemic state where a constant fraction of the nodes remain infected). A similar result has also been shown in the continuous-time case. We tie in these results with the ""true"" underlying Markov chain model by showing that the linear model is the tightest upper-bound on the true probabilities of infection that involves only marginals, and that, even though the nonlinear model is not an upper-bound on the true probabilities in general, it does provide an upper-bound on the probability of the chain not being absorbed. As a consequence, we also show that when the disease-free fixed point is globally stable for the mean-field model, the Markov chain has an $O(\log n)$ mixing time, which means the epidemic dies out quickly. We compare and summarize the results on different propagation models."
"We propose an in-depth study of lending behaviors in Kiva using a mix of quantitative and large-scale data mining techniques. Kiva is a non-profit organization that offers an online platform to connect lenders with borrowers. Their site, kiva.org, allows citizens to microlend small amounts of money to entrepreneurs (borrowers) from different countries. The borrowers are always affiliated with a Field Partner (FP) which can be a microfinance institution (MFI) or other type of local organization that has partnered with Kiva. Field partners give loans to selected businesses based on their local knowledge regarding the country, the business sector including agriculture, health or manufacture among others, and the borrower.Our objective is to understand the relationship between lending activity and various features offered by the online platform. Specifically, we focus on two research questions: (i) the role that MFI ratings play in driving lending activity and (ii) the role that various loan features have in the lending behavior. The first question analyzes whether there exists a relationship between the MFI ratings - that lenders can explore online - and their lending volumes. The second research question attempts to understand if certain loan features - available online at Kiva - such as the type of small business, the gender of the borrower, or the loan's country information might affect the way lenders lend."
"While the reversibility of multidimensional cellular automata is undecidable and there exists a criterion for determining if a multidimensional linear cellular automaton is reversible, there are only a few results about the reversibility problem of multidimensional linear cellular automata under boundary conditions. This work proposes a criterion for testing the reversibility of a multidimensional linear cellular automaton under null boundary condition and an algorithm for the computation of its reverse, if it exists. The investigation of the dynamical behavior of a multidimensional linear cellular automaton under null boundary condition is equivalent to elucidating the properties of block Toeplitz matrix. The proposed criterion significantly reduce the computational cost whenever the number of cells or the dimension is large; the discussion can also apply to cellular automata under periodic boundary condition with a minor modification."
"Motor imagery (MI) is a mental representation of motor behavior that has been widely used as a control method for a brain-computer interface (BCI), allowing communication for the physically impaired. The performance of MI based BCI mainly depends on the subject's ability to self-modulate EEG signals. Proper training can help naive subjects learn to modulate brain activity proficiently. However, training subjects typically involves abstract motor tasks and is time-consuming. To improve the performance of naive subjects during motor imagery, a novel paradigm was presented that would guide naive subjects to modulate brain activity effectively. In this new paradigm, pictures of the left or right hand were used as cues for subjects to finish the motor imagery task. Fourteen healthy subjects (11 male, aged 22-25 years, mean 23.6+/-1.16) participated in this study. The task was to imagine writing a Chinese character. Specifically, subjects could imagine hand movements following the sequence of writing strokes in the Chinese character. This paradigm was meant to find an effective and familiar action for most Chinese people, to provide them with a specific, extensively practiced task and help them modulate brain activity. Results showed that the writing task paradigm yielded significantly better performance than the traditional arrow paradigm (p<0.001). Questionnaire replies indicated that most subjects thought the new paradigm was easier and more comfortable. The proposed new motor imagery paradigm could guide subjects to help them modulate brain activity effectively. Results showed that there were significant improvements using new paradigm, both in classification accuracy and usability."
"This paper discusses lexicon word learning in high-dimensional meaning spaces from the viewpoint of referential uncertainty. We investigate various state-of-the-art Machine Learning algorithms and discuss the impact of scaling, representation and meaning space structure. We demonstrate that current Machine Learning techniques successfully deal with high-dimensional meaning spaces. In particular, we show that exponentially increasing dimensions linearly impact learner performance and that referential uncertainty from word sensitivity has no impact."
"""People want an authority to tell them how to value things. But they chose this authority not based on facts or results. They chose it because it seems authoritative and familiar."" - The Big Short   The pavement condition index is one such a familiar measure used by many US cities to measure street quality and justify billions of dollars spent every year on street repair. These billion-dollar decisions are based on evaluation criteria that are subjective and not representative. In this paper, we build upon our initial submission to D4GX 2015 that approaches this problem of information asymmetry in municipal decision-making.   We describe a process to identify street-defects using computer vision techniques on data collected using the Street Quality Identification Device (SQUID). A User Interface to host a large quantity of image data towards digitizing the street inspection process and enabling actionable intelligence for a core public service is also described. This approach of combining device, data and decision-making around street repair enables cities make targeted decisions about street repair and could lead to an anticipatory response which can result in significant cost savings. Lastly, we share lessons learnt from the deployment of SQUID in the city of Syracuse, NY."
"In this paper, we derive simple closed-form expressions for the $n$-queens problem and three related problems in terms of permanents of $(0,1)$ matrices. These formulas are the first of their kind. Moreover, they provide the first method for solving these problems with polynomial space that has a nontrivial time complexity bound. We then show how a closed-form for the number of Latin squares of order $n$ follows from our method. Finally, we prove lower bounds. In particular, we show that the permanent of Schur's complex valued matrix is a lower bound for the toroidal semi-queens problem, or equivalently, the number of transversals in a cyclic Latin square."
"In this paper, we study one-Lee weight and two-Lee weight codes over $\mathbb{Z}_{2}\mathbb{Z}_{2}[u]$, where $u^{2}=0$. Some properties of one-Lee weight $\mathbb{Z}_{2}\mathbb{Z}_{2}[u]$-additive codes are given, and a complete classification of one-Lee weight $\mathbb{Z}_2\mathbb{Z}_2[u]$-additive formally self-dual codes is obtained. The structure of two-Lee weight projective $\mathbb{Z}_2\mathbb{Z}_2[u]$ codes are determined. Some optimal binary linear codes are obtained directly from one-Lee weight and two-Lee weight $\mathbb{Z}_{2}\mathbb{Z}_{2}[u]$-additive codes via the extended Gray map."
"The problem of event-triggered control with rate limited communication is considered. For continuous-time scalar systems without disturbances, a phase transition behavior of the transmission rate required for stabilization as a function of the communication delay is revealed. It is shown that for low values of the delay the timing information carried by the triggering events is large and the system can be stabilized with any positive rate. On the other hand, when the delay exceeds a certain threshold that depends on the given triggering strategy, the timing information alone is not enough to achieve stabilization and the rate must begin to grow, eventually becoming larger than what required by the classic data-rate theorem. The critical point where the transmission rate equals the one imposed by the data-rate theorem occurs when the delay equals the inverse of the entropy rate of the plant, representing the intrinsic rate at which the system generates information. At this critical point, the timing information supplied by event triggering is completely balanced by the information loss due to the communication delay. Exponential convergence guarantees are also discussed, and an explicit construction providing a sufficient condition for stabilization is given."
"Direction of arrival (DOA) estimation refers to the process of retrieving the direction information of several electromagnetic waves/sources from the outputs of a number of receiving antennas that form a sensor array. DOA estimation is a major problem in array signal processing and has wide applications in radar, sonar, wireless communications, etc. The purpose of this article is to provide an overview of the recent work on sparse DOA estimation methods."
"Mobile big data contains vast statistical features in various dimensions, including spatial, temporal, and the underlying social domain. Understanding and exploiting the features of mobile data from a social network perspective will be extremely beneficial to wireless networks, from planning, operation, and maintenance to optimization and marketing. In this paper, we categorize and analyze the big data collected from real wireless cellular networks. Then, we study the social characteristics of mobile big data and highlight several research directions for mobile big data in the social computing areas."
"The trending integrations of Battery Energy Storage System (BESS, stationary battery) and Electric Vehicles (EV, mobile battery) to distribution grids call for advanced Demand Side Management (DSM) technique that addresses the scalability concerns of the system and stochastic availabilities of EVs. Towards this goal, a stochastic DSM is proposed to capture the uncertainties in EVs. Numerical approximation is then used to make the problem tractable. To accelerate the computational speed, the proposed DSM is tightly relaxed to a convex form using second-order cone programming. Furthermore, in light of the continuous increasing problem scale, we use a distributed method with a guaranteed convergence to shift the computational burden to local controllers. To verify the proposed DSM, we use real-world EV data collected on UCLA campus and test the DSM in a modified IEEE distribution benchmark test system. Numerical results demonstrates the correctness and merits of the proposed approach."
"Management and analysis of big data are systematically associated with a data distributed architecture in the Hadoop and now Spark frameworks. This article offers an introduction for statisticians to these technologies by comparing the performance obtained by the direct use of three reference environments: R, Python Scikit-learn, Spark MLlib on three public use cases: character recognition, recommending films, categorizing products. As main result, it appears that, if Spark is very efficient for data munging and recommendation by collaborative filtering (non-negative factorization), current implementations of conventional learning methods (logistic regression, random forests) in MLlib or SparkML do not ou poorly compete habitual use of these methods (R, Python Scikit-learn) in an integrated or undistributed architecture"
"This paper proposes a CNN cascade for semantic part segmentation guided by pose-specific information encoded in terms of a set of landmarks (or keypoints). There is large amount of prior work on each of these tasks separately, yet, to the best of our knowledge, this is the first time in literature that the interplay between pose estimation and semantic part segmentation is investigated. To address this limitation of prior work, in this paper, we propose a CNN cascade of tasks that firstly performs landmark localisation and then uses this information as input for guiding semantic part segmentation. We applied our architecture to the problem of facial part segmentation and report large performance improvement over the standard unguided network on the most challenging face datasets. Testing code and models will be published online at http://cs.nott.ac.uk/~psxasj/."
"We show that any quantum circuit of treewidth $t$, built from $r$-qubit gates, requires at least $\Omega(\frac{n^{2}}{2^{O(r\cdot t)}\cdot \log^4 n})$ gates to compute the element distinctness function. Our result generalizes a near-quadratic lower bound for quantum formula size obtained by Roychowdhury and Vatan [SIAM J. on Computing, 2001]. The proof of our lower bound follows by an extension of Ne\v{c}iporuk's method to the context of quantum circuits of constant treewidth. This extension is made via a combination of techniques from structural graph theory, tensor-network theory, and the connected-component counting method, which is a classic tool in algebraic geometry."
"Efficient construction of the suffix tree given an input text is an active area of research from the time it was first introduced. Both theoretical computer scientists and engineers tackled the problem. In this paper we focus on the fastest practical suffix tree construction algorithm to date, ERA. We first provide a theoretical analysis of the algorithm assuming the uniformly random text as an input and using the PEM model of computation with respect to the lower bounds. Secondly, we empirically confirm the theoretical results in different test scenarios exposing the critical terms. Thirdly, we discuss the fundamental characteristics of the input text where the fastest suffix tree construction algorithms in practice fail. This paper serves as a foundation for further research in the parallel text indexing area."
"Notwithstanding the advancement of software engineering for enterprise applications, the process of software implementation is still time-consuming and error-prone. This situation is more severe when developing the custom software, because the requirements are always changing during the whole development processes. This issue could be alleviated by reusing exiting code or services in the private and public repositories. Nevertheless, the reuse could fail if no existing service is matched or existing services could not be composited to satisfy the requirements. Eventually, the developers have to implement the business logic manually for the un-satisfied requirements. In this paper, we present an approach which can automated generate business logic from requirement model for enterprise applications. Unlike other works, our approach does not need to specify design model such as sequence diagram, the business logic could be directly generated from operation contracts of requirement model. Operation contracts only contain the preconditions before executing the action, and the postconditions after execution. Moreover, the generated off-the-shelf code is adopted the same multi-layer structure as Java EE and .NET platforms which are robust, scalable, and widely used in enterprise application developments. Finally, a case study of library management system demonstrates the feasibility and efficiency of the proposed approach in our implemented RMCode tool."
"This paper begins with considering the identification of sparse linear time-invariant networks described by multivariable ARX models. Such models possess relatively simple structure thus used as a benchmark to promote further research. With identifiability of the network guaranteed, this paper presents an identification method that infers both the Boolean structure of the network and the internal dynamics between nodes. Identification is performed directly from data without any prior knowledge of the system, including its order. The proposed method solves the identification problem using Maximum a posteriori estimation (MAP) but with inseparable penalties for complexity, both in terms of element (order of nonzero connections) and group sparsity (network topology). Such an approach is widely applied in Compressive Sensing (CS) and known as Sparse Bayesian Learning (SBL). We then propose a novel scheme that combines sparse Bayesian and group sparse Bayesian to efficiently solve the problem. The resulted algorithm has a similar form of the standard Sparse Group Lasso (SGL) while with known noise variance, it simplifies to exact re-weighted SGL. The method and the developed toolbox can be applied to infer networks from a wide range of fields, including systems biology applications such as signaling and genetic regulatory networks."
"Filter bank-based multicarrier (FBMC) systems are currently being considered as a prevalent candidate for replacing the long established cyclic prefix (CP)-based orthogonal frequency division multiplexing (CP-OFDM) in the physical layer of next generation communications systems. In particular, FBMC/OQAM has received increasing attention due to, among other features, its potential for maximum spectral efficiency. It suffers, however, from an intrinsic self-interference effect, which complicates signal processing tasks at the receiver, including synchronization, channel estimation and equalization. In a multiple-input multiple-output (MIMO) configuration, the multi-antenna interference has also to be taken into account. (Semi-)blind FBMC/OQAM receivers have been little studied so far and mainly for single-antenna systems. The problem of joint channel estimation and data detection in a MIMO-FBMC/OQAM system, given limited or no training information, is studied in this paper through a tensor-based approach in the light of the success of such techniques in OFDM applications. Simulation-based comparisons with CP-OFDM are included, for realistic transmission models."
"In this paper, we study a relative two-weight $\mathbb{Z}_2 \mathbb{Z}_4$-additive codes. It is shown that the Gray image of a two-distance $\mathbb{Z}_2 \mathbb{Z}_4$-additive code is a binary two-distance code and that the Gray image of a relative two-weight $\mathbb{Z}_2 \mathbb{Z}_4$-additive code, with nontrivial binary part, is a linear binary relative two-weight code. The structure of relative two-weight $\mathbb{Z}_2 \mathbb{Z}_4$-additive codes are described. Finally, we discussed permutation automorphism group of a $\mathbb{Z}_2 \mathbb{Z}_4$-additive codes."
"Convolutional Neural Networks (CNNs) have gained significant traction in the field of machine learning, particularly due to their high accuracy in visual recognition. Recent works have pushed the performance of GPU implementations of CNNs to significantly improve their classification and training times. With these improvements, many frameworks have become available for implementing CNNs on both CPUs and GPUs, with no support for FPGA implementations. In this work we present a modified version of the popular CNN framework Caffe, with FPGA support. This allows for classification using CNN models and specialized FPGA implementations with the flexibility of reprogramming the device when necessary, seamless memory transactions between host and device, simple-to-use test benches, and the ability to create pipelined layer implementations. To validate the framework, we use the Xilinx SDAccel environment to implement an FPGA-based Winograd convolution engine and show that the FPGA layer can be used alongside other layers running on a host processor to run several popular CNNs (AlexNet, GoogleNet, VGG A, Overfeat). The results show that our framework achieves 50 GFLOPS across 3x3 convolutions in the benchmarks. This is achieved within a practical framework, which will aid in future development of FPGA-based CNNs."
"Quasi-trees generalize trees in that the unique ""path"" between two nodes may be infinite and have any countable order type. They are used to define the rank-width of a countable graph in such a way that it is equal to the least upper-bound of the rank-widths of its finite induced subgraphs. Join-trees are the corresponding directed trees. They are useful to define the modular decomposition of a countable graph. We also consider ordered join-trees, that generalize rooted trees equipped with a linear order on the set of sons of each node. We define algebras with finitely many operations that generate (via infinite terms) these generalized trees. We prove that the associated regular objects (those defined by regular terms) are exactly the ones that are the unique models of monadic second-order sentences. These results use and generalize a similar result by W.Thomas for countable linear orders."
"We study the version of the C-Planarity problem in which edges connecting the same pair of clusters must be grouped into pipes, which generalizes the Strip Planarity problem. We give algorithms to decide several families of instances for the two variants in which the order of the pipes around each cluster is given as part of the input or can be chosen by the algorithm."
"The objective of this dissertation is to shed light on some fundamental impediments in learning control laws in continuous state spaces. In particular, if one wants to build artificial devices capable to learn motor tasks the same way they learn to classify signals and images, one needs to establish control rules that do not necessitate comparisons between quantities of the surrounding space. We propose, in that context, to take inspiration from the ""end effector control"" principle, as suggested by neuroscience studies, as opposed to the ""displacement control"" principle used in the classical control theory."
"Caching popular content at the edge of future mobile networks has been widely considered in order to alleviate the impact of the data tsunami on both the access and backhaul networks. A number of interesting techniques have been proposed, including femto-caching and ""delayed"" or opportunistic cache access. Nevertheless, the majority of these approaches suffer from the rather limited storage capacity of the edge caches, compared to the tremendous and rapidly increasing size of the Internet content catalog. We propose to depart from the assumption of hard cache misses, common in most existing works, and consider ""soft"" cache misses, where if the original content is not available, an alternative content that is locally cached can be recommended. Given that Internet content consumption is increasingly entertainment-oriented, we believe that a related content could often lead to complete or at least partial user satisfaction, without the need to retrieve the original content over expensive links. In this paper, we formulate the problem of optimal edge caching with soft cache hits, in the context of delayed access, and analyze the expected gains. We then show using synthetic and real datasets of related video contents that promising caching gains could be achieved in practice."
"Analyzing pseudo-telepathy graph games, we propose a way to build contextuality scenarios exhibiting the quantum supremacy using graph states. We consider the combinatorial structures generating equivalent scenarios. We investigate which scenarios are more multipartite and show that there exist graphs generating scenarios with a linear multipartiteness width."
"Emerging cellular technologies such as those proposed for use in 5G communications will accommodate a wide range of usage scenarios with diverse link requirements. This will include the necessity to operate over a versatile set of wireless channels ranging from indoor to outdoor, from line-of-sight (LOS) to non-LOS, and from circularly symmetric scattering to environments which promote the clustering of scattered multipath waves. Unfortunately, many of the conventional fading models adopted in the literature to develop network models lack the flexibility to account for such disparate signal propagation mechanisms. To bridge the gap between theory and practical channels, we consider $\kappa$-$\mu$ shadowed fading, which contains as special cases, the majority of the linear fading models proposed in the open literature, including Rayleigh, Rician, Nakagami-m, Nakagami-q, One-sided Gaussian, $\kappa$-$\mu$, $\eta$-$\mu$, and Rician shadowed to name but a few. In particular, we apply an orthogonal expansion to represent the $\kappa$-$\mu$ shadowed fading distribution as a simplified series expression. Then using the series expressions with stochastic geometry, we propose an analytic framework to evaluate the average of an arbitrary function of the SINR over $\kappa$-$\mu$ shadowed fading channels. Using the proposed method, we evaluate the spectral efficiency, moments of the SINR, bit error probability and outage probability of a $K$-tier HetNet with $K$ classes of BSs, differing in terms of the transmit power, BS density, shadowing characteristics and small-scale fading. Building upon these results, we provide important new insights into the network performance of these emerging wireless applications while considering a diverse range of fading conditions and link qualities."
"We propose an entirely data-driven approach to estimating the 3D pose of a hand given a depth image. We show that we can correct the mistakes made by a Convolutional Neural Network trained to predict an estimate of the 3D pose by using a feedback loop. The components of this feedback loop are also Deep Networks, optimized using training data. They remove the need for fitting a 3D model to the input data, which requires both a carefully designed fitting function and algorithm. We show that our approach outperforms state-of-the-art methods, and is efficient as our implementation runs at over 400 fps on a single GPU."
"In this paper we describe how to leverage higher-order unification to type check a dependently typed language with meta-variables. The literature usually presents the unification algorithm as a standalone component, however the need to check definitional equality of terms while type checking gives rise to a tight interplay between type checking and unification. This interplay is a major source of complexity in the type-checking algorithm for existing dependently typed programming languages. We propose an algorithm that encodes a type-checking problem entirely in the form of unification constraints, reducing the complexity of the type-checking code by taking advantage of higher order unification, which is already part of the implementation of many dependently typed languages."
"Convolutional Neural Networks (CNNs) trained on large scale RGB databases have become the secret sauce in the majority of recent approaches for object categorization from RGB-D data. Thanks to colorization techniques, these methods exploit the filters learned from 2D images to extract meaningful representations in 2.5D. Still, the perceptual signature of these two kind of images is very different, with the first usually strongly characterized by textures, and the second mostly by silhouettes of objects. Ideally, one would like to have two CNNs, one for RGB and one for depth, each trained on a suitable data collection, able to capture the perceptual properties of each channel for the task at hand. This has not been possible so far, due to the lack of a suitable depth database. This paper addresses this issue, proposing to opt for synthetically generated images rather than collecting by hand a 2.5D large scale database. While being clearly a proxy for real data, synthetic images allow to trade quality for quantity, making it possible to generate a virtually infinite amount of data. We show that the filters learned from such data collection, using the very same architecture typically used on visual data, learns very different filters, resulting in depth features (a) able to better characterize the different facets of depth images, and (b) complementary with respect to those derived from CNNs pre-trained on 2D datasets. Experiments on two publicly available databases show the power of our approach."
"Jolie is a programming language that follows the microservices paradigm. As an open source project, it has built a community of developers worldwide - both in the industry as well as in academia - taken care of the development, continuously improved its usability, and therefore broadened the adoption. In this paper, we present some of the most recent results and work in progress that has been made within our research team."
"The paper presents a proof of concept to calibrate iCub's skin using vacuum bags. The method's main idea consists in inserting the skin in a vacuum bag, and then decreasing the pressure in the bag to create a uniform pressure distribution on the skin surface. Acquisition and data processing of the bag pressure and sensors' measured capacitance allow us to characterize the relationship between the pressure and the measured capacitance of each sensor. After calibration, integration of the pressure distribution over the skin geometry provides us with the net normal force applied to the skin. Experiments are conducted using the forearm skin of the iCub humanoid robot, and validation results indicate acceptable average errors in force prediction."
"Rate adaptation in 802.11 WLANs has received a lot of attention from the research community, with most of the proposals aiming at maximising throughput based on network conditions. Considering energy consumption, an implicit assumption is that optimality in throughput implies optimality in energy efficiency, but this assumption has been recently put into question. In this paper, we address via analysis and experimentation the relation between throughput performance and energy efficiency in multi-rate 802.11 scenarios. We demonstrate the trade-off between these performance figures, confirming that they may not be simultaneously optimised, and analyse their sensitivity towards the energy consumption parameters of the device. Our results provide the means to design novel rate adaptation schemes that takes energy consumption into account."
"Bounded context switching (BCS) is an under-approximate method for finding violations to safety properties in shared memory concurrent programs. Technically, BCS is a reachability problem that is known to be NP-complete if the context bound is part of the input. Our contribution is a parameterized analysis of BCS. The main finding is a new algorithm that solves BCS when parameterized by the number of context switches and the size of the memory. Notably, there are no restrictions on the number, the size, nor the behavior of the threads. Moreover, the algorithm is generic in that it applies to various classes of programs. The runtime is O*(m^cs * 4^cs) where m is the size of the memory. We show an O*(2^cs) lower bound, assuming the exponential time hypothesis. The idea behind the algorithm is to iterate over the memory sequences: sequences of states in the memory that are seen upon context switches. We also study a generalization of the parameterization that only limits the size of the memory each thread may access."
This lecture discusses the mathematical relationship between network structure and network utilization of transportation network. Network structure means the graph itself. Network utilization represent the aggregation of trajectories of agents in using the network graph. I show the similarity and relationship between the structural pattern of the network and network utilization.
"Most existing methods in binaural sound source localization rely on some kind of aggregation of phase-and level-difference cues in the time-frequency plane. While different ag-gregation schemes exist, they are often heuristic and suffer in adverse noise conditions. In this paper, we introduce the rectified binaural ratio as a new feature for sound source local-ization. We show that for Gaussian-process point source signals corrupted by stationary Gaussian noise, this ratio follows a complex t-distribution with explicit parameters. This new formulation provides a principled and statistically sound way to aggregate binaural features in the presence of noise. We subsequently derive two simple and efficient methods for robust relative transfer function and time-delay estimation. Experiments on heavily corrupted simulated and speech signals demonstrate the robustness of the proposed scheme."
"We consider the problem of estimating the phases of K mixed complex signals from a multichannel observation, when the mixing matrix and signal magnitudes are known. This problem can be cast as a non-convex quadratically constrained quadratic program which is known to be NP-hard in general. We propose three approaches to tackle it: a heuristic method, an alternate minimization method, and a convex relaxation into a semi-definite program. These approaches are showed to outperform the oracle multichannel Wiener filter in under-determined informed source separation tasks, using simulated and speech signals. The convex relaxation approach yields best results, including the potential for exact source separation in under-determined settings."
"This paper introduces a new framework for supervised sound source localization referred to as virtually-supervised learning. An acoustic shoe-box room simulator is used to generate a large number of binaural single-source audio scenes. These scenes are used to build a dataset of spatial binaural features annotated with acoustic properties such as the 3D source position and the walls' absorption coefficients. A probabilis-tic high-to low-dimensional regression framework is used to learn a mapping from these features to the acoustic properties. Results indicate that this mapping successfully estimates the azimuth and elevation of new sources, but also their range and even the walls' absorption coefficients solely based on binau-ral signals. Results also reveal that incorporating random-diffusion effects in the data significantly improves the estimation of all parameters."
"Crowdsourcing platforms enable to propose simple human intelligence tasks to a large number of participants who realise these tasks. The workers often receive a small amount of money or the platforms include some other incentive mechanisms, for example they can increase the workers reputation score, if they complete the tasks correctly. We address the problem of identifying experts among participants, that is, workers, who tend to answer the questions correctly. Knowing who are the reliable workers could improve the quality of knowledge one can extract from responses. As opposed to other works in the literature, we assume that participants can give partial or incomplete responses, in case they are not sure that their answers are correct. We model such partial or incomplete responses with the help of belief functions, and we derive a measure that characterizes the expertise level of each participant. This measure is based on precise and exactitude degrees that represent two parts of the expertise level. The precision degree reflects the reliability level of the participants and the exactitude degree reflects the knowledge level of the participants. We also analyze our model through simulation and demonstrate that our richer model can lead to more reliable identification of experts."
"Individual neighborhoods within large cities can benefit from independent analysis of public data in the context of ongoing efforts to improve the community. Yet existing tools for public data analysis and visualization are often mismatched to community needs, for reasons including geographic granularity that does not correspond to community boundaries, siloed data sets, inaccurate assumptions about data literacy, and limited user input in design and implementation phases. In Atlanta this need is being addressed through a Data Dashboard developed under the auspices of the Westside Communities Alliance (WCA), a partnership between Georgia Tech and community stakeholders. In this paper we present an interactive analytic and visualization tool for public safety data within the WCA Data Dashboard. We describe a human-centered approach to understand the needs of users and to build accessible mapping tools for visualization and analysis. The tools include a variety of overlays that allow users to spatially correlate features of the built environment, such as vacant properties with criminal activity as well as crime prevention efforts. We are in the final stages of developing the first version of the tool, with plans for a public release in fall of 2016."
"The American Community Survey (ACS) is the bedrock underpinning any analysis of the US population, urban areas included. The Census Bureau delivers the ACS data in multiple formats, yet in each the raw data is difficult to export in bulk and difficult to sift through. We argue that Enigma's approach to the data delivery, such as our raw data and metadata presentation, reflects the survey's logical structure. It can be explored, interlinked, and searched; making it easier to retrieve the appropriate data applicable to a question at hand. We make the use of data more liquid via curated tables and API access; even metadata and notes from technical documentation are programmatically accessible. Additionally, we are working towards opening our scalable and reproducible ingestion process of ACS estimations. This paper details all of the ways the Census Bureau currently makes the data available, the barriers each of these raise to applying this data in analysis and how our approach overcomes them. Finally, this paper will address other recent innovations in making Census datasets more usable, the use cases suited to each and how they fit into the wider application of data science."
"In this article algebraic constructions are introduced in order to study the variety defined by a radical parametrization (a tuple of functions involving complex numbers, $n$ variables, the four field operations and radical extractions). We provide algorithms to implicitize radical parametrizations and to check whether a radical parametrization can be reparametrized into a rational parametrization."
"Interest makes one hold her attention on the object of interest. Automatic recognition of interest has numerous applications in human-computer interaction. In this paper, we study the facial expressions associated with interest and its underlying and closely related components, namely, curiosity, coping potential, novelty and complexity. To this end, we conducted an experiment in which participants watched images and micro-videos while a front-facing camera recorded their expressions. After watching each item they self-reported their level of interest, curiosity, coping potential and perceived novelty and complexity. Using an automated method, we tracked facial action units (AU) and studied the relationship between the presence of facial movements with interest and its related components. We then tracked the facial landmarks, e.g., corners of lips, and extracted features from each response. We trained random forests regression models to detect the level of interest, curiosity, and appraisals. We found a large difference between the way people report and react to interesting visual content. The expressions in response to images and micro-videos were not always pronounced depending on the participants. This makes the direct detection of interest from facial expressions a challenging problem. With this work, for the first time, we demonstrate the feasibility of detecting cognitive appraisals from facial expressions which will open the door for appraisal-driven emotion recognition methods."
"Medical research is one area where collecting data is usually hard and expensive. With the launch of ResearchKit, Apple and Sage Bionetworks made large-scale personal data collection increasingly popular via simple text-based survey apps running on mobile phones. But such surveys can be a barrier in terms of usability and richness of the data being collected. In this paper, we present SDL-R X , a powerful software library designed for ResearchKit that enables study-specific, personalized, and rich visual surveys, for both iOS and Android platforms."
"The stringent requirements of a 1,000 times increase in data traffic and one millisecond round trip latency have made limiting the potentially tremendous ensuing energy consumption one of the most challenging problems for the design of the upcoming fifth-generation (5G) networks. To enable sustainable 5G networks, new technologies have been proposed to improve the system energy efficiency and alternative energy sources are introduced to reduce our dependence on traditional fossil fuels. In particular, various 5G techniques target the reduction of the energy consumption without sacrificing the quality-of-service. Meanwhile, energy harvesting technologies, which enable communication transceivers to harvest energy from various renewable resources and ambient radio frequency signals for communi- cation, have drawn significant interest from both academia and industry. In this article, we provide an overview of the latest research on both green 5G techniques and energy harvesting for communication. In addition, some technical challenges and potential research topics for realizing sustainable green 5G networks are also identified."
"In this invited talk, I will review five basic concepts of Axiomatic Rewriting Theory, an axiomatic and diagrammatic theory of rewriting started 25 years ago in a LICS paper with Georges Gonthier and Jean-Jacques L\'evy, and developed along the subsequent years into a full-fledged 2-dimensional theory of causality and residuation in rewriting. I will give a contemporary view on the theory, informed by my later work on categorical semantics and higher-dimensional algebra, and also indicate a number of current research directions in the field."
"Public transit systems are a critical component of major metropolitan areas. However, in the face of increasing demand, most of these systems are operating close to capacity. Under normal operating conditions, station crowding and boarding denial are becoming a major concern for transit agencies. As such, any disruption in service will have even more severe consequences, affecting huge number of passengers. Considering the aging infrastructure of many large cities, such as New York and London, these disruptions are to be expected, amplifying the need for better demand management and strategies to deal with congested transit facilities. Opportunistic sensors such as smart cards (AFC), automatic vehicle location systems (AVL), GPS, etc. provide a wealth of information about system's performance and passengers' trip making patterns. We develop a hybrid data/model-driven decision support system, using real-time predictive models, to help transit operators manage and respond proactively to disruptions and mitigate consequences in a timely fashion. These models include station arrival and origin-destination predictions in real-time to help transit agencies, and predictive information systems for assisting passengers' trip making decisions."
"We consider the problem of using polar codes with higher order modulation over AWGN channels. Unlike prior work, we focus on using modulation independent polar codes. That is, the polar codes are not re-designed based on the modulation used. Instead, we propose bit-permuted coded modulation (BPCM): a technique for using the multilevel coding (MLC) approach for an arbitrary polar code. The BPCM technique exploits a natural connection between MLC and polar codes. It involves applying bit permutations prior to mapping the polar code to a higher order modulation. The bit permutations are designed, via density evolution, to match the rates provided by various bit levels of the higher order modulation to that of the polar code.   We demonstrate performance of the BPCM technique using link simulations and density evolution for the AWGN channel. We compare the BPCM technique with the bit-interleaved coded modulation (BICM) technique. When using polar codes designed for BPSK modulation, we show gains for BPCM over BICM with random interleaver of up to 0.2 dB, 0.7 dB and 1.4 dB for 4-ASK, 8-ASK, and 16-ASK respectively."
"This paper deals with the identification of piecewise affine state-space models. These models are obtained by partitioning the state or input domain into a finite number of regions and by considering affine submodels in each region. The proposed framework uses the Expectation Maximization (EM) algorithm to identify the parameters of the model. In most of the current literature, a discrete random variable with a discrete transition density is introduced to describe the transition between each submodel, leading to a further approximation of the dynamical system by a jump Markov model. On the contrary, we use the cumulative distribution function (CDF) to compute the probability of each submodel given the measurement at that time step. Then, given the submodel at each time step the latent state is estimated using the Kalman smoother. Subsequently, the parameters are estimated by maximizing a surrogate function for the likelihood. The performance of the proposed method is illustrated using the simulated model of the JAS 39 Gripen aircraft."
"In the classical Travelling Salesman Problem (TSP), the objective function sums the costs for travelling from one city to the next city along the tour. In the q-stripe TSP with q larger than 1, the objective function sums the costs for travelling from one city to each of the next q cities along the tour. The resulting q-stripe TSP generalizes the TSP and forms a special case of the quadratic assignment problem. We analyze the computational complexity of the q-stripe TSP for various classes of specially structured distance matrices. We derive NP-hardness results as well as polyomially solvable cases. One of our main results generalizes a well-known theorem of Kalmanson from the classical TSP to the q-stripe TSP."
"Many spectral unmixing methods rely on the non-negative decomposition of spectral data onto a dictionary of spectral templates. In particular, state-of-the-art music transcription systems decompose the spectrogram of the input signal onto a dictionary of representative note spectra. The typical measures of fit used to quantify the adequacy of the decomposition compare the data and template entries frequency-wise. As such, small displacements of energy from a frequency bin to another as well as variations of timber can disproportionally harm the fit. We address these issues by means of optimal transportation and propose a new measure of fit that treats the frequency distributions of energy holistically as opposed to frequency-wise. Building on the harmonic nature of sound, the new measure is invariant to shifts of energy to harmonically-related frequencies, as well as to small and local displacements of energy. Equipped with this new measure of fit, the dictionary of note templates can be considerably simplified to a set of Dirac vectors located at the target fundamental frequencies (musical pitch values). This in turns gives ground to a very fast and simple decomposition algorithm that achieves state-of-the-art performance on real musical data."
"In this paper, a method for stabilizing biped robots stepping by a combination of Divergent Component of Motion (DCM) tracking and step adjustment is proposed. In this method, the DCM trajectory is generated, consistent with the predefined footprints. Furthermore, a swing foot trajectory modification strategy is proposed to adapt the landing point, using DCM measurement. In order to apply the generated trajectories to the full robot, a Hierarchical Inverse Dynamics (HID) is employed. The HID enables us to use different combinations of the DCM tracking and step adjustment for stabilizing different biped robots. Simulation experiments on two scenarios for two different simulated robots, one with active ankles and the other with passive ankles, are carried out. Simulation results demonstrate the effectiveness of the proposed method for robots with both active and passive ankles."
"Distributed learning platforms for processing large scale data-sets are becoming increasingly prevalent. In typical distributed implementations, a centralized master node breaks the data-set into smaller batches for parallel processing across distributed workers to achieve speed-up and efficiency. Several computational tasks are of sequential nature, and involve multiple passes over the data. At each iteration over the data, it is common practice to randomly re-shuffle the data at the master node, assigning different batches for each worker to process. This random re-shuffling operation comes at the cost of extra communication overhead, since at each shuffle, new data points need to be delivered to the distributed workers.   In this paper, we focus on characterizing the information theoretically optimal communication overhead for the distributed data shuffling problem. We propose a novel coded data delivery scheme for the case of no excess storage, where every worker can only store the assigned data batches under processing. Our scheme exploits a new type of coding opportunity and is applicable to any arbitrary shuffle, and for any number of workers. We also present an information theoretic lower bound on the minimum communication overhead for data shuffling, and show that the proposed scheme matches this lower bound for the worst-case communication overhead."
"Modern distributed storage systems often use erasure codes to protect against disk and node failures to increase reliability, while trying to meet the latency requirements of the applications and clients. Storage systems may have caches at the proxy or client ends in order to reduce the latency. In this paper, we consider a novel caching framework with erasure code called {\em functional caching}. Functional Caching involves using erasure-coded chunks in the cache such that the code formed by the chunks in storage nodes and cache combined are maximal-distance-separable (MDS) erasure codes. Based on the arrival rates of different files, placement of file chunks on the servers, and service time distribution of storage servers, an optimal functional caching placement and the access probabilities of the file request from different disks are considered. The proposed algorithm gives significant latency improvement in both simulations and a prototyped solution in an open-source, cloud storage deployment."
"The Hermite methods of Goodrich, Hagstrom, and Lorenz (2006) use Hermite interpolation to construct high order numerical methods for hyperbolic initial value problems. The structure of the method has several favorable features for parallel computing. In this work, we propose algorithms that take advantage of the many-core architecture of Graphics Processing Units. The algorithm exploits the compact stencil of Hermite methods and uses data structures that allow for efficient data load and stores. Additionally the highly localized evolution operator of Hermite methods allows us to combine multi-stage time-stepping methods within the new algorithms incurring minimal accesses of global memory. Using a scalar linear wave equation, we study the algorithm by considering Hermite interpolation and evolution as individual kernels and alternatively combined them into a monolithic kernel. For both approaches we demonstrate strategies to increase performance. Our numerical experiments show that although a two kernel approach allows for better performance on the hardware, a monolithic kernel can offer a comparable time to solution with less global memory usage."
"Minutiae have played an important role in fingerprint identification. Extracting effective minutiae is difficult for latent fingerprints which are usually of poor quality. Instead of conventional hand-craft features, a fully convolutional network(FCN) is learned end-to-end to extract minutiae from raw fingerprints in pixel level. FCN is used to map raw fingerprints to a correspondingly-sized minutia-score map with a fixed stride. And thus a large number of minutiae will be extracted through a given thresh. Then small regions centering at these minutia points are put into a convolutional neural network(CNN) to reclassify these minutiae and calculate their orientations. The CNN shares the convolutional layers with the fully convolutional network to speed up. For the VGG model~\cite{simonyan2014very}, 0.45 second is used on average to detect one fingerprint on a GPU. On the NIST SD27 database we achieve 53\% recall rate and 53\% precise rate that beats many other algorithms. Our trained model is also visualized to see that we have successfully extracted features preserving ridge information of a latent fingerprint."
"In this paper, we present an efficient Dynamic Programing framework for optimal planning and control of legged robots. First we formulate this problem as an optimal control problem for switched systems. Then we propose a multi--level optimization approach to find the optimal switching times and the optimal continuous control inputs. Through this scheme, the decomposed optimization can potentially be done more efficiently than the combined approach. Finally, we present a continuous-time constrained LQR algorithm which simultaneously optimizes the feedforward and feedback controller with $O(n)$ time-complexity. In order to validate our approach, we show the performance of our framework on a quadrupedal robot. We choose the Center of Mass dynamics and the full kinematic formulation as the switched system model where the switching times as well as the contact forces and the joint velocities are optimized for different locomotion tasks such as gap crossing, walking and trotting."
"Structured sparse optimization is an important and challenging problem for analyzing high-dimensional data in a variety of applications such as bioinformatics, medical imaging, social networks, and astronomy. Although a number of structured sparsity models have been explored, such as trees, groups, clusters, and paths, connected subgraphs have been rarely explored in the current literature. One of the main technical challenges is that there is no structured sparsity-inducing norm that can directly model the space of connected subgraphs, and there is no exact implementation of a projection oracle for connected subgraphs due to its NP-hardness. In this paper, we explore efficient approximate projection oracles for connected subgraphs, and propose two new efficient algorithms, namely, Graph-IHT and Graph-GHTP, to optimize a generic nonlinear objective function subject to connectivity constraint on the support of the variables. Our proposed algorithms enjoy strong guarantees analogous to several current methods for sparsity-constrained optimization, such as Projected Gradient Descent (PGD), Approximate Model Iterative Hard Thresholding (AM-IHT), and Gradient Hard Thresholding Pursuit (GHTP) with respect to convergence rate and approximation accuracy. We apply our proposed algorithms to optimize several well-known graph scan statistics in several applications of connected subgraph detection as a case study, and the experimental results demonstrate that our proposed algorithms outperform state-of-the-art methods."
"Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood."
"This paper presents a study of improvement in stability in a single machine connected to infinite bus (SMIB) power system by using static compensator (STATCOM). The gains of Proportional-Integral-Derivative (PID) controller in STATCOM are being optimized by heuristic technique based on Particle swarm optimization (PSO). Further, Bacterial Foraging Optimization (BFO) as an alternative heuristic method is also applied to select optimal gains of PID controller. The performance of STATCOM with the above soft-computing techniques are studied and compared with the conventional PID controller under various scenarios. The simulation results are accompanied with performance indices based quantitative analysis. The analysis clearly signifies the robustness of the new scheme in terms of stability and voltage regulation when compared with conventional PID."
"This paper presents a formulation for brittle fracture in 3D elastic solids within the context of configurational mechanics. The local form of the first law of thermodynamics provides a condition for equilibrium of the crack front. The direction of the crack propagation is shown to be given by the direction of the configurational forces on the crack front that maximise the local dissipation. The evolving crack front is continuously resolved by the finite element mesh, without the need for face splitting or the use of enrichment techniques. A monolithic solution strategy is adopted, solving simultaneously for both the material displacements (i.e. crack extension) and the spatial displacements, is adopted. In order to trace the dissipative loading path, an arc-length procedure is developed that controls the incremental crack area growth. In order to maintain mesh quality, smoothing of the mesh is undertaken as a continuous process, together with face flipping, node merging and edge splitting where necessary. Hierarchical basis functions of arbitrary polynomial order are adopted to increase the order of approximation without the need to change the finite element mesh. Performance of the formulation is demonstrated by means of three representative numerical simulations, demonstrating both accuracy and robustness."
"In this paper we investigate the optimal latency of communications. Focusing on fixed rate communication without any feedback channel, this paper encompasses low-latency strategies with which one hop and multi-hop communication issues are treated from an information theoretic perspective. By defining the latency as the time required to make decisions, we prove that if short messages can be transmitted in parallel Gaussian channels, for example, via orthogonal frequency-division multiplexing (OFDM)-like signals, there exists an optimal low-latency strategy for every code. This can be achieved via early-detection schemes or asynchronous detections. We first provide the optimal achievable latency in additive white Gaussian noise (AWGN) channels for every channel code given a probability block error $\epsilon$. This can be obtained via sequential ratio tests or a ""genie"" aided, \textit{e.g}. error-detecting codes. Results demonstrate the effectiveness of the approach. Next, we show how early-detection can be effective with OFDM signals while maintaining its spectral efficiency via random coding or pre-coding random matrices. Finally, we explore the optimal low-latency strategy in multi-hop relaying schemes. For amplify-and-forward (AF) and decode-and-forward (DF) relaying schemes there exist an optimal achievable latency. In particular, we first show that there exist a better low-latency strategy, for which AF relays could transmit while receiving. This can be achieved by using amplify and forward combined with early detection."
"Revenue optimization of large data centers is an open and challenging problem. The intricacy of the problem is due to the presence of too many parameters posing as costs or investment. This paper proposes a model to optimize the revenue in cloud data center and analyzes the model, revenue and different investment or cost commitments of organizations investing in data centers. The model uses the Cobb-Douglas production function to quantify the boundaries and the most significant factors to generate the revenue. The dynamics between revenue and cost is explored by designing an experiment (DoE) which is an interpretation of revenue as function of cost/investment as factors with different levels/fluctuations. Optimal elasticities associated with these factors of the model for maximum revenue are computed and verified . The model response is interpreted in light of the business scenario of data centers."
"Homotopy type theory offers the promise of a formal system for the univalent foundations of mathematics. However, if we simply add the univalence axiom to type theory, then we lose the property of canonicity --- that every term computes to a normal form. A computation becomes `stuck' when it reaches the point that it needs to evaluate a proof term that is an application of the univalence axiom. So we wish to find a way to compute with the univalence axiom.   As a first step, we present here a system of higher-order propositional logic, with a universe Omega of propositions closed under implication and quantification over any simple type over Omega. We add a type M =A N for any terms M, N of type A, and two ways to prove an equality: reflexivity, and the univalence axiom. We present reduction relations for this system, and prove the reduction confluent and strongly normalizing on the well-typed terms."
"Microscopic pedestrian studies consider detailed interaction of pedestrians to control their movement in pedestrian traffic flow. The tools to collect the microscopic data and to analyze microscopic pedestrian flow are still very much in its infancy. The microscopic pedestrian flow characteristics need to be understood. Manual, semi manual and automatic image processing data collection systems were developed. It was found that the microscopic speed resemble a normal distribution with a mean of 1.38 m/second and standard deviation of 0.37 m/second. The acceleration distribution also bear a resemblance to the normal distribution with an average of 0.68 m/ square second. A physical based microscopic pedestrian simulation model was also developed. Both Microscopic Video Data Collection and Microscopic Pedestrian Simulation Model generate a database called NTXY database. The formulations of the flow performance or microscopic pedestrian characteristics are explained. Sensitivity of the simulation and relationship between the flow performances are described. Validation of the simulation using real world data is then explained through the comparison between average instantaneous speed distributions of the real world data with the result of the simulations. The simulation model is then applied for some experiments on a hypothetical situation to gain more understanding of pedestrian behavior in one way and two way situations, to know the behavior of the system if the number of elderly pedestrian increases and to evaluate a policy of lane-like segregation toward pedestrian crossing and inspects the performance of the crossing. It was revealed that the microscopic pedestrian studies have been successfully applied to give more understanding to the behavior of microscopic pedestrians flow, predict the theoretical and practical situation and evaluate some design policies before its implementation."
"This paper presents a number of experiments to model changes in a historical Portuguese corpus composed of literary texts for the purpose of temporal text classification. Algorithms were trained to classify texts with respect to their publication date taking into account lexical variation represented as word n-grams, and morphosyntactic variation represented by part-of-speech (POS) distribution. We report results of 99.8% accuracy using word unigram features with a Support Vector Machines classifier to predict the publication date of documents in time intervals of both one century and half a century. A feature analysis is performed to investigate the most informative features for this task and how they are linked to language change."
"We present an analysis of the performance of machine learning classifiers on discriminating between similar languages and language varieties. We carried out a number of experiments using the results of the two editions of the Discriminating between Similar Languages (DSL) shared task. We investigate the progress made between the two tasks, estimate an upper bound on possible performance using ensemble and oracle combination, and provide learning curves to help us understand which languages are more challenging. A number of difficult sentences are identified and investigated further with human annotation."
This paper considers distributed storage systems (DSSs) from a graph theoretic perspective. A DSS is constructed by means of the path decomposition of a 3- regular graph into P4 paths. The paths represent the disks of the DSS and the edges of the graph act as the blocks of storage. We deduce the properties of the DSS from a related graph and show their optimality.
"Cognitive radio (CR) has been considered as a promising technology to enhance spectrum efficiency via opportunistic transmission at link level. Basic CR features allow SUs to transmit only when the licensed primary channel is not occupied by PUs. However, waiting for idle time slot may include large packet delay and high energy consumption. We further consider that the SU may decide, at any moment, to use another dedicated way of communication (3G) in order to transmit its packets. Thus, we consider an Opportunistic Spectrum Access (OSA) mechanism that takes into account packet delay and energy consumption. We formulate the OSA problem as a Partially Observable Markov Decision Process (POMDP) by explicitly considering the energy consumption as well as packets' delay, which are often ignored in existing OSA solutions. Specifically, we consider a POMDP with an average reward criterion. We derive structural properties of the value function and we show the existence of optimal strategies in the class of the threshold strategies. For implementation purposes, we propose online learning mechanisms that estimate the PU activity and determine the appropriate threshold strategy on the fly. In particular, numerical illustrations validate our theoretical findings."
"Fault tolerance is essential for building reliable services; however, it comes at the price of redundancy, mainly the ""replication factor"" and ""diversity"". With the increasing reliance on Internet-based services, more machines (mainly servers) are needed to scale out, multiplied with the extra expense of replication. This paper revisits the very fundamentals of fault tolerance and presents ""artificial redundancy"": a formal generalization of ""exact copy"" redundancy in which new sources of redundancy are exploited to build fault tolerant systems. On this concept, we show how to build ""artificial replication"" and design ""artificial fault tolerance"" (AFT). We discuss the properties of these new techniques showing that AFT extends current fault tolerant approaches to use other forms of redundancy aiming at reduced cost and high diversity."
"We propose a hybrid semiconductor-superconductor hardware platform for the implementation of neural networks and large-scale neuromorphic computing. The platform combines semiconducting few-photon light-emitting diodes with superconducting-nanowire single-photon detectors to behave as spiking neurons. These processing units are connected via a network of optical waveguides, and variable weights of connection can be implemented using several approaches. The use of light as a signaling mechanism overcomes the requirement for time-multiplexing that has limited the event rates of purely electronic platforms. The proposed processing units can operate at $20$ MHz with fully asynchronous activity, light-speed-limited latency, and power densities on the order of 1 mW/cm$^2$ for neurons with 700 connections operating at full speed at 2 K. The processing units achieve an energy efficiency of $\approx 20$ aJ per synapse event. By leveraging multilayer photonics with low-temperature-deposited waveguides and superconductors with feature sizes $>$ 100 nm, this approach could scale to massive interconnectivity near that of the human brain, and could surpass the brain in speed and energy efficiency."
"Detecting a small number of outliers from a set of data observations is always challenging. This problem is more difficult in the setting of multiple network samples, where computing the anomalous degree of a network sample is generally not sufficient. In fact, explaining why the network is exceptional, expressed in the form of subnetwork, is also equally important. In this paper, we develop a novel algorithm to address these two key problems. We treat each network sample as a potential outlier and identify subnetworks that mostly discriminate it from nearby regular samples. The algorithm is developed in the framework of network regression combined with the constraints on both network topology and L1-norm shrinkage to perform subnetwork discovery. Our method thus goes beyond subspace/subgraph discovery and we show that it converges to a global optimum. Evaluation on various real-world network datasets demonstrates that our algorithm not only outperforms baselines in both network and high dimensional setting, but also discovers highly relevant and interpretable local subnetworks, further enhancing our understanding of anomalous networks."
"Orthogonal Frequency Division Multiplexing (OFDM) has gained a lot of popularity over the years. Due to its popularity, OFDM has been adopted as a standard in cellular technology and Wireless Local Area Network (WLAN) communication systems. To improve the bit error rate (BER) performance, forward error correction (FEC) codes are often utilized to protect signals against unknown interference and channel degradations. In this paper, we apply soft-decision FEC, more specifically polar codes and a convolutional code, to an OFDM system in a quasi-static multipath fading channel, and compare BER performance in various channels. We investigate the effect of interleaving bits within a polar codeword. Finally, the simulation results for each case are presented in the paper."
"In this work, we propose buffer-aided distributed space-time coding (DSTC) schemes and relay selection algorithms for cooperative direct-sequence code-division multiple access (DS-CDMA) systems. We first devise a relay pair selection algorithm that can form relay pairs and then select the optimum set of relays among both the source-relay phase and the relay-destination phase according to the signal-to-interference-plus-noise ratio (SINR) criterion. Multiple relays equipped with dynamic buffers are then introduced in the network, which allows the relays to store data received from the sources and wait until the most appropriate time for transmission. { A greedy relay pair selection algorithm is then developed to reduce the high cost of the exhaustive search required when a large number of relays are involved.} The proposed techniques effectively improve the quality of the transmission with an acceptable delay as the buffer size is adjustable. An analysis of the computational complexity of the proposed algorithms, the delay and a study of the greedy algorithm are then carried out. Simulation results show that the proposed dynamic buffer-aided DSTC schemes and algorithms outperform prior art."
"This paper investigates the minimal number of actuators required to guarantees the controllability of a system, under the condition that its state transition matrix (STM) is prescribed. It has been proved that this minimal number is equal to the maximum value of the dimension of the space spanned by left eigenvectors of the STM associated with the same eigenvalue. These results can be directly modified for the determination of the minimal number of sensors required to assure system observability. The obtained conclusions are in sharp contrast to those established for the problems of finding the sparest input/output matrix under the restriction of system controllability/observabilty, which have been proved to be NP-hard, and even impossible to be approximated within a multiplicative factor."
"While state-of-the-art kernels for graphs with discrete labels scale well to graphs with thousands of nodes, the few existing kernels for graphs with continuous attributes, unfortunately, do not scale well. To overcome this limitation, we present hash graph kernels, a general framework to derive kernels for graphs with continuous attributes from discrete ones. The idea is to iteratively turn continuous attributes into discrete labels using randomized hash functions. We illustrate hash graph kernels for the Weisfeiler-Lehman subtree kernel and for the shortest-path kernel. The resulting novel graph kernels are shown to be, both, able to handle graphs with continuous attributes and scalable to large graphs and data sets. This is supported by our theoretical analysis and demonstrated by an extensive experimental evaluation."
"In this paper, for multichannel synthetic aperture radar (SAR) systems we first formulate the effects of Doppler ambiguities on the radial velocity (RV) estimation of a ground moving target in range-compressed domain, range-Doppler domain and image domain, respectively, where cascaded time-space Doppler ambiguity (CTSDA) may occur, that is, time domain Doppler ambiguity (TDDA) in each channel occurs at first and then spatial domain Doppler ambiguity (SDDA) among multi-channels occurs subsequently. Accordingly, the multichannel SAR systems with different parameters are divided into three cases with different Doppler ambiguity properties, i.e., only TDDA occurs in Case I, and CTSDA occurs in Cases II and III, while the CTSDA in Case II can be simply seen as the SDDA. Then, a multi-frequency SAR is proposed to obtain the RV estimation by solving the ambiguity problem based on Chinese remainder theorem (CRT). For Cases I and II, the ambiguity problem can be solved by the existing closed-form robust CRT. For Case III, we show that the problem is different from the conventional CRT problem and we call it a double remaindering problem. We then propose a sufficient condition under which the double remaindering problem, i.e., the CTSDA, can be solved by the closed-form robust CRT. When the sufficient condition is not satisfied, a searching based method is proposed. Finally, some numerical experiments are provided to demonstrate the effectiveness of the proposed methods."
"Cooperative communication can improve communication quality in wireless communication networks through strategic relay selection. However, wireless cooperative communication networks are vulnerable to the attacks initiated on relays. Although applying authentication protocols can secure cooperative communication when the selected relay is malicious, better system throughput could be obtained without executing authentication protocol when the selected relay is free from attacker's attack. In this paper, a game theoretic approach is proposed to quantitatively analyze the attacking strategies of the attacker who chooses one relay to attack so as to make rational decision on relay selection and extent of applying authentication protocols, which reaches the trade-off between system security requirement and quality of service (QoS) in wireless cooperative communication networks."
Classical translation models constrain the space of possible outputs by selecting a subset of translation rules based on the input sentence. Recent work on improving the efficiency of neural translation models adopted a similar strategy by restricting the output vocabulary to a subset of likely candidates given the source. In this paper we experiment with context and embedding-based selection methods and extend previous work by examining speed and accuracy trade-offs in more detail. We show that decoding time on CPUs can be reduced by up to 90% and training time by 25% on the WMT15 English-German and WMT16 English-Romanian tasks at the same or only negligible change in accuracy. This brings the time to decode with a state of the art neural translation system to just over 140 msec per sentence on a single CPU core for English-German.
"Forecasting the flow of crowds is of great importance to traffic management and public safety, yet a very challenging task affected by many complex factors, such as inter-region traffic, events and weather. In this paper, we propose a deep-learning-based approach, called ST-ResNet, to collectively forecast the in-flow and out-flow of crowds in each and every region through a city. We design an end-to-end structure of ST-ResNet based on unique properties of spatio-temporal data. More specifically, we employ the framework of the residual neural networks to model the temporal closeness, period, and trend properties of the crowd traffic, respectively. For each property, we design a branch of residual convolutional units, each of which models the spatial properties of the crowd traffic. ST-ResNet learns to dynamically aggregate the output of the three residual neural networks based on data, assigning different weights to different branches and regions. The aggregation is further combined with external factors, such as weather and day of the week, to predict the final traffic of crowds in each and every region. We evaluate ST-ResNet based on two types of crowd flows in Beijing and NYC, finding that its performance exceeds six well-know methods."
"We consider the problem of allocating a set $I$ of $m$ indivisible resources (items) to a set $P$ of $n$ customers (players) competing for the resources. Each resource $j \in I$ has a same value $v_j > 0$ for a subset of customers interested in $j$, and zero value for the remaining customers. The utility received by each customer is the sum of the values of the resources allocated to her. The goal is to find a feasible allocation of the resources to the interested customers such that for the Max-Min allocation problem (Min-Max allocation problem) the minimum of the utilities (maximum of the utilities) received by the customers is maximized (minimized). The Max-Min allocation problem is also known as the \textit{Fair Allocation problem}, or the \textit{Santa Claus problem}. The Min-Max allocation problem is the problem of Scheduling on Unrelated Parallel Machines, and is also known as the $R \, | \, | C_{\max}$ problem.   In this paper, we are interested in instances of the problem that admit a Polynomial Time Approximation Scheme (PTAS). We show that an ordering property on the resources and the customers is important and paves the way for a PTAS. For the Max-Min allocation problem, we start with instances of the problem that can be viewed as a \textit{convex bipartite graph}; a bipartite graph for which there exists an ordering of the resources such that each customer is interested in (has a positive evaluation for) a set of \textit{consecutive} resources. We demonstrate a PTAS for the inclusion-free cases. This class of instances is equivalent to the class of bipartite permutation graphs. For the Min-Max allocation problem, we also obtain a PTAS for inclusion-free instances. These instances are not only of theoretical interest but also have practical applications."
"Latent tree analysis seeks to model the correlations among a set of random variables using a tree of latent variables. It was proposed as an improvement to latent class analysis --- a method widely used in social sciences and medicine to identify homogeneous subgroups in a population. It provides new and fruitful perspectives on a number of machine learning areas, including cluster analysis, topic detection, and deep probabilistic modeling. This paper gives an overview of the research on latent tree analysis and various ways it is used in practice."
"Web Service is one of the most significant current discussions in information sharing technologies and one of the examples of service oriented processing. To ensure accurate execution of web services operations, it must be adaptable with policies of the social networks in which it signs up. This adaptation implements using controls called 'Commitment'. This paper describes commitments structure and existing research about commitments and social web services, then suggests an algorithm for consistency of commitments in social web services. As regards the commitments may be executed concurrently, a key challenge in web services execution based on commitment structure is consistency ensuring in execution time. The purpose of this research is providing an algorithm for consistency ensuring between web services operations based on commitments structure."
"Learning acoustic models directly from the raw waveform data with minimal processing is challenging. Current waveform-based models have generally used very few (~2) convolutional layers, which might be insufficient for building high-level discriminative features. In this work, we propose very deep convolutional neural networks (CNNs) that directly use time-domain waveforms as inputs. Our CNNs, with up to 34 weight layers, are efficient to optimize over very long sequences (e.g., vector of size 32000), necessary for processing acoustic waveforms. This is achieved through batch normalization, residual learning, and a careful design of down-sampling in the initial layers. Our networks are fully convolutional, without the use of fully connected layers and dropout, to maximize representation learning. We use a large receptive field in the first convolutional layer to mimic bandpass filters, but very small receptive fields subsequently to control the model capacity. We demonstrate the performance gains with the deeper models. Our evaluation shows that the CNN with 18 weight layers outperform the CNN with 3 weight layers by over 15% in absolute accuracy for an environmental sound recognition task and matches the performance of models using log-mel features."
"UAV control system is a huge and complex system, and to design and test a UAV control system is time-cost and money-cost. This paper considered the simulation of identification of a nonlinear system dynamics using artificial neural networks approach. This experiment develops a neural network model of the plant that we want to control. In the control design stage, experiment uses the neural network plant model to design (or train) the controller. We use Matlab to train the network and simulate the behavior. This chapter provides the mathematical overview of MRC technique and neural network architecture to simulate nonlinear identification of UAV systems. MRC provides a direct and effective method to control a complex system without an equation-driven model. NN approach provides a good framework to implement MEC by identifying complicated models and training a controller for it."
"The availability of large bandwidth at millimeter wave (mmWave) frequencies is one of the major factors that rendered very high frequencies a promising candidate enabler for fifth generation (5G) mobile communication networks. To confront with the intrinsic characteristics of signal propagation at frequencies of tens of GHz and being able to achieve data rates of the order of gigabits per second, mmWave systems are expected to employ large antenna arrays that implement highly directional beamforming. In this paper, we consider mmWave wireless systems comprising of nodes equipped with large antenna arrays and being capable of performing hybrid analog and digital (A/D) processing. Intending at realizing channel-aware transmit and receive beamforming, we focus on designing low complexity compressed sensing channel estimation. In particular, by adopting a temporally correlated mmWave channel model, we present two compressed sensing algorithms that exploit the temporal correlation to reduce the complexity of sparse channel estimation, one being greedy and the other one being iterative. Our representative performance evaluation results offer useful insights on the interplay among some system and operation parameters, and the accuracy of channel estimation."
"We present Dithen, a novel computation-as-a-service (CaaS) cloud platform specifically tailored to the parallel execution of large-scale multimedia tasks. Dithen handles the upload/download of both multimedia data and executable items, the assignment of compute units to multimedia workloads, and the reactive control of the available compute units to minimize the cloud infrastructure cost under deadline-abiding execution. Dithen combines three key properties: (i) the reactive assignment of individual multimedia tasks to available computing units according to availability and predetermined time-to-completion constraints; (ii) optimal resource estimation based on Kalman-filter estimates; (iii) the use of additive increase multiplicative decrease (AIMD) algorithms (famous for being the resource management in the transport control protocol) for the control of the number of units servicing workloads. The deployment of Dithen over Amazon EC2 spot instances is shown to be capable of processing more than 80,000 video transcoding, face detection and image processing tasks (equivalent to the processing of more than 116 GB of compressed data) for less than $1 in billing cost from EC2. Moreover, the proposed AIMD-based control mechanism, in conjunction with the Kalman estimates, is shown to provide for more than 27% reduction in EC2 spot instance cost against methods based on reactive resource estimation. Finally, Dithen is shown to offer a 38% to 500% reduction of the billing cost against the current state-of-the-art in CaaS platforms on Amazon EC2 (Amazon Lambda and Amazon Autoscale). A baseline version of Dithen is currently available at http://www.dithen.com under the ""AutoScale"" option."
"Blelloch and Farzan (2010) showed how we can represent succinctly any planar embedding of a connected planar simple graph while still supporting constant-time navigation queries, but their representation does not allow multi-edges. Other authors have shown how to represent any connected planar multigraph compactly while supporting fast navigation, but not with an arbitrary planar embedding of such a graph. In this paper we show how we can represent any given planar embedding of any connected planar (multi)graph using asymptotically four bits per edge such that later we can list the edges incident to any vertex in counter-clockwise order using constant time per edge, and determine whether two vertices are neighbours or find a vertex's degree in almost constant time."
"Age and gender are complementary soft biometric traits for face recognition. Successful estimation of age and gender from facial images taken under real-world conditions can contribute improving the identification results in the wild. In this study, in order to achieve robust age and gender classification in the wild, we have benefited from Deep Convolutional Neural Networks based representation. We have explored transferability of existing deep convolutional neural network (CNN) models for age and gender classification. The generic AlexNet-like architecture and domain specific VGG-Face CNN model are employed and fine-tuned with the Adience dataset prepared for age and gender classification in uncontrolled environments. In addition, task specific GilNet CNN model has also been utilized and used as a baseline method in order to compare with transferred models. Experimental results show that both transferred deep CNN models outperform the GilNet CNN model, which is the state-of-the-art age and gender classification approach on the Adience dataset, by an absolute increase of 7% and 4.5% in accuracy, respectively. This outcome indicates that transferring a deep CNN model can provide better classification performance than a task specific CNN model, which has a limited number of layers and trained from scratch using a limited amount of data as in the case of GilNet. Domain specific VGG-Face CNN model has been found to be more useful and provided better performance for both age and gender classification tasks, when compared with generic AlexNet-like model, which shows that transfering from a closer domain is more useful."
"We undertake an investigation of combinatorial designs engendered by cellular automata (CA), focusing in particular on orthogonal Latin squares and orthogonal arrays. The motivation is of cryptographic nature. Indeed, we consider the problem of employing CA to define threshold secret sharing schemes via orthogonal Latin squares. We first show how to generate Latin squares through bipermutive CA. Then, using a characterization based on Sylvester matrices, we prove that two linear CA induce a pair of orthogonal Latin squares if and only if the polynomials associated to their local rules are relatively prime."
"This paper proposes a data-unit-size distribution model to represent the retransmitted packet size preservation (RPSP) property in a scenario where independently lost packets are retransmitted by a stop-and-wait protocol. RPSP means that retransmitted packets with the same sequence number are equal in size to the packet of the original transmission, which is identical to the packet generated from a message through the segmentation function, namely, generated packet. Furthermore, we derive goodput formula using an approach to derive the data-unit-size distribution. We investigate the effect of RPSP on frame size distributions and goodput in a simple case when no collision happens over the bit-error prone wireless network equipped with IEEE 802.11 Distributed Coordination Function, which is a typical example of the stop-and-wait protocol. Numerical results show that the effect gets stronger as bit error rate increases and the maximum size of the generated packets is larger than the mean size for large enough packet retry limits because longer packets will be repeatedly corrupted and retransmitted more times as a result of RPSP."
"Hilbert's two-dimensional space-filling curve is appreciated for its good locality-preserving properties and easy implementation for many applications. However, Hilbert did not describe how to generalize his construction to higher dimensions. In fact, the number of ways in which this may be done ranges from zero to infinite, depending on what properties of the Hilbert curve one considers to be essential.   In this work we take the point of view that a Hilbert curve should at least be self-similar and traverse cubes octant by octant. We organize and explore the space of possible three-dimensional Hilbert curves and the potentially useful properties which they may have. We discuss a notation system that allows us to distinguish the curves from one another and enumerate them. This system has been implemented in a software prototype, available from the author's website. Several examples of possible three-dimensional Hilbert curves are presented, including a curve that visits the points on most sides of the unit cube in the order of the two-dimensional Hilbert curve; curves of which not only the eight octants are similar to each other, but also the four quarters; a curve with excellent locality-preserving properties and endpoints that are not vertices of the cube; a curve in which all but two octants are each other's images with respect to reflections in axis-parallel planes; and curves that can be sketched on a grid without using vertical line segments. In addition, we discuss several four-dimensional Hilbert curves."
"With the rapid growth of the cloud computing marketplace, the issue of pricing resources in the cloud has been the subject of much study in recent years. In this paper, we identify and study a new issue: how to price resources in the cloud so that the customer's risk is minimized, while at the same time ensuring that the provider accrues his fair share. We do this by correlating the revenue stream of the customer to the prices charged by the provider. We show that our mechanism is incentive compatible in that it is in the best interest of the customer to provide his true revenue as a function of the resources rented. We next add another restriction to the price function, i.e., that it be linear. This removes the distortion that creeps in when the customer has to pay more money for less resources. Our algorithms for both the schemes mentioned above are efficient."
"We answer a question in [Landsberg, Ressayre, 2015], showing the regular determinantal complexity of the determinant det_m is O(m^3). We answer questions in, and generalize results of [Aravind, Joglekar, 2015], showing there is no rank one determinantal expression for perm_m or det_m when m >= 3. Finally we state and prove several ""folklore"" results relating different models of computation."
"In this paper we propose cross-modal convolutional neural networks (X-CNNs), a novel biologically inspired type of CNN architectures, treating gradient descent-specialised CNNs as individual units of processing in a larger-scale network topology, while allowing for unconstrained information flow and/or weight sharing between analogous hidden layers of the network---thus generalising the already well-established concept of neural network ensembles (where information typically may flow only between the output layers of the individual networks). The constituent networks are individually designed to learn the output function on their own subset of the input data, after which cross-connections between them are introduced after each pooling operation to periodically allow for information exchange between them. This injection of knowledge into a model (by prior partition of the input data through domain knowledge or unsupervised methods) is expected to yield greatest returns in sparse data environments, which are typically less suitable for training CNNs. For evaluation purposes, we have compared a standard four-layer CNN as well as a sophisticated FitNet4 architecture against their cross-modal variants on the CIFAR-10 and CIFAR-100 datasets with differing percentages of the training data being removed, and find that at lower levels of data availability, the X-CNNs significantly outperform their baselines (typically providing a 2--6% benefit, depending on the dataset size and whether data augmentation is used), while still maintaining an edge on all of the full dataset tests."
"The advent of abundant on-board sensors and electronic devices in vehicles populates the paradigm of participatory sensing for intelligent transportation systems, by harnessing crowd-sourced data gathering to enable knowledge discovery in various applications, such as distance-to-empty prediction and eco-routing. While participatory sensing can provide various driving and vehicle data, there lacks a systematic study on effectively utilizing the data for personalized prediction applications. There are considerable challenges on how to interpolate the missing data from a sparse dataset, which often arises from participatory sensing. This paper presents and compares various personalized prediction approaches for driving energy consumption, including a blackbox approach that identifies driver/vehicle/environment-dependent factors and a collaborative filtering approach that uses matrix factorization. To evaluate the effectiveness of our approaches, a case study of distance-to-empty prediction for electric vehicles is conducted based on the participatory sensing data. Our approaches are shown to effectively improve the prediction accuracy."
"Near-infrared imaging can capture haze-free near-infrared gray images and visible color images, according to physical scattering models, e.g., Rayleigh or Mie models. However, there exist serious discrepancies in brightness and image structures between the near-infrared gray images and the visible color images. The direct use of the near-infrared gray images brings about another color distortion problem in the dehazed images. Therefore, the color distortion should also be considered for near-infrared dehazing. To reflect this point, this paper presents an approach of adding a new color regularization to conventional dehazing framework. The proposed color regularization can model the color prior for unknown haze-free images from two captured images. Thus, natural-looking colors and fine details can be induced on the dehazed images. The experimental results show that the proposed color regularization model can help remove the color distortion and the haze at the same time. Also, the effectiveness of the proposed color regularization is verified by comparing with other conventional regularizations. It is also shown that the proposed color regularization can remove the edge artifacts which arise from the use of the conventional dark prior model."
"Recently, communication systems that are both spectrum and energy efficient have attracted significant attention. Different from the existing research, we investigate the throughput and energy efficiency of a general class of multiple-input and multiple-output systems with arbitrary inputs when they are subject to statistical quality-of-service (QoS) constraints, which are imposed as limits on the delay violation and buffer overflow probabilities. We employ the effective capacity as the performance metric. We obtain the optimal input covariance matrix that maximizes the effective capacity under a short-term average power constraint. Following that, we perform an asymptotic analysis of the effective capacity in the low signal-to-noise ratio and large-scale antenna regimes. In the low signal-to-noise ratio regime analysis, we utilize the first and second derivatives of the effective capacity when the signal-to-noise ratio approaches zero in order to determine the minimum energy-per-bit and also the slope of the effective capacity versus energy-per-bit curve at the minimum energy-per-bit. We observe that the minimum energy-per-bit is independent of the input distribution, whereas the slope depends on the input distribution. In the large-scale antenna analysis, we show that the effective capacity approaches the average transmission rate in the channel with the increasing number of transmit and/or receive antennas. Particularly, the gap between the effective capacity and the average transmission rate in the channel, which is caused by the QoS constraints, is minimized with the number of antennas. In addition, we put forward the non-asymptotic backlog and delay violation bounds by utilizing the effective capacity. Finally, we substantiate our analytical results through numerical illustrations."
"In this article, we introduce the method of urban association rules and its uses for extracting frequently appearing combinations of stores that are visited together to characterize shoppers' behaviors. The Apriori algorithm is used to extract the association rules (i.e., if -> result) from customer transaction datasets in a market-basket analysis. An application to our large-scale and anonymized bank card transaction dataset enables us to output linked trips for shopping all over the city: the method enables us to predict the other shops most likely to be visited by a customer given a particular shop that was already visited as an input. In addition, our methodology can consider all transaction activities conducted by customers for a whole city in addition to the location of stores dispersed in the city. This approach enables us to uncover not only simple linked trips such as transition movements between stores but also the edge weight for each linked trip in the specific district. Thus, the proposed methodology can complement conventional research methods. Enhancing understanding of people's shopping behaviors could be useful for city authorities and urban practitioners for effective urban management. The results also help individual retailers to rearrange their services by accommodating the needs of their customers' habits to enhance their shopping experience."
"A major task in systematic reviews is abstract screening, i.e., excluding, often hundreds or thousand of, irrelevant citations returned from a database search based on titles and abstracts. Thus, a systematic review platform that can automate the abstract screening process is of huge importance. Several methods have been proposed for this task. However, it is very hard to clearly understand the applicability of these methods in a systematic review platform because of the following challenges: (1) the use of non-overlapping metrics for the evaluation of the proposed methods, (2) usage of features that are very hard to collect, (3) using a small set of reviews for the evaluation, and (4) no solid statistical testing or equivalence grouping of the methods. In this paper, we use feature representation that can be extracted per citation. We evaluate SVM-based methods (commonly used) on a large set of reviews ($61$) and metrics ($11$) to provide equivalence grouping of methods based on a solid statistical test. Our analysis also includes a strong variability of the metrics using $500$x$2$ cross validation. While some methods shine for different metrics and for different datasets, there is no single method that dominates the pack. Furthermore, we observe that in some cases relevant (included) citations can be found after screening only 15-20% of them via a certainty based sampling. A few included citations present outlying characteristics and can only be found after a very large number of screening steps. Finally, we present an ensemble algorithm for producing a $5$-star rating of citations based on their relevance. Such algorithm combines the best methods from our evaluation and through its $5$-star rating outputs a more easy-to-consume prediction."
"Subspace learning and matrix factorization problems have a great many applications in science and engineering, and efficient algorithms are critical as dataset sizes continue to grow. Many relevant problem formulations are non-convex, and in a variety of contexts it has been observed that solving the non-convex problem directly is not only efficient but reliably accurate. We discuss convergence theory for a particular method: first order incremental gradient descent constrained to the Grassmannian. The output of the algorithm is an orthonormal basis for a $d$-dimensional subspace spanned by an input streaming data matrix. We study two sampling cases: where each data vector of the streaming matrix is fully sampled, or where it is undersampled by a sampling matrix $A_t\in \R^{m\times n}$ with $m\ll n$. We propose an adaptive stepsize scheme that depends only on the sampled data and algorithm outputs. We prove that with fully sampled data, the stepsize scheme maximizes the improvement of our convergence metric at each iteration, and this method converges from any random initialization to the true subspace, despite the non-convex formulation and orthogonality constraints. For the case of undersampled data, we establish monotonic improvement on the defined convergence metric for each iteration with high probability."
"We give a strongly polynomial time algorithm which determines whether or not a bivariate polynomial is real stable. As a corollary, this implies an algorithm for testing whether a given linear transformation on univariate polynomials preserves real-rootedness. The proof exploits properties of hyperbolic polynomials to reduce real stability testing to testing nonnegativity of a finite number of polynomials on an interval."
"Automated discourse analysis tools based on Natural Language Processing (NLP) aiming at the diagnosis of language-impairing dementias generally extract several textual metrics of narrative transcripts. However, the absence of sentence boundary segmentation in the transcripts prevents the direct application of NLP methods which rely on these marks in order to function properly, such as taggers and parsers. We present the first steps taken towards automatic neuropsychological evaluation based on narrative discourse analysis, presenting a new automatic sentence segmentation method for impaired speech. Our model uses recurrent convolutional neural networks with prosodic, Part of Speech (PoS) features, and word embeddings. It was evaluated intrinsically on impaired, spontaneous speech as well as normal, prepared speech. The results suggest that our model is robust for impaired speech and can be used in automated discourse analysis tools to differentiate narratives produced by Mild Cognitive Impairment and healthy elderly patients."
"The movement of a user's face, easily detected by a smartphone's front camera, is an underexploited input modality for mobile interactions. We introduce three sets of face-engaged interaction techniques for augmenting the traditional mobile inputs, which leverages the combination of the head movements with touch gestures and device motions, all sensed via the phone's built-in sensors. We systematically present the space of design considerations for mobile interactions using one or more of the three input modalities (i.e., touch, motion, and head). The additional affordances of the proposed techniques expand the mobile interaction vocabulary, and can facilitate unique usage scenarios such as one-hand or touch-free interaction. An initial evaluation was conducted and users had positive reactions to the new techniques, indicating the promise of an intuitive and convenient user experience."
"A text network refers to a data type that each vertex is associated with a text document and the relationship between documents is represented by edges. The proliferation of text networks such as hyperlinked webpages and academic citation networks has led to an increasing demand for quickly developing a general sense of a new text network, namely text network exploration. In this paper, we address the problem of text network exploration through constructing a heterogeneous web of topics, which allows people to investigate a text network associating word level with document level. To achieve this, a probabilistic generative model for text and links is proposed, where three different relationships in the heterogeneous topic web are quantified. We also develop a prototype demo system named TopicAtlas to exhibit such heterogeneous topic web, and demonstrate how this system can facilitate the task of text network exploration. Extensive qualitative analyses are included to verify the effectiveness of this heterogeneous topic web. Besides, we validate our model on real-life text networks, showing that it preserves good performance on objective evaluation metrics."
"A broad range of linear and non-linear equalization and precoding algorithms for wideband massive multi-user (MU) multiple-input multiple-output (MIMO) wireless systems that rely on orthogonal frequency-division multiplexing (OFDM) or single-carrier frequency-division multiple access (SC-FDMA) requires the computation of the Gram matrix for each active subcarrier, which results in excessively high computational complexity. In this paper, we propose novel, approximate algorithms that reduce the complexity of Gram-matrix computation for linear equalization and precoding by exploiting correlation across subcarriers. We analytically show that a small fraction of Gram-matrix computations in combination with approximate interpolation schemes are sufficient to achieve near-optimal error-rate performance at low computational complexity in wideband massive MU-MIMO systems. We furthermore demonstrate that the proposed methods exhibit improved robustness against channel-estimation errors compared to exact Gram-matrix interpolation algorithms that typically require high computational complexity."
"Convolutional networks have marked their place over the last few years as the best performing model for various visual tasks. They are, however, most suited for supervised learning from large amounts of labeled data. Previous attempts have been made to use unlabeled data to improve model performance by applying unsupervised techniques. These attempts require different architectures and training methods. In this work we present a novel approach for unsupervised training of Convolutional networks that is based on contrasting between spatial regions within images. This criterion can be employed within conventional neural networks and trained using standard techniques such as SGD and back-propagation, thus complementing supervised methods."
"This paper introduces a novel framework for modeling temporal events with complex longitudinal dependency that are generated by dependent sources. This framework takes advantage of multidimensional point processes for modeling time of events. The intensity function of the proposed process is a mixture of intensities, and its complexity grows with the complexity of temporal patterns of data. Moreover, it utilizes a hierarchical dependent nonparametric approach to model marks of events. These capabilities allow the proposed model to adapt its temporal and topical complexity according to the complexity of data, which makes it a suitable candidate for real world scenarios. An online inference algorithm is also proposed that makes the framework applicable to a vast range of applications. The framework is applied to a real world application, modeling the diffusion of contents over networks. Extensive experiments reveal the effectiveness of the proposed framework in comparison with state-of-the-art methods."
"In everyday life. Technological advancement can be found in many facets of life, including personal computers, mobile devices, wearables, cloud services, video gaming, web-powered messaging, social media, Internet-connected devices, etc. This technological influence has resulted in these technologies being employed by criminals to conduct a range of crimes -- both online and offline. Both the number of cases requiring digital forensic analysis and the sheer volume of information to be processed in each case has increased rapidly in recent years. As a result, the requirement for digital forensic investigation has ballooned, and law enforcement agencies throughout the world are scrambling to address this demand. While more and more members of law enforcement are being trained to perform the required investigations, the supply is not keeping up with the demand. Current digital forensic techniques are arduously time-consuming and require a significant amount of man power to execute. This paper discusses a novel solution to combat the digital forensic backlog. This solution leverages a deduplication-based paradigm to eliminate the reacquisition, redundant storage, and reanalysis of previously processed data."
"Seismic full-waveform inversion tries to estimate subsurface medium parameters from seismic data. Areas with subsurface salt bodies are of particular interest because they often have hydrocarbon reservoirs on their sides or underneath. Accurate reconstruction of their geometry is a challenge for current techniques. This paper presents a parametric level-set method for the reconstruction of salt-bodies in seismic full-waveform inversion. We split the subsurface model in two parts: a background velocity model and the salt body with known velocity but undetermined shape. The salt geometry is represented by a level-set function that evolves during the inversion. We choose radial basis functions to represent the level-set function, leading to an optimization problem with a modest number of parameters. A common problem with level-set methods is to fine tune the width of the level-set boundary for optimal sensitivity. We propose a robust algorithm that dynamically adapts the width of the level-set boundary to ensure faster convergence. Tests on a suite of idealized salt geometries show that the proposed method is stable against a modest amount of noise. We also extend the method to joint inversion of both the background velocity model and the salt-geometry."
"Coordination is essential for dynamic distributed systems whose components exhibit interactive and autonomous behaviors. Spatially distributed, locally interacting, propagating computational fields are particularly appealing for allowing components to join and leave with little or no overhead. Computational fields are a key ingredient of aggregate programming, a promising software engineering methodology particularly relevant for the Internet of Things. In our approach, space topology is represented by a fixed graph-shaped field, namely a network with attributes on both nodes and arcs, where arcs represent interaction capabilities between nodes. We propose a SMuC calculus where mu-calculus- like modal formulas represent how the values stored in neighbor nodes should be combined to update the present node. Fixpoint operations can be understood globally as recursive definitions, or locally as asynchronous converging propagation processes. We present a distributed implementation of our calculus. The translation is first done mapping SMuC programs into normal form, purely iterative programs and then into distributed programs. Some key results are presented that show convergence of fixpoint computations under fair asynchrony and under reinitialization of nodes. The first result allows nodes to proceed at different speeds, while the second one provides robustness against certain kinds of failure. We illustrate our approach with a case study based on a disaster recovery scenario, implemented in a prototype simulator that we use to evaluate the performance of a recovery strategy."
"A so-called maximum correntropy filtering methodology has been recently developed for estimating a hidden state of stochastic dynamic systems in the presence of non-Gaussian noise. It allows for extracting a higher order information than the second order statistics as in the classical Kalman filter, and is shown to be robust against outliers or impulsive noises. This paper is concerned with a numerical stability (with respect to roundoff errors) of the recently designed maximum correntropy criterion Kalman filter (MCC-KF). Previous works on the filter derivation yield the conventional implementation that is given in the stabilized Joseph form. The purpose of this technical note is two-fold. First, we revise the previously proposed MCC-KF implementation and reveal an inconsistency in the suggested filtering equations. We resolve it here by proving equality conditions for the filter and, then, present a {\it consistent} variant of the MCC-KF. Second, we design some new array square-root algorithms that are inherently more stable than the conventional or Joseph filter implementations. All algorithms are given in terms of covariance quantities. They are demonstrated to outperform the previously proposed MCC-KF method in two numerical examples."
"In this letter, a novel weighted ensemble classifier is proposed that improves classification accuracy and minimizes the number of classifiers. Ensemble weight finding problem is modeled as a cost function with following terms: (a) a data fidelity term aiming to decrease misclassification rate, (b) a sparsity term aiming to decrease the number of classifiers, and (c) a non-negativity constraint on the weights of the classifiers. The proposed cost function is a non-convex and hard to solve; thus, convex relaxation techniques and novel approximations are employed to obtain a numerically efficient solution. The proposed method achieves better or similar performance compared to state-of-the art classifier ensemble methods, while using lower number of classifiers."
"This paper describes the extension and optimization of our previous work on very deep convolutional neural networks (CNNs) for effective recognition of noisy speech in the Aurora 4 task. The appropriate number of convolutional layers, the sizes of the filters, pooling operations and input feature maps are all modified: the filter and pooling sizes are reduced and dimensions of input feature maps are extended to allow adding more convolutional layers. Furthermore appropriate input padding and input feature map selection strategies are developed. In addition, an adaptation framework using joint training of very deep CNN with auxiliary features i-vector and fMLLR features is developed. These modifications give substantial word error rate reductions over the standard CNN used as baseline. Finally the very deep CNN is combined with an LSTM-RNN acoustic model and it is shown that state-level weighted log likelihood score combination in a joint acoustic model decoding scheme is very effective. On the Aurora 4 task, the very deep CNN achieves a WER of 8.81%, further 7.99% with auxiliary feature joint training, and 7.09% with LSTM-RNN joint decoding."
"In this paper, we show an approach to build deep learning algorithms for recognizing signals in distributed fiber optic monitoring and security systems for long perimeters. Synthesizing such detection algorithms poses a non-trivial research and development challenge, because these systems face stringent error (type I and II) requirements and operate in difficult signal-jamming environments, with intensive signal-like jamming and a variety of changing possible signal portraits of possible recognized events. To address these issues, we have developed a twolevel event detection architecture, where the primary classifier is based on an ensemble of deep convolutional networks, can recognize 7 classes of signals and receives time-space data frames as input. Using real-life data, we have shown that the applied methods result in efficient and robust multiclass detection algorithms that have a high degree of adaptability."
"We propose Flock; a simple and scalable protocol that enables live migration of Virtual Machines (VMs) across heterogeneous edge and conventional cloud platforms to improve the responsiveness of cloud services. Flock is designed with properties that are suitable for the use cases of the Internet of Things (IoT). We describe the properties of regularized latency measurements that Flock can use for asynchronous and autonomous migration decisions. Such decisions allow communicating VMs to follow a flocking-like behavior that consists of three simple rules: separation, alignment, and cohesion. Using game theory, we derive analytical bounds on Flock's Price of Anarchy (PoA), and prove that flocking VMs converge to a Nash Equilibrium while settling in the best possible cloud platforms. We verify the effectiveness of Flock through simulations and discuss how its generic objective can simply be tweaked to achieve other objectives, such as cloud load balancing and energy consumption minimization."
"We present a novel method for constructing Variational Autoencoder (VAE). Instead of using pixel-by-pixel loss, we enforce deep feature consistency between the input and the output of a VAE, which ensures the VAE's output to preserve the spatial correlation characteristics of the input, thus leading the output to have a more natural visual appearance and better perceptual quality. Based on recent deep learning works such as style transfer, we employ a pre-trained deep convolutional neural network (CNN) and use its hidden features to define a feature perceptual loss for VAE training. Evaluated on the CelebA face dataset, we show that our model produces better results than other methods in the literature. We also show that our method can produce latent vectors that can capture the semantic information of face expressions and can be used to achieve state-of-the-art performance in facial attribute prediction."
"In this article, we consider a collection of geometric problems involving points colored by two colors (red and blue), referred to as bichromatic problems. The motivation behind studying these problems is two fold; (i) these problems appear naturally and frequently in the fields like Machine learning, Data mining, and so on, and (ii) we are interested in extending the algorithms and techniques for single point set (monochromatic) problems to bichromatic case. For all the problems considered in this paper, we design low polynomial time exact algorithms. These algorithms are based on novel techniques which might be of independent interest."
"Most of the crowd abnormal event detection methods rely on complex hand-crafted features to represent the crowd motion and appearance. Convolutional Neural Networks (CNN) have shown to be a powerful tool with excellent representational capacities, which can leverage the need for hand-crafted features. In this paper, we show that keeping track of the changes in the CNN feature across time can facilitate capturing the local abnormality. We specifically propose a novel measure-based method which allows measuring the local abnormality in a video by combining semantic information (inherited from existing CNN models) with low-level Optical-Flow. One of the advantage of this method is that it can be used without the fine-tuning costs. The proposed method is validated on challenging abnormality detection datasets and the results show the superiority of our method compared to the state-of-the-art methods."
"We assign binary and ternary error-correcting codes to the data of syntactic structures of world languages and we study the distribution of code points in the space of code parameters. We show that, while most codes populate the lower region approximating a superposition of Thomae functions, there is a substantial presence of codes above the Gilbert-Varshamov bound and even above the asymptotic bound and the Plotkin bound. We investigate the dynamics induced on the space of code parameters by spin glass models of language change, and show that, in the presence of entailment relations between syntactic parameters the dynamics can sometimes improve the code. For large sets of languages and syntactic data, one can gain information on the spin glass dynamics from the induced dynamics in the space of code parameters."
"Content-based medical image retrieval can support diagnostic decisions by clinical experts. Examining similar images may provide clues to the expert to remove uncertainties in his/her final diagnosis. Beyond conventional feature descriptors, binary features in different ways have been recently proposed to encode the image content. A recent proposal is ""Radon barcodes"" that employ binarized Radon projections to tag/annotate medical images with content-based binary vectors, called barcodes. In this paper, MinMax Radon barcodes are introduced which are superior to ""local thresholding"" scheme suggested in the literature. Using IRMA dataset with 14,410 x-ray images from 193 different classes, the advantage of using MinMax Radon barcodes over \emph{thresholded} Radon barcodes are demonstrated. The retrieval error for direct search drops by more than 15\%. As well, SURF, as a well-established non-binary approach, and BRISK, as a recent binary method are examined to compare their results with MinMax Radon barcodes when retrieving images from IRMA dataset. The results demonstrate that MinMax Radon barcodes are faster and more accurate when applied on IRMA images."
"Medical images can be a valuable resource for reliable information to support medical diagnosis. However, the large volume of medical images makes it challenging to retrieve relevant information given a particular scenario. To solve this challenge, content-based image retrieval (CBIR) attempts to characterize images (or image regions) with invariant content information in order to facilitate image search. This work presents a feature extraction technique for medical images using stacked autoencoders, which encode images to binary vectors. The technique is applied to the IRMA dataset, a collection of 14,410 x-ray images in order to demonstrate the ability of autoencoders to retrieve similar x-rays given test queries. Using IRMA dataset as a benchmark, it was found that stacked autoencoders gave excellent results with a retrieval error of 376 for 1,733 test images with a compression of 74.61%."
"To reduce the potential radiation risk, low-dose CT has attracted much attention. However, simply lowering the radiation dose will lead to significant deterioration of the image quality. In this paper, we propose a noise reduction method for low-dose CT via deep neural network without accessing original projection data. A deep convolutional neural network is trained to transform low-dose CT images towards normal-dose CT images, patch by patch. Visual and quantitative evaluation demonstrates a competing performance of the proposed method."
"A polyomino is said to be L-convex if any two of its cells are connected by a 4-connected inner path that changes direction at most once. The 2-dimensional language representing such polyominoes has been recently proved to be recognizable by tiling systems by S. Brocchi, A. Frosini, R. Pinzani and S. Rinaldi. In an attempt to compare recognition power of tiling systems and cellular automata, we have proved that this language can be recognized by 2-dimensional cellular automata working on the von Neumann neighborhood in real time.   Although the construction uses a characterization of L-convex polyominoes that is similar to the one used for tiling systems, the real time constraint which has no equivalent in terms of tilings requires the use of techniques that are specific to cellular automata."
"We explore techniques to significantly improve the compute efficiency and performance of Deep Convolution Networks without impacting their accuracy. To improve the compute efficiency, we focus on achieving high accuracy with extremely low-precision (2-bit) weight networks, and to accelerate the execution time, we aggressively skip operations on zero-values. We achieve the highest reported accuracy of 76.6% Top-1/93% Top-5 on the Imagenet object classification challenge with low-precision network\footnote{github release of the source code coming soon} while reducing the compute requirement by ~3x compared to a full-precision network that achieves similar accuracy. Furthermore, to fully exploit the benefits of our low-precision networks, we build a deep learning accelerator core, dLAC, that can achieve up to 1 TFLOP/mm^2 equivalent for single-precision floating-point operations (~2 TFLOP/mm^2 for half-precision)."
"The operation of most signalized intersections is governed by predefined timing plans that are applied during specified times of the day. These plans are designed to accommodate average conditions and are unable to respond to large deviations in traffic flow. We propose a control approach that adjusts time-of-day signaling plans based on a prediction of future traffic flow. The prediction algorithm identifies correlated, low rank structure in historical measurement data and predicts future traffic flow from real-time measurements by determining which structural trends are prominent in the measurements. From this prediction, the controller then determines the optimal time of day to apply new timing plans. We demonstrate the potential benefits of this approach using eight months of high resolution data collected at an intersection in Beaufort, South Carolina."
"We study the influence of the dimension of cellular automata (CA) for real time language recognition of one-dimensional languages with parallel input. Specifically, we focus on the question of determining whether every language that can be recognized in real time on a 2-dimensional CA working on the Moore neighborhood can also be recognized in real time by a 1-dimensional CA working on the standard two-way neighborhood.   We show that 2-dimensional CA in real time can perform a linear number of simulations of a 1-dimensional real time CA. If the two classes are equal then the number of simulated instances can be polynomial."
"We study two-dimensional rotation-symmetric number-conserving cellular automata working on the von Neumann neighborhood (RNCA). It is known that such automata with 4 states or less are trivial, so we investigate the possible rules with 5 states. We give a full characterization of these automata and show that they cannot be strongly Turing universal. However, we give example of constructions that allow to embed some boolean circuit elements in a 5-states RNCA."
"Linear acceleration theorems are known for most computational models. Although such results have been proved for two-dimensional cellular automata working on specific neighborhoods, no general construction was known. We present here a technique of linear acceleration for all two-dimensional languages recognized by cellular automata working on complete neighborhoods."
"The estimation of probability densities based on available data is a central task in many statistical applications. Especially in the case of large ensembles with many samples or high-dimensional sample spaces, computationally efficient methods are needed. We propose a new method that is based on a decomposition of the distribution to be estimated in terms of so-called distribution elements (DEs). These elements enable an adaptive and hierarchical discretization of the sample space with small or large elements in regions with high and variable or low densities, respectively. The refinement strategy that we propose is based on statistical goodness-of-fit and independence tests that evaluate the local approximation of the distribution in terms of DEs. The capabilities of our new method are inspected based on several low and high-dimensional examples."
"This paper focuses on the design of a stabilizing control law for an aerial vehicle which is physically connected to a ground station by means of a tether cable. By taking advantage of the tensile force acting along the taut cable, it is shown that the tethered UAV is able to maintain a non-zero attitude while hovering in a constant position. The control objective is to stabilize the desired configuration while simultaneously ensuring that the cable remains taut at all times. This leads to a nonlinear control problem subject to constraints. This paper provides a two-step solution. First, the system is stabilized using a cascade control scheme based on thrust vectoring. Then, constraint satisfaction is guaranteed using a novel Reference Governor scheme."
"We present a polynomial-sized linear program (LP) for the n-city TSP drawing upon ""complex flow"" modeling ideas by the first two authors who used an O(n^9)xO(n^8) model*. Here we have only O(n^5) variables and O(n^4) constraints. We do not model explicit cycles of the cities, and our modeling does not involve the city-to-city variables-based, traditional TSP polytope referred to in the literature as ""The TSP Polytope."" Optimal TSP objective value and tours are achieved by solving our proposed LP. In the case of a unique optimum, the integral solution representing the optimal tour is obtained using any LP solver (solution algorithm). In the case of alternate optima, an LP solver (e.g., an interior-point solver) may stop with a fractional (interior-point) solution, which (we prove) is a convex combination of alternate optimal TSP tours. In such cases, one of the optimal tours can be trivially retrieved from the solution using a simple iterative elimination procedure we propose. We have solved over a million problems with up to 27 cities using the barrier methods of CPLEX, consistently obtaining all integer solutions. Since LP is solvable in polynomial time and we have a model which is of polynomial size in n, the paper is thus offering (although, incidentally) a proof of the equality of the computational complexity classes ""P"" and ""NP"". The non-applicability and numerical refutations of existing extended formulations results (such as Braun et al. (2015) or Fiorini et al. (2015) in particular) are briefly discussed in an appendix. [*: Advances in Combinatorial Optimization: Linear Programming Formulation of the Traveling Salesman and Other Hard Combinatorial Optimization Problems (World Scientific, January 2016).]"
"The Internet Protocol (IP) is the lifeblood of the modern Internet. Its simplicity and universality have fueled the unprecedented and lasting global success of the current Internet. Nonetheless, some limitations of IP have been emerging in recent years. Its original design envisaged supporting perhaps tens of thousands of static hosts operating in a friendly academic-like setting, mainly in order to facilitate email communication and remote access to scarce computing resources. At present IP interconnects billions of static and mobile devices (ranging from supercomputers to IoT gadgets) with a large and dynamic set of popular applications. Starting in mid-1990s, the advent of mobility, wirelessness and the web substantially shifted Internet usage and communication paradigms. This accentuated long-term concerns about the current Internet architecture and prompted interest in alternative designs.   The U.S. National Science Foundation (NSF) has been one of the key supporters of efforts to design a set of candidate next-generation Internet architectures. As a prominent design requirement, NSF emphasized ""security and privacy by design"" in order to avoid the long and unhappy history of incremental patching and retrofitting that characterizes the current Internet architecture. To this end, as a result of a competitive process, four prominent research projects were funded by the NSF in 2010: Nebula, Named-Data Networking (NDN), MobilityFirst (MF), and Expressive Internet Architecture (XIA). This paper provides a comprehensive and neutral analysis of salient security and privacy features (and issues) in these NSF-funded Future Internet Architectures. It also compares the four candidate designs with the current IP-based architecture and discusses similarities, differences, and possible improvements."
"Given $n$ line segments in the plane, do they form the edge set of a \emph{weakly simple polygon}; that is, can the segment endpoints be perturbed by at most $\varepsilon$, for any $\varepsilon>0$, to obtain a simple polygon? While the analogous question for \emph{simple polygons} can easily be answered in $O(n\log n)$ time, we show that it is NP-complete for weakly simple polygons. We give $O(n)$-time algorithms in two special cases: when all segments are collinear, or the segment endpoints are in general position. These results extend to the variant in which the segments are \emph{directed}, and the counterclockwise traversal of a polygon should follow the orientation.   We study related problems for the case that the union of the $n$ input segments is connected. (i) If each segment can be subdivided into several segments, find the minimum number of subdivision points to form a weakly simple polygon. (ii) If new line segments can be added, find the minimum total length of new segments that creates a weakly simple polygon. We give worst-case upper and lower bounds for both problems."
"Demand response is designed to motivate electricity customers to modify their loads at critical time periods. The accurate estimation of impact of demand response signals to customers' consumption is central to any successful program. In practice, learning these response is nontrivial because operators can only send a limited number of signals. In addition, customer behavior also depends on a large number of exogenous covariates. These two features lead to a high dimensional inference problem with limited number of observations. In this paper, we formulate this problem by using a multivariate linear model and adopt an experimental design approach to estimate the impact of demand response signals. We show that randomized assignment, which is widely used to estimate the average treatment effect, is not efficient in reducing the variance of the estimator when a large number of covariates is present. In contrast, we present a tractable algorithm that strategically assigns demand response signals to customers. This algorithm achieves the optimal reduction in estimation variance, independent of the number of covariates. The results are validated from simulations on synthetic data."
"Bayesian optimization has become a fundamental global optimization algorithm in many problems where sample efficiency is of paramount importance. Recently, there has been proposed a large number of new applications in fields such as robotics, machine learning, experimental design, simulation, etc. In this paper, we focus on several problems that appear in robotics and autonomous systems: algorithm tuning, automatic control and intelligent design. All those problems can be mapped to global optimization problems. However, they become hard optimization problems. Bayesian optimization internally uses a probabilistic surrogate model (e.g.: Gaussian process) to learn from the process and reduce the number of samples required. In order to generalize to unknown functions in a black-box fashion, the common assumption is that the underlying function can be modeled with a stationary process. Nonstationary Gaussian process regression cannot generalize easily and it typically requires prior knowledge of the function. Some works have designed techniques to generalize Bayesian optimization to nonstationary functions in an indirect way, but using techniques originally designed for regression, where the objective is to improve the quality of the surrogate model everywhere. Instead optimization should focus on improving the surrogate model near the optimum. In this paper, we present a novel kernel function specially designed for Bayesian optimization, that allows nonstationary behavior of the surrogate model in an adaptive local region. In our experiments, we found that this new kernel results in an improved local search (exploitation), without penalizing the global search (exploration). We provide results in well-known benchmarks and real applications. The new method outperforms the state of the art in Bayesian optimization both in stationary and nonstationary problems."
"Security and privacy are major concerns in modern communication networks. In recent years, the information theory of covert communications, where the very presence of the communication is undetectable to a watchful and determined adversary, has been of great interest. This emerging body of work has focused on additive white Gaussian noise (AWGN), discrete memoryless channels (DMCs), and optical channels. In contrast, our recent work introduced the information-theoretic limits for covert communications over packet channels whose packet timings are governed by a Poisson point process. However, actual network packet arrival times do not generally conform to the Poisson process assumption, and thus here we consider the extension of our work to timing channels characterized by more general renewal processes of rate $\lambda$. We consider two scenarios. In the first scenario, the source of the packets on the channel cannot be authenticated by Willie, and therefore Alice can insert packets into the channel. We show that if the total number of transmitted packets by Jack is $N$, Alice can covertly insert $\mathcal{O}\left(\sqrt{N}\right)$ packets and, if she transmits more, she will be detected by Willie. In the second scenario, packets are authenticated by Willie but we assume that Alice and Bob share a secret key; hence, Alice alters the timings of the packets according to a pre-shared codebook with Bob to send information to him over a $G/M/1$ queue with service rate $\mu>\lambda$. We show that Alice can covertly and reliably transmit $\mathcal{O}(N)$ bits to Bob when the total number of packets sent from Jack to Steve is $N$."
"Sentiment Analysis (SA) is an action research area in the digital age. With rapid and constant growth of online social media sites and services, and the increasing amount of textual data such as - statuses, comments, reviews etc. available in them, application of automatic SA is on the rise. However, most of the research works on SA in natural language processing (NLP) are based on English language. Despite being the sixth most widely spoken language in the world, Bangla still does not have a large and standard dataset. Because of this, recent research works in Bangla have failed to produce results that can be both comparable to works done by others and reusable as stepping stones for future researchers to progress in this field. Therefore, we first tried to provide a textual dataset - that includes not just Bangla, but Romanized Bangla texts as well, is substantial, post-processed and multiple validated, ready to be used in SA experiments. We tested this dataset in Deep Recurrent model, specifically, Long Short Term Memory (LSTM), using two types of loss functions - binary crossentropy and categorical crossentropy, and also did some experimental pre-training by using data from one validation to pre-train the other and vice versa. Lastly, we documented the results along with some analysis on them, which were promising."
"Myasnikov et al. have introduced the knapsack problem for arbitrary finitely generated groups. In previous work, the authors proved that for each graph group, the knapsack problem can be solved in $\mathsf{NP}$. Here, we determine the exact complexity of the problem for every graph group. While the problem is $\mathsf{TC}^0$-complete for complete graphs, it is $\mathsf{LogCFL}$-complete for each (non-complete) transitive forest. For every remaining graph, the problem is $\mathsf{NP}$-complete."
"A number of attempts have been made to improve accuracy and/or scalability of the PC (Peter and Clark) algorithm, some well known (Buhlmann, et al., 2010; Kalisch and Buhlmann, 2007; 2008; Zhang, 2012, to give some examples). We add here one more tool to the toolbox: the simple observation that if one is forced to choose between a variety of possible conditioning sets for a pair of variables, one should choose the one with the highest p-value. One can use the CPC (Conservative PC, Ramsey et al., 2012) algorithm as a guide to possible sepsets for a pair of variables. However, whereas CPC uses a voting rule to classify colliders versus noncolliders, our proposed algorithm, PC-Max, picks the conditioning set with the highest p-value, so that there are no ambiguities. We combine this with two other optimizations: (a) avoiding bidirected edges in the orientation of colliders, and (b) parallelization. For (b) we borrow ideas from the PC-Stable algorithm (Colombo and Maathuis, 2014). The result is an algorithm that scales quite well both in terms of accuracy and time, with no risk of bidirected edges."
"Consider a channel where authorized transmitter Jack sends packets to authorized receiver Steve according to a Poisson process with rate $\lambda$ packets per second for a time period $T$. Suppose that covert transmitter Alice wishes to communicate information to covert receiver Bob on the same channel without being detected by a watchful adversary Willie. We consider two scenarios. In the first scenario, we assume that warden Willie cannot look at packet contents but rather can only observe packet timings, and Alice must send information by inserting her own packets into the channel. We show that the number of packets that Alice can covertly transmit to Bob is on the order of the square root of the number of packets that Jack transmits to Steve; conversely, if Alice transmits more than that, she will be detected by Willie with high probability. In the second scenario, we assume that Willie can look at packet contents but that Alice can communicate across an $M/M/1$ queue to Bob by altering the timings of the packets going from Jack to Steve. First, Alice builds a codebook, with each codeword consisting of a sequence of packet timings to be employed for conveying the information associated with that codeword. However, to successfully employ this codebook, Alice must always have a packet to send at the appropriate time. Hence, leveraging our result from the first scenario, we propose a construction where Alice covertly slows down the packet stream so as to buffer packets to use during a succeeding codeword transmission phase. Using this approach, Alice can covertly and reliably transmit $\mathcal{O}(\lambda T)$ covert bits to Bob in time period $T$ over an $M/M/1$ queue with service rate $\mu > \lambda$."
"Near-infrared gray images captured together with corresponding visible color images have recently proven useful for image restoration and classification. This paper introduces a new coloring method to add colors to near-infrared gray images based on a contrast-preserving mapping model. A naive coloring method directly adds the colors from the visible color image to the near-infrared gray image; however, this method results in an unrealistic image because of the discrepancies in brightness and image structure between the captured near-infrared gray image and the visible color image. To solve the discrepancy problem, first we present a new contrast-preserving mapping model to create a new near-infrared gray image with a similar appearance in the luminance plane to the visible color image, while preserving the contrast and details of the captured near-infrared gray image. Then based on the proposed contrast-preserving mapping model, we develop a method to derive realistic colors that can be added to the newly created near-infrared gray image. Experimental results show that the proposed method can not only preserve the local contrasts and details of the captured near-infrared gray image, but transfers the realistic colors from the visible color image to the newly created near-infrared gray image. Experimental results also show that the proposed approach can be applied to near-infrared denoising."
"Covert communication, also known as low probability of detection (LPD) communication, prevents the adversary from knowing that a communication is taking place. Recent work has demonstrated that, in a three-party scenario with a transmitter (Alice), intended recipient (Bob), and adversary (Warden Willie), the maximum number of bits that can be transmitted reliably from Alice to Bob without detection by Willie, when additive white Gaussian noise (AWGN) channels exist between all parties, is on the order of the square root of the number of channel uses. In this paper, we begin consideration of network scenarios by studying the case where there are additional ""friendly"" nodes present in the environment that can produce artificial noise to aid in hiding the communication. We establish achievability results by considering constructions where the system node closest to the warden produces artificial noise and demonstrate a significant improvement in the throughput achieved covertly, without requiring close coordination between Alice and the noise-generating node. Conversely, under mild restrictions on the communication strategy, we demonstrate no higher covert throughput is possible. Extensions to the consideration of the achievable covert throughput when multiple wardens randomly located in the environment collaborate to attempt detection of the transmitter are also considered."
"This paper introduces a new rain removal model based on the shrinkage of the sparse codes for a single image. Recently, dictionary learning and sparse coding have been widely used for image restoration problems. These methods can also be applied to the rain removal by learning two types of rain and non-rain dictionaries and forcing the sparse codes of the rain dictionary to be zero vectors. However, this approach can generate unwanted edge artifacts and detail loss in the non-rain regions. Based on this observation, a new approach for shrinking the sparse codes is presented in this paper. To effectively shrink the sparse codes in the rain and non-rain regions, an error map between the input rain image and the reconstructed rain image is generated by using the learned rain dictionary. Based on this error map, both the sparse codes of rain and non-rain dictionaries are used jointly to represent the image structures of objects and avoid the edge artifacts in the non-rain regions. In the rain regions, the correlation matrix between the rain and non-rain dictionaries is calculated. Then, the sparse codes corresponding to the highly correlated signal-atoms in the rain and non-rain dictionaries are shrunk jointly to improve the removal of the rain structures. The experimental results show that the proposed shrinkage-based sparse coding can preserve image structures and avoid the edge artifacts in the non-rain regions, and it can remove the rain structures in the rain regions. Also, visual quality evaluation confirms that the proposed method outperforms the conventional texture and rain removal methods."
"Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively."
"We introduce the {\em polygon cloud}, also known as a polygon set or {\em soup}, as a compressible representation of 3D geometry (including its attributes, such as color texture) intermediate between polygonal meshes and point clouds. Dynamic or time-varying polygon clouds, like dynamic polygonal meshes and dynamic point clouds, can take advantage of temporal redundancy for compression, if certain challenges are addressed. In this paper, we propose methods for compressing both static and dynamic polygon clouds, specifically triangle clouds. We compare triangle clouds to both triangle meshes and point clouds in terms of compression, for live captured dynamic colored geometry. We find that triangle clouds can be compressed nearly as well as triangle meshes, while being far more robust to noise and other structures typically found in live captures, which violate the assumption of a smooth surface manifold, such as lines, points, and ragged boundaries. We also find that triangle clouds can be used to compress point clouds with significantly better performance than previously demonstrated point cloud compression methods."
"Images are formed by counting how many photons traveling from a given set of directions hit an image sensor during a given time interval. When photons are few and far in between, the concept of `image' breaks down and it is best to consider directly the flow of photons. Computer vision in this regime, which we call `scotopic', is radically different from the classical image-based paradigm in that visual computations (classification, control, search) have to take place while the stream of photons is captured and decisions may be taken as soon as enough information is available. The scotopic regime is important for biomedical imaging, security, astronomy and many other fields. Here we develop a framework that allows a machine to classify objects with as few photons as possible, while maintaining the error rate below an acceptable threshold. A dynamic and asymptotically optimal speed-accuracy tradeoff is a key feature of this framework. We propose and study an algorithm to optimize the tradeoff of a convolutional network directly from lowlight images and evaluate on simulated images from standard datasets. Surprisingly, scotopic systems can achieve comparable classification performance as traditional vision systems while using less than 0.1% of the photons in a conventional image. In addition, we demonstrate that our algorithms work even when the illuminance of the environment is unknown and varying. Last, we outline a spiking neural network coupled with photon-counting sensors as a power-efficient hardware realization of scotopic algorithms."
"In Fourier-based medical imaging, sampling below the Nyquist rate results in an underdetermined system, in which linear reconstructions will exhibit artifacts. Another consequence of under-sampling is lower signal to noise ratio (SNR) due to fewer acquired measurements. Even if an oracle provided the information to perfectly disambiguate the underdetermined system, the reconstructed image could still have lower image quality than a corresponding fully sampled acquisition because of the reduced measurement time. The effects of lower SNR and the underdetermined system are coupled during reconstruction, making it difficult to isolate the impact of lower SNR on image quality. To this end, we present an image quality prediction process that reconstructs fully sampled, fully determined data with noise added to simulate the loss of SNR induced by a given under-sampling pattern. The resulting prediction image empirically shows the effect of noise in under-sampled image reconstruction without any effect from an underdetermined system.   We discuss how our image quality prediction process can simulate the distribution of noise for a given under-sampling pattern, including variable density sampling that produces colored noise in the measurement data. An interesting consequence of our prediction model is that we can show that recovery from underdetermined non-uniform sampling is equivalent to a weighted least squares optimization that accounts for heterogeneous noise levels across measurements.   Through a series of experiments with synthetic and in vivo datasets, we demonstrate the efficacy of the image quality prediction process and show that it provides a better estimation of reconstruction image quality than the corresponding fully-sampled reference image."
"This letter proposes a simple method of transferring rain structures of a given exemplar rain image into a target image. Given the exemplar rain image and its corresponding masked rain image, rain patches including rain structures are extracted randomly, and then residual rain patches are obtained by subtracting those rain patches from their mean patches. Next, residual rain patches are selected randomly, and then added to the given target image along a raster scanning direction. To decrease boundary artifacts around the added patches on the target image, minimum error boundary cuts are found using dynamic programming, and then blending is conducted between overlapping patches. Our experiment shows that the proposed method can generate realistic rain images that have similar rain structures in the exemplar images. Moreover, it is expected that the proposed method can be used for rain removal. More specifically, natural images and synthetic rain images generated via the proposed method can be used to learn classifiers, for example, deep neural networks, in a supervised manner."
"This paper presents the verified methodology for wireless authentication system using infrared barcode based scanner. An alternate approach of scanning a barcode using an infrared communication is implemented here, comprises of a card on which a barcode is printed is illuminated with an infrared beam. The reflections from the card are received by an infrared receiver and transmit them to a remote machine over wireless channel where the retrieve information is further processed and converted into the corresponding barcode digits which are further matched against the database stored in the server and the authenticity of the user is established. The system developed is cost-effective, efficient, provide double security and robust."
"Many real-world problems involving constraints can be regarded as instances of the Max-SAT problem, which is the optimization variant of the classic satisfiability problem. In this paper, we propose a novel probabilistic approach for Max-SAT called ProMS. Our algorithm relies on a stochastic local search strategy using a novel probability distribution function with two strategies for picking variables, one based on available information and another purely random one. Moreover, while most previous algorithms based on WalkSAT choose unsatisfied clauses randomly, we introduce a novel clause selection strategy to improve our algorithm. Experimental results illustrate that ProMS outperforms many state-of-the-art stochastic local search solvers on hard unweighted random Max-SAT benchmarks."
"We study algebraic synchronization trees, i.e., initial solutions of algebraic recursion schemes over the continuous categorical algebra of synchronization trees. In particular, we investigate the relative expressive power of algebraic recursion schemes over two signatures, which are based on those for Basic CCS and Basic Process Algebra, as a means for defining synchronization trees up to isomorphism as well as modulo bisimilarity and language equivalence. The expressiveness of algebraic recursion schemes is also compared to that of the low levels in Caucal's pushdown hierarchy."
"In Wireless Sensor Networks, sensors are used for tracking objects, monitoring health and observing a region/territory for different environmental parameters. Coverage problem in sensor network ensures quality of monitoring a given region. Depending on applications different measures of coverage are there. Barrier coverage is a type of coverage, which ensures all paths that cross the boundary of a region intersect at least one sensor's sensing region. The goal of the sensors is to detect intruders as they cross the boundary or as they penetrate a protected area. The sensors are dependent on their battery life. Restoring barrier coverage on sensor failure using mobile sensors with minimum total displacement is the primary objective of this paper. A centralized barrier coverage restoring scheme is proposed to increase the robustness of the network. We formulate restoring barrier coverage as bipartite matching problem. A distributed restoring of barrier coverage algorithm is also proposed, which restores it by first finding existing alternate barrier. If alternate barrier is not found, an alternate barrier is reconstructed by shifting existing sensors in a cascaded manner. Detailed simulation results are shown to evaluate the performance of our algorithms."
"The recent development of smartphone and wearable sensor technologies enable general public to carry self-tracking tasks more easily. Much work has been devoted to life data collection and visualisation to help people with better self-understanding. We believe that although (awareness/knowledge discovery is an important aspect of personal informatics, knowledge maintenance is more, or at least equally, important. In this paper, we propose a proactive approach that uses the knowledge mined from people's activity data to nudge them towards a good lifestyle (better knowledge maintenance). For demonstration purpose, a trial study was designed and implemented for good sleep maintenance. In the study, we first use smartphones as activity trackers to collect various features in a non-intrusive manner. We then use those data to learn users' activity patterns, including daily step amount, app usages, bedding time, wake-up time and sleep duration. Subsequently, we analyse correlations that may have the positive or negative impact on users' sleep qualities and finally we designed and implemented three proactive services that are able to generate customised advice in the ""right"" context to nudge users towards a better lifestyle. The experiments results are positive showing that with the use of the proposed services 1. daily step amount have been increased by 3.03% on average in a 10 days study and 2. sleep durations are increased by 7% for two subjects."
"In an ideal network, every packet would be attributable to its sender, while host identities and transmitted content would remain private. Designing such a network is challenging because source accountability and communication privacy are typically viewed as conflicting properties. In this paper, we propose an architecture that guarantees source accountability and privacy-preserving communication by enlisting ISPs as accountability agents and privacy brokers. While ISPs can link every packet in their network to their customers, customer identity remains unknown to the rest of the Internet. In our architecture, network communication is based on Ephemeral Identifiers (EphIDs)---cryptographic tokens that can be linked to a source only by the source's ISP. We demonstrate that EphIDs can be generated and processed efficiently, and we analyze the practical considerations for deployment."
"Perturb and Combine (P&C) group of methods generate multiple versions of the predictor by perturbing the training set or construction and then combining them into a single predictor (Breiman, 1996b). The motive is to improve the accuracy in unstable classification and regression methods. One of the most well known method in this group is Bagging. Arcing or Adaptive Resampling and Combining methods like AdaBoost are smarter variants of P&C methods. In this extended abstract, we lay the groundwork for a new family of methods under the P&C umbrella, known as Evolutionary Sampling (ES). We employ Evolutionary algorithms to suggest smarter sampling in both the feature space (sub-spaces) as well as training samples. We discuss multiple fitness functions to assess ensembles and empirically compare our performance against randomized sampling of training data and feature sub-spaces."
"Notes in a musical piece are building blocks employed in non-random ways to create melodies. It is the ""interaction"" among a limited amount of notes that allows constructing the variety of musical compositions that have been written in centuries and within different cultures. Networks are a modeling tool that is commonly employed to represent a set of entities interacting in some way. Thus, notes composing a melody can be seen as nodes of a network that are connected whenever these are played in sequence. The outcome of such a process results in a directed graph. By using complex network theory, some main metrics of musical graphs can be measured, which characterize the related musical pieces. In this paper, we define a framework to represent melodies as networks. Then, we provide an analysis on a set of guitar solos performed by main musicians. Results of this study indicate that the presented model can have an impact on audio and multimedia applications such as music classification, identification, e-learning, automatic music generation, multimedia entertainment."
"In this paper we introduce a novel method for linear system identification with quantized output data. We model the impulse response as a zero-mean Gaussian process whose covariance (kernel) is given by the recently proposed stable spline kernel, which encodes information on regularity and exponential stability. This serves as a starting point to cast our system identification problem into a Bayesian framework. We employ Markov Chain Monte Carlo methods to provide an estimate of the system. In particular, we design two methods based on the so-called Gibbs sampler that allow also to estimate the kernel hyperparameters by marginal likelihood maximization via the Expectation-Maximization method. Numerical simulations show the effectiveness of the proposed scheme, as compared to the state-of-the-art kernel-based methods when these are employed in system identification with quantized data."
"Automatically identifying data types of web structured data is a key step in the process of web data integration. Web structured data is usually associated with entities or objects in a particular domain. In this paper, we aim to map attributes of an entity in a given domain to pre-specified classes of attributes in the same domain based on their values. To perform this task, we propose a hybrid deep learning network that relies on the format of the attributes' values. It does so without any pre-processing or using pre-defined hand-crafted features. The hybrid network combines sequence-based neural networks, namely convolutional neural networks (CNN) and recurrent neural networks (RNN), to learn the sequence structure of attributes' values. The CNN captures short-distance dependencies in these sequences through a sliding window approach, and the RNN captures long-distance dependencies by storing information of previous characters. These networks create different vector representations of the input sequence which are combined using a pooling layer. This layer applies a specific operation on these vectors in order to capture their most useful patterns for the task. Finally, on top of the pooling layer, a softmax function predicts the label of a given attribute value. We evaluate our strategy in four different web domains. The results show that the pooling network outperforms previous approaches, which use some kind of input pre-processing, in all domains."
"We consider and analyze properties of large sets of randomly selected (i.i.d.) points in high dimensional spaces. In particular, we consider the problem of whether a single data point that is randomly chosen from a finite set of points can be separated from the rest of the data set by a linear hyperplane. We formulate and prove stochastic separation theorems, including: 1) with probability close to one a random point may be separated from a finite random set by a linear functional; 2) with probability close to one for every point in a finite random set there is a linear functional separating this point from the rest of the data. The total number of points in the random sets are allowed to be exponentially large with respect to dimension. Various laws governing distributions of points are considered, and explicit formulae for the probability of separation are provided. These theorems reveal an interesting implication for machine learning and data mining applications that deal with large data sets (big data) and high-dimensional data (many attributes): simple linear decision rules and learning machines are surprisingly efficient tools for separating and filtering out arbitrarily assigned points in large dimensions."
"Many security and software testing applications require checking whether certain properties of a program hold for any possible usage scenario. For instance, a tool for identifying software vulnerabilities may need to rule out the existence of any backdoor to bypass a program's authentication. One approach would be to test the program using different, possibly random inputs. As the backdoor may only be hit for very specific program workloads, automated exploration of the space of possible inputs is of the essence. Symbolic execution provides an elegant solution to the problem, by systematically exploring many possible execution paths at the same time without necessarily requiring concrete inputs. Rather than taking on fully specified input values, the technique abstractly represents them as symbols, resorting to constraint solvers to construct actual instances that would cause property violations. Symbolic execution has been incubated in dozens of tools developed over the last four decades, leading to major practical breakthroughs in a number of prominent software reliability applications. The goal of this survey is to provide an overview of the main ideas, challenges, and solutions developed in the area, distilling them for a broad audience."
"In phylogenetics, the consensus problem consists in summarizing a set of phylogenetic trees that all classify the same set of species into a single tree. Several definition of consensus exists in the literature; in this paper we focus on the Weighted Quartet Consensus problem, a problem with unknown complexity status. Here we prove that the Weighted Quartet Consensus problem is NP-hard and we give several results on the approximability and fixed-parameter tractability of this problem."
"We propose the application of a semi-supervised learning method to improve the performance of acoustic modelling for automatic speech recognition based on deep neural net- works. As opposed to unsupervised initialisation followed by supervised fine tuning, our method takes advantage of both unlabelled and labelled data simultaneously through mini- batch stochastic gradient descent. We tested the method with varying proportions of labelled vs unlabelled observations in frame-based phoneme classification on the TIMIT database. Our experiments show that the method outperforms standard supervised training for an equal amount of labelled data and provides competitive error rates compared to state-of-the-art graph-based semi-supervised learning techniques."
"We propose a probabilistic video model, the Video Pixel Network (VPN), that estimates the discrete joint distribution of the raw pixel values in a video. The model and the neural architecture reflect the time, space and color structure of video tensors and encode it as a four-dimensional dependency chain. The VPN approaches the best possible performance on the Moving MNIST benchmark, a leap over the previous state of the art, and the generated videos show only minor deviations from the ground truth. The VPN also produces detailed samples on the action-conditional Robotic Pushing benchmark and generalizes to the motion of novel objects."
"We present a policy search method for learning complex feedback control policies that map from high-dimensional sensory inputs to motor torques, for manipulation tasks with discontinuous contact dynamics. We build on a prior technique called guided policy search (GPS), which iteratively optimizes a set of local policies for specific instances of a task, and uses these to train a complex, high-dimensional global policy that generalizes across task instances. We extend GPS in the following ways: (1) we propose the use of a model-free local optimizer based on path integral stochastic optimal control (PI2), which enables us to learn local policies for tasks with highly discontinuous contact dynamics; and (2) we enable GPS to train on a new set of task instances in every iteration by using on-policy sampling: this increases the diversity of the instances that the policy is trained on, and is crucial for achieving good generalization. We show that these contributions enable us to learn deep neural network policies that can directly perform torque control from visual input. We validate the method on a challenging door opening task and a pick-and-place task, and we demonstrate that our approach substantially outperforms the prior LQR-based local policy optimizer on these tasks. Furthermore, we show that on-policy sampling significantly increases the generalization ability of these policies."
"For a finite group $G$ and a finite set $A$, we study various algebraic aspects of cellular automata over the configuration space $A^G$. In this situation, the set $\text{CA}(G;A)$ of all cellular automata over $A^G$ is a finite monoid whose basic algebraic properties had remained unknown. First, we investigate the structure of the group of units $\text{ICA}(G;A)$ of $\text{CA}(G;A)$. We obtain a decomposition of $\text{ICA}(G;A)$ into a direct product of wreath products of groups that depends on the numbers $\alpha_{[H]}$ of periodic configurations for conjugacy classes $[H]$ of subgroups of $G$. We show how the numbers $\alpha_{[H]}$ may be computed using the M\""obius function of the subgroup lattice of $G$, and we use this to improve the lower bound recently found by Gao, Jackson and Seward on the number of aperiodic configurations of $A^G$. Furthermore, we study generating sets of $\text{CA}(G;A)$; in particular, we prove that $\text{CA}(G;A)$ cannot be generated by cellular automata with small memory set, and, when all subgroups of $G$ are normal, we determine the relative rank of $\text{ICA}(G;A)$ on $\text{CA}(G;A)$, i.e. the minimal size of a set $V \subseteq \text{CA}(G;A)$ such that $\text{CA}(G;A) = \langle \text{ICA}(G;A) \cup V \rangle$."
"We present an extension of a theorem by Michael Drmota and Michele Soria [Images and Preimages in Random Mappings, 1997] which can be used to identify the limiting distribution for a class of combinatorial schemata. This is achieved by determining analytic and algebraic properties of the associated bivariate generating function. We give sufficient conditions implying a half-normal limiting distribution, extending the known conditions leading to either a Rayleigh, a Gaussian, or a convolution of the last two distributions. We conclude with three natural appearances of such a limiting distribution in the domain of lattice paths."
"In this paper, a neural network based real-time speech recognition (SR) system is developed using an FPGA for very low-power operation. The implemented system employs two recurrent neural networks (RNNs); one is a speech-to-character RNN for acoustic modeling (AM) and the other is for character-level language modeling (LM). The system also employs a statistical word-level LM to improve the recognition accuracy. The results of the AM, the character-level LM, and the word-level LM are combined using a fairly simple N-best search algorithm instead of the hidden Markov model (HMM) based network. The RNNs are implemented using massively parallel processing elements (PEs) for low latency and high throughput. The weights are quantized to 6 bits to store all of them in the on-chip memory of an FPGA. The proposed algorithm is implemented on a Xilinx XC7Z045, and the system can operate much faster than real-time."
"We consider a network of processor-sharing queues with state-dependent service rates. These are allocated according to balanced fairness within a polymatroid capacity set. Balanced fairness is known to be both insensitive and Pareto-efficient in such networks, which ensures that the performance metrics, when computable, will provide robust insights into the real performance of the system considered. We first show that these performance metrics can be evaluated with a complexity that is polynomial in the system size when we allow for some controlled asymmetry, in the sense that the network contains a fixed number of parts wherein all queues are `exchangeable'. This in turn allows us to derive stochastic bounds for a larger class of networks which satisfy less restrictive symmetry assumptions. These results are applied to practical examples of tree data networks and computer clusters."
We investigate sequence machine learning techniques on raw radio signal time-series data. By applying deep recurrent neural networks we learn to discriminate between several application layer traffic types on top of a constant envelope modulation without using an expert demodulation algorithm. We show that complex protocol sequences can be learned and used for both classification and generation tasks using this approach.
"Dynamic complexity is concerned with updating the output of a problem when the input is slightly changed. We study the dynamic complexity of two-player parity games over graphs of bounded tree-width, where updates may add or delete edges, or change the owner or color of states. We show that this problem is in DynFO (with LOGSPACE precomputation); this is achieved by a reduction to a Dyck-path problem on an acyclic automaton."
"We describe an Arabic-Hebrew parallel corpus of TED talks built upon WIT3, the Web inventory that repurposes the original content of the TED website in a way which is more convenient for MT researchers. The benchmark consists of about 2,000 talks, whose subtitles in Arabic and Hebrew have been accurately aligned and rearranged in sentences, for a total of about 3.5M tokens per language. Talks have been partitioned in train, development and test sets similarly in all respects to the MT tasks of the IWSLT 2016 evaluation campaign. In addition to describing the benchmark, we list the problems encountered in preparing it and the novel methods designed to solve them. Baseline MT results and some measures on sentence length are provided as an extrinsic evaluation of the quality of the benchmark."
"Due to rapid development of the Internet, recent years have witnessed an explosion in the rate of data generation. Dealing with data at current scales brings up unprecedented challenges. From the algorithmic view point, executing existing linear algorithms in information retrieval and machine learning on such tremendous amounts of data incur intolerable computational and storage costs. To address this issue, there is a growing interest to map data points in large-scale datasets to binary codes. This can significantly reduce the storage complexity of large-scale datasets. However, one of the most compelling reasons for using binary codes or any discrete representation is that they can be used as direct indices into a hash table. Incorporating hash table offers fast query execution; one can look up the nearby buckets in a hash table populated with binary codes to retrieve similar items. Nonetheless, if binary codes are compared in terms of the cosine similarity rather than the Hamming distance, there is no fast exact sequential procedure to find the $K$ closest items to the query other than the exhaustive search. Given a large dataset of binary codes and a binary query, the problem that we address is to efficiently find $K$ closest codes in the dataset that yield the largest cosine similarities to the query. To handle this issue, we first elaborate on the relation between the Hamming distance and the cosine similarity. This allows finding the sequence of buckets to check in the hash table. Having this sequence, we propose a multi-index hashing approach that can increase the search speed up to orders of magnitude in comparison to the exhaustive search and even approximation methods such as LSH. We empirically evaluate the performance of the proposed algorithm on real world datasets."
"In this paper we study the facility leasing problem with penalties. We present a primal-dual algorithm which is a 3-approximation, based on the algorithm by Nagarajan and Williamson for the facility leasing problem and on the algorithm by Charikar et al. for the facility location problem with penalties."
"Detecting and quantifying anomalies in urban traffic is critical for real-time alerting or re-routing in the short run and urban planning in the long run. We describe a two-step framework that achieves these two goals in a robust, fast, online, and unsupervised manner. First, we adapt stable principal component pursuit to detect anomalies for each road segment. This allows us to pinpoint traffic anomalies early and precisely in space. Then we group the road-level anomalies across time and space into meaningful anomaly events using a simple graph expansion procedure. These events can be easily clustered, visualized, and analyzed by urban planners. We demonstrate the effectiveness of our system using 7 weeks of anonymized and aggregated cellular location data in Dallas-Fort Worth. We suggest potential opportunities for urban planners and policy makers to use our methodology to make informed changes. These applications include real-time re-routing of traffic in response to abnormally high traffic, or identifying candidates for high-impact infrastructure projects."
"Recovery from the Flint Water Crisis has been hindered by uncertainty in both the water testing process and the causes of contamination. In this work, we develop an ensemble of predictive models to assess the risk of lead contamination in individual homes and neighborhoods. To train these models, we utilize a wide range of data sources, including voluntary residential water tests, historical records, and city infrastructure data. Additionally, we use our models to identify the most prominent factors that contribute to a high risk of lead contamination. In this analysis, we find that lead service lines are not the only factor that is predictive of the risk of lead contamination of water. These results could be used to guide the long-term recovery efforts in Flint, minimize the immediate damages, and improve resource-allocation decisions for similar water infrastructure crises."
"We study space and time efficient quantum algorithms for two graph problems -- deciding whether an $n$-vertex graph is a forest, and whether it is bipartite. Via a reduction to the s-t connectivity problem, we describe quantum algorithms for deciding both properties in $\tilde{O}(n^{3/2})$ time and using $O(\log n)$ classical and quantum bits of storage in the adjacency matrix model. We then present quantum algorithms for deciding the two properties in the adjacency array model, which run in time $\tilde{O}(n\sqrt{d_m})$ and also require $O(\log n)$ space, where $d_m$ is the maximum degree of any vertex in the input graph."
"In this paper, we describe a system for generating three-dimensional visual simulations of natural language motion expressions. We use a rich formal model of events and their participants to generate simulations that satisfy the minimal constraints entailed by the associated utterance, relying on semantic knowledge of physical objects and motion events. This paper outlines technical considerations and discusses implementing the aforementioned semantic models into such a system."
"In a real-world network, shared by several users, telehaptic applications involving delay-sensitive multimedia communication between remote locations demand distinct Quality of Service (QoS) guarantees for different media components. These QoS constraints pose a variety of challenges, especially when the network cross-traffic is unknown and time-varying. In this work, we propose an application layer congestion control protocol for telehaptic applications operating over shared networks, termed as dynamic packetization module (DPM). DPM is a lossless, network-aware protocol which tunes the telehaptic packetization rate based on the level of congestion in the network. To monitor the network congestion, we devise a novel network feedback module, which communicates the end-to-end delays encountered by the telehaptic packets to the respective transmitters, with negligible overhead. Via extensive simulations, we show that DPM meets the QoS requirements of telehaptic applications over a wide range of network cross-traffic conditions. We also report the qualitative results of a real-time telepottery experiment with several human subjects, which reveal that DPM preserves the quality of telehaptic activity even under heavily congested network scenarios. Finally, we compare the performance of DPM with existing telehaptic communication protocols, and conclude that DPM outperforms other techniques."
"Excessive tail end-to-end latency occurs with conventional message brokers as a result of having massive numbers of geographically distributed devices communicate through a message broker. On the other hand, broker-less messaging systems, though ensure low latency, are highly dependent on the limitation of direct device-to-device (D2D) communication technologies, and cannot scale well as large numbers of resource-limited devices exchange messages. In this paper, we propose FogMQ, a cloud-based message broker system that overcomes the limitations of conventional systems by enabling autonomous discovery, self-deployment, and online migration of message brokers across heterogeneous cloud platforms. For each device, FogMQ provides a high capacity device cloning service that subscribes to device messages. The clones facilitate near-the-edge data analytics in resourceful cloud compute nodes. Clones in FogMQ apply Flock, an algorithm mimicking flocking-like behavior to allow clones to dynamically select and autonomously migrate to different heterogeneous cloud platforms in a distributed manner."
"Enterprises are investing heavily in cloud data centers to meet the ever surging business demand. Data Center is a facility, which houses computer systems and associated components, such as telecommunications and storage systems. It generally includes power supply equipment, communication connections and cooling equipment. A large data center can use as much electricity as a small town. Due to the emergence of data center based computing services, it has become necessary to examine how the costs associated with data centers evolve over time, mainly in view of efficiency issues. We have presented a quasi form of Cobb Douglas model, which addresses revenue and profit issues in running large data centers. The stochastic form has been introduced and explored along with the quasi Cobb Douglas model to understand the behavior of the model in depth. Harrod neutrality and Solow neutrality are incorporated in the model to identify the technological progress in cloud data centers. This allows us to shed light on the stochastic uncertainty of cloud data center operations. A general approach to optimizing the revenue cost of data centers using Cobb Douglas Stochastic Frontier Analysis,CDSFA is presented. Next, we develop the optimization model for large data centers. The mathematical basis of CDSFA has been utilized for cost optimization and profit maximization in data centers. The results are found to be quite useful in view of production reorganization in large data centers around the world."
"In this report, we apply the proposed ""para-model"" framework in order to control the trajectory of a dynamical system-based robot. The optimization of the dynamical performances in closed-loop is performed using a derivative-free optimization algorithm."
"Not long ago, it was thought that only software applications and general purpose digital systems i.e. computers were prone to various types of attacks against their security. The underlying hardware, hardware implementations of these software applications, embedded systems, and hardware devices were considered to be secure and out of reach of these attacks. However, during the previous few years, it has been demonstrated that novel attacks against the hardware and embedded systems can also be mounted. Not only viruses, but worms and Trojan horses have been developed for them, and they have also been demonstrated to be effective. Whereas a lot of research has already been done in the area of security of general purpose computers and software applications, hardware and embedded systems security is a relatively new and emerging area of research. This chapter provides details of various types of existing attacks against hardware devices and embedded systems, analyzes existing design methodologies for their vulnerability to new types of attacks, and along the way describes solutions and countermeasures against them for the design and development of secure systems."
"Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations."
"We explore the use of the orthographic syllable, a variable-length consonant-vowel sequence, as a basic unit of translation between related languages which use abugida or alphabetic scripts. We show that orthographic syllable level translation significantly outperforms models trained over other basic units (word, morpheme and character) when training over small parallel corpora."
"The Ethernet as we see it today is a result of many stages of evolution. As a result, the Ethernet frame format at the link layer also has seen many changes. One might expect a consistent format of link layer frames as we usually study in text books, in actual practice this is not the case. Please refer to the manuscript for complete abstract."
"There is no denying the fact that with the widespread usage of computers and the Internet in our daily lives, security of information and data has gained increased attention. Information stored in electronic form is more susceptible to being leaked to unauthorized individuals intentionally or without intent. One of the major reasons for this breach of security has been attributed to proprietary software whose source is available only to the company which made it. Thus you have no surety that the proprietary and pre-compiled software has no hole to help an individual break security of your computer or network. Philosophy of free and open source software as against this provides everyone an opportunity to view the source code for any possible vulnerabilities and compile and change it according to ones need. This paper discusses this philosophy in length with some examples and also some open source tools that help maintain computer and Internet security."
"In this paper, a two-stage dual tree complex wavelet packet transform (DTCWPT) based speech enhancement algorithm has been proposed, in which a speech presence probability (SPP) estimator and a generalized minimum mean squared error (MMSE) estimator are developed. To overcome the drawback of signal distortions caused by down sampling of WPT, a two-stage analytic decomposition concatenating undecimated WPT (UWPT) and decimated WPT is employed. An SPP estimator in the DTCWPT domain is derived based on a generalized Gamma distribution of speech, and Gaussian noise assumption. The validation results show that the proposed algorithm can obtain enhanced perceptual evaluation of speech quality (PESQ), and segmental signal-to-noise ratio (SegSNR) at low SNR nonstationary noise, compared with other four state-of-the-art speech enhancement algorithms, including optimally modified LSA (OM-LSA), soft masking using a posteriori SNR uncertainty (SMPO), a posteriori SPP based MMSE estimation (MMSE-SPP), and adaptive Bayesian wavelet thresholding (BWT)."
"In future practical deployments of massive multi-input multi-output (MIMO) systems, the number of radio frequency (RF) chains at the base stations (BSs) may be much smaller than the number of BS antennas to reduce the overall expenditure. In this paper, we propose a novel design framework for joint data and artificial noise (AN) precoding in a multiuser massive MIMO system with limited number of RF chains, which improves the wireless security performance. With imperfect channel state information (CSI), we analytically derive an achievable lower bound on the ergodic secrecy rate of any mobile terminal (MT), for both analog and hybrid precoding schemes. The closed-form lower bound is used to determine optimal power splitting between data and AN that maximizes the secrecy rate through simple one-dimensional search. Analytical and numerical results together reveal that the proposed hybrid precoder, although suffers from reduced secrecy rate compared with theoretical full-dimensional precoder, is free of the high computational complexity of large-scale matrix inversion and null-space calculations, and largely reduces the hardware cost."
"We present a new model of incomplete information games without private information in which the players use a distributionally robust optimization approach to cope with the payoff uncertainty. With some specific restrictions, we show that our ""Distributionally Robust Game"" constitutes a true generalization of three popular finite games. These are the Complete Information Games, Bayesian Games and Robust Games. Subsequently, we prove that the set of equilibria of an arbitrary distributionally robust game with specified ambiguity set can be computed as the component-wise projection of the solution set of a multi-linear system of equations and inequalities. For special cases of such games we show equivalence to complete information finite games (Nash Games) with the same number of players and same action spaces. Thus, when our game falls within these special cases one can simply solve the corresponding Nash Game. Finally, we demonstrate the applicability of our new model of games and highlight its importance."
"Distance Geometry is based on the inverse problem that asks to find the positions of points, in a Euclidean space of given dimension, that are compatible with a given set of distances. We briefly introduce the field, and discuss some open and promising research areas."
"We study a simple model pertinent to distributed algorithms and control, that has been well studied in the prior literature in the presence of additive noise. The model in question belongs to the class of so-called network consensus (or gossip) algorithms, which provide a local algorithms for computing global means of values distributed over the nodes of a network. We show that this simple model exhibits non-trivial behavior when time-dependent multiplicative noise is introduced, affecting either the values at the network nodes, or the edges connecting the nodes. In contrast with the case of additive noise, the system exhibits a phase transition to an exponentially growing phase once the multiplicative noise strength crosses a threshold. The transition threshold will exhibit a strong dimensionality dependance and the phase diagram will be fully characterize in the case where conservation of the mean value is conserved, either exactly or in average. Connections with well-known results in physics of interfaces and disordered systems will be made, making the phenomenology of the system more transparent."
"In the context of the global obesity epidemic, it is important to know who becomes obese and why. However, the processes that determine the changing shape of Body Mass Index (BMI) distributions in high-income societies are not well-understood. Here we establish the statistical mechanics of human weight change, providing a fundamental new understanding of human weight distributions. By compiling and analysing the largest data set so far of year-over-year BMI changes, we find, strikingly, that heavy people on average strongly decrease their weight year-over-year, and light people increase their weight. This drift towards the centre of the BMI distribution is balanced by diffusion resulting from random fluctuations in diet and physical activity that are, notably, proportional in size to BMI. We formulate a stochastic mathematical model for BMI dynamics, deriving a theoretical shape for the BMI distribution and offering a mechanism to explain the ongoing right-skewed broadening of BMI distributions over time. The model also provides new quantitative support for the hypothesis that peer-to-peer social influence plays a measurable role in BMI dynamics. More broadly, our results demonstrate a remarkable analogy with drift-diffusion mechanisms that are well-known from the physical sciences and finance."
"Face Recognition (FR) has been the interest to several researchers over the past few decades due to its passive nature of biometric authentication. Despite high accuracy achieved by face recognition algorithms under controlled conditions, achieving the same performance for face images obtained in surveillance scenarios, is a major hurdle. Some attempts have been made to super-resolve the low-resolution face images and improve the contrast, without considerable degree of success. The proposed technique in this paper tries to cope with the very low resolution and low contrast face images obtained from surveillance cameras, for FR under surveillance conditions. For Support Vector Machine classification, the selection of appropriate kernel has been a widely discussed issue in the research community. In this paper, we propose a novel kernel selection technique termed as MFKL (Multi-Feature Kernel Learning) to obtain the best feature-kernel pairing. Our proposed technique employs a effective kernel selection by Multiple Kernel Learning (MKL) method, to choose the optimal kernel to be used along with unsupervised domain adaptation method in the Reproducing Kernel Hilbert Space (RKHS), for a solution to the problem. Rigorous experimentation has been performed on three real-world surveillance face datasets : FR\_SURV, SCface and ChokePoint. Results have been shown using Rank-1 Recognition Accuracy, ROC and CMC measures. Our proposed method outperforms all other recent state-of-the-art techniques by a considerable margin."
"Obtaining high quality sensor information is critical in vehicular emergencies. However, existing standards such as IEEE 802.11p/DSRC and LTE-A cannot support either the required data rates or the latency requirements. One solution to this problem is for municipalities to invest in dedicated base stations to ensure that drivers have the information they need to make safe decisions in or near accidents. In this paper we further propose that these municipality-owned base stations form a Single Frequency Network (SFN). In order to ensure that transmissions are reliable, we derive tight bounds on the outage probability when the SFN is overlaid on an existing cellular network. Using our bounds, we propose a transmission power allocation algorithm. We show that our power allocation model can reduce the total instantaneous SFN transmission power up to $20$ times compared to a static uniform power allocation solution, for the considered scenarios. The result is particularly important when base stations rely on an off-grid power source (i.e., batteries)."
"This paper introduces a multi-round interaction problem with privacy constraints between two agents that observe correlated data. The agents alternately share data with one another for a total of K rounds such that each agent initiates sharing over K/2 rounds. The interactions are modeled as a collection of K random mechanisms (mappings), one for each round. The goal is to jointly design the K private mechanisms to determine the set of all achievable distortion-leakage pairs at each agent. Arguing that a mutual information-based leakage metric can be appropriate for streaming data settings, this paper: (i) determines the set of all achievable distortion- leakage tuples ; (ii) shows that the K mechanisms allow for precisely composing the total privacy budget over K rounds without loss; and (ii) develops conditions under which interaction reduces the net leakage at both agents and illustrates it for a specific class of sources. The paper then focuses on log-loss distortion to better understand the effect on leakage of using a commonly used utility metric in learning theory. The resulting interaction problem leads to a non-convex sum-leakage-distortion optimization problem that can be viewed as an interactive version of the information bottleneck problem. A new merge-and-search algorithm that extends the classical agglomerative information bottleneck algorithm to the interactive setting is introduced to determine a provable locally optimal solution. Finally, the benefit of interaction under log-loss is illustrated for specific source classes and the optimality of one-shot is proved for Gaussian sources under both mean-square and log-loss distortions constraints."
"Synthetic graph generators facilitate research in graph algorithms and processing systems by providing access to data, for instance, graphs resembling social networks, while circumventing privacy and security concerns. Nevertheless, their practical value lies in their ability to capture important metrics of real graphs, such as degree distribution and clustering properties. Graph generators must also be able to produce such graphs at the scale of real-world industry graphs, that is, hundreds of billions or trillions of edges.   In this paper, we propose Darwini, a graph generator that captures a number of core characteristics of real graphs. Importantly, given a source graph, it can reproduce the degree distribution and, unlike existing approaches, the local clustering coefficient and joint-degree distributions. Furthermore, Darwini maintains metrics such node PageRank, eigenvalues and the K-core decomposition of a source graph. Comparing Darwini with state-of-the-art generative models, we show that it can reproduce these characteristics more accurately. Finally, we provide an open source implementation of our approach on the vertex-centric Apache Giraph model that allows us to create synthetic graphs with one trillion edges."
"In principle, reinforcement learning and policy search methods can enable robots to learn highly complex and general skills that may allow them to function amid the complexity and diversity of the real world. However, training a policy that generalizes well across a wide range of real-world conditions requires far greater quantity and diversity of experience than is practical to collect with a single robot. Fortunately, it is possible for multiple robots to share their experience with one another, and thereby, learn a policy collectively. In this work, we explore distributed and asynchronous policy learning as a means to achieve generalization and improved training times on challenging, real-world manipulation tasks. We propose a distributed and asynchronous version of Guided Policy Search and use it to demonstrate collective policy learning on a vision-based door opening task using four robots. We show that it achieves better generalization, utilization, and training times than the single robot alternative."
"We construct optimal estimation algorithms over distributed networks for state estimation in the mean-square error (MSE) sense. Here, we have a distributed collection of agents with processing and cooperation capabilities. These agents continually observe a noisy version of a desired state of the nature through a linear model and seek to learn this state by interacting with each other. Although this problem has attracted significant attention and extensively been studied in several different fields including machine learning theory to signal processing, all the well-known strategies achieve suboptimal learning performance in the MSE sense. To this end, we provide algorithms that achieve distributed minimum MSE (MMSE) performance over an arbitrary network topology based on the aggregation of information at each agent. This approach differs from the diffusion of information across network, i.e., exchange of local estimates per time instance. Importantly, we show that exchange of local estimates is sufficient only over the certain network topologies. By inspecting these network structures, we also propose strategies that achieve the distributed MMSE performance also through the diffusion of information such that we can substantially reduce the communication load while achieving the best possible MSE performance. For practical implementations we provide approaches to reduce the complexity of the algorithms through the time-windowing of the observations. Finally, in the numerical examples, we demonstrate the superior performance of the introduced algorithms in the MSE sense due to optimal estimation."
"High-Throughput materials discovery involves the rapid synthesis, measurement, and characterization of many different but structurally-related materials. A key problem in materials discovery, the phase map identification problem, involves the determination of the crystal phase diagram from the materials' composition and structural characterization data. We present Phase-Mapper, a novel AI platform to solve the phase map identification problem that allows humans to interact with both the data and products of AI algorithms, including the incorporation of human feedback to constrain or initialize solutions. Phase-Mapper affords incorporation of any spectral demixing algorithm, including our novel solver, AgileFD, which is based on a convolutive non-negative matrix factorization algorithm. AgileFD can incorporate constraints to capture the physics of the materials as well as human feedback. We compare three solver variants with previously proposed methods in a large-scale experiment involving 20 synthetic systems, demonstrating the efficacy of imposing physical constrains using AgileFD. Phase-Mapper has also been used by materials scientists to solve a wide variety of phase diagrams, including the previously unsolved Nb-Mn-V oxide system, which is provided here as an illustrative example."
"A key challenge in scaling up robot learning to many skills and environments is removing the need for human supervision, so that robots can collect their own data and improve their own performance without being limited by the cost of requesting human feedback. Model-based reinforcement learning holds the promise of enabling an agent to learn to predict the effects of its actions, which could provide flexible predictive models for a wide range of tasks and environments, without detailed human supervision. We develop a method for combining deep action-conditioned video prediction models with model-predictive control that uses entirely unlabeled training data. Our approach does not require a calibrated camera, an instrumented training set-up, nor precise sensing and actuation. Our results show that our method enables a real robot to perform nonprehensile manipulation -- pushing objects -- and can handle novel objects not seen during training."
"Traversing environments with arbitrary obstacles poses significant challenges for bipedal robots. In some cases, whole body motions may be necessary to maneuver around an obstacle, but most existing footstep planners can only select from a discrete set of predetermined footstep actions, they are unable to utilize the continuum of whole body motion that is truly available to the robot platform. Existing motion planners that can utilize whole body motion tend to struggle with the complexity of large-scale problems. We introduce a planning method, called the ""Possibility Graph"", which uses high-level approximations of constraint manifolds to rapidly explore the ""possibility"" of actions, thereby allowing lower-level motion planners to be utilized more efficiently. We demonstrate simulations of the method working in a variety of semi-unstructured environments. In this context, ""semi-unstructured"" means the walkable terrain is flat and even, but there are arbitrary 3D obstacles throughout the environment which may need to be stepped over or maneuvered around using whole body motions."
"Locomotion for legged robots poses considerable challenges when confronted by obstacles and adverse environments. Footstep planners are typically only designed for one mode of locomotion, but traversing unfavorable environments may require several forms of locomotion to be sequenced together, such as walking, crawling, and jumping. Multi-modal motion planners can be used to address some of these problems, but existing implementations tend to be time-consuming and are limited to quasi-static actions. This paper presents a motion planning method to traverse complex environments using multiple categories of continuous actions. To this end, this paper formulates and exploits the Possibility Graph---which uses high-level approximations of constraint manifolds to rapidly explore the ""possibility"" of actions---to utilize lower-level single-action motion planners more effectively. We show that the Possibility Graph can quickly find routes through several different challenging environments which require various combinations of actions in order to traverse."
"We develop a thermodynamic framework for modeling nonlinear ultrasonic damage sensing and prognosis in materials undergoing progressive damage. The framework is based on the internal variable approach and relies on the construction of a pseudo-elastic strain energy function that captures the energetics associated with the damage progression. The pseudo-elastic strain energy function is composed of two energy functions - one that describes how a material stores energy in an elastic fashion and the other describes how material dissipates energy or stores it in an inelastic fashion. Experimental motivation for the choice of the above two functionals is discussed and some specific choices pertaining to damage progression during fatigue and creep are presented. The thermodynamic framework is employed to model the nonlinear response of material undergoing stress relaxation and creep-like degradation. For each of the above cases, evolution of the nonlinearity parameter with damage as well as with macroscopic measurables like accumulated plastic strain are obtained."
"A language $L$ over an alphabet $\Sigma$ is suffix-convex if, for any words $x,y,z\in\Sigma^*$, whenever $z$ and $xyz$ are in $L$, then so is $yz$. Suffix-convex languages include three special cases: left-ideal, suffix-closed, and suffix-free languages. We examine complexity properties of these three special classes of suffix-convex regular languages. In particular, we study the quotient/state complexity of boolean operations, product (concatenation), star, and reversal on these languages, as well as the size of their syntactic semigroups, and the quotient complexity of their atoms."
"For state-of-the-art semantic segmentation task, training convolutional neural networks (CNNs) requires dense pixelwise ground truth (GT) labeling, which is expensive and involves extensive human effort. In this work, we study the possibility of using auxiliary ground truth, so-called \textit{pseudo ground truth} (PGT) to improve the performance. The PGT is obtained by propagating the labels of a GT frame to its subsequent frames in the video using a simple CRF-based, cue integration framework. Our main contribution is to demonstrate the use of noisy PGT along with GT to improve the performance of a CNN. We perform a systematic analysis to find the right kind of PGT that needs to be added along with the GT for training a CNN. In this regard, we explore three aspects of PGT which influence the learning of a CNN: i) the PGT labeling has to be of good quality; ii) the PGT images have to be different compared to the GT images; iii) the PGT has to be trusted differently than GT. We conclude that PGT which is diverse from GT images and has good quality of labeling can indeed help improve the performance of a CNN. Also, when PGT is multiple folds larger than GT, weighing down the trust on PGT helps in improving the accuracy. Finally, We show that using PGT along with GT, the performance of Fully Convolutional Network (FCN) on Camvid data is increased by $2.7\%$ on IoU accuracy. We believe such an approach can be used to train CNNs for semantic video segmentation where sequentially labeled image frames are needed. To this end, we provide recommendations for using PGT strategically for semantic segmentation and hence bypass the need for extensive human efforts in labeling."
"Detecting emergence of a low-rank signal from high-dimensional data is an important problem arising from many applications such as camera surveillance and swarm monitoring using sensors. We consider a procedure based on the largest eigenvalue of the sample covariance matrix over a sliding window to detect the change. To achieve dimensionality reduction, we present a sketching-based approach for rank change detection using the low-dimensional linear sketches of the original high-dimensional observations. The premise is that when the sketching matrix is a random Gaussian matrix, and the dimension of the sketching vector is sufficiently large, the rank of sample covariance matrix for these sketches equals the rank of the original sample covariance matrix with high probability. Hence, we may be able to detect the low-rank change using sample covariance matrices of the sketches without having to recover the original covariance matrix. We character the performance of the largest eigenvalue statistic in terms of the false-alarm-rate and the expected detection delay, and present an efficient online implementation via subspace tracking."
"Probabilistic language models are widely used in Information Retrieval (IR) to rank documents by the probability that they generate the query. However, the implementation of the probabilistic representations with programming languages that favor matrix calculations is challenging. In this paper, we utilize matrix representations to reformulate the probabilistic language models. The matrix representation is a superstructure for the probabilistic language models to organize the calculated probabilities and a potential formalism for standardization of language models and for further mathematical analysis. It facilitates implementations by matrix friendly programming languages. In this paper, we consider the matrix formulation of conventional language model with Dirichlet smoothing, and two language models based on Latent Dirichlet Allocation (LDA), i.e., LBDM and LDI. We release a Java software package--MatLM--implementing the proposed models. Code is available at: https://github.com/yanshanwang/JGibbLDA-v.1.0-MatLM."
"Pedestrian detection is one of the most popular topics in computer vision and robotics. Considering challenging issues in multiple pedestrian detection, we present a real-time depth-based template matching people detector. In this paper, we propose different approaches for training the depth-based template. We train multiple templates for handling issues due to various upper-body orientations of the pedestrians and different levels of detail in depth-map of the pedestrians with various distances from the camera. And, we take into account the degree of reliability for different regions of sliding window by proposing the weighted template approach. Furthermore, we combine the depth-detector with an appearance based detector as a verifier to take advantage of the appearance cues for dealing with the limitations of depth data. We evaluate our method on the challenging ETH dataset sequence. We show that our method outperforms the state-of-the-art approaches."
"One of the objectives of the pedestrian analysis is to evaluate the effects of proposed policy on the pedestrian facilities before its implementation. The implementation of a policy without pedestrian analysis might lead to a very costly trial and error due to the implementation cost (i.e. user cost, construction time and cost, etc.). On the other hand, using good analysis tools, the trial and error of policy could be done in the analysis level. Once the analysis could prove a good performance, the implementation of the policy is straightforward. The problem is how to evaluate the impact of the policy quantitatively toward the behavior of pedestrians before its implementation. Since the interaction of pedestrians cannot be well address using a macroscopic level of analysis, a microscopic level of analysis is the choice. However, the analytical solution of the microscopic pedestrian model is very difficult and simulation models are more practical approach. To evaluate the impact of the policy quantitatively toward the behavior of pedestrians before its implementation, a microscopic pedestrian simulation model was developed. The model was based on physical forces, which work upon each pedestrian dynamically. To demonstrate the numerical analysis of the model, an experimental policy on pedestrian crossing was performed. The simulation results showed that the keep right policy or the lane-like segregation policy is inclined to be superior to do minimum or mix-lane policy in terms of average speed, average delay and dissipation time."
"Modern System-on-Chip (SoC) platforms typically consist of multiple processors and a communication interconnect between them. Network-on-Chip (NoC) arises as a solution to interconnect these systems, which provides a scalable, reusable, and an efficient interconnect. For these SoC platforms, multicast communication is significantly used for parallel applications. Cache coherency in distributed sharedmemory,clock synchronization, replication, or barrier synchronization are examples of these requests. This paper presents an overview of research on NoC with support for multicast communication and delineates the major issues addressed so far by the scientific community in this investigation area."
"Persistent homology is a method from computational algebraic topology that can be used to study the ""shape"" of data. We illustrate two filtrations --- the weight rank clique filtration and the Vietoris--Rips (VR) filtration --- that are commonly used in persistent homology, and we apply these filtrations to a pair of data sets that are both related to the 2016 European Union ""Brexit"" referendum in the United Kingdom. These examples consider a topical situation and give useful illustrations of the strengths and weaknesses of these methods."
"Looking at a person's hands one often can tell what the person is going to do next, how his/her hands are moving and where they will be, because an actor's intentions shape his/her movement kinematics during action execution. Similarly, active systems with real-time constraints must not simply rely on passive video-segment classification, but they have to continuously update their estimates and predict future actions. In this paper, we study the prediction of dexterous actions. We recorded from subjects performing different manipulation actions on the same object, such as ""squeezing"", ""flipping"", ""washing"", ""wiping"" and ""scratching"" with a sponge. In psychophysical experiments, we evaluated human observers' skills in predicting actions from video sequences of different length, depicting the hand movement in the preparation and execution of actions before and after contact with the object. We then developed a recurrent neural network based method for action prediction using as input patches around the hand. We also used the same formalism to predict the forces on the finger tips using for training synchronized video and force data streams. Evaluations on two new datasets showed that our system closely matches human performance in the recognition task, and demonstrate the ability of our algorithm to predict what and how a dexterous action is performed."
"We present encube $-$ a qualitative, quantitative and comparative visualisation and analysis system, with application to high-resolution, immersive three-dimensional environments and desktop displays. encube extends previous comparative visualisation systems by considering: 1) the integration of comparative visualisation and analysis into a unified system; 2) the documentation of the discovery process; and 3) an approach that enables scientists to continue the research process once back at their desktop. Our solution enables tablets, smartphones or laptops to be used as interaction units for manipulating, organising, and querying data. We highlight the modularity of encube, allowing additional functionalities to be included as required. Additionally, our approach supports a high level of collaboration within the physical environment. We show how our implementation of encube operates in a large-scale, hybrid visualisation and supercomputing environment using the CAVE2 at Monash University, and on a local desktop, making it a versatile solution. We discuss how our approach can help accelerate the discovery rate in a variety of research scenarios."
"Lexical sets contain the words filling the argument positions of a verb in one of its senses. They can be grounded empirically through their automatic extraction from corpora. The purpose of this paper is demonstrating that their vector representation based on word embedding provides insights onto many linguistic phenomena, and in particular about verbs undergoing the causative-inchoative alternation. A first experiment aims at investigating the internal structure of the sets, which are known to be radial and continuous categories cognitively. A second experiment shows that the distance between the subject set and object set is correlated with a semantic factor, namely the spontaneity of the verb."
"cleverhans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure.   This technical report is structured as follows. Section 1 provides an overview of adversarial examples in machine learning and of the cleverhans software. Section 2 presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section 3 describes how to report benchmark results using the library. Section 4 describes the versioning system."
"Networks are used to represent relationships between entities in many complex systems, spanning from online social networks to biological cell development and brain activity. These networks model relationships which present various challenges. In many cases, relationships between entities are unambiguously known: are two users friends in a social network? Do two researchers collaborate on a published paper? Do two road segments in a transportation system intersect? These are unambiguous and directly observable in the system in question. In most cases, relationship between nodes are not directly observable and must be inferred: does one gene regulate the expression of another? Do two animals who physically co-locate have a social bond? Who infected whom in a disease outbreak?   Existing approaches use specialized knowledge in different home domains to infer and measure the goodness of inferred network for a specific task. However, current research lacks a rigorous validation framework which employs standard statistical validation. In this survey, we examine how network representations are learned from non-network data, the variety of questions and tasks on these data over several domains, and validation strategies for measuring the inferred network's capability of answering questions on the original system of interest."
"This paper develops a distributed resource allocation game to study countries' pursuit of targets such as self-survival in the networked international environment. The paper has two general contributions: firstly, it contributes the basic idea that countries' behavior, which is power allocation, is a basic human behavior of resource allocation and the development of this game is the first time countries' behavior has ever been rigorously studied from a resource allocation perspective; secondly, the game itself has an intrinsically interesting and novel mathematical structure --- it actually presents a new technical problem with a surprising amount of informative predictions which arise from the rich parameter space that defines all kinds of possibilities for the networked international environments. The predictions both motivate major theoretical results (e.g., Nash equilibrium existence) and shed light on real world politics."
"We present novel techniques to accelerate the convergence of Deep Learning algorithms by conducting low overhead removal of redundant neurons -- apoptosis of neurons -- which do not contribute to model learning, during the training phase itself. We provide in-depth theoretical underpinnings of our heuristics (bounding accuracy loss and handling apoptosis of several neuron types), and present the methods to conduct adaptive neuron apoptosis. Specifically, we are able to improve the training time for several datasets by 2-3x, while reducing the number of parameters by up to 30x (4-5x on average) on datasets such as ImageNet classification. For the Higgs Boson dataset, our implementation improves the accuracy (measured by Area Under Curve (AUC)) for classification from 0.88/1 to 0.94/1, while reducing the number of parameters by 3x in comparison to existing literature. The proposed methods achieve a 2.44x speedup in comparison to the default (no apoptosis) algorithm."
"A distributed control architecture is presented that is intended to make a collection of heterogeneous loads appear to the grid operator as a nearly perfect battery. Local control is based on randomized decision rules advocated in prior research, and extended in this paper to any load with a discrete number of power states. Additional linear filtering at the load ensures that the input-output dynamics of the aggregate has a nearly flat input-output response: the behavior of an ideal, multi-GW battery system."
"A well-designed fine-grained categorization system usually has three contradictory requirements: accuracy (the ability to identify objects among subordinate categories); interpretability (the ability to provide human-understandable explanation of recognition system behavior); and efficiency (the speed of the system). To handle the trade-off between accuracy and interpretability, we propose a novel ""Deeper Part-Stacked CNN"" architecture armed with interpretability by modeling subtle differences between object parts. The proposed architecture consists of a part localization network, a two-stream classification network that simultaneously encodes object-level and part-level cues, and a feature vectors fusion component. Specifically, the part localization network is implemented by exploring a new paradigm for key point localization that first samples a small number of representable pixels and then determine their labels via a convolutional layer followed by a softmax layer. We also use a cropping layer to extract part features and propose a scale mean-max layer for feature fusion learning. Experimentally, our proposed method outperform state-of-the-art approaches both in part localization task and classification task on Caltech-UCSD Birds-200-2011. Moreover, by adopting a set of sharing strategies between the computation of multiple object parts, our single model is fairly efficient running at 32 frames/sec."
"Dataflow matrix machines are self-referential generalized recurrent neural nets. The self-referential mechanism is provided via a stream of matrices defining the connectivity and weights of the network in question. A natural question is: what should play the role of untyped lambda-calculus for this programming architecture? The proposed answer is a discipline of programming with only one kind of streams, namely the streams of appropriately shaped matrices. This yields Pure Dataflow Matrix Machines which are networks of transformers of streams of matrices capable of defining a pure dataflow matrix machine."
"This survey aims at reviewing recent techniques used in the assessment of image aesthetic quality. The assessment of image aesthetic quality is the process of computationally distinguishing high-quality photos from low-quality ones based on photographic rules or artistic perceptions. A variety of approaches have been proposed in the literature trying to solve this challenging problem. In this survey, we present a systematic listing of the reviewed approaches based on feature types (hand-crafted features and deep features) and evaluation criteria (dataset characteristics and evaluation metrics). Main contributions and novelties of the reviewed approaches are highlighted and discussed. In addition, following the emergence of deep learning techniques, we systematically evaluate recent deep learning settings that are useful for developing a robust deep model for aesthetic scoring. Experiments are conducted using simple yet solid baselines that are competitive with the current state-of-the-arts. Moreover, we discuss the relation between image aesthetic assessment and automatic image cropping. We hope that this survey could serve as a comprehensive reference source for future research on the study of image aesthetic assessment."
"Revolution in the area of information technology has brought about changes in many spheres of life. Today, information systems are being used in very sensitive areas such as defence and missile control systems, nuclear plants, etc. Not only has it changed how business is conducted, it has also brought about entirely new paradigms like that of information and cyber warfare. Similarly, one of the many impacts that it has made, is how wars are fought. For all what it has contributed, the information stored on digital devices and computers has become a precious resource and special measures are taken to guard it against attacks from malicious users. These special measures are needed by any enterprise be it a business firm, a commercial entity, a government agency or a military organization. However, requirements and specifications for information security and assurance for a military organization are essentially different from those of commercial or business applications. This paper highlights and discusses various aspects related to the security of information resources of military importance and outlines certain parameters that should be taken into consideration when talking about the security of military information resources. W also describe the role satellite reconnaissance can play in cyber war. Authors proclaim that this paper is first such attempt to correlate cyber war with satellite reconnaissance."
"A lot of prior work on event extraction has exploited a variety of features to represent events. Such methods have several drawbacks: 1) the features are often specific for a particular domain and do not generalize well; 2) the features are derived from various linguistic analyses and are error-prone; and 3) some features may be expensive and require domain expert. In this paper, we develop a Chinese event extraction system that uses word embedding vectors to represent language, and deep neural networks to learn the abstract feature representation in order to greatly reduce the effort of feature engineering. In addition, in this framework, we leverage large amount of unlabeled data, which can address the problem of limited labeled corpus for this task. Our experiments show that our proposed method performs better compared to the system using rich language features, and using unlabeled data benefits the word embeddings. This study suggests the potential of DNN and word embedding for the event extraction task."
"We consider the task of learning the parameters of a {\em single} component of a mixture model, for the case when we are given {\em side information} about that component; we call this the ""search problem"" in mixture models. We would like to solve this with computational and sample complexity lower than solving the overall original problem, where one learns parameters of all components.   Our main contributions are the development of a simple but general model for the notion of side information, and a corresponding simple matrix-based algorithm for solving the search problem in this general setting. We then specialize this model and algorithm to four common scenarios: Gaussian mixture models, LDA topic models, subspace clustering, and mixed linear regression. For each one of these we show that if (and only if) the side information is informative, we obtain better sample complexity than existing standard mixture model algorithms (e.g. tensor methods). We also illustrate several natural ways one can obtain such side information, for specific problem instances. Our experiments on real datasets (NY Times, Yelp, BSDS500) further demonstrate the practicality of our algorithms showing significant improvement in runtime and accuracy."
"General isometries of cyclic codes, including multipliers and translations, are introduced; and isometrically self-dual cyclic codes are defined. In terms of Type-I duadic splittings given by multipliers and translations, a necessary and sufficient condition for the existence of isometrically self-dual cyclic codes is obtained. A program to construct isometrically self-dual cyclic codes is provided, and illustrated by several examples. In particular, a class of isometrically self-dual MDS cyclic codes, which are alternant codes from a class of generalized Reed-Solomon codes, is presented."
"The performances of the fifth generation (5G) wireless communication systems are significantly affected by edge cache and transport network. These emerging components bring substantial costs of the placement and utilization, and the evaluation of the cost impact is beyond the capability of traditional performance metrics, including spectral efficiency (SE) and energy efficiency (EE). In this article, economical energy efficiency (E3) is proposed, whose core idea is to take SE/EE and cost into account to evaluate comprehensive gains when different kinds of advanced technologies are used in 5G systems. The E3results are shown when the transport network and edge cache are separately or jointly used. Open issues in terms of modeling the cost, E3optimization based radio resource allocation, and E3optimization for internet of things, are identified as well."
"Motivated by recent advances in Deep Learning for robot control, this paper considers two learning algorithms in terms of how they acquire demonstrations. ""Human-Centric"" (HC) sampling is the standard supervised learning algorithm, where a human supervisor demonstrates the task by teleoperating the robot to provide trajectories consisting of state-control pairs. ""Robot-Centric"" (RC) sampling is an increasingly popular alternative used in algorithms such as DAgger, where a human supervisor observes the robot executing a learned policy and provides corrective control labels for each state visited. RC sampling can be challenging for human supervisors and prone to mislabeling. RC sampling can also induce error in policy performance because it repeatedly visits areas of the state space that are harder to learn. Although policies learned with RC sampling can be superior to HC sampling for standard learning models such as linear SVMs, policies learned with HC sampling may be comparable with highly-expressive learning models such as deep learning and hyper-parametric decision trees, which have little model error. We compare HC and RC using a grid world and a physical robot singulation task, where in the latter the input is a binary image of a connected set of objects on a planar worksurface and the policy generates a motion of the gripper to separate one object from the rest. We observe in simulation that for linear SVMs, policies learned with RC outperformed those learned with HC but that with deep models this advantage disappears. We also find that with RC, the corrective control labels provided by humans can be highly inconsistent. We prove there exists a class of examples where in the limit, HC is guaranteed to converge to an optimal policy while RC may fail to converge."
"The evolution of the internet has created an abundance of unstructured data on the web, a significant part of which is textual. The task of author profiling seeks to find the demographics of people solely from their linguistic and content-based features in text. The ability to describe traits of authors clearly has applications in fields such as security and forensics, as well as marketing. Instead of seeing age as just a classification problem, we also frame age as a regression one, but use an ensemble chain method that incorporates the power of both classification and regression to learn the authors exact age."
"\emph{Strictly Chordality-$k$ graphs ($SC_k$)} are graphs which are either cycle-free or every induced cycle is of length exactly $k, k \geq 3$. Strictly chordality-3 and strictly chordality-4 graphs are well known chordal and chordal bipartite graphs, respectively. For $k\geq 5$, the study has been recently initiated in \cite{sadagopan} and various structural and algorithmic results are reported. In this paper, we show that maximum independent set (MIS), minimum vertex cover, minimum dominating set, feedback vertex set (FVS), odd cycle transversal (OCT), even cycle transversal (ECT) and Steiner tree problem are polynomial time solvable on $SC_k$ graphs, $k\geq 5$. We next consider $2K_2$-free graphs and show that FVS, OCT, ECT, Steiner tree problem are polynomial time solvable on subclasses of $2K_2$-free graphs."
"In this paper, we investigate the well-studied Hamiltonian cycle problem, and present an interesting dichotomy result on split graphs. Akiyama et al. have shown that the Hamiltonian cycle problem is NP-complete in planar bipartite graph with maximum degree $3$. Using this reduction, we show that the Hamiltonian cycle problem is NP-complete in split graphs. In particular, we show that the problem is NP-complete in $K_{1,5}$-free split graphs. Further, we present polynomial-time algorithms for Hamiltonian cycle in $K_{1,3}$-free and $K_{1,4}$-free split graphs. We believe that the structural results presented in this paper can be used to show similar dichotomy result for Hamiltonian path and other variants of Hamiltonian cycle problem."
"This paper presents the first Network-Coded Multiple Access (NCMA) system with multiple users adopting different signal modulations, referred to as rate-diverse NCMA. A distinguishing feature of NCMA is the joint use of physical-layer network coding (PNC) and multiuser decoding (MUD) to boost throughput of multipacket reception systems. In previous NCMA systems, users adopt the same modulation regardless of their individual channel conditions. This leads to suboptimal throughput for many practical scenarios, especially when different users have widely varying channel conditions. A rate-diverse NCMA system allows different users to use modulations that are commensurate with their channel conditions. A key challenge is the design of the PNC mapping and decoding mechanisms in NCMA when different users adopt different modulations. While there have been past work on non-channel-coded rate-diverse PNC, this paper is the first attempt to design channel-coded rate-diverse PNC to ensure the reliability of the overall NCMA system. Specifically, we put forth a symbol-splitting channel coding and modulation design so that PNC/NCMA can work over different modulations. We implemented our rate-diverse NCMA system on software-defined radios. Experimental results show that the throughput of rate-diverse NCMA can outperform the state-of-the-art rate-homogeneous NCMA by 80%. Overall, the introduction of rate diversity significantly boosts the NCMA system throughput in practical scenarios."
"We consider the function computation problem in a three node network with one encoder and two decoders. The encoder has access to two correlated sources $X$ and $Y$. The encoder encodes $X^n$ and $Y^n$ into a message which is given to two decoders. Decoder 1 and decoder 2 have access to $X$ and $Y$ respectively, and they want to compute two functions $f(X,Y)$ and $g(X,Y)$ respectively using the encoded message and their respective side information. We want to find the optimum (minimum) encoding rate under the zero error and $\epsilon$-error (i.e. vanishing error) criteria. For the special case of this problem with $f(X,Y) = Y$ and $g(X,Y) = X$, we show that the $\epsilon$-error optimum rate is also achievable with zero error. This result extends to a more general `complementary delivery index coding' problem with arbitrary number of messages and decoders. For other functions, we show that the cut-set bound is achievable under $\epsilon$-error if $X$ and $Y$ are binary, or if the functions are from a special class of `compatible' functions which includes the case $f=g$."
"The design of block codes for short information blocks (e.g., a thousand or less information bits) is an open research problem which is gaining relevance thanks to emerging applications in wireless communication networks. In this work, we review some of the most recent code constructions targeting the short block regime, and we compare then with both finite-length performance bounds and classical error correction coding schemes. We will see how it is possible to effectively approach the theoretical bounds, with different performance vs. decoding complexity trade-offs."
"Alcohol abuse may lead to unsociable behavior such as crime, drunk driving, or privacy leaks. We introduce automatic drunk-texting prediction as the task of identifying whether a text was written when under the influence of alcohol. We experiment with tweets labeled using hashtags as distant supervision. Our classifiers use a set of N-gram and stylistic features to detect drunk tweets. Our observations present the first quantitative evidence that text contains signals that can be exploited to detect drunk-texting."
"This paper makes a simple increment to state-of-the-art in sarcasm detection research. Existing approaches are unable to capture subtle forms of context incongruity which lies at the heart of sarcasm. We explore if prior work can be enhanced using semantic similarity/discordance between word embeddings. We augment word embedding-based features to four feature sets reported in the past. We also experiment with four types of word embeddings. We observe an improvement in sarcasm detection, irrespective of the word embedding used or the original feature set to which our features are augmented. For example, this augmentation results in an improvement in F-score of around 4\% for three out of these four feature sets, and a minor degradation in case of the fourth, when Word2Vec embeddings are used. Finally, a comparison of the four embeddings shows that Word2Vec and dependency weight-based features outperform LSA and GloVe, in terms of their benefit to sarcasm detection."
"The growing popularity of mobile and wearable devices with built-in cameras, the bright prospect of camera related applications such as augmented reality and life-logging system, the increased ease of taking and sharing photos, and advances in computer vision techniques have greatly facilitated people's lives in many aspects, but have also inevitably raised people's concerns about visual privacy at the same time. Motivated by recent user studies that people's privacy concerns are dependent on the context, in this paper, we propose Cardea, a context-aware and interactive visual privacy protection framework that enforces privacy protection according to people's privacy preferences. The framework provides people with fine-grained visual privacy protection using: i) personal privacy profiles, with which people can define their context-dependent privacy preferences; and ii) visual indicators: face features, for devices to automatically locate individuals who request privacy protection; and iii) hand gestures, for people to flexibly interact with cameras to temporarily change their privacy preferences. We design and implement the framework consisting of the client app on Android devices and the cloud server. Our evaluation results confirm this framework is practical and effective with 86% overall accuracy, showing promising future for context-aware visual privacy protection from pervasive cameras."
"Compressed Sensing (CS) and Total Variation (TV)- based iterative image reconstruction algorithms have received increased attention recently. This is due to the ability of such methods to reconstruct from limited and noisy data. Local TV methods fail to preserve texture details and fine structures, which are tedious for the method to distinguish from noise. In many cases local methods also create additional artifacts due to over smoothing. Non-Local Total Variation (NLTV) has been increasingly used for medical imaging applications. However, it is not updated in every iteration of the algorithm, has a high computational complexity and depends on the scale of pairwise parameters. In this work we propose using Adaptive Graph- based TV in combination with CS (ACSGT). Similar to NLTV our proposed method goes beyond spatial similarity between different regions of an image being reconstructed by establishing a connection between similar regions in the image regardless of spatial distance. However, it is computationally much more efficient and scalable when compared to NLTV due to the use of approximate nearest neighbor search algorithm. Moreover, our method is adaptive, i.e, it involves updating the graph prior every iteration making the connection between similar regions stronger. Since TV is a special case of graph TV the proposed method can be seen as a generalization of CS and TV methods. We test our proposed algorithm by reconstructing a variety of different phantoms from limited and corrupted data and observe that we achieve a better result with ACSGT in every case."
"In this paper, we study self-dual codes over $\mathbb{Z}_2 \times (\mathbb{Z}_2+u\mathbb{Z}_2) $, where $u^2=0$. Three types of self-dual codes are defined. For each type, the possible values $\alpha,\beta$ such that there exists a code $\mathcal{C}\subseteq \mathbb{Z}_{2}^\alpha\times (\mathbb{Z}_2+u\mathbb{Z}_2)^\beta$ are established. We also present several approaches to construct self-dual codes over $\mathbb{Z}_2 \times (\mathbb{Z}_2+u\mathbb{Z}_2) $. Moreover, the structure of two-weight self-dual codes is completely obtained for $\alpha \cdot\beta\neq 0$."
"We study a type of reverse (procurement) auction problems in the presence of budget constraints. The general algorithmic problem is to purchase a set of resources, which come at a cost, so as not to exceed a given budget and at the same time maximize a given valuation function. This framework captures the budgeted version of several well known optimization problems, and when the resources are owned by strategic agents the goal is to design truthful and budget feasible mechanisms, i.e. elicit the true cost of the resources and ensure the payments of the mechanism do not exceed the budget. Budget feasibility introduces more challenges in mechanism design, and we study instantiations of this problem for certain classes of submodular and XOS valuation functions. We first obtain mechanisms with an improved approximation ratio for weighted coverage valuations, a special class of submodular functions that has already attracted attention in previous works. We then provide a general scheme for designing randomized and deterministic polynomial time mechanisms for a class of XOS problems. This class contains problems whose feasible set forms an independence system (a more general structure than matroids), and some representative problems include, among others, finding maximum weighted matchings, maximum weighted matroid members, and maximum weighted 3D-matchings. For most of these problems, only randomized mechanisms with very high approximation ratios were known prior to our results."
"We consider the optical fiber channel modelled by the nonlinear Shr\""{o}dinger equation with zero dispersion and additive Gaussian noise. Using Feynman path-integral approach for the model we find corrections to conditional probability density function, output signal distribution, conditional and output signal entropies, and the channel capacity at large signal-to-noise ratio. We demonstrate that the correction to the channel capacity is positive for large signal power. Therefore, this correction increases the earlier calculated capacity for a nondispersive nonlinear optical fiber channel in the intermediate power region."
"This paper addresses the motion planning problem for a team of aerial agents under high level goals. We propose a hybrid control strategy that guarantees the accomplishment of each agent's local goal specification, which is given as a temporal logic formula, while guaranteeing inter-agent collision avoidance. In particular, by defining 3-D spheres that bound the agents' volume, we extend previous work on decentralized navigation functions and propose control laws that navigate the agents among predefined regions of interest of the workspace while avoiding collision with each other. This allows us to abstract the motion of the agents as finite transition systems and, by employing standard formal verification techniques, to derive a high-level control algorithm that satisfies the agents' specifications. Simulation and experimental results with quadrotors verify the validity of the proposed method."
"This paper addresses the problem of cooperative manipulation of a single object by N robotic agents under local goal specifications given as Metric Interval Temporal Logic (MITL) formulas. In particular, we propose a distributed model-free control protocol for the trajectory tracking of the cooperatively manipulated object without necessitating feedback of the contact forces/torques or inter-agent communication. This allows us to abstract the motion of the coupled object-agents system as a finite transition system and, by employing standard automata-based methodologies, we derive a hybrid control algorithm for the satisfaction of a given MITL formula. In addition, we use load sharing coefficients to represent potential differences in power capabilities among the agents. Finally, simulation studies verify the validity of the proposed scheme."
"Many fields are now snowed under with an avalanche of data, which raises considerable challenges for computer scientists. Meanwhile, robotics (among other fields) can often only use a few dozen data points because acquiring them involves a process that is expensive or time-consuming. How can an algorithm learn with only a few data points?"
"In this paper we show that a point visibility graph can have chromatic number 6 while its clique number is only 4, thus settling an open problem. Then we show that the problem of colouring the visibility graph of a given point set with 5 colours is NP-hard."
"Due to increasing shares of renewable energy sources, more frequency reserves are required to maintain power system stability. In this paper, we present a decentralized control scheme that allows a large aggregation of refrigerators to provide Primary Frequency Control (PFC) reserves to the grid based on local frequency measurements and without communication.   The control is based on stochastic switching of refrigerators depending on the frequency deviation. We develop methods to account for typical lockout constraints of compressors and increased power consumption during the startup phase. In addition, we propose a procedure to dynamically reset the thermostat temperature limits in order to provide reliable PFC reserves, as well as a corrective temperature feedback loop to build robustness to biased frequency deviations. Furthermore, we introduce an additional randomization layer in the controller to account for thermostat resolution limitations, and finally, we modify the control design to account for refrigerator door openings.   Extensive simulations with actual frequency signal data and with different aggregation sizes, load characteristics, and control parameters, demonstrate that the proposed controller outperforms a relevant state-of-the-art controller."
"There is a practically unlimited amount of natural language data available. Still, recent work in text comprehension has focused on datasets which are small relative to current computing possibilities. This article is making a case for the community to move to larger data and as a step in that direction it is proposing the BookTest, a new dataset similar to the popular Children's Book Test (CBT), however more than 60 times larger. We show that training on the new data improves the accuracy of our Attention-Sum Reader model on the original CBT test data by a much larger margin than many recent attempts to improve the model architecture. On one version of the dataset our ensemble even exceeds the human baseline provided by Facebook. We then show in our own human study that there is still space for further improvement."
"We consider the rates achievable by a user in a multibeam satellite system for unicast applications, and propose alternatives to the conventional single-user symbol-by-symbol detection applied at user terminals. Single-user detection is known to suffer from strong degradation when the terminal is located near the edge of the coverage area of a beam, and when aggressive frequency reuse is adopted. For this reason, we consider multiuser detection, and take into account the strongest interfering signal. We also analyze two additional transmission strategies requiring modifications at medium access control layer. We describe an information-theoretic framework to compare the different strategies by computing the information rate of the user in the reference beam. Furthermore, we analyze the performance of coded schemes that could approach the information-theoretic limits. We show that classical codes from the DVB-S2(X) standard are not suitable when multiuser detection is adopted, and we propose two ways to improve the performance, based on the redesign of the code and of the bit mapping."
"Stochastic optimization algorithms with variance reduction have proven successful for minimizing large finite sums of functions. However, in the context of empirical risk minimization, it is often helpful to augment the training set by considering random perturbations of input examples. In this case, the objective is no longer a finite sum, and the main candidate for optimization is the stochastic gradient descent method (SGD). In this paper, we introduce a variance reduction approach for this setting when the objective is strongly convex. After an initial linearly convergent phase, the algorithm achieves a $O(1/t)$ convergence rate in expectation like SGD, but with a constant factor that is typically much smaller, depending on the variance of gradient estimates due to perturbations on a single example."
"The cooperative medium access control (CoopMAC) protocol in the presence of randomly-distributed nodes and shadowing is considered. The nodes are assumed to be distributed according to a homogeneous two-dimensional Poisson point process. A new approach is proposed for helper selection and throughput performance analysis which depends on the shadowing parameters as well as the distribution of helpers. In the proposed protocol, the potential helpers are divided into several tiers based on their distances from the source and destination in a way that the lower the tier index, the higher its priority. When there are several helpers of the same tier, the helper that is less affected by shadowing is chosen for cooperation. Upper and lower bounds are derived for the average cooperative throughput of the proposed CoopMAC protocol. It is observed that the proposed scheme readily outperforms the conventional CoopMAC protocol in having larger average throughput. It is also seen that the cooperative throughput of the proposed scheme approaches the upper bound when the density of nodes increases."
"Over the years, several meta-heuristic algorithms were proposed and are now emerging as common methods for constrained optimization problems. Among them, genetic algorithms (GA's) shine as popular evolutionary algorithms (EA's) in engineering optimization. Most engineering design problems are difficult to resolve with conventional optimization algorithms because they are highly nonlinear and contain constraints. In order to handle these constraints, the most common technique is to apply penalty functions. The major drawback is that they require tuning of parameters, which can be very challenging. In this paper, we present a constraint-handling technique for GA's solely using the violation factor, called VCH (Violation Constraint-Handling) method. Several benchmark problems from the literature are examined. The VCH technique was able to provide a consistent performance and match results from other GA-based techniques."
"We investigate transmission energy minimization via optimizing wireless relay selection in orthogonal-frequency-division multiple access (OFDMA) networks. We take into account the impact of the load of cells on transmission energy. We prove the NP-hardness of the energy-aware wireless relay selection problem. To tackle the computational complexity, a partial optimality condition is derived for providing insights in respect of designing an effective and efficient algorithm. Numerical results show that the resulting algorithm achieves high energy performance."
"This paper lays out the current landscape of tools used in statistics education. In particular, it considers graphing calculators, spreadsheets, applets and microworlds, standalone educational software, statistical programming tools, tools for reproducible research and bespoke tools. The strengths and weaknesses of the tools are considered, particularly in the context of McNamara (2016)'s list of attributes for a statistical computing tool. Best practices for computing in introductory statistics are suggested."
"In the 1990s, statisticians began thinking in a principled way about how computation could support statistics and statistics education. Since then, the pace of software development has accelerated, advancements in computing and data science have moved the goalposts, and it is time to reassess. Software continues to be developed to help do and learn statistics, but there is little critical evaluation of the resulting tools, and no accepted framework with which to critique them. In this paper, we present a set of attributes necessary for a modern statistical computing tool. This framework is designed to be broadly applicable to both novice and expert users, but the particular focus is on making a more supportive statistical computing environment.   A modern statistical computing tool should be accessible, provide easy entry, privilege data as a first-order object, support exploratory and confirmatory analysis, allow for flexible plot creation, support randomization, be interactive, include inherent documentation, support narrative, publishing, and reproducibility, and be flexible to extensions. Ideally, all these attributes could be incorporated into one tool, supporting users at all levels, but a more reasonable goal is for tools designed for novices and professionals to `reach across the gap,' taking inspiration from each others' strengths."
"Transactional memory is a mechanism that manages thread synchronisation on behalf of a programmer so that blocks of code execute with an illusion of atomicity. The main safety criterion for transactional memory is opacity, which defines conditions for serialising concurrent transactions.   Proving opacity is complicated because it allows concurrent transactions to observe distinct memory states, while TM implementations are typically based on one single shared store. This paper presents a sound and complete method, based on coarse-grained abstraction, for reducing proofs of opacity to the relatively simpler correctness condition: linearizability. We use our methods to verify TML and NORec from the literature and show our techniques extend to relaxed memory models by showing that both are opaque under TSO without requiring additional fences. Our methods also elucidate TM designs at higher level of abstraction; as an application, we develop a variation of NORec with fast-path reads transactions. All our proofs have been mechanised, either in the Isabelle theorem prover or the PAT model checker."
"In this paper we study the approximability of (Finite-)Valued Constraint Satisfaction Problems (VCSPs) with a fixed finite constraint language {\Gamma} consisting of finitary functions on a fixed finite domain. An instance of VCSP is given by a finite set of variables and a sum of functions belonging to {\Gamma} and depending on a subset of the variables. Each function takes values in [0, 1] specifying costs of assignments of labels to its variables, and the goal is to find an assignment of labels to the variables that minimizes the sum. A recent result of Ene et al. says that, under the mild technical condition that {\Gamma} contains the equality relation, the basic LP relaxation is optimal for constant-factor approximation for VCSP({\Gamma}) unless the Unique Games Conjecture fails. Using the algebraic approach to the CSP, we give new natural algebraic conditions for the finiteness of the integrality gap for the basic LP relaxation of VCSP({\Gamma}). We also show how these algebraic conditions can in principle be used to round solutions of the basic LP relaxation, and how, for several examples that cover all previously known cases, this leads to efficient constant-factor approximation algorithms. Finally, we show that the absence of another algebraic condition leads to NP-hardness of constant-factor approximation."
"During natural or man-made disasters, humanitarian response organizations look for useful information to support their decision-making processes. Social media platforms such as Twitter have been considered as a vital source of useful information for disaster response and management. Despite advances in natural language processing techniques, processing short and informal Twitter messages is a challenging task. In this paper, we propose to use Deep Neural Network (DNN) to address two types of information needs of response organizations: 1) identifying informative tweets and 2) classifying them into topical classes. DNNs use distributed representation of words and learn the representation as well as higher level features automatically for the classification task. We propose a new online algorithm based on stochastic gradient descent to train DNNs in an online fashion during disaster situations. We test our models using a crisis-related real-world Twitter dataset."
"We develop here a computationally effective approach for producing high-quality $\mathcal{H}_\infty$-approximations to large scale linear dynamical systems having multiple inputs and multiple outputs (MIMO). We extend an approach for $\mathcal{H}_\infty$ model reduction introduced by Flagg, Beattie, and Gugercin for the single-input/single-output (SISO) setting, which combined ideas originating in interpolatory $\mathcal{H}_2$-optimal model reduction with complex Chebyshev approximation. Retaining this framework, our approach to the MIMO problem has its principal computational cost dominated by (sparse) linear solves, and so it can remain an effective strategy in many large-scale settings. We are able to avoid computationally demanding $\mathcal{H}_\infty$ norm calculations that are normally required to monitor progress within each optimization cycle through the use of ""data-driven"" rational approximations that are built upon previously computed function samples. Numerical examples are included that illustrate our approach. We produce high fidelity reduced models having consistently better $\mathcal{H}_\infty$ performance than models produced via balanced truncation; these models often are as good as (and occasionally better than) models produced using optimal Hankel norm approximation as well. In all cases considered, the method described here produces reduced models at far lower cost than is possible with either balanced truncation or optimal Hankel norm approximation."
We outline a program in the area of formalization of mathematics to automate theorem proving in algebra and algebraic geometry. We propose a construction of a dictionary between automated theorem provers and (La)TeX exploiting syntactic parsers. We describe its application to a repository of human-written facts and definitions in algebraic geometry (The Stacks Project). We use deep learning techniques.
This work addresses the problem of fusing two random vectors with unknown cross-correlations. We present a formulation and a numerical method for computing the optimal estimate in the minimax sense. We extend our formulation to linear measurement models that depend on two random vectors with unknown cross-correlations. As an application we consider the problem of decentralized state estimation for a group of agents. The proposed estimator takes cross-correlations into account while being less conservative than the widely used Covariance Intersection. We demonstrate the superiority of the proposed method compared to Covariance Intersection with numerical examples and simulations within the specific application of decentralized state estimation using relative position measurements.
"We propose RSFT, which is an extension of the one dimensional Sparse Fourier Transform algorithm to higher dimensions in a way that it can be applied to real, noisy data. The RSFT allows for off-grid frequencies. Furthermore, by incorporating Neyman-Pearson detection, the frequency detection stages in RSFT do not require knowledge of the exact sparsity of the signal and are more robust to noise. We analyze the asymptotic performance of RSFT, and study the computational complexity versus the worst case signal SNR tradeoff. We show that by choosing the proper parameters, the optimal tradeoff can be achieved. We discuss the application of RSFT on short range ubiquitous radar signal processing, and demonstrate its feasibility via simulations."
"Protein tertiary structure defines its functions, classification and binding sites. Similar structural characteristics between two proteins often lead to the similar characteristics thereof. Determining structural similarity accurately in real time is a crucial research issue. In this paper, we present a novel and effective scoring scheme that is dependent on novel features extracted from protein alpha carbon distance matrices. Our scoring scheme is inspired from pattern recognition and computer vision. Our method is significantly better than the current state of the art methods in terms of family match of pairs of protein structures and other statistical measurements. The effectiveness of our method is tested on standard benchmark structures. A web service is available at http://research.buet.ac.bd:8080/Comograd/score.html where you can get the similarity measurement score between two protein structures based on our method."
"We consider the stochastic $k$-TSP problem where rewards at vertices are random and the objective is to minimize the expected length of a tour that collects reward $k$. We present an adaptive $O(\log k)$-approximation algorithm, and a non-adaptive $O(\log^2k)$-approximation algorithm. We also show that the adaptivity gap of this problem is between $e$ and $O(\log^2k)$."
"Due to geometrical errors and possible misalignment of the sensors, wheel speed measurements provided by incremental encoders in road vehicles are usually affected by significant periodic noises. This paper presents an online wheel speed filtering procedure, based on a model of the sensor, aimed at processing the speed measurement to make it suitable for advanced vehicle dynamics applications. In particular, differently from low-pass and notch filtering, this strategy is reliable for the cycling cadence estimation from the wheel speed on bicycles. Experimental data are used to show the effectiveness of the proposed approach."
"Cheating is a real problem in the Internet of Things. The fundamental question that needs to be answered is how we can trust the validity of the data being generated in the first place. The problem, however, isn't inherent in whether or not to embrace the idea of an open platform and open-source software, but to establish a methodology to verify the trustworthiness and control any access. This paper focuses on building an access control model and system based on trust computing. This is a new field of access control techniques which includes Access Control, Trust Computing, Internet of Things, network attacks, and cheating technologies. Nevertheless, the target access control systems can be very complex to manage. This paper presents an overview of the existing work on trust computing, access control models and systems in IoT. It not only summarizes the latest research progress, but also provides an understanding of the limitations and open issues of the existing work. It is expected to provide useful guidelines for future research."
"Sparsity constrained single image super-resolution (SR) has been of much recent interest. A typical approach involves sparsely representing patches in a low-resolution (LR) input image via a dictionary of example LR patches, and then using the coefficients of this representation to generate the high-resolution (HR) output via an analogous HR dictionary. However, most existing sparse representation methods for super resolution focus on the luminance channel information and do not capture interactions between color channels. In this work, we extend sparsity based super-resolution to multiple color channels by taking color information into account. Edge similarities amongst RGB color bands are exploited as cross channel correlation constraints. These additional constraints lead to a new optimization problem which is not easily solvable; however, a tractable solution is proposed to solve it efficiently. Moreover, to fully exploit the complementary information among color channels, a dictionary learning method is also proposed specifically to learn color dictionaries that encourage edge similarities. Merits of the proposed method over state of the art are demonstrated both visually and quantitatively using image quality metrics."
This paper presents a novel approach to visual objects classification based on generating simple fuzzy classifiers using local image features to distinguish between one known class and other classes. Boosting meta learning is used to find the most representative local features. The proposed approach is tested on a state-of-the-art image dataset and compared with the bag-of-features image representation model combined with the Support Vector Machine classification. The novel method gives better classification accuracy and the time of learning and testing process is more than 30% shorter.
"Together with the development of more accurate methods in Computer Vision and Natural Language Understanding, holistic architectures that answer on questions about the content of real-world images have emerged. In this tutorial, we build a neural-based approach to answer questions about images. We base our tutorial on two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the models that we present here can achieve a competitive performance on both datasets, in fact, they are among the best methods that use a combination of LSTM with a global, full frame CNN representation of an image. We hope that after reading this tutorial, the reader will be able to use Deep Learning frameworks, such as Keras and introduced Kraino, to build various architectures that will lead to a further performance improvement on this challenging task."
"Detection rules have traditionally been designed for rational agents that minimize the Bayes risk (average decision cost). With the advent of crowd-sensing systems, there is a need to redesign binary hypothesis testing rules for behavioral agents, whose cognitive behavior is not captured by traditional utility functions such as Bayes risk. In this paper, we adopt prospect theory based models for decision makers. We consider special agent models namely optimists and pessimists in this paper, and derive optimal detection rules under different scenarios. Using an illustrative example, we also show how the decision rule of a human agent deviates from the Bayesian decision rule under various behavioral models, considered in this paper."
"Livestreaming platforms have become increasingly popular in recent years as a means of sharing and advertising creative content. Popular content streamers who attract large viewership to their live broadcasts can earn a living by means of ad revenue, donations and channel subscriptions. Unfortunately, this incentivized popularity has simultaneously resulted in incentive for fraudsters to provide services to astroturf, or artificially inflate viewership metrics by providing fake ""live"" views to customers. Our work provides a number of major contributions: (a) formulation: we are the first to introduce and characterize the viewbot fraud problem in livestreaming platforms, (b) methodology: we propose FLOCK, a principled and unsupervised method which efficiently and effectively identifies botted broadcasts and their constituent botted views, and (c) practicality: our approach achieves over 98% precision in identifying botted broadcasts and over 90% precision/recall against sizable synthetically generated viewbot attacks on a real-world livestreaming workload of over 16 million views and 92 thousand broadcasts. FLOCK successfully operates on larger datasets in practice and is regularly used at a large, undisclosed livestreaming corporation."
"Machine learning theory typically assumes that training data is unbiased and not adversarially generated. When real training data deviates from these assumptions, trained models make erroneous predictions, sometimes with disastrous effects. Robust losses, such as the huber norm, were designed to mitigate the effects of such contaminated data, but they are limited to the regression context.   In this paper, we show how to transform any optimization problem that arises from fitting a machine learning model into one that (1) detects and removes contaminated data from the training set while (2) simultaneously fitting the trimmed model on the uncontaminated data that remains. To solve the resulting nonconvex optimization problem, we introduce a fast stochastic proximal-gradient algorithm that incorporates prior knowledge through nonsmooth regularization. For datasets of size $n$, our approach requires $O(n^{2/3}/\varepsilon)$ gradient evaluations to reach $\varepsilon$-accuracy and, when a certain error bound holds, the complexity improves to $O(\kappa n^{2/3}\log(1/\varepsilon))$. These rates are $n^{1/3}$ times better than those achieved by typical, full gradient methods."
"The impact of Phasor Measurement Units (PMUs) for providing situational awareness to transmission system operators has been widely documented. Micro-PMUs ($\mu$PMUs) are an emerging sensing technology that can provide similar benefits to Distribution System Operators (DSOs), enabling a level of visibility into the distribution grid that was previously unattainable. In order to support the deployment of these high resolution sensors, the automation of data analysis and prioritizing communication to the DSO becomes crucial. In this paper, we explore the use of $\mu$PMUs to detect anomalies on the distribution grid. Our methodology is motivated by growing concern about failures and attacks to distribution automation equipment. The effectiveness of our approach is demonstrated through both real and simulated data."
"In this paper we provide the largest published comparison of translation quality for phrase-based SMT and neural machine translation across 30 translation directions. For ten directions we also include hierarchical phrase-based MT. Experiments are performed for the recently published United Nations Parallel Corpus v1.0 and its large six-way sentence-aligned subcorpus. In the second part of the paper we investigate aspects of translation speed, introducing AmuNMT, our efficient neural machine translation decoder. We demonstrate that current neural machine translation could already be used for in-production systems when comparing words-per-second ratios."
"An ordered graph $G$ is a graph whose vertex set is a subset of integers. The edges are interpreted as tuples $(u,v)$ with $u < v$. For a positive integer $s$, a matrix $M \in \mathbb{Z}^{s \times 4}$, and a vector $\mathbf{p} = (p,\ldots,p) \in \mathbb{Z}^s$ we build a conflict graph by saying that edges $(u,v)$ and $(x,y)$ are conflicting if $M(u,v,x,y)^\top \geq \mathbf{p}$ or $M(x,y,u,v)^\top \geq \mathbf{p}$, where the comparison is componentwise. This new framework generalizes many natural concepts of ordered and unordered graphs, such as the page-number, queue-number, band-width, interval chromatic number and forbidden ordered matchings.   For fixed $M$ and $p$, we investigate how the chromatic number of $G$ depends on the structure of its conflict graph. Specifically, we study the maximum chromatic number $X_\text{cli}(M,p,w)$ of ordered graphs $G$ with no $w$ pairwise conflicting edges and the maximum chromatic number $X_\text{ind}(M,p,a)$ of ordered graphs $G$ with no $a$ pairwise non-conflicting edges. We determine $X_\text{cli}(M,p,w)$ and $X_\text{ind}(M,p,a)$ exactly whenever $M$ consists of one row with entries in $\{-1,0,+1\}$ and moreover consider several cases in which $M$ consists of two rows or has arbitrary entries from $\mathbb{Z}$."
"Autonomous learning of robotic skills can allow general-purpose robots to learn wide behavioral repertoires without requiring extensive manual engineering. However, robotic skill learning methods typically make one of several trade-offs to enable practical real-world learning, such as requiring manually designed policy or value function representations, initialization from human-provided demonstrations, instrumentation of the training environment, or extremely long training times. In this paper, we propose a new reinforcement learning algorithm for learning manipulation skills that can train general-purpose neural network policies with minimal human engineering, while still allowing for fast, efficient learning in stochastic environments. Our approach builds on the guided policy search (GPS) algorithm, which transforms the reinforcement learning problem into supervised learning from a computational teacher (without human demonstrations). In contrast to prior GPS methods, which require a consistent set of initial states to which the system must be reset after each episode, our approach can handle randomized initial states, allowing it to be used in environments where deterministic resets are impossible. We compare our method to existing policy search techniques in simulation, showing that it can train high-dimensional neural network policies with the same sample efficiency as prior GPS methods, and present real-world results on a PR2 robotic manipulator."
"In this paper, we propose a compact twisted string actuation system that achieves a high contraction percentage (81%) on two phases: multi string twist and overtwist. This type of system can be used in many robotic applications, such as robotic hands and exoskeletons. The overtwist phase enables the development of more compact actuators based on the twisted string systems. Furthermore, by analyzing the previously developed mathematical models, we found out that a constant radius model should be applied for the overtwisting phase. Moreover, we propose an improvement of an existing model for prediction of the radius of the multi string system after they twist around each other. This model helps to better estimate the bundle diameter which results in a more precise mathematical model for multi string systems. The model was validated by performing experiments with 2, 4, 6 and 8 string systems. Finally, we performed extensive life cycle tests with different loads and contractions to find out the expected life of the system."
"Thanks to the available large-scale scene datasets such as Places and Places2, Convolutional Neural Networks (CNNs) have made remarkable progress on the problem of scene recognition. However, scene categories are often defined according its functions and there exist large intra-class variations in a single scene category. Meanwhile, as the number of scene classes is increasing, some classes tend to overlap with others and label ambiguity is becoming a problem. This paper focuses on large-scale scene recognition and makes two major contributions to tackle these issues. First, we propose a multi-resolution CNN architecture to capture visual content and structure at different scales. Our proposed multi-resolution CNNs are composed of coarse resolution CNNs and fine resolution CNNs, whose performance is complementary to each other. Second, we design two knowledge guided disambiguation techniques to deal with the problem of label ambiguity. In the first scenario, we exploit the knowledge from confusion matrix at validation data to merge similar classes into a super category, while in the second scenario, we utilize the knowledge of extra networks to produce a soft label for each image. Both the information of super category and soft labels are exploited to train CNNs on the Places2 datasets. We conduct experiments on three large-scale image classification datasets (ImangeNet, Places, Places2) to demonstrate the effectiveness of our proposed approach. In addition, our method takes part in two major scene recognition challenges, and we achieve the 2$^{nd}$ place at the Places2 challenge 2015 and 1$^{st}$ place at the LSUN challenge 2016. Finally, we transfer the learned representations to the datasets of MIT Indoor67 and SUN397, which yields the state-of-the-art performance (86.7% and 72.0%) on both datasets."
"We give a novel formal theoretical framework for unsupervised learning with two distinctive characteristics. First, it does not assume any generative model and based on a worst-case performance metric. Second, it is comparative, namely performance is measured with respect to a given hypothesis class. This allows to avoid known computational hardness results and improper algorithms based on convex relaxations. We show how several families of unsupervised learning models, which were previously only analyzed under probabilistic assumptions and are otherwise provably intractable, can be efficiently learned in our framework by convex optimization."
"This paper presents Mathematical Execution (ME), a new, unified approach for testing numerical code. The key idea is to (1) capture the desired testing objective via a representing function and (2) transform the automated testing problem to the minimization problem of the representing function. The minimization problem is to be solved via mathematical optimization. The main feature of ME is that it directs input space exploration by only executing the representing function, thus avoiding static or symbolic reasoning about the program semantics, which is particularly challenging for numerical code. To illustrate this feature, we develop an ME-based algorithm for coverage-based testing of numerical code. We also show the potential of applying and adapting ME to other related problems, including path reachability testing, boundary value analysis, and satisfiability checking.   To demonstrate ME's practical benefits, we have implemented CoverMe, a proof-of-concept realization for branch coverage based testing, and evaluated it on Sun's C math library (used in, for example, Android, Matlab, Java and JavaScript). We have compared CoverMe with random testing and Austin, a publicly available branch coverage based testing tool that supports numerical code (Austin combines symbolic execution and search-based heuristics). Our experimental results show that CoverMe achieves near-optimal and substantially higher coverage ratios than random testing on all tested programs, across all evaluated coverage metrics. Compared with Austin, CoverMe improves branch coverage from 43% to 91%, with significantly less time (6.9 vs. 6058.4 seconds on average)."
"We define in the setting of homotopy type theory an H-space structure on $\mathbb S^3$. Hence we obtain a description of the quaternionic Hopf fibration $\mathbb S^3\hookrightarrow\mathbb S^7\twoheadrightarrow\mathbb S^4$, using only homotopy invariant tools."
"The Chord distributed hash table (DHT) is well-known and often used to implement peer-to-peer systems. Chord peers find other peers, and access their data, through a ring-shaped pointer structure in a large identifier space. Despite claims of proven correctness, i.e., eventual reachability, previous work has shown that the Chord ring-maintenance protocol is not correct under its original operating assumptions. Previous work has not, however, discovered whether Chord could be made correct under the same assumptions. The contribution of this paper is to provide the first specification of correct operations and initialization for Chord, an inductive invariant that is necessary and sufficient to support a proof of correctness, and two independent proofs of correctness. One proof is informal and intuitive, and applies to networks of any size. The other proof is based on a formal model in Alloy, and uses fully automated analysis to prove the assertions for networks of bounded size. The two proofs complement each other in several important ways."
"This paper deals with the problem of preventive maintenance (PM) scheduling of pipelines subject to external corrosion defects. The preventive maintenance strategy involves an inspection step at some epoch, together with a repair schedule. This paper proposes to determine the repair schedule as well as an inspection time minimizing the maintenance cost. This problem is formulated as a binary integer non-linear programming model and we approach it under a decision support framework. We derive a polynomial-time algorithm that computes the optimum PM schedule and suggests different PM strategies in order to assist practitioners in making decision."
"The article presents the results of a study of the current state of higher IT education system in Ukraine. The problems of reforming higher IT education system of Ukraine in accordance with the commitments made by Ukraine in connection with the ratification of the EU-Ukraine Agreement Law of Ukraine N 1678-VII of September 16, 2014. An indicator of the presence or absence of a real reform of the system of higher IT education in Ukraine is detected. A comparative analysis of lists of IT-specialties of higher education in Ukraine in 2005 and 2015 with similar lists adopted by the international system of higher IT education is made. A discrepancy between the list of IT-specialties in Ukraine and international list of IT specialties are identified. The conclusion about the need for immediate correction of the list of higher education in Ukraine IT-specialties in order to bring it into line with international standards. It recommended a series of actions that will lead to the solution of the problem."
"Modulo inverse is an important arithmetic operation. Many famous algorithms in public key cryptography require to compute modulo inverse. It is argued that the method of DaYan deriving one of Jiushao Qin provides the most concise and transparent way of computing modulo inverse. Based on the rule of taking the least positive remainder in division, this paper presents a more precise algorithmic description of the method of DaYan deriving one to reflect Qin's original idea. Our form of the algorithm is straightforward and different from the ones in the literature. Some additional information can be revealed easily from the process of DaYan deriving one, e.g., the invariance property of the permanent of the state, natural connection to continued fractions. Comparison of Qin'a algorithm and the modern form of the Extended Euclidean algorithm is also given. Since DaYan deriving one is the key technical ingredient of Jiushao Qin's DaYan aggregation method (aka the Chinese Remainder Theorem), we include some explanation to the latter as well."
"Deep learning is a branch of artificial intelligence employing deep neural network architectures that has significantly advanced the state-of-the-art in computer vision, speech recognition, natural language processing and other domains. In November 2015, Google released $\textit{TensorFlow}$, an open source deep learning software library for defining, training and deploying machine learning models. In this paper, we review TensorFlow and put it in context of modern deep learning concepts and software. We discuss its basic computational paradigms and distributed execution model, its programming interface as well as accompanying visualization toolkits. We then compare TensorFlow to alternative libraries such as Theano, Torch or Caffe on a qualitative as well as quantitative basis and finally comment on observed use-cases of TensorFlow in academia and industry."
"The challenging requirements of 5G--from both the applications and the architecture perspectives--motivate the need to explore the feasibility of delivering services over new network architectures. As 5G proposes application-centric network slicing, which enables the use of new data planes realizable over a programmable compute, storage, and transport infrastructure, we consider Information-centric Networking (ICN) as a candidate network architecture to realize 5G objectives. This can co-exist with end-to-end IP services that are offered today. To this effect, we first propose a 5G-ICN architecture and compare its benefits (i.e., innovative services offered by leveraging ICN features) to current 3GPP-based mobile architectures. We then introduce a general application-driven framework that emphasizes on the flexibility afforded by Network Function Virtualization (NFV) and Software Defined Networking (SDN) over which 5G-ICN can be realized. We specifically focus on the issue of how mobility-as-a-service (MaaS) can be realized as a 5G-ICN slice, and give an in-depth overview on resource provisioning and inter-dependencies and -coordinations among functional 5G-ICN slices to meet the MaaS objectives."
"For which sets A does there exist a mapping, computed by a total or partial recursive function, such that the mapping, when its domain is restricted to A, is a 1-to-1, onto mapping to $\Sigma^*$? And for which sets A does there exist such a mapping that respects the lexicographical ordering within A? Both cases are types of perfect, minimal hash functions. The complexity-theoretic versions of these notions are known as compression functions and ranking functions. The present paper defines and studies the recursion-theoretic versions of compression and ranking functions, and in particular studies the question of which sets have, or lack, such functions."
"Recent results of Kaplan et al., building on previous work by Kuwakado and Morii, have shown that a wide variety of classically-secure symmetric-key cryptosystems are completely broken when exposed to quantum CPA attacks. In such an attack, the quantum adversary has the ability to query the cryptographic functionality in superposition. The vulnerable cryptosystems include the Even-Mansour block cipher, the three-round Feistel network, the Encrypted-CBC-MAC, and many others. In this work, we study simple algebraic adaptations of such schemes that replace $(\mathbb Z/2)^n$ addition with operations over alternate finite groups--such as $\mathbb Z/{2^n}$--and provide evidence that these adaptations are secure against quantum CPA attacks. These adaptations furthermore retain the classical security properties (and basic structural features) enjoyed by the original schemes.   We establish security by treating the (quantum) hardness of the well-studied Hidden Shift problem as a basic cryptographic assumption. We observe that this problem has a number of attractive features in this cryptographic context, including random self-reducibility, hardness amplification, and--in many cases of interest--a reduction from the ""search version"" to the ""decisional version."" We then establish, under this assumption, the security of several such hidden-shift adaptations of symmetric-key constructions against quantum CPA attack. We show that a Hidden Shift version of the Even-Mansour block cipher yields a quantum-secure pseudorandom function, and that a Hidden Shift version of the Encrypted CBC-MAC yields a collision-resistant hash function. Finally, we observe that such adaptations frustrate the direct Simon's algorithm-based attacks in more general circumstances, e.g., Feistel networks and slide attacks."
"With the roll-out of smart meters the importance of effective non-intrusive load monitoring (NILM) techniques has risen rapidly. NILM estimates the power consumption of individual devices given their aggregate consumption. In this way, the combined consumption must only be monitored at a single, central point in the household, providing various advantages such as reduced cost for metering equipment. In this paper we discuss the fundamental building-blocks of NILM, first giving a taxonomy of appliance models and device signatures and then explaining common supervised and unsupervised learning methods. Furthermore, we outline a fundamental algorithm that tackles the task of NILM. Subsequently, this paper reviews recent research that has brought novel insight to the field and more effective techniques. Finally, we formulate future challenges in the domain of NILM and smart meters."
"Recently, multi-view representation learning has become a rapidly growing direction in machine learning and data mining areas. This paper first reviews the root methods and theories on multi-view representation learning, especially on canonical correlation analysis (CCA) and its several extensions. And then we investigate the advancement of multi-view representation learning that ranges from shallow methods including multi-modal topic learning, multi-view sparse coding, and multi-view latent space Markov networks, to deep methods including multi-modal restricted Boltzmann machines, multi-modal autoencoders, and multi-modal recurrent neural networks. Further, we also provide an important perspective from manifold alignment for multi-view representation learning. Overall, this survey aims to provide an insightful overview of theoretical basis and current developments in the field of multi-view representation learning and to help researchers find the most appropriate tools for particular applications."
"This paper presents an open platform, which collects multimodal environmental data related to air quality from several sources including official open sources, social media and citizens. Collecting and fusing different sources of air quality data into a unified air quality indicator is a highly challenging problem, leveraging recent advances in image analysis, open hardware, machine learning and data fusion and is expected to result in increased geographical coverage and temporal granularity of air quality data."
"Network complexity is increasing, making network control and orchestration a challenging task. The proliferation of network information and tools for data analytics can provide an important insight into resource provisioning and optimisation. The network knowledge incorporated in software defined networking can facilitate the knowledge driven control, leveraging the network programmability. We present Seer: a flexible, highly configurable data analytics platform for network intelligence based on software defined networking and big data principles. Seer combines a computational engine with a distributed messaging system to provide a scalable, fault tolerant and real-time platform for knowledge extraction. Our first prototype uses Apache Spark for streaming analytics and open network operating system (ONOS) controller to program a network in real-time. The first application we developed aims to predict the mobility pattern of mobile devices inside a smart city environment."
"Several methods have recently been proposed to analyze speech and automatically infer the personality of the speaker. These methods often rely on prosodic and other hand crafted speech processing features extracted with off-the-shelf toolboxes. To achieve high accuracy, numerous features are typically extracted using complex and highly parameterized algorithms. In this paper, a new method based on feature learning and spectrogram analysis is proposed to simplify the feature extraction process while maintaining a high level of accuracy. The proposed method learns a dictionary of discriminant features from patches extracted in the spectrogram representations of training speech segments. Each speech segment is then encoded using the dictionary, and the resulting feature set is used to perform classification of personality traits. Experiments indicate that the proposed method achieves state-of-the-art results with a significant reduction in complexity when compared to the most recent reference methods. The number of features, and difficulties linked to the feature extraction process are greatly reduced as only one type of descriptors is used, for which the 6 parameters can be tuned automatically. In contrast, the simplest reference method uses 4 types of descriptors to which 6 functionals are applied, resulting in over 20 parameters to be tuned."
"If classifiers are selected from a hypothesis class to form an ensemble, bounds on average error rate over the selected classifiers include a component for selectivity, which grows as the fraction of hypothesis classifiers selected for the ensemble shrinks, and a component for variety, which grows with the size of the hypothesis class or in-sample data set. We show that the component for selectivity asymptotically dominates the component for variety, meaning that variety is essentially free."
"We present a weakly-supervised approach to segmenting proposed drivable paths in images with the goal of autonomous driving in complex urban environments. Using recorded routes from a data collection vehicle, our proposed method generates vast quantities of labelled images containing proposed paths and obstacles without requiring manual annotation, which we then use to train a deep semantic segmentation network. With the trained network we can segment proposed paths and obstacles at run-time using a vehicle equipped with only a monocular camera without relying on explicit modelling of road or lane markings. We evaluate our method on the large-scale KITTI and Oxford RobotCar datasets and demonstrate reliable path proposal and obstacle segmentation in a wide variety of environments under a range of lighting, weather and traffic conditions. We illustrate how the method can generalise to multiple path proposals at intersections and outline plans to incorporate the system into a framework for autonomous urban driving."
"Deep neural networks (DNN) have been proven to be quite effective in many applications such as image recognition and using software to process security or traffic camera footage, for example to measure traffic flows or spot suspicious activities. Despite the superior performance of DNN in these applications, it has recently been shown that a DNN is susceptible to a particular type of attack that exploits a fundamental flaw in its design. Specifically, an attacker can craft a particular synthetic example, referred to as an adversarial sample, causing the DNN to produce an output behavior chosen by attackers, such as misclassification. Addressing this flaw is critical if a DNN is to be used in critical applications such as those in cybersecurity. Previous work provided various defence mechanisms by either increasing the model nonlinearity or enhancing model complexity. However, after a thorough analysis of the fundamental flaw in the DNN, we discover that the effectiveness of such methods is limited. As such, we propose a new adversary resistant technique that obstructs attackers from constructing impactful adversarial samples by randomly nullifying features within samples. Using the MNIST dataset, we evaluate our proposed technique and empirically show our technique significantly boosts DNN's robustness against adversarial samples while maintaining high accuracy in classification."
"This paper studies the problem of constructing in-block controllable (IBC) regions for affine systems. That is, we are concerned with constructing regions in the state space of affine systems such that all the states in the interior of the region are mutually accessible through the region's interior by applying uniformly bounded inputs. We first show that existing results for checking in-block controllability on given polytopic regions cannot be easily extended to address the question of constructing IBC regions. We then explore the geometry of the problem to provide a computationally efficient algorithm for constructing IBC regions. We also prove the soundness of the algorithm. We then use the proposed algorithm to construct safe speed profiles for different robotic systems, including fully-actuated robots, ground robots modeled as unicycles with acceleration limits, and unmanned aerial vehicles (UAVs). Finally, we present several experimental results on UAVs to verify the effectiveness of the proposed algorithm. For instance, we use the proposed algorithm for real-time collision avoidance for UAVs."
"The test suite is essential for fault detection during software development. First-order mutation coverage is an accurate metric to quantify the quality of the test suite. However, it is computationally expensive. Hence, the adoption of this metric is limited. In this study, we address this issue by proposing a realistic model able to estimate first-order mutation coverage using only higher-order mutation coverage. Our study shows how the estimation evolves along with the order of mutation. We validate the model with an empirical study based on 17 open-source projects."
"This paper introduces the Event Capture Annotation Tool (ECAT), a user-friendly, open-source interface tool for annotating events and their participants in video, capable of extracting the 3D positions and orientations of objects in video captured by Microsoft's Kinect(R) hardware. The modeling language VoxML (Pustejovsky and Krishnaswamy, 2016) underlies ECAT's object, program, and attribute representations, although ECAT uses its own spec for explicit labeling of motion instances. The demonstration will show the tool's workflow and the options available for capturing event-participant relations and browsing visual data. Mapping ECAT's output to VoxML will also be addressed."
"Machine learning algorithms are increasingly influencing our decisions and interacting with us in all parts of our daily lives. Therefore, just like for power plants, highways, and a myriad of other engineered socio-technical systems, we must consider the safety of systems involving machine learning. Heretofore, the definition of safety has not been formalized in the machine learning context; in this paper, we do so by defining machine learning safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. We then use this definition to examine safety in all sorts of applications in cyber-physical systems, decision sciences and data products, finding that the foundational principle of modern statistical machine learning, empirical risk minimization, is not always a sufficient objective. In particular, we note an emerging dichotomy of applications: ones in which safety is important and risk minimization is not the complete story (we name these Type A applications), and ones in which safety is not so critical and risk minimization is sufficient (we name these Type B applications). Finally, we discuss how four different strategies for achieving safety in engineering (inherently safe design, safety reserves, safe fail, and procedural safeguards) can be mapped to the machine learning context through interpretability and causality of predictive models, objectives beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software and open data."
"We explore the performance and advantages/disadvantages of using unconditionally stable explicit super time-stepping (STS) algorithms versus implicit schemes with Krylov solvers for integrating parabolic operators in thermodynamic MHD models of the solar corona. Specifically, we compare the second-order Runge-Kutta Legendre (RKL2) STS method with the implicit backward Euler scheme computed using the preconditioned conjugate gradient (PCG) solver with both a point-Jacobi and a non-overlapping domain decomposition ILU0 preconditioner. The algorithms are used to integrate anisotropic Spitzer thermal conduction and artificial kinematic viscosity at time-steps much larger than classic explicit stability criteria allow. A key component of the comparison is the use of an established MHD model (MAS) to compute a real-world simulation on a large HPC cluster. Special attention is placed on the parallel scaling of the algorithms. It is shown that, for a specific problem and model, the RKL2 method is comparable or surpasses the implicit method with PCG solvers in performance and scaling, but suffers from some accuracy limitations. These limitations, and the applicability of RKL methods are briefly discussed."
"Despite computation becomes much complex on data with unprecedented large-scale, we argue computers or smart devices should and will consistently provide information and knowledge to human being in the order of a few tens milliseconds. We coin a new term 10-millisecond computing to call attention to this class of workloads. Public reports indicate that internet service users are sensitive to the service or job-level response time outliers, so we propose a very simple but powerful metric-outlier proportion to characterize the system behaviors.The outlier proportion is defined as follows: for N completed requests or jobs, if M jobs or requests' latencies exceed the outlier limit t, e.g. 10 milliseconds, the outlier proportion is M/N. 10-millisecond computing raises many challenges for both software and hardware stacks. In this paper, as a case study we investigate the challenges raised for conventional operating systems. For typical latency-critical services running with Linux on a 40-core server - a main-stream server hardware system in near future, we found, when the outlier limit decreases, the outlier proportion of a single server will significantly deteriorate. Meanwhile, the outlier proportion is further amplified by the system scale, including the system core number. For a 1K-scale system, we surprisingly find that to reduce the service or job-level outlier proportion to 10%, running Linux (version 2.6.32) or LXC (version 0.7.5 ) or XEN (version 4.0.0), respectively, the outlier proportion of a single server needs to be reduced by 871X, 2372X, 2372X accordingly. We also conducted a list of experiments to reveal the current Linux systems still suffer from poor outlier performance, including Linux kernel version 3.17.4, Linux kernel version 2.6.35M, a modified version of 2.6.35 integrated with sloppy counters and two representative real time schedulers."
"It is known that a PR-BOX (PR), a non-local resource and $(2\rightarrow 1)$ random access code (RAC), a functionality (wherein Alice encodes 2 bits into 1 bit message and Bob learns one of randomly chosen Alice's inputs) are equivalent under the no-signaling condition. In this work we introduce generalizations to PR and $(2\rightarrow 1)$ RAC and study their inter-convertibility. We introduce generalizations based on the number of inputs provided to Alice, $B_n$-BOX and $(n\rightarrow 1)$ RAC. We show that a $B_n$-BOX is equivalent to a no-signaling $(n\rightarrow 1)$ RACBOX (RB). Further we introduce a signaling $(n\rightarrow 1)$ RB which cannot simulate a $B_n$-BOX. Finally to quantify the same we provide a resource inequality between $(n\rightarrow 1)$ RB and $B_n$-BOX, and show that it is saturated. As an application we prove that one requires atleast $(n-1)$ PRs supplemented with a bit of communication to win a $(n\rightarrow 1)$ RAC. We further introduce generalizations based on the dimension of inputs provided to Alice and the message she sends, $B_n^d(+)$-BOX, $B_n^d(-)$-BOX and $(n\rightarrow 1,d)$ RAC ($d>2$). We show that no-signaling condition is not enough to enforce strict equivalence in the case of $d>2$. We introduce classes of no-signaling $(n\rightarrow 1,d)$ RB, one which can simulate $B_n^d(+)$-BOX, second which can simulate $B_n^d(-)$-BOX and third which cannot simulate either. Finally to quantify the same we provide a resource inequality between $(n\rightarrow 1,d)$ RB and $B_n^d(+)$-BOX, and show that it is saturated."
"Mobile network performance measurement is important for understanding mobile user experience, problem diagnosis, and service comparison. A number of crowdsourcing measurement apps (e.g., MobiPerf [4, 6] and Netalyzr [5, 7]) have been embarked for the last few years. Unlike existing apps that use active measurement methods, we employ a novel passive-active approach to continuously monitor per-app network performance on unrooted smartphones without injecting additional network traffic. By leveraging the VpnService API on Android, MopEye, our measurement app, intercepts all network traffic and then relays them to their destinations using socket APIs. Therefore, not only MopEye can measure the round-trip time accurately, it can do so without injecting additional traffic. As a result, the bandwidth cost (and monetary cost of data usage) for conducting such a measurement is eliminated, and the measurement can be conducted free of user intervention. Our evaluation shows that MopEye's RTT measurement is very close to result of tcpdump and is more accurate than MobiPerf. We have used MopEye to conduct a one-week measurement revealing multiple interesting findings on different apps' performance."
"Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks -- especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including to unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation."
"In this work we consider a communication problem in which a sender, Alice, wishes to communicate with a receiver, Bob, over a channel controlled by an adversarial jammer, James, who is {\em myopic}. Roughly speaking, for blocklength $n$, the codeword $X^n$ transmitted by Alice is corrupted by James who must base his adversarial decisions (of which locations of $X^n$ to corrupt and how to corrupt them) not on the codeword $X^n$ but on $Z^n$, an image of $X^n$ through a noisy memoryless channel. More specifically, our communication model may be described by two channels. A memoryless channel $p(z|x)$ from Alice to James, and an {\it Arbitrarily Varying Channel} from Alice to Bob, $p(y|x,s)$ governed by a state $X^n$ determined by James. In standard adversarial channels the states $S^n$ may depend on the codeword $X^n$, but in our setting $S^n$ depends only on James's view $Z^n$.   The myopic channel captures a broad range of channels and bridges between the standard models of memoryless and adversarial (zero-error) channels. In this work we present upper and lower bounds on the capacity of myopic channels. For a number of special cases of interest we show that our bounds are tight. We extend our results to the setting of {\em secure} communication in which we require that the transmitted message remain secret from James. For example, we show that if (i) James may flip at most a $p$ fraction of the bits communicated between Alice and Bob, and (ii) James views $X^n$ through a binary symmetric channel with parameter $q$, then once James is ""sufficiently myopic"" (in this case, when $q>p$), then the optimal communication rate is that of an adversary who is ""blind"" (that is, an adversary that does not see $X^n$ at all), which is $1-H(p)$ for standard communication, and $H(q)-H(p)$ for secure communication. A similar phenomenon exists for our general model of communication."
"This paper presents an approach combining lexico-semantic resources and distributed representations of words applied to the evaluation in machine translation (MT). This study is made through the enrichment of a well-known MT evaluation metric: METEOR. This metric enables an approximate match (synonymy or morphological similarity) between an automatic and a reference translation. Our experiments are made in the framework of the Metrics task of WMT 2014. We show that distributed representations are a good alternative to lexico-semantic resources for MT evaluation and they can even bring interesting additional information. The augmented versions of METEOR, using vector representations, are made available on our Github page."
"Cognitive radio networks (CRNs) propose a smart solution for spectrum usage inefficiency. Routing protocols for CRNs follow different criteria to choose the best route to the destination and to avoid the interference with primary users. Some protocols use cooperative communication techniques to achieve the full coexistence between secondary users (SUs) and primary users (PUs). Although using such cross-layer techniques have a great impact on the quality of the chosen routes, the existing work do not choose the best channel to send the data over. Thus, the available spectrum is not utilized efficiently. In this work, we propose CSCR, a channel selection scheme for cooperation-based routing protocols in CRNs. The proposed scheme increases the spectrum utilization through integrating the channels selection in the route discovery phase of the cooperation-based routing protocols. The best channels, that are less congested with primary users and that lead to minimum switching overhead, are chosen while constructing the cooperative group. Evaluating CSCR via NS2 simulations shows that it outperforms its counterparts in terms of goodput, end-to-end delay, and packet delivery ratio. The proposed scheme can enhance the network goodput, in some cases, by more than 150%, as compared to other related protocols."
"This paper proposes a task-space control protocol for the collaborative manipulation of a single object by two robotic arms. No feedback of the contact forces/torques is required, therefore employment of corresponding sensors is avoided. The proposed methodology is decentralized in the sense that each agent utilizes information associated with its own and the object's dynamic/kinematic parameters and no on-line communication takes place. An adaptive version of the control scheme is also introduced, where the agents' and object's dynamic parameters are considered unknown. Moreover, load sharing coefficients between the agents are employed and internal force regulation is guaranteed. Unit quaternions are also employed to represent the object's orientation. Finally, realistic simulation studies verify the effectiveness of the introduced control scheme."
"To reduce automobile exhaust pollution, traffic congestion and parking difficulties, bike-sharing systems are rapidly developed in many countries and more than 500 major cities in the world over the past decade. In this paper, we discuss a large-scale bike-sharing system under Markovian environment, and propose a mean-field matrix-analytic method in the study of bike-sharing systems through combining the mean-field theory with the time-inhomogeneous queues as well as the nonlinear QBD processes. Firstly, we establish an empirical measure process to express the states of this bike-sharing system. Secondly, we apply the mean-field theory to establishing a time-inhomogeneous MAP(t)/MAP(t)/1/K+2L+1 queue, and then to setting up a system of mean-field equations. Thirdly, we use the martingale limit theory to show the asymptotic independence of this bike-sharing system, and further analyze the limiting interchangeability as N goes to infinity and t goes to infinity. Based on this, we discuss and compute the fixed point in terms of a nonlinear QBD process. Finally, we analyze performance measures of this bike-sharing system, such as, the mean of stationary bike number at any station and the stationary probability of problematic stations. Furthermore, we use numerical examples to show how the performance measures depend on the key parameters of this bike-sharing system. We hope the methodology and results of this paper are applicable in the study of more general large-scale bike-sharing systems."
"Informative path planning (IPP) is used to design paths for robotic sensor platforms to extract the best/maximum possible information about a quantity of interest while operating under a set of constraints, such as the dynamic feasibility of vehicles. The key challenges of IPP are the strong coupling in multiple layers of decisions: the selection of locations to visit, the allocation of sensor platforms to those locations; and the processing of the gathered information along the paths. This paper presents a systematic procedure for IPP and environmental mapping using multiple UAV sensor platforms. It (a) selects the best locations to observe, (b) calculates the cost and finds the best paths for each UAV, and (c) estimates the measurement value within a given region using the Gaussian process (GP) regression framework. An illustrative example of RF intensity field mapping is presented to demonstrate the validity and applicability of the proposed approach."
"The current framework of Internet interconnections, based on transit and settlement-free peering relations, has systemic problems that often cause peering disputes. We propose a new techno-economic interconnection framework called Nash-Peering, which is based on the principles of Nash Bargaining in game theory and economics. Nash-Peering constitutes a radical departure from current interconnection practices, providing a broader and more economically efficient set of interdomain relations. In particular, the direction of payment is not determined by the direction of traffic or by rigid customer-provider relationships but based on which AS benefits more from the interconnection. We argue that Nash-Peering can address the root cause of various types of peering disputes."
"We propose a novel system for optical encryption based on an optical XOR and optical Linear Feedback Shift Register (oLFSRs). Though we choose LFSR for its ability to process optical signals at line rate, we consider the fact that it offers no cryptographic security. To address the security shortfall, we propose implementation of parallel oLFSRs, whereby the resulting key-stream at line rate is controlled electronically by a nonlinear random number generator at speeds much lower than the optical line rate, which makes the system practically relevant. The analysis of computational security shows that the proposed system is secure against wiretapping and can be engineered with the state of the art optical components."
"In recent years, the numbers of life-size humanoids as well as their mobile capabilities have steadily grown. Stable walking motion and control for humanoid robots are active fields of research. In this scenario an open question is how to model and analyse the scene so that a motion planning algorithm can generate an appropriate walking pattern. This paper presents the current work towards scene modelling and understanding, using an RGBD sensor. The main objective is to provide the humanoid robot iCub with capabilities to navigate safely and interact with various parts of the environment. In this sense we address the problem of traversability analysis of the scene, focusing on classification of point clouds as a function of mobility, and hence walking safety."
"Much of the information processed by Information Retrieval (IR) systems is unreliable, biased, and generally untrustworthy [1], [2], [3]. Yet, factuality & objectivity detection is not a standard component of IR systems, even though it has been possible in Natural Language Processing (NLP) in the last decade. Motivated by this, we ask if and how factuality & objectivity detection may benefit IR. We answer this in two parts. First, we use state-of-the-art NLP to compute the probability of document factuality & objectivity in two TREC collections, and analyse its relation to document relevance. We find that factuality is strongly and positively correlated to document relevance, but objectivity is not. Second, we study the impact of factuality & objectivity to retrieval effectiveness by treating them as query independent features that we combine with a competitive language modelling baseline. Experiments with 450 TREC queries show that factuality improves precision >10% over strong baselines, especially for uncurated data used in web search; objectivity gives mixed results. An overall clear trend is that document factuality & objectivity is much more beneficial to IR when searching uncurated (e.g. web) documents vs. curated (e.g. state documentation and newswire articles). To our knowledge, this is the first study of factuality & objectivity for back-end IR, contributing novel findings about the relation between relevance and factuality/objectivity, and statistically significant gains to retrieval effectiveness in the competitive web search task."
"In this paper, we consider the satisfiability problem for string logic with equations, regular membership and Presburger constraints over length functions. The difficulty comes from multiple occurrences of string variables making state-of-the-art algorithms non-terminating. Our main contribution is to show that the satisfiability problem in a fragment where no string variable occurs more than twice in an equation is decidable. In particular, we propose a semi-decision procedure for arbitrary string formulae with word equations, regular membership and length functions. The essence of our procedure is an algorithm to enumerate an equivalent set of solvable disjuncts for the formula. We further show that the algorithm always terminates for the aforementioned decidable fragment. Finally, we provide a complexity analysis of our decision procedure to prove that it runs, in the worst case, in factorial time."
"While spectrum at millimeter wave (mmWave) frequencies is less scarce than at traditional frequencies below 6 GHz, still it is not unlimited, in particular if we consider the requirements from other services using the same band and the need to license mmWave bands to multiple mobile operators. Therefore, an efficient spectrum access scheme is critical to harvest the maximum benefit from emerging mmWave technologies. In this paper, we introduce a new hybrid spectrum access scheme for mmWave networks, where data is aggregated through two mmWave carriers with different characteristics. In particular, we consider the case of a hybrid spectrum scheme between a mmWave band with exclusive access and a mmWave band where spectrum is pooled between multiple operators. To the best of our knowledge, this is the first study proposing hybrid spectrum access for mmWave networks and providing a quantitative assessment of its benefits. Our results show that this approach provides major advantages with respect to traditional fully licensed or fully unlicensed spectrum access schemes, though further work is needed to achieve a more complete understanding of both technical and non technical implications."
"A $ B_h $ set (or Sidon set of order $ h $) in an Abelian group $ G $ is any subset $ \{b_0, b_1, \ldots,b_{n}\} \subset G $ with the property that all the sums $ b_{i_1} + \cdots + b_{i_h} $ are different up to the order of the summands. Let $ \phi(h,n) $ denote the order of the smallest Abelian group containing a $ B_h $ set of cardinality $ n + 1 $. It is shown that, as $ h \to \infty $ and $ n $ is kept fixed, \[ \phi(h,n) \sim \frac{1}{n! \ \delta_{L}(\triangle^n)} h^n , \] where $ \delta_{L}(\triangle^n) $ is the lattice-packing density of an $ n $-simplex in the Euclidean space. This determines the asymptotics exactly in cases where this density is known ($ n \leq 3 $), and gives an improved upper bound on $ \phi(h,n) $ in the remaining cases. Covering analogs of Sidon sets are also introduced and their characterization in terms of lattice-coverings by simplices is given."
"In 2004, Muzereau et al. showed how to use a reduction algorithm of the discrete logarithm problem to Diffie-Hellman problem in order to estimate lower bound on Diffie-Hellman problem on elliptic curves. They presented their estimates for various elliptic curves that are used in practical applications. In this paper, we show that a much tighter lower bound for Diffie-Hellman problem on those curves can be achieved, if one uses the multiplicative group of a finite field as an auxiliary group. Moreover, improved lower bound estimates on Diffie-Hellman problem for various recommended curves are also given which are the tightest; thus, leading us towards the equivalence of Diffie-Hellman problem and the discrete logarithm problem for these recommended elliptic curves."
"We estimate sentiment categories proportions for retrieval within large retrieval sets. In general, estimates are produced by counting the classification outcomes and then by adjusting such category sizes taking into account misclassification error matrix. However, both the accuracy of the classifier and the precision of the retrieval produce a large number of errors that makes difficult the application of an aggregative approach to sentiment analysis as a reliable and efficient estimation of proportions for sentiment categories.   The challenge for real time analytics during retrieval is thus to overcome misclassification errors, and more importantly, to apply sentiment classification or any other similar post-processing analytics at retrieval time. We present a non-aggregative approach that can be applied to very large retrieval sets of queries."
"A Pascal challenge entitled monaural multi-talker speech recognition was developed, targeting the problem of robust automatic speech recognition against speech like noises which significantly degrades the performance of automatic speech recognition systems. In this challenge, two competing speakers say a simple command simultaneously and the objective is to recognize speech of the target speaker. Surprisingly during the challenge, a team from IBM research, could achieve a performance better than human listeners on this task. The proposed method of the IBM team, consist of an intermediate speech separation and then a single-talker speech recognition. This paper reconsiders the task of this challenge based on gain adapted factorial speech processing models. It develops a joint-token passing algorithm for direct utterance decoding of both target and masker speakers, simultaneously. Comparing it to the challenge winner, it uses maximum uncertainty during the decoding which cannot be used in the past two-phased method. It provides detailed derivation of inference on these models based on general inference procedures of probabilistic graphical models. As another improvement, it uses deep neural networks for joint-speaker identification and gain estimation which makes these two steps easier than before producing competitive results for these steps. The proposed method of this work outperforms past super-human results and even the results were achieved recently by Microsoft research, using deep neural networks. It achieved 5.5% absolute task performance improvement compared to the first super-human system and 2.7% absolute task performance improvement compared to its recent competitor."
"Face recognition (FR) is the most preferred mode for biometric-based surveillance, due to its passive nature of detecting subjects, amongst all different types of biometric traits. FR under surveillance scenario does not give satisfactory performance due to low contrast, noise and poor illumination conditions on probes, as compared to the training samples. A state-of-the-art technology, Deep Learning, even fails to perform well in these scenarios. We propose a novel soft-margin based learning method for multiple feature-kernel combinations, followed by feature transformed using Domain Adaptation, which outperforms many recent state-of-the-art techniques, when tested using three real-world surveillance face datasets."
"With a large proportion of people carrying location-aware smartphones, we have an unprecedented platform from which to understand individuals and predict their future actions. This work builds upon the Context Tree data structure that summarises the historical contexts of individuals from augmented geospatial trajectories, and constructs a predictive model for their likely future contexts. The Predictive Context Tree (PCT) is constructed as a hierarchical classifier, capable of predicting both the future locations that a user will visit and the contexts that a user will be immersed within. The PCT is evaluated over real-world geospatial trajectories, and compared against existing location extraction and prediction techniques, as well as a proposed hybrid approach that uses identified land usage elements in combination with machine learning to predict future interactions. Our results demonstrate that higher predictive accuracies can be achieved using this hybrid approach over traditional extracted location datasets, and the PCT itself matches the performance of the hybrid approach at predicting future interactions, while adding utility in the form of context predictions. Such a prediction system is capable of understanding not only where a user will visit, but also their context, in terms of what they are likely to be doing."
"Besides spoken words, speech signals also carry information about speaker gender, age, and emotional state which can be used in a variety of speech analysis applications. In this paper, a divide and conquer strategy for ensemble classification has been proposed to recognize emotions in speech. Intrinsic hierarchy in emotions has been utilized to construct an emotions tree, which assisted in breaking down the emotion recognition task into smaller sub tasks. The proposed framework generates predictions in three phases. Firstly, emotions are detected in the input speech signal by classifying it as neutral or emotional. If the speech is classified as emotional, then in the second phase, it is further classified into positive and negative classes. Finally, individual positive or negative emotions are identified based on the outcomes of the previous stages. Several experiments have been performed on a widely used benchmark dataset. The proposed method was able to achieve improved recognition rates as compared to several other approaches."
"Purpose: The main purpose of this study was to assess the reliability of shape and heterogeneity features in both Positron Emission Tomography (PET) and low-dose Computed Tomography (CT) components of PET/CT. A secondary objective was to investigate the impact of image quantization.Material and methods: A Health Insurance Portability and Accountability Act -compliant secondary analysis of deidentified prospectively acquired PET/CT test-retest datasets of 74 patients from multi-center Merck and ACRIN trials was performed. Metabolically active volumes were automatically delineated on PET with Fuzzy Locally Adaptive Bayesian algorithm. 3DSlicerTM was used to semi-automatically delineate the anatomical volumes on low-dose CT components. Two quantization methods were considered: a quantization into a set number of bins (quantizationB) and an alternative quantization with bins of fixed width (quantizationW). Four shape descriptors, ten first-order metrics and 26 textural features were computed. Bland-Altman analysis was used to quantify repeatability. Features were subsequently categorized as very reliable, reliable, moderately reliable and poorly reliable with respect to the corresponding volume variability. Results: Repeatability was highly variable amongst features. Numerous metrics were identified as poorly or moderately reliable. Others were (very) reliable in both modalities, and in all categories (shape, 1st-, 2nd- and 3rd-order metrics). Image quantization played a major role in the features repeatability. Features were more reliable in PET with quantizationB, whereas quantizationW showed better results in CT.Conclusion: The test-retest repeatability of shape and heterogeneity features in PET and low-dose CT varied greatly amongst metrics. The level of repeatability also depended strongly on the quantization step, with different optimal choices for each modality. The repeatability of PET and low-dose CT features should be carefully taken into account when selecting metrics to build multiparametric models."
"We describe an end-to-end framework for learning parameters of min-cost flow multi-target tracking problem with quadratic trajectory interactions including suppression of overlapping tracks and contextual cues about cooccurrence of different objects. Our approach utilizes structured prediction with a tracking-specific loss function to learn the complete set of model parameters. In this learning framework, we evaluate two different approaches to finding an optimal set of tracks under a quadratic model objective, one based on an LP relaxation and the other based on novel greedy variants of dynamic programming that handle pairwise interactions. We find the greedy algorithms achieve almost equivalent accuracy to the LP relaxation while being up to 10x faster than a commercial LP solver. We evaluate trained models on three challenging benchmarks. Surprisingly, we find that with proper parameter learning, our simple data association model without explicit appearance/motion reasoning is able to achieve comparable or better accuracy than many state-of-the-art methods that use far more complex motion features or appearance affinity metric learning."
"It is well known that the emptiness problem for binary probabilistic automata and so for quantum automata is undecidable. We present the current status of the emptiness problems for unary probabilistic and quantum automata with connections with Skolem's and positivity problems. We also introduce the concept of linear recurrence automata in order to show the connection naturally. Then, we also give possible generalizations of linear recurrence relations and automata on vectors."
"We investigate in this work a versatile convex framework for multiple image segmentation, relying on the regularized optimal mass transport theory. In this setting, several transport cost functions are considered and used to match statistical distributions of features. In practice, global multidimensional histograms are estimated from the segmented image regions, and are compared to referring models that are either fixed histograms given a priori, or directly inferred in the non-supervised case. The different convex problems studied are solved efficiently using primal-dual algorithms. The proposed approach is generic and enables multi-phase segmentation as well as co-segmentation of multiple images."
"This paper considers a distributed stochastic optimization problem where the goal is to minimize the time average of a cost function subject to a set of constraints on the time averages of a related stochastic processes called penalties. We assume that a delayed information about an event in the system is available as a common information at every user, and the state of the system is evolving in an independent and non-stationary fashion. We show that an approximate Drift-plus-penalty (DPP) algorithm that we propose achieves a time average cost that is within some positive constant epsilon of the optimal cost with high probability. Further, we provide a condition on the waiting time for this result to hold. The condition is shown to be a function of the mixing coefficient, the number of samples (w) used to compute an estimate of the distribution of the state, and the delay. Unlike the existing work, the method used in the paper can be adapted to prove high probability results when the state is evolving in a non-i.i.d and non-stationary fashion. Under mild conditions, we show that the dependency of the error bound on w is exponential, which is a significant improvement compared to the exiting work."
The recently introduced Intelligent Trial and Error algorithm (IT\&E) enables robots to creatively adapt to damage in a matter of minutes by combining an off-line evolutionary algorithm and an on-line learning algorithm based on Bayesian Optimization. We extend the IT\&E algorithm to allow for robots to learn to compensate for damages while executing their task(s). This leads to a semi-episodic learning scheme that increases the robot's lifetime autonomy and adaptivity. Preliminary experiments on a toy simulation and a 6-legged robot locomotion task show promising results.
"These lecture notes concern the basics of the theory of process behaviour. First the concept of a (labelled) transition system receives ample treatment and then the following issues concerning process behaviour are elaborated in the setting of transition systems: (i) concurrency and interaction, (ii) abstraction, (iii) sequential composition, alternative composition, and iteration, (iv) expressions that represent process behaviours, and (v) structural operational semantics and equational laws for such expressions. To quicken an intuitive understanding, direct connections with programs and automata are established wherever appropriate. For the interested reader, direct connections with Petri nets are also established. In each chapter, except the last one, it shows that what has been dealt with so far still has certain limitations. Each time, the next chapter is devoted to reducing the limitations concerned."
"Privacy preserving networks can be modelled as decentralized networks (e.g., sensors, connected objects, smartphones), where communication between nodes of the network is not controlled by an all-knowing, central node. For this type of networks, the main issue is to gather/learn global information on the network (e.g., by optimizing a global cost function) while keeping the (sensitive) information at each node. In this work, we focus on text information that agents do not want to share (e.g., text messages, emails, confidential reports). We use recent advances on decentralized optimization and topic models to infer topics from a graph with limited communication. We propose a method to adapt latent Dirichlet allocation (LDA) model to decentralized optimization and show on synthetic data that we still recover similar parameters and similar performance at each node than with stochastic methods accessing to the whole information in the graph."
"The wait-free read-write memory model has been characterized as an iterated \emph{Immediate Snapshot} (IS) task. The IS task is \emph{affine}---it can be defined as a (sub)set of simplices of the standard chromatic subdivision. It is known that the task of \emph{Weak Symmetry Breaking} (WSB) cannot be represented as an affine task. In this paper, we highlight the phenomenon of a ""natural"" model that can be captured by an iterated affine task and, thus, by a subset of runs of the iterated immediate snapshot model. We show that the read-write memory model in which, additionally, $k$-set-consensus objects can be used is, unlike WSB, ""natural"" by presenting the corresponding simple affine task captured by a subset of $2$-round IS runs. Our results imply the first combinatorial characterization of models equipped with abstractions other than read-write memory that applies to generic tasks."
"In this paper, a spatial multiplexing multiple-input multiple-output (MIMO) system when hardware along with RF imperfections occur during the communication setup is analytically investigated. More specifically, the scenario of hardware impairments at the transceiver and imperfect channel state information (CSI) at the receiver is considered, when successive interference cancellation (SIC) is implemented. Two popular linear detection schemes are analyzed, namely, zero forcing SIC (ZF-SIC) and minimum mean-square error SIC (MMSE-SIC). New analytical expressions for the outage probability of each SIC stage are provided, when independent and identically distributed Rayleigh fading channels are considered. In addition, the well-known error propagation effect between consecutive SIC stages is analyzed, while closed-form expressions are derived for some special cases of interest. Finally, useful engineering insights are manifested, such as the achievable diversity order, the performance difference between ZF- and MMSE-SIC, and the impact of imperfect CSI and/or the presence of hardware impairments to the overall system performance."
Layers is an open source neural network toolkit aim at providing an easy way to implement modern neural networks. The main user target are students and to this end layers provides an easy scriptting language that can be early adopted. The user has to focus only on design details as network totpology and parameter tunning.
"Phase noise correction is crucial to exploit full advantage of orthogonal frequency division multiplexing (OFDM) in modern high-data-rate communications. OFDM channel estimation with simultaneous phase noise compensation has therefore drawn much attention and stimulated continuing efforts. Existing methods, however, either have not taken into account the fundamental properties of phase noise or are only able to provide estimates of limited applicability owing to considerable computational complexity. In this paper, we have reformulated the joint estimation problem in the time domain as opposed to existing frequency-domain approaches, which enables us to develop much more efficient algorithms using the majorization-minimization technique. In addition, we propose a method based on dimensionality reduction and the Bayesian Information Criterion (BIC) that can adapt to various phase noise levels and accomplish much lower mean squared error than the benchmarks without incurring much additional computational cost. Several numerical examples with phase noise generated by free-running oscillators or phase-locked loops demonstrate that our proposed algorithms outperform existing methods with respect to both computational efficiency and mean squared error within a large range of signal-to-noise ratios."
"Neural networks are known to be effective function approximators. Recently, deep neural networks have proven to be very effective in pattern recognition, classification tasks and human-level control to model highly nonlinear realworld systems. This paper investigates the effectiveness of deep neural networks in the modeling of dynamical systems with complex behavior. Three deep neural network structures are trained on sequential data, and we investigate the effectiveness of these networks in modeling associated characteristics of the underlying dynamical systems. We carry out similar evaluations on select publicly available system identification datasets. We demonstrate that deep neural networks are effective model estimators from input-output data"
"We study efficiency and budget balance for designing mechanisms in general quasi-linear domains. Green and Laffont (1979) proved that one cannot generically achieve both. We consider strategyproof budget-balanced mechanisms that are approximately efficient. For deterministic mechanisms, we show that a strategyproof and budget-balanced mechanism must have a sink agent whose valuation function is ignored in selecting an alternative, and she is compensated with the payments made by the other agents. We assume the valuations of the agents come from a bounded open interval. Using this result, we find a tight lower bound on the inefficiencies of strategyproof, budget-balanced mechanisms in this domain. The bound shows that the inefficiency asymptotically disappears when the number of agents is large---a result close in spirit to Green and Laffont (1979, Theorem 9.4). However, our results provide worst-case bounds and the best possible rate of convergence.   Next, we consider minimizing any convex combination of inefficiency and budget imbalance. We show that if the valuations are unrestricted, no deterministic mechanism can do asymptotically better than minimizing inefficiency alone.   Finally, we investigate randomized mechanisms and provide improved lower bounds on expected inefficiency. We give a tight lower bound for an interesting class of strategyproof, budget-balanced, randomized mechanisms. We also use an optimization-based approach---in the spirit of automated mechanism design---to provide a lower bound on the minimum achievable inefficiency of any randomized mechanism.   Experiments with real data from two applications show that the inefficiency for a simple randomized mechanism is 5--100 times smaller than the worst case. This relative difference increases with the number of agents."
"The lack of large video databases obtained from real patients with respiratory disorders makes the design and optimization of video-based monitoring systems quite critical. The purpose of this study is the development of suitable models and simulators of breathing behaviors and disorders, such as respiratory pauses and apneas, in order to allow efficient design and test of video-based monitoring systems. More precisely, a novel Continuous-Time Markov Chain (CTMC) statistical model of breathing patterns is presented. The Respiratory Rate (RR) pattern, estimated by measured vital signs of hospital-monitored patients, is approximated as a CTMC, whose states and parameters are selected through an appropriate statistical analysis. Then, two simulators, software- and hardware-based, are proposed. After validation of the CTMC model, the proposed simulators are tested with previously developed video-based algorithms for the estimation of the RR and the detection of apnea events. Examples of application to assess the performance of systems for video-based RR estimation and apnea detection are presented. The results, in terms of Kullback-Leibler divergence, show that realistic breathing patterns, including specific respiratory disorders, can be accurately described by the proposed model; moreover, the simulators are able to reproduce practical breathing patterns for video analysis. The presented CTMC statistical model can be strategic to describe realistic breathing patterns and devise simulators useful to develop and test novel and effective video processing-based monitoring systems."
"This paper studies the operation and scheduling of electric loads in micro-grid, a highly automated and distributed cyber-physical energy system (CPES). We establish rigorous mathematical expressions for electric loads and battery banks in the micro-grid by considering their characteristics and constraints. Based on these mathematical models, we propose a novel real-time scheduling analysis method for priority-based energy management in micro-grid, named Significant Moments Analysis (SMA). SMA pinpoints all the crucial moments when electrical operations are requested among the micro-grid and establishes a dynamic model to describe the scheduling behavior of electric loads. Using SMA, we can check the scheduling feasibility and predict whether the micro-grid can generate enough power to support the execution of electric loads. In the case where the power is insufficient to supply load demands, SMA can provide accurate information about the amount of insufficient power and the time when the insufficiency happens. Simulated results are presented to show the effectiveness of the proposed analysis method."
"We consider the following distributed pursuit-evasion problem. A team of mobile agents called searchers starts at an arbitrary node of an unknown $n$-node network. Their goal is to execute a search strategy that guarantees capturing a fast and invisible intruder regardless of its movements using as few agents as possible. We restrict our attention to networks that are embedded into partial grids: nodes are placed on the plane at integer coordinates and only nodes at distance one can be adjacent. We give a distributed algorithm for the searchers that allow them to compute a connected and monotone strategy that guarantees searching any unknown partial grid with the use of $O(\sqrt{n})$ searchers. As for a lower bound, not only there exist partial grids that require $\Omega(\sqrt{n})$ searchers, but we prove that for each distributed searching algorithm there is a partial grid that forces the algorithm to use $\Omega(\sqrt{n})$ searchers but $O(\log n)$ searchers are sufficient in the offline scenario. This gives a lower bound of $\Omega(\sqrt{n}/\log n)$ in terms of achievable competitive ratio of any distributed algorithm."
"In this paper, we consider the cooperative output regulation problem for heterogeneous linear multi-agent systems in the presence of communication constraints. Under standard assumptions on the agents dynamics, we propose a distributed control algorithm relying on intermittent and asynchronous discrete-time information exchange that can be subject to unknown time-varying delays and information losses. We show that cooperative output regulation can be reached for arbitrary characteristics of the discrete-time communication process and under mild assumptions on the interconnection topology between agents. A numerical example is given to illustrate the effectiveness of our theoretical results."
"Visual Question Answering (VQA) is a recent problem in computer vision and natural language processing that has garnered a large amount of interest from the deep learning, computer vision, and natural language processing communities. In VQA, an algorithm needs to answer text-based questions about images. Since the release of the first VQA dataset in 2014, several additional datasets have been released and many algorithms have been proposed. In this review, we critically examine the current state of VQA in terms of problem formulation, existing datasets, evaluation metrics, and algorithms. In particular, we discuss the limitations of current datasets with regard to their ability to properly train and assess VQA algorithms. We then exhaustively review existing algorithms for VQA. Finally, we discuss possible future directions for VQA and image understanding research."
"Data vectors generalise finite multisets: they are finitely supported functions into a commutative monoid. We study the question if a given data vector can be expressed as a finite sum of others, only assuming that 1) the domain is countable and 2) the given set of base vectors is finite up to permutations of the domain.   Based on a succinct representation of the involved permutations as integer linear constraints, we derive that positive instances can be witnessed in a bounded subset of the domain.   For data vectors over a group we moreover study when a data vector is reversible, that is, if its inverse is expressible using only nonnegative coefficients. We show that if all base vectors are reversible then the expressibility problem reduces to checking membership in finitely generated subgroups. Moreover, checking reversibility also reduces to such membership tests.   These questions naturally appear in the analysis of counter machines extended with unordered data: namely, for data vectors over $(\mathbb{Z}^d,+)$ expressibility directly corresponds to checking state equations for Coloured Petri nets where tokens can only be tested for equality. We derive that in this case, expressibility is in NP, and in P for reversible instances. These upper bounds are tight: they match the lower bounds for standard integer vectors (over singleton domains)."
"Let $\mathbb{F}_{p^m}$ be a finite field of cardinality $p^m$, where $p$ is a prime, and $k, N$ be any positive integers. We denote $R_k=F_{p^m}[u]/\langle u^k\rangle =F_{p^m}+uF_{p^m}+\ldots+u^{k-1}F_{p^m}$ ($u^k=0$) and $\lambda=a_0+a_1u+\ldots+a_{k-1}u^{k-1}$ where $a_0, a_1,\ldots, a_{k-1}\in F_{p^m}$ satisfying $a_0\neq 0$ and $a_1=1$. Let $r$ be a positive integer satisfying $p^{r-1}+1\leq k\leq p^r$. We defined a Gray map from $R_k$ to $F_{p^m}^{p^r}$ first, then prove that the Gray image of any linear $\lambda$-constacyclic code over $R_k$ of length $N$ is a distance invariant linear $a_0^{p^r}$-constacyclic code over $F_{p^m}$ of length $p^rN$. Furthermore, the generator polynomials for each linear $\lambda$-constacyclic code over $R_k$ of length $N$ and its Gray image are given respectively. Finally, some optimal constacyclic codes over $F_{3}$ and $F_{5}$ are constructed."
"In this paper, we study the Temporal Difference (TD) learning with linear value function approximation. It is well known that most TD learning algorithms are unstable with linear function approximation and off-policy learning. Recent development of Gradient TD (GTD) algorithms has addressed this problem successfully. However, the success of GTD algorithms requires a set of well chosen features, which are not always available. When the number of features is huge, the GTD algorithms might face the problem of overfitting and being computationally expensive. To cope with this difficulty, regularization techniques, in particular $\ell_1$ regularization, have attracted significant attentions in developing TD learning algorithms. The present work combines the GTD algorithms with $\ell_1$ regularization. We propose a family of $\ell_1$ regularized GTD algorithms, which employ the well known soft thresholding operator. We investigate convergence properties of the proposed algorithms, and depict their performance with several numerical experiments."
"This paper describes the probabilistic behaviour of a random Sturmian word. It performs the probabilistic analysis of the recurrence function which can be viewed as a waiting time to discover all the factors of length $n$ of the Sturmian word. This parameter is central to combinatorics of words. Having fixed a possible length $n$ for the factors, we let $\alpha$ to be drawn uniformly from the unit interval $[0,1]$, thus defining a random Sturmian word of slope $\alpha$. Thus the waiting time for these factors becomes a random variable, for which we study the limit distribution and the limit density."
This work presents an on-going investigation of the control of a pneumatic soft-robot actuator addressing accurate patient positioning systems in maskless head and neck cancer radiotherapy. We employ two RGB-D sensors in a sensor fusion scheme to better estimate a patient's head pitch motion. A system identification prediction error model is used to obtain a linear time invariant state space model. We then use the model to design a linear quadratic Gaussian feedback controller to manipulate the patient head position based on sensed head pitch motion. Experiments demonstrate the success of our approach.
"We present DASH, a C++ template library that offers distributed data structures and parallel algorithms and implements a compiler-free PGAS (partitioned global address space) approach. DASH offers many productivity and performance features such as global-view data structures, efficient support for the owner-computes model, flexible multidimensional data distribution schemes and inter-operability with STL (standard template library) algorithms. DASH also features a flexible representation of the parallel target machine and allows the exploitation of several hierarchically organized levels of locality through a concept of Teams. We evaluate DASH on a number of benchmark applications and we port a scientific proxy application using the MPI two-sided model to DASH. We find that DASH offers excellent productivity and performance and demonstrate scalability up to 9800 cores."
"Legged locomotion has received increasing attention from the robotics community. Contacts stability plays a critical role in ensuring that robots do not fall and it is a key element of balancing and walking controllers. The Center of Pressure is a contact stability criterion that defines a point which must be kept strictly inside the support polygon in order to ensure postural stability. We introduce the concept of the sensitivity of the static center of pressure, roughly speaking the rate of change of the center of pressure with respect to the system equilibrium configurations, as an additional criterion to assess the robustness of the contact stability. We show how this new concept can be used as a metric to assess balancing controllers performance and we apply it to evaluate the performance of two different balancing controllers. The analytical analysis is performed on a simplified model and validated during balancing tasks on the iCub humanoid robot."
"In this paper we present a maneuver regulation scheme for Vertical Take-Off and Landing (VTOL) micro aerial vehicles (MAV). Differently from standard trajectory tracking, maneuver regulation has an intrinsic robustness due to the fact that the vehicle is not required to chase a virtual target, but just to stay on a (properly designed) desired path with a given velocity profile. In this paper we show how a robust maneuver regulation controller can be easily designed by converting an existing tracking scheme. The resulting maneuvering controller has three main appealing features, namely it: (i) inherits the robustness properties of the tracking controller, (ii) gains the appealing features of maneuver regulation, and (iii) does not need any additional tuning with respect to the tracking controller. We prove the correctness of the proposed scheme and show its effectiveness in experiments on a nano-quadrotor. In particular, we show on a nontrivial maneuver how external disturbances acting on the quadrotor cause instabilities in the standard tracking, while marginally affect the maneuver regulation scheme."
"We use tools from geometric statistics to analyze the usual estimation procedure of a template shape. This applies to shapes from landmarks, curves, surfaces, images etc. We demonstrate the asymptotic bias of the template shape estimation using the stratified geometry of the shape space. We give a Taylor expansion of the bias with respect to a parameter $\sigma$ describing the measurement error on the data. We propose two bootstrap procedures that quantify the bias and correct it, if needed. They are applicable for any type of shape data. We give a rule of thumb to provide intuition on whether the bias has to be corrected. This exhibits the parameters that control the bias' magnitude. We illustrate our results on simulated and real shape data."
"We present the specification for a modeling language, VoxML, which encodes semantic knowledge of real-world objects represented as three-dimensional models, and of events and attributes related to and enacted over these objects. VoxML is intended to overcome the limitations of existing 3D visual markup languages by allowing for the encoding of a broad range of semantic knowledge that can be exploited by a variety of systems and platforms, leading to multimodal simulations of real-world scenarios using conceptual objects that represent their semantic values."
"With current efforts to design Future Internet Architectures (FIAs), the evaluation and comparison of different proposals is an interesting research challenge. Previously, metrics such as bandwidth or latency have commonly been used to compare FIAs to IP networks. We suggest the use of power consumption as a metric to compare FIAs. While low power consumption is an important goal in its own right (as lower energy use translates to smaller environmental impact as well as lower operating costs), power consumption can also serve as a proxy for other metrics such as bandwidth and processor load.   Lacking power consumption statistics about either commodity FIA routers or widely deployed FIA testbeds, we propose models for power consumption of FIA routers. Based on our models, we simulate scenarios for measuring power consumption of content delivery in different FIAs. Specifically, we address two questions: 1) which of the proposed FIA candidates achieves the lowest energy footprint; and 2) which set of design choices yields a power-efficient network architecture? Although the lack of real-world data makes numerous assumptions necessary for our analysis, we explore the uncertainty of our calculations through sensitivity analysis of input parameters."
"In vivo wireless medical devices have a critical role in healthcare technologies due to their continuous health monitoring and noninvasive surgery capabilities. In order to fully exploit the potential of such devices, it is necessary to characterize the in vivo wireless communication channel which will help to build reliable and high-performance communication systems. This paper presents preliminary results of experimental characterization for this fascinating communications medium on a human cadaver and compares the results with numerical studies."
"We discuss the use of elliptic curves in cryptography on high-dimensional surfaces. In particular, instead of a Diffie-Hellman key exchange protocol written in the form of a bi-dimensional row, where the elements are made up with 256 bits, we propose a key exchange protocol given in a matrix form, with four independent entries each of them constructed with 64 bits. Apart from the great advantage of significantly reducing the number of used bits, this methodology appears to be immune to attacks of the style of Western, Miller, and Adleman, and at the same time it is also able to reach the same level of security as the cryptographic system presently obtained by the Microsoft Digital Rights Management. A nonlinear differential equation (NDE) admitting the elliptic curves as a special case is also proposed. The study of the class of solutions of this NDE is in progress."
"Word embeddings have been extensively studied in large text datasets. However, only a few studies analyze semantic representations of small corpora, particularly relevant in single-person text production studies. In the present paper, we compare Skip-gram and LSA capabilities in this scenario, and we test both techniques to extract relevant semantic patterns in single-series dreams reports. LSA showed better performance than Skip-gram in small size training corpus in two semantic tests. As a study case, we show that LSA can capture relevant words associations in dream reports series, even in cases of small number of dreams or low-frequency words. We propose that LSA can be used to explore words associations in dreams reports, which could bring new insight into this classic research area of psychology"
"We introduce the lifted Generalized Belief Propagation (GBP) message passing algorithm, for the computation of sum-product queries in Probabilistic Relational Models (e.g. Markov logic network). The algorithm forms a compact region graph and establishes a modified version of message passing, which mimics the GBP behavior in a corresponding ground model. The compact graph is obtained by exploiting a graphical representation of clusters, which reduces cluster symmetry detection to isomorphism tests on small local graphs. The framework is thus capable of handling complex models, while remaining domain-size independent."
"Online social networks represent a main source of communication and information exchange in today's life. They facilitate exquisitely news sharing, knowledge elicitation, and forming groups of same interests. Researchers in the last two decades studied the growth dynamics of the online social networks extensively questing a clear understanding of the behavior of humans in online social networks that helps in many directions, like engineering better recommendation systems and attracting new members. However, not all of social networks achieved the desired growth, for example, online social networks like MySpace, Orkut, and Friendster are out of service today. In this work, we present a probabilistic theoretical model that captures the dynamics of the social decay due to the inactivity of the members of a social network. The model is proved to have some interesting mathematical properties, namely \textit{submodularity}, which imply achieving the model optimization in a reasonable performance. That means the maximization problem can be approximated within a factor of $(1-1/e)$ and the minimization problem can be achieved in polynomial time."
"We will demonstrate a conversational products recommendation agent. This system shows how we combine research in personalized recommendation systems with research in dialogue systems to build a virtual sales agent. Based on new deep learning technologies we developed, the virtual agent is capable of learning how to interact with users, how to answer user questions, what is the next question to ask, and what to recommend when chatting with a human user.   Normally a descent conversational agent for a particular domain requires tens of thousands of hand labeled conversational data or hand written rules. This is a major barrier when launching a conversation agent for a new domain. We will explore and demonstrate the effectiveness of the learning solution even when there is no hand written rules or hand labeled training data."
"Deep Neural Networks (DNNs) have become very popular for prediction in many areas. Their strength is in representation with a high number of parameters that are commonly learned via gradient descent or similar optimization methods. However, the representation is non-standardized, and the gradient calculation methods are often performed using component-based approaches that break parameters down into scalar units, instead of considering the parameters as whole entities. In this work, these problems are addressed. Standard notation is used to represent DNNs in a compact framework. Gradients of DNN loss functions are calculated directly over the inner product space on which the parameters are defined. This framework is general and is applied to two common network types: the Multilayer Perceptron and the Deep Autoencoder."
"The use of microblogging platforms such as Twitter during crises has become widespread. More importantly, information disseminated by affected people contains useful information like reports of missing and found people, requests for urgent needs etc. For rapid crisis response, humanitarian organizations look for situational awareness information to understand and assess the severity of the crisis. In this paper, we present a novel framework (i) to generate abstractive summaries useful for situational awareness, and (ii) to capture sub-topics and present a short informative summary for each of these topics. A summary is generated using a two stage framework that first extracts a set of important tweets from the whole set of information through an Integer-linear programming (ILP) based optimization technique and then follows a word graph and concept event based abstractive summarization technique to produce the final summary. High accuracies obtained for all the tasks show the effectiveness of the proposed framework."
"Here we present DeepGaze II, a model that predicts where people look in images. The model uses the features from the VGG-19 deep neural network trained to identify objects in images. Contrary to other saliency models that use deep features, here we use the VGG features for saliency prediction with no additional fine-tuning (rather, a few readout layers are trained on top of the VGG features to predict saliency). The model is therefore a strong test of transfer learning. After conservative cross-validation, DeepGaze II explains about 87% of the explainable information gain in the patterns of fixations and achieves top performance in area under the curve metrics on the MIT300 hold-out benchmark. These results corroborate the finding from DeepGaze I (which explained 56% of the explainable information gain), that deep features trained on object recognition provide a versatile feature space for performing related visual tasks. We explore the factors that contribute to this success and present several informative image examples. A web service is available to compute model predictions at http://deepgaze.bethgelab.org."
"Identity verification based on authenticity assessment of a handwritten signature is an important issue in biometrics. There are many effective methods for signature verification taking into account dynamics of a signing process. Methods based on partitioning take a very important place among them. In this paper we propose a new approach to signature partitioning. Its most important feature is the possibility of selecting and processing of hybrid partitions in order to increase a precision of the test signature analysis. Partitions are formed by a combination of vertical and horizontal sections of the signature. Vertical sections correspond to the initial, middle, and final time moments of the signing process. In turn, horizontal sections correspond to the signature areas associated with high and low pen velocity and high and low pen pressure on the surface of a graphics tablet. Our previous research on vertical and horizontal sections of the dynamic signature (created independently) led us to develop the algorithm presented in this paper. Selection of sections, among others, allows us to define the stability of the signing process in the partitions, promoting signature areas of greater stability (and vice versa). In the test of the proposed method two databases were used: public MCYT-100 and paid BioSecure."
"In this paper, the problem of proactive deployment of cache-enabled unmanned aerial vehicles (UAVs) for optimizing the quality-of-experience (QoE) of wireless devices in a cloud radio access network (CRAN) is studied. In the considered model, the network can leverage human-centric information such as users' visited locations, requested contents, gender, job, and device type to predict the content request distribution and mobility pattern of each user. Then, given these behavior predictions, the proposed approach seeks to find the user-UAV associations, the optimal UAVs' locations, and the contents to cache at UAVs. This problem is formulated as an optimization problem whose goal is to maximize the users' QoE while minimizing the transmit power used by the UAVs. To solve this problem, a novel algorithm based on the machine learning framework of conceptor-based echo state networks (ESNs) is proposed. Using ESNs, the network can effectively predict each user's content request distribution and its mobility pattern when limited information on the states of users and the network is available. Based on the predictions of the users' content request distribution and their mobility patterns, we derive the optimal user-UAV association, optimal locations of the UAVs as well as the content to cache at UAVs. Simulation results using real pedestrian mobility patterns from BUPT and actual content transmission data from Youku show that the proposed algorithm can yield 40% and 61% gains, respectively, in terms of the average transmit power and the percentage of the users with satisfied QoE compared to a benchmark algorithm without caching and a benchmark solution without UAVs."
"For a seamless deployment of the Internet of Things (IoT), there is a need for self-organizing solutions to overcome key IoT challenges that include data processing, resource management, coexistence with existing wireless networks, and improved IoT-wide event detection. One of the most promising solutions to address these challenges is via the use of innovative learning frameworks that will enable the IoT devices to operate autonomously in a dynamic environment. However, developing learning mechanisms for the IoT requires coping with unique IoT properties in terms of resource constraints, heterogeneity, and strict quality-of-service requirements. In this paper, a number of emerging learning frameworks suitable for IoT applications are presented. In particular, the advantages, limitations, IoT applications, and key results pertaining to machine learning, sequential learning, and reinforcement learning are studied. For each type of learning, the computational complexity, required information, and learning performance are discussed. Then, to handle the heterogeneity of the IoT, a new framework based on the powerful tools of cognitive hierarchy theory is introduced. This framework is shown to efficiently capture the different IoT device types and varying levels of available resources among the IoT devices. In particular, the different resource capabilities of IoT devices are mapped to different levels of rationality in cognitive hierarchy theory, thus enabling the IoT devices to use different learning frameworks depending on their available resources. Finally, key results on the use of cognitive hierarchy theory in the IoT are presented."
"Domain adaptation, adapting models from domains rich in labeled training data to domains poor in such data, is a fundamental NLP challenge. We introduce a neural network model that marries together ideas from two prominent strands of research on domain adaptation through representation learning: structural correspondence learning (SCL, (Blitzer et al., 2006)) and autoencoder neural networks. Particularly, our model is a three-layer neural network that learns to encode the nonpivot features of an input example into a low-dimensional representation, so that the existence of pivot features (features that are prominent in both domains and convey useful information for the NLP task) in the example can be decoded from that representation. The low-dimensional representation is then employed in a learning algorithm for the task. Moreover, we show how to inject pre-trained word embeddings into our model in order to improve generalization across examples with similar pivot features. On the task of cross-domain product sentiment classification (Blitzer et al., 2007), consisting of 12 domain pairs, our model outperforms both the SCL and the marginalized stacked denoising autoencoder (MSDA, (Chen et al., 2012)) methods by 3.77% and 2.17% respectively, on average across domain pairs."
"In the IEEE 802.11p standard addressing vehicular communications, Basic Safety Messages (BSMs) can be bundled together and relayed as to increase the effective communication range of transmitting vehicles. This process forms a vehicular ad hoc network (VANET) for the dissemination of safety information. The number of ""shortest multihop paths"" (or geodesics) connecting two network nodes is an important statistic which can be used to enhance throughput, validate threat events, protect against collusion attacks, infer location information, and also limit redundant broadcasts thus reducing interference. To this end, we analytically calculate for the first time the mean and variance of the number of geodesics in 1D VANETs."
"We introduce a machine learning approach for extracting fine-grained representations of protein evolution from molecular dynamics datasets. Metastable switching linear dynamical systems extend standard switching models with a physically-inspired stability constraint. This constraint enables the learning of nuanced representations of protein dynamics that closely match physical reality. We derive an EM algorithm for learning, where the E-step extends the forward-backward algorithm for HMMs and the M-step requires the solution of large biconvex optimization problems. We construct an approximate semidefinite program solver based on the Frank-Wolfe algorithm and use it to solve the M-step. We apply our EM algorithm to learn accurate dynamics from large simulation datasets for the opioid peptide met-enkephalin and the proto-oncogene Src-kinase. Our learned models demonstrate significant improvements in temporal coherence over HMMs and standard switching models for met-enkephalin, and sample transition paths (possibly useful in rational drug design) for Src-kinase."
"Neural network models have a reputation for being black boxes. We propose a new method to understand better the roles and dynamics of the intermediate layers. This has direct consequences on the design of such models and it enables the expert to be able to justify certain heuristics (such as the auxiliary heads in the Inception model). Our method uses linear classifiers, referred to as ""probes"", where a probe can only use the hidden units of a given intermediate layer as discriminating features. Moreover, these probes cannot affect the training phase of a model, and they are generally added after training. They allow the user to visualize the state of the model at multiple steps of training. We demonstrate how this can be used to develop a better intuition about a known model and to diagnose potential problems."
"We present a new game semantics for Martin-L\""{o}f type theory (MLTT); our aim is to give a mathematical and intensional explanation for MLTT. Specifically, we propose a category with families of a novel variant of games, which induces a fully complete interpretation of the intensional variant of MLTT equipped with unit-, empty-, N-, dependent product-, dependent sum- and Id-types as well as a cumulative hierarchy of universes for the first time in the literature (as far as we are aware). Our variant of games generalizes an existing notion of games, and achieves an interpretation of dependent types and the hierarchy of universes in an intuitive yet mathematically precise manner. In particular, our variant of strategies can be seen as effective algorithms underlying programs (or proofs) in MLTT, achieving the research aim. We intend to extend the present work to homotopy type theory, which would in particular provides a computational interpretation for univalence axiom and higher-inductive types."
"We describe the Gun Violence Database (GVDB), a large and growing database of gun violence incidents in the United States. The GVDB is built from the detailed information found in local news reports about gun violence, and is constructed via a large-scale crowdsourced annotation effort through our web site, http://gun-violence.org/. We argue that centralized and publicly available data about gun violence can facilitate scientific, fact-based discussion about a topic that is often dominated by politics and emotion. We describe our efforts to automate the construction of the database using state-of-the-art natural language processing (NLP) technologies, eventually enabling a fully-automated, highly-scalable resource for research on this important public health problem."
"The phylogenetic network emerged in the 1990s as a new model to represent the evolution of species in the case where coexisting species transfer genetic information through hybridization, recombination, lateral gene transfer, etc. As is true for many rapidly evolving fields, there is considerable fragmentation and diversity in methodologies, standards and vocabulary in phylogenetic network research, thus creating the need for an integrated database of articles, authors, techniques, keywords and software. We describe such a database, ""Who is Who in Phylogenetic Networks"", available at http://phylnet.univ-mlv.fr. ""Who is Who in Phylogenetic Networks"" comprises more than 600 publications and 500 authors interlinked with a rich set of more than 200 keywords related to phylogenetic networks. The database is integrated with web-based tools to visualize authorship and collaboration networks and analyze these networks using common graph and social network metrics such as centrality (betweenness, eigenvector, degree and closeness) and clustering. We provide downloads of raw information about entries in the database, and a facility to suggest modifications and contribute new information to the database. We also present in this article common use cases of the database and identify trends in the research on phylogenetic networks using the information in the database and textual analysis."
"Inverse classification is the process of perturbing an instance in a meaningful way such that it is more likely to conform to a specific class. Historical methods that address such a problem are often framed to leverage only a single classifier, or specific set of classifiers. These works are often accompanied by naive assumptions. In this work we propose generalized inverse classification (GIC), which avoids restricting the classification model that can be used. We incorporate this formulation into a refined framework in which GIC takes place. Under this framework, GIC operates on features that are immediately actionable. Each change incurs an individual cost, either linear or non-linear. Such changes are subjected to occur within a specified level of cumulative change (budget). Furthermore, our framework incorporates the estimation of features that change as a consequence of direct actions taken (indirectly changeable features). To solve such a problem, we propose three real-valued heuristic-based methods and two sensitivity analysis-based comparison methods, each of which is evaluated on two freely available real-world datasets. Our results demonstrate the validity and benefits of our formulation, framework, and methods."
"We used convolutional neural networks (CNNs) for automatic sleep stage scoring based on single-channel electroencephalography (EEG) to learn task-specific filters for classification without using prior domain knowledge. We used an openly available dataset from 20 healthy young adults for evaluation and applied 20-fold cross-validation. We used class-balanced random sampling within the stochastic gradient descent (SGD) optimization of the CNN to avoid skewed performance in favor of the most represented sleep stages. We achieved high mean F1-score (81%, range 79-83%), mean accuracy across individual sleep stages (82%, range 80-84%) and overall accuracy (74%, range 71-76%) over all subjects. By analyzing and visualizing the filters that our CNN learns, we found that rules learned by the filters correspond to sleep scoring criteria in the American Academy of Sleep Medicine (AASM) manual that human experts follow. Our method's performance is balanced across classes and our results are comparable to state-of-the-art methods with hand-engineered features. We show that, without using prior domain knowledge, a CNN can automatically learn to distinguish among different normal sleep stages."
"There has been a recent paradigm shift in robotics to data-driven learning for planning and control. Due to large number of experiences required for training, most of these approaches use a self-supervised paradigm: using sensors to measure success/failure. However, in most cases, these sensors provide weak supervision at best. In this work, we propose an adversarial learning framework that pits an adversary against the robot learning the task. In an effort to defeat the adversary, the original robot learns to perform the task with more robustness leading to overall improved performance. We show that this adversarial framework forces the the robot to learn a better grasping model in order to overcome the adversary. By grasping 82% of presented novel objects compared to 68% without an adversary, we demonstrate the utility of creating adversaries. We also demonstrate via experiments that having robots in adversarial setting might be a better learning strategy as compared to having collaborative multiple robots."
"Fictitious play is a simple and widely studied adaptive heuristic for playing repeated games. It is well known that fictitious play fails to be Hannan consistent. Several variants of fictitious play including regret matching, generalized regret matching and smooth fictitious play, are known to be Hannan consistent. In this note, we consider sampled fictitious play: at each round, the player samples past times and plays the best response to previous moves of other players at the sampled time points. We show that sampled fictitious play, using Bernoulli sampling, is Hannan consistent. Unlike several existing Hannan consistency proofs that rely on concentration of measure results, ours instead uses anti-concentration results from Littlewood-Offord theory."
"The low-tubal-rank tensor model has been recently proposed for real-world multidimensional data. In this paper, we study the low-tubal-rank tensor completion problem, i.e., to recover a third-order tensor by observing a subset of its elements selected uniformly at random. We propose a fast iterative algorithm, called {\em Tubal-Alt-Min}, that is inspired by a similar approach for low-rank matrix completion. The unknown low-tubal-rank tensor is represented as the product of two much smaller tensors with the low-tubal-rank property being automatically incorporated, and Tubal-Alt-Min alternates between estimating those two tensors using tensor least squares minimization. First, we note that tensor least squares minimization is different from its matrix counterpart and nontrivial as the circular convolution operator of the low-tubal-rank tensor model is intertwined with the sub-sampling operator. Second, the theoretical performance guarantee is challenging since Tubal-Alt-Min is iterative and nonconvex in nature. We prove that 1) Tubal-Alt-Min guarantees exponential convergence to the global optima, and 2) for an $n \times n \times k$ tensor with tubal-rank $r \ll n$, the required sampling complexity is $O(nr^2k \log^3 n)$ and the computational complexity is $O(n^2rk^2 \log^2 n)$. Third, on both synthetic data and real-world video data, evaluation results show that compared with tensor-nuclear norm minimization (TNN-ADMM), Tubal-Alt-Min improves the recovery error dramatically (by orders of magnitude). It is estimated that Tubal-Alt-Min converges at an exponential rate $10^{-0.4423 \text{Iter}}$ where $\text{Iter}$ denotes the number of iterations, which is much faster than TNN-ADMM's $10^{-0.0332 \text{Iter}}$, and the running time can be accelerated by more than $5$ times for a $200 \times 200 \times 20$ tensor."
"We present a system to capture video footage of human subjects in the real world. Our system leverages a quadrotor camera to automatically capture well-composed video of two subjects. Subjects are tracked in a large-scale outdoor environment using RTK GPS and IMU sensors. Then, given the tracked state of our subjects, our system automatically computes static shots based on well-established visual composition principles and canonical shots from cinematography literature. To transition between these static shots, we calculate feasible, safe, and visually pleasing transitions using a novel real-time trajectory planning algorithm. We evaluate the performance of our tracking system, and experimentally show that RTK GPS significantly outperforms conventional GPS in capturing a variety of canonical shots. Lastly, we demonstrate our system guiding a consumer quadrotor camera autonomously capturing footage of two subjects in a variety of use cases. This is the first end-to-end system that enables people to leverage the mobility of quadrotors, as well as the knowledge of expert filmmakers, to autonomously capture high-quality footage of people in the real world."
"We present an isogeometric analysis of time-harmonic exterior acoustic problems. The infinite space is truncated by a fictitious boundary and (simple) absorbing boundary conditions are applied. The truncation error is included in the exact solution so that the reported error is an indicator of the performance of the isogeometric analysis, in particular of the related pollution error. Numerical results performed with high-order basis functions (third or fourth orders) showed no visible pollution error even for very high frequencies. This property combined with exact geometrical representation makes isogeometric analysis a very promising platform to solve high-frequency acoustic problems."
"Subjective expected utility theory assumes that decision-makers possess unlimited computational resources to reason about their choices; however, virtually all decisions in everyday life are made under resource constraints - i.e. decision-makers are bounded in their rationality. Here we experimentally tested the predictions made by a formalization of bounded rationality based on ideas from statistical mechanics and information-theory. We systematically tested human subjects in their ability to solve combinatorial puzzles under different time limitations. We found that our bounded-rational model accounts well for the data. The decomposition of the fitted model parameter into the subjects' expected utility function and resource parameter provide interesting insight into the subjects' information capacity limits. Our results confirm that humans gradually fall back on their learned prior choice patterns when confronted with increasing resource limitations."
"Augmenting RGB data with measured depth has been shown to improve the performance of a range of tasks in computer vision including object detection and semantic segmentation. Although depth sensors such as the Microsoft Kinect have facilitated easy acquisition of such depth information, the vast majority of images used in vision tasks do not contain depth information. In this paper, we show that augmenting RGB images with estimated depth can also improve the accuracy of both object detection and semantic segmentation. Specifically, we first exploit the recent success of depth estimation from monocular images and learn a deep depth estimation model. Then we learn deep depth features from the estimated depth and combine with RGB features for object detection and semantic segmentation. Additionally, we propose an RGB-D semantic segmentation method which applies a multi-task training scheme: semantic label prediction and depth value regression. We test our methods on several datasets and demonstrate that incorporating information from estimated depth improves the performance of object detection and semantic segmentation remarkably."
"Traditional saliency models usually adopt hand-crafted image features and human-designed mechanisms to calculate local or global contrast. In this paper, we propose a novel computational saliency model, i.e., deep spatial contextual long-term recurrent convolutional network (DSCLRCN) to predict where people looks in natural scenes. DSCLRCN first automatically learns saliency related local features on each image location in parallel. Then, in contrast with most other deep network based saliency models which infer saliency in local contexts, DSCLRCN can mimic the cortical lateral inhibition mechanisms in human visual system to incorporate global contexts to assess the saliency of each image location by leveraging the deep spatial long short-term memory (DSLSTM) model. Moreover, we also integrate scene context modulation in DSLSTM for saliency inference, leading to a novel deep spatial contextual LSTM (DSCLSTM) model. The whole network can be trained end-to-end and works efficiently when testing. Experimental results on two benchmark datasets show that DSCLRCN can achieve state-of-the-art performance on saliency detection. Furthermore, the proposed DSCLSTM model can significantly boost the saliency detection performance by incorporating both global spatial interconnections and scene context modulation, which may uncover novel inspirations for studies on them in computational saliency models."
"Tests for Esophageal cancer can be expensive, uncomfortable and can have side effects. For many patients, we can predict non-existence of disease with 100% certainty, just using demographics, lifestyle, and medical history information. Our objective is to devise a general methodology for customizing tests using user preferences so that expensive or uncomfortable tests can be avoided. We propose to use classifiers trained from electronic health records (EHR) for selection of tests. The key idea is to design classifiers with 100% false normal rates, possibly at the cost higher false abnormals. We compare Naive Bayes classification (NB), Random Forests (RF), Support Vector Machines (SVM) and Logistic Regression (LR), and find kernel Logistic regression to be most suitable for the task. We propose an algorithm for finding the best probability threshold for kernel LR, based on test set accuracy. Using the proposed algorithm, we describe schemes for selecting tests, which appear as features in the automatic classification algorithm, using preferences on costs and discomfort of the users. We test our methodology with EHRs collected for more than 3000 patients, as a part of project carried out by a reputed hospital in Mumbai, India. Kernel SVM and kernel LR with a polynomial kernel of degree 3, yields an accuracy of 99.8% and sensitivity 100%, without the MP features, i.e. using only clinical tests. We demonstrate our test selection algorithm using two case studies, one using cost of clinical tests, and other using ""discomfort"" values for clinical tests. We compute the test sets corresponding to the lowest false abnormals for each criterion described above, using exhaustive enumeration of 15 clinical tests. The sets turn out to different, substantiating our claim that one can customize test sets based on user preferences."
"In this paper, we describe a computational model for motion events in natural language that maps from linguistic expressions, through a dynamic event interpretation, into three-dimensional temporal simulations in a model. Starting with the model from (Pustejovsky and Moszkowicz, 2011), we analyze motion events using temporally-traced Labelled Transition Systems. We model the distinction between path- and manner-motion in an operational semantics, and further distinguish different types of manner-of-motion verbs in terms of the mereo-topological relations that hold throughout the process of movement. From these representations, we generate minimal models, which are realized as three-dimensional simulations in software developed with the game engine, Unity. The generated simulations act as a conceptual ""debugger"" for the semantics of different motion verbs: that is, by testing for consistency and informativeness in the model, simulations expose the presuppositions associated with linguistic expressions and their compositions. Because the model generation component is still incomplete, this paper focuses on an implementation which maps directly from linguistic interpretations into the Unity code snippets that create the simulations."
"Nowadays a lot of data is collected in online forums. One of the key tasks is to determine the social structure of these online groups, for example the identification of subgroups within a larger group. We will approach the grouping of individual as a classification problem. The classifier will be based on fuzzy logic. The input to the classifier will be linguistic features and degree of relationships (among individuals). The output of the classifiers are the groupings of individuals. We also incorporate a method that ranks the members of the detected subgroup to identify the hierarchies in each subgroup. Data from the HBO television show The Wire is used to analyze the efficacy and usefulness of fuzzy logic based methods as alternative methods to classical statistical methods usually used for these problems. The proposed methodology could detect automatically the most influential members of each organization The Wire with 90% accuracy."
"We introduce Symbolic Alternating Finite Automata (s-AFA) as an expressive, succinct, and decidable model for describing sets of finite sequences over arbitrary alphabets. Boolean operations over s-AFAs have linear complexity, which is in sharp contrast with the quadratic cost of intersection and union for non-alternating symbolic automata. Due to this succinctness, emptiness and equivalence checking are PSpace-hard.   We introduce an algorithm for checking the equivalence of two s-AFAs based on bisimulation up to congruence. This algorithm allows us to exploit the power of SAT and SMT solvers to efficiently search the state space of the s-AFAs. We evaluate our decision procedure on two verification and security applications: 1) checking satisfiability of linear temporal logic formulas over finite traces, and 2) checking equivalence of Boolean combinations of regular expressions. Our experiments show that our technique often outperforms existing techniques and it can be beneficial in both such applications."
"Machine-type devices (MTDs) will lie at the heart of the Internet of Things (IoT) system. A key challenge in such a system is sharing network resources between small MTDs, which have limited memory and computational capabilities. In this paper, a novel learning \emph{with finite memory} framework is proposed to enable MTDs to effectively learn about each others message state, so as to properly adapt their transmission parameters. In particular, an IoT system in which MTDs can transmit both delay tolerant, periodic messages and critical alarm messages is studied. For this model, the characterization of the exponentially growing delay for critical alarm messages and the convergence of the proposed learning framework in an IoT are analyzed. Simulation results show that the delay of critical alarm messages is significantly reduced up to $94\%$ with very minimal memory requirements. The results also show that the proposed learning with finite memory framework is very effective in mitigating the limiting factors of learning that prevent proper learning procedures."
"In the presence of accelerated fault rates, which are projected to be the norm on future exascale systems, it will become increasingly difficult for high-performance computing (HPC) applications to accomplish useful computation. Due to the fault-oblivious nature of current HPC programming paradigms and execution environments, HPC applications are insufficiently equipped to deal with errors. We believe that HPC applications should be enabled with capabilities to actively search for and correct errors in their computations. The redundant multithreading (RMT) approach offers lightweight replicated execution streams of program instructions within the context of a single application process. However, the use of complete redundancy incurs significant overhead to the application performance.   In this paper we present RedThreads, an interface that provides application-level fault detection and correction based on RMT, but applies the thread-level redundancy adaptively. We describe the RedThreads syntax and semantics, and the supporting compiler infrastructure and runtime system. Our approach enables application programmers to scope the extent of redundant computation. Additionally, the runtime system permits the use of RMT to be dynamically enabled, or disabled, based on the resiliency needs of the application and the state of the system. Our experimental results demonstrate how adaptive RMT exploits programmer insight and runtime inference to dynamically navigate the trade-off space between an application's resilience coverage and the associated performance overhead of redundant computation."
"Semantic segmentation of functional magnetic resonance imaging (fMRI) makes great sense for pathology diagnosis and decision system of medical robots. The multi-channel fMRI data provide more information of the pathological features. But the increased amount of data causes complexity in feature detection. This paper proposes a principal component analysis (PCA)-aided fully convolutional network to particularly deal with multi-channel fMRI. We transfer the learned weights of contemporary classification networks to the segmentation task by fine-tuning. The experiments results are compared with various methods e.g. k-NN. A new labelling strategy is proposed to solve the semantic segmentation problem with unclear boundaries. Even with a small-sized training dataset, the test results demonstrate that our model outperforms other pathological feature detection methods. Besides, its forward inference only takes 90 milliseconds for a single set of fMRI data. To our knowledge, this is the first time to realize pixel-wise labeling of multi-channel magnetic resonance image using FCN."
"Exploration in an unknown environment is the core functionality for mobile robots. Learning-based exploration methods, including convolutional neural networks, provide excellent strategies without human-designed logic for the feature extraction. But the conventional supervised learning algorithms cost lots of efforts on the labeling work of datasets inevitably. Scenes not included in the training set are mostly unrecognized either. We propose a deep reinforcement learning method for the exploration of mobile robots in an indoor environment with the depth information from an RGB-D sensor only. Based on the Deep Q-Network framework, the raw depth image is taken as the only input to estimate the Q values corresponding to all moving commands. The training of the network weights is end-to-end. In arbitrarily constructed simulation environments, we show that the robot can be quickly adapted to unfamiliar scenes without any man-made labeling. Besides, through analysis of receptive fields of feature representations, deep reinforcement learning motivates the convolutional networks to estimate the traversability of the scenes. The test results are compared with the exploration strategies separately based on deep learning or reinforcement learning. Even trained only in the simulated environment, experimental results in real-world environment demonstrate that the cognitive ability of robot controller is dramatically improved compared with the supervised method. We believe it is the first time that raw sensor information is used to build cognitive exploration strategy for mobile robots through end-to-end deep reinforcement learning."
"The purpose of this research was to synthesize basic and fundamental findings in quantum computing, as applied to the attack and defense of conventional computer networks. The concept focuses on uses of radio waves as a shield for, and attack against traditional computers. A logic bomb is analogous to a landmine in a computer network, and if one was to implement it as non-trivial mitigation, it will aid computer network defense. As has been seen in kinetic warfare, the use of landmines has been devastating to geopolitical regions in that they are severely difficult for a civilian to avoid triggering given the unknown position of a landmine. Thus, the importance of understanding a logic bomb is relevant and has corollaries to quantum mechanics as well. The research synthesizes quantum logic phase shifts in certain respects using the Dynamic Data Exchange protocol in software written for this work, as well as a C-NOT gate applied to a virtual quantum circuit environment by implementing a Quantum Fourier Transform. The research focus applies the principles of coherence and entanglement from quantum physics, the concept of expert systems in artificial intelligence, principles of prime number based cryptography with trapdoor functions, and modeling radio wave propagation against an event from unknown parameters. This comes as a program relying on the artificial intelligence concept of an expert system in conjunction with trigger events for a trapdoor function relying on infinite recursion, as well as system mechanics for elliptic curve cryptography along orbital angular momenta. Here trapdoor both denotes the form of cipher, as well as the implied relationship to logic bombs."
"Sleep stages pattern provides important clues in diagnosing the presence of sleep disorder. By analyzing sleep stages pattern and extracting its features from EEG, EOG, and EMG signals, we can classify sleep stages. This study presents a novel classification model for predicting sleep stages with a high accuracy. The main idea is to combine the generative capability of Deep Belief Network (DBN) with a discriminative ability and sequence pattern recognizing capability of Long Short-term Memory (LSTM). We use DBN that is treated as an automatic higher level features generator. The input to DBN is 28 ""handcrafted"" features as used in previous sleep stages studies. We compared our method with other techniques which combined DBN with Hidden Markov Model (HMM).In this study, we exploit the sequence or time series characteristics of sleep dataset. To the best of our knowledge, most of the present sleep analysis from polysomnogram relies only on single instanced label (nonsequence) for classification. In this study, we used two datasets: an open data set that is treated as a benchmark; the other dataset is our sleep stages dataset (available for download) to verify the results further. Our experiments showed that the combination of DBN with LSTM gives better overall accuracy 98.75\% (Fscore=0.9875) for benchmark dataset and 98.94\% (Fscore=0.9894) for MKG dataset. This result is better than the state of the art of sleep stages classification that was 91.31\%."
"We derive a general formula of the minimum achievable rate for fixed-to-variable length coding with a regular cost function by allowing the error probability up to a constant $\varepsilon$. For a fixed-to-variable length code, we call the set of source sequences that can be decoded without error the dominant set of source sequences. For any two regular cost functions, it is revealed that the dominant set of source sequences for a code attaining the minimum achievable rate with a cost function is also the dominant set for a code attaining the minimum achievable rate with the other cost function. We also give a general formula of the second-order minimum achievable rate."
"We consider collaborative graph exploration with a set of $k$ agents. All agents start at a common vertex of an initially unknown graph and need to collectively visit all other vertices. We assume agents are deterministic, vertices are distinguishable, moves are simultaneous, and we allow agents to communicate globally. For this setting, we give the first non-trivial lower bounds that bridge the gap between small ($k \leq \sqrt n$) and large ($k \geq n$) teams of agents. Remarkably, our bounds tightly connect to existing results in both domains.   First, we significantly extend a lower bound of $\Omega(\log k / \log\log k)$ by Dynia et al. on the competitive ratio of a collaborative tree exploration strategy to the range $k \leq n \log^c n$ for any $c \in \mathbb{N}$. Second, we provide a tight lower bound on the number of agents needed for any competitive exploration algorithm. In particular, we show that any collaborative tree exploration algorithm with $k = Dn^{1+o(1)}$ agents has a competitive ratio of $\omega(1)$, while Dereniowski et al. gave an algorithm with $k = Dn^{1+\varepsilon}$ agents and competitive ratio $O(1)$, for any $\varepsilon > 0$ and with $D$ denoting the diameter of the graph. Lastly, we show that, for any exploration algorithm using $k = n$ agents, there exist trees of arbitrarily large height $D$ that require $\Omega(D^2)$ rounds, and we provide a simple algorithm that matches this bound for all trees."
"In 2015, stroke was the number one cause of death in Indonesia. The majority type of stroke is ischemic. The standard tool for diagnosing stroke is CT-Scan. For developing countries like Indonesia, the availability of CT-Scan is very limited and still relatively expensive. Because of the availability, another device that potential to diagnose stroke in Indonesia is EEG. Ischemic stroke occurs because of obstruction that can make the cerebral blood flow (CBF) on a person with stroke has become lower than CBF on a normal person (control) so that the EEG signal have a deceleration. On this study, we perform the ability of 1D Convolutional Neural Network (1DCNN) to construct classification model that can distinguish the EEG and EOG stroke data from EEG and EOG control data. To accelerate training process our model we use Batch Normalization. Involving 62 person data object and from leave one out the scenario with five times repetition of measurement we obtain the average of accuracy 0.86 (F-Score 0.861) only at 200 epoch. This result is better than all over shallow and popular classifiers as the comparator (the best result of accuracy 0.69 and F-Score 0.72 ). The feature used in our study were only 24 handcrafted feature with simple feature extraction process."
"Civic Crowdfunding is emerging as a popular means to mobilize funding from citizens for public projects. A popular mechanism deployed on civic crowdfunding platforms is a provision point mechanism, wherein, the total contributions must reach a predetermined threshold in order for the project to be provisioned (undertaken). Such a mechanism may have multiple equilibria; unfortunately, in many of these equilibria, the project is not funded even if it is highly valued among the agents. Recent work has proposed mechanisms with refund bonuses where the project gets funded in equilibrium if its net value is higher than a threshold among the agents who are aware of the crowdfunding effort. In this paper, we formalize the notion of social desirability of a public project and propose mechanisms which use the idea of referrals to expand the pool of participants and achieve an equilibrium in which the project gets funded if its net value exceeds a threshold among the entire agent population. We call this new class of mechanisms Referral-Embedded Provision Point Mechanisms (REPPM). We specifically propose two variants of REPPM and both these mechanisms have the remarkable property that, at equilibrium, referral bonuses are offered but there is no need for actual payment of these bonuses. We establish that any given agent's equilibrium strategy is to refer other agents and to contribute in proportion to the agent's true value for the project. By referring others to contribute, an agent can, in fact, reduce his equilibrium contribution. In our view, the proposed mechanisms will lead to an increase in the number of projects that are funded on civic crowdfunding platforms."
"The microservice architecture is a style inspired by service-oriented computing that has recently started gaining popularity and that promises to change the way in which software is perceived, conceived and designed. In this paper we offer a short overview intended as a collection of bibliographic references and links in the field of Microservices Science and Engineering (MSE)."
"Energy efficiency has become one of the top design criteria for current computing systems. The dynamic voltage and frequency scaling (DVFS) has been widely adopted by laptop computers, servers, and mobile devices to conserve energy, while the GPU DVFS is still at a certain early age. This paper aims at exploring the impact of GPU DVFS on the application performance and power consumption, and furthermore, on energy conservation. We survey the state-of-the-art GPU DVFS characterizations, and then summarize recent research works on GPU power and performance models. We also conduct real GPU DVFS experiments on NVIDIA Fermi and Maxwell GPUs. According to our experimental results, GPU DVFS has significant potential for energy saving. The effect of scaling core voltage/frequency and memory voltage/frequency depends on not only the GPU architectures, but also the characteristic of GPU applications."
"Despite an extensive literature has been devoted to mine and model mobility features, forecasting where, when and whom people will encounter/colocate still deserve further research efforts. Forecasting people's encounter and colocation features is the key point for the success of many applications ranging from epidemiology to the design of new networking paradigms and services such as delay tolerant and opportunistic networks. While many algorithms which rely on both mobility and social information have been proposed, we propose a novel encounter and colocation predictive model which predicts user's encounter and colocation events and their features by exploiting the spatio-temporal regularity in the history of these events. We adopt weighted features Bayesian predictor and evaluate its accuracy on two large scales WiFi and cellular datasets. Results show that our approach could improve prediction accuracy w.r.t standard naive Bayesian and some of the state-of-the-art predictors."
"This study uses remote sensing technology that can provide information about the condition of the earth's surface area, fast, and spatially. The study area was in Karawang District, lying in the Northern part of West Java-Indonesia. We address a paddy growth stages classification using LANDSAT 8 image data obtained from multi-sensor remote sensing image taken in October 2015 to August 2016. This study pursues a fast and accurate classification of paddy growth stages by employing multiple regularizations learning on some deep learning methods such as DNN (Deep Neural Networks) and 1-D CNN (1-D Convolutional Neural Networks). The used regularizations are Fast Dropout, Dropout, and Batch Normalization. To evaluate the effectiveness, we also compared our method with other machine learning methods such as (Logistic Regression, SVM, Random Forest, and XGBoost). The data used are seven bands of LANDSAT-8 spectral data samples that correspond to paddy growth stages data obtained from i-Sky (eye in the sky) Innovation system. The growth stages are determined based on paddy crop phenology profile from time series of LANDSAT-8 images. The classification results show that MLP using multiple regularization Dropout and Batch Normalization achieves the highest accuracy for this dataset."
"Audio tagging aims to assign one or several tags to an audio clip. Most of the datasets are weakly labelled, which means only the tags of the clip are known, without knowing the occurrence time of the tags. The labeling of an audio clip is often based on the audio events in the clip and no event level label is provided to the user. Previous works have used the bag of frames model assume the tags occur all the time, which is not the case in practice. We propose a joint detection-classification (JDC) model to detect and classify the audio clip simultaneously. The JDC model has the ability to attend to informative and ignore uninformative sounds. Then only informative regions are used for classification. Experimental results on the ""CHiME Home"" dataset show that the JDC model reduces the equal error rate (EER) from 19.0% to 16.9%. More interestingly, the audio event detector is trained successfully without needing the event level label."
"In this paper we propose to represent a scene as an abstraction of 'things'. We start from 'things' as generated by modern object proposals, and we investigate their immediately observable properties: position, size, aspect ratio and color, and those only. Where the recent successes and excitement of the field lie in object identification, we represent the scene composition independent of object identities. We make three contributions in this work. First, we study simple observable properties of 'things', and call it things syntax. Second, we propose translating the things syntax in linguistic abstract statements and study their descriptive effect to retrieve scenes. Thirdly, we propose querying of scenes with abstract block illustrations and study their effectiveness to discriminate among different types of scenes. The benefit of abstract statements and block illustrations is that we generate them directly from the images, without any learning beforehand as in the standard attribute learning. Surprisingly, we show that even though we use the simplest of features from 'things' layout and no learning at all, we can still retrieve scenes reasonably well."
"The rapid growth of emerging information technologies and application patterns in modern society, e.g., Internet, Internet of Things, Cloud Computing and Tri-network Convergence, has caused the advent of the era of big data. Big data contains huge values, however, mining knowledge from big data is a tremendously challenging task because of data uncertainty and inconsistency. Attribute reduction (also known as feature selection) can not only be used as an effective preprocessing step, but also exploits the data redundancy to reduce the uncertainty. However, existing solutions are designed 1) either for a single machine that means the entire data must fit in the main memory and the parallelism is limited; 2) or for the Hadoop platform which means that the data have to be loaded into the distributed memory frequently and therefore become inefficient. In this paper, we overcome these shortcomings for maximum efficiency possible, and propose a unified framework for Parallel Large-scale Attribute Reduction, termed PLAR, for big data analysis. PLAR consists of three components: 1) Granular Computing (GrC)-based initialization: it converts a decision table (i.e., original data representation) into a granularity representation which reduces the amount of space and hence can be easily cached in the distributed memory: 2) model-parallelism: it simultaneously evaluates all feature candidates and makes attribute reduction highly parallelizable; 3) data-parallelism: it computes the significance of an attribute in parallel using a MapReduce-style manner. We implement PLAR with four representative heuristic feature selection algorithms on Spark, and evaluate them on various huge datasets, including UCI and astronomical datasets, finding our method's advantages beyond existing solutions."
"This paper describes the design of a 1024-core processor chip in 16nm FinFet technology. The chip (""Epiphany-V"") contains an array of 1024 64-bit RISC processors, 64MB of on-chip SRAM, three 136-bit wide mesh Networks-On-Chip, and 1024 programmable IO pins. The chip has taped out and is being manufactured by TSMC.   This research was developed with funding from the Defense Advanced Research Projects Agency (DARPA). The views, opinions and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government."
"This article reports on a case study on teachers' video-enhanced education. Considering the fact that video exemplification is a thriving practice in the field although there is little consensus in the literature regarding its instructional issues, it seems appropriate to focus on the activity carried out by teachers in video-enhanced devices in order to identify promising characteristics (nature of the examples, types of associations between them, documentation methods, organization and scenarization, etc.). This study involved six trainees who used, during two sessions of 45 minutes, a digital device based on a "" pedagogy of typical professional paths "" (Durand, 2014; Ria \& Leblanc, 2011). The goal was to better understand their use of video exemplification and sense-making. The results indicate that student teachers (i) preferentially target resources about ""economic rules""; (ii) plebiscite classroom situations, always consulted; (iii) are particularly interested (even if they are sometimes disappointed) in the experts' testimonies; (iv) initially show a distrust toward resources assumed to be theoretical; (v) get gradually interested in these resources during the second session. We conclude with several empirical and technological prospects for the design of video-enhanced devices for teacher induction."
"We propose a new compressive imaging method for reconstructing 2D or 3D objects from their scattered wave-field measurements. Our method relies on a novel, nonlinear measurement model that can account for the multiple scattering phenomenon, which makes the method preferable in applications where linear measurement models are inaccurate. We construct the measurement model by expanding the scattered wave-field with an accelerated-gradient method, which is guaranteed to converge and is suitable for large-scale problems. We provide explicit formulas for computing the gradient of our measurement model with respect to the unknown image, which enables image formation with a sparsity- driven numerical optimization algorithm. We validate the method both analytically and with numerical simulations."
"We study to what extend Chinese, Japanese and Korean faces can be classified and which facial attributes offer the most important cues. First, we propose a novel way of obtaining large numbers of facial images with nationality labels. Then we train state-of-the-art neural networks with these labeled images. We are able to achieve an accuracy of 75.03% in the classification task, with chances being 33.33% and human accuracy 38.89% . Further, we train multiple facial attribute classifiers to identify the most distinctive features for each group. We find that Chinese, Japanese and Koreans do exhibit substantial differences in certain attributes, such as bangs, smiling, and bushy eyebrows. Along the way, we uncover several gender-related cross-country patterns as well. Our work, which complements existing APIs such as Microsoft Cognitive Services and Face++, could find potential applications in tourism, e-commerce, social media marketing, criminal justice and even counter-terrorism."
"An emerging challenge in the online classification of social media data streams is to keep the categories used for classification up-to-date. In this paper, we propose an innovative framework based on an Expert-Machine-Crowd (EMC) triad to help categorize items by continuously identifying novel concepts in heterogeneous data streams often riddled with outliers. We unify constrained clustering and outlier detection by formulating a novel optimization problem: COD-Means. We design an algorithm to solve the COD-Means problem and show that COD-Means will not only help detect novel categories but also seamlessly discover human annotation errors and improve the overall quality of the categorization process. Experiments on diverse real data sets demonstrate that our approach is both effective and efficient."
"The distortion varieties of a given projective variety are parametrized by duplicating coordinates and multiplying them with monomials. We study their degrees and defining equations. Exact formulas are obtained for the case of one-parameter distortions. These are based on Chow polytopes and Gr\""obner bases. Multi-parameter distortions are studied using tropical geometry. The motivation for distortion varieties comes from multi-view geometry in computer vision. Our theory furnishes a new framework for formulating and solving minimal problems for camera models with image distortion."
"Strategic network formation models the uncoordinated creation of a network by selfish agents. Inspired by real world examples, e.g. the Internet, researchers have introduced an abundance of strategic games to study natural phenomena in networks. Most of these games have the conceptual drawback of being computationally intractable. For example, computing a best response strategy or checking whether an equilibrium is reached is NP-hard. Thus, a main challenge in the field is to find models which incorporate many interesting features and to devise efficient algorithms for solving the entailed computational tasks.   We address this challenge by providing an efficient algorithm to compute a best response strategy for a recently introduced model, thereby answering the open question posed by Goyal et al. [WINE'16]. Their promising model focuses on network robustness by considering an adversary who attacks (and kills) nodes in the network and lets this attack spread virus-like to neighboring nodes.   Additionally, we augment their model by introducing a less predictable adversary and show that our algorithm, with minor modifications, can cope with this more complex scenario."
"In this paper, we apply an equivalent color transform (ECT) for a minimal $k$-coloring of any graph $G$. It contracts each color class of the graph to a single vertex and produces a complete graph $K_k$ for $G$ by removing redundant edges between any two vertices. Based on ECT, a simple proof for four color theorem for planar graph is then proposed."
"Word embeddings have been demonstrated to benefit NLP tasks impressively. Yet, there is room for improvement in the vector representations, because current word embeddings typically contain unnecessary information, i.e., noise. We propose two novel models to improve word embeddings by unsupervised learning, in order to yield word denoising embeddings. The word denoising embeddings are obtained by strengthening salient information and weakening noise in the original word embeddings, based on a deep feed-forward neural network filter. Results from benchmark tasks show that the filtered word denoising embeddings outperform the original word embeddings."
"In this paper we consider a 2-vehicle routing problem which can be viewed as a building block for the varieties of the vehicle routing problems (VRPs). To approach this problem, we suggest a framework based on the Held and Karp dynamic programming algorithm for the classical travelling salesman problem. An algorithm based on this framework shows an exceptionally good performance on published test data. Our approach can be easily extended to a variety of constraints/attributes in the VRP, hence the wording ""small but rich"" in the title of our paper."
"One essential task in information extraction from the medical corpus is drug name recognition. Compared with text sources come from other domains, the medical text is special and has unique characteristics. In addition, the medical text mining poses more challenges, e.g., more unstructured text, the fast growing of new terms addition, a wide range of name variation for the same drug. The mining is even more challenging due to the lack of labeled dataset sources and external knowledge, as well as multiple token representations for a single drug name that is more common in the real application setting. Although many approaches have been proposed to overwhelm the task, some problems remained with poor F-score performance (less than 0.75). This paper presents a new treatment in data representation techniques to overcome some of those challenges. We propose three data representation techniques based on the characteristics of word distribution and word similarities as a result of word embedding training. The first technique is evaluated with the standard NN model, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two deep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked Denoising Encoders). The third technique represents the sentence as a sequence that is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term Memory). In extracting the drug name entities, the third technique gives the best F-score performance compared to the state of the art, with its average F-score being 0.8645."
"We consider a gossip approach for finding a Nash equilibrium in a distributed multi-player network game. We extend previous results on Nash equilibrium seeking to the case when the players' cost functions may be affected by the actions of any subset of players. An interference graph is employed to illustrate the partially-coupled cost functions and the asymmetric information requirements. For a given interference graph, we design a generalized communication graph so that players with possibly partially-coupled cost functions exchange only their required information and make decisions based on them. Using a set of standard assumptions on the cost functions, interference and communication graphs, we prove almost sure convergence to a Nash equilibrium for diminishing step sizes. We then quantify the effect of the second largest eigenvalue of the expected communication matrix on the convergence rate, and illustrate the trade-off between the parameters associated with the communication and the interference graphs. Finally, the efficacy of the proposed algorithm on a large-scale networked game is demonstrated via simulation."
"In this letter, we propose a new coordinated multipoint (CoMP) technique based on mutual information (MI) accumulation using rateless codes. Using a stochastic geometry model for the cellular downlink, we quantify the performance enhancements in coverage probability and rate due to MI accumulation. By simulation and analysis, we show that MI accumulation using rateless codes leads to remarkable improvements in coverage and rate for general users and specific cell edge users."
"The concept of distance rationalizability of voting rules has been explored in recent years by several authors. Most previous work has dealt with a definition in terms of preference profiles. However, most voting rules in common use are anonymous and homogeneous. In this case there is a much more succinct representation (using the voting simplex) of the inputs to the rule. This representation has been widely used in the voting literature, but rarely in the context of distance rationalizability.   Recently, the present authors showed, as a special case of general results on quotient spaces, exactly how to connect distance rationalizability on profiles for anonymous and homogeneous rules to geometry in the simplex. In this article we develop the connection for the important special case of votewise distances, recently introduced and studied by Elkind, Faliszewski and Slinko in several papers. This yields a direct interpretation in terms of well-developed mathematical topics not seen before in the voting literature, namely Kantorovich (also called Wasserstein) distances and the geometry of Minkowski spaces.   As an application of this approach, we prove some positive and some negative results about the decisiveness of distance rationalizable anonymous and homogeneous rules. The positive results connect with the recent theory of hyperplane rules, while the negative ones deal with distances that are not metrics, controversial notions of consensus, and the fact that the $\ell^1$-norm is not strictly convex.   We expect that the above novel geometric interpretation will aid the analysis of rules defined by votewise distances, and the discovery of new rules with desirable properties."
"We propose a framework for discriminative Information Retrieval (IR) atop linguistic features, trained to improve the recall of tasks such as answer candidate passage retrieval, the initial step in text-based Question Answering (QA). We formalize this as an instance of linear feature-based IR (Metzler and Croft, 2007), illustrating how a variety of knowledge discovery tasks are captured under this approach, leading to a 44% improvement in recall for candidate triage for QA."
"The concept of distance rationalizability of social choice rules has been explored in recent years by several authors. We deal here with several foundational questions, and unify, correct, and generalize previous work. For example, we study a new question involving uniqueness of representation in the distance rationalizability framework, and present a counterexample.   For rules satisfying various axiomatic properties such as anonymity, neutrality and homogeneity, the standard profile representation of input can be compressed substantially. We explain in detail using quotient constructions and symmetry groups how distance rationalizability is interpreted in this situation. This enables us to connect the theory of distance rationalizability with geometric concepts such as Earth Mover distance and optimal transportation. We expect this connection to prove fruitful in future work.   We improve on the best-known sufficient conditions for rules rationalized via votewise distances to satisfy anonymity, neutrality, homogeneity, consistency and continuity. This leads to a class of well-behaved rules which deserve closer scrutiny in future."
"With the help of a map and GPS, outdoor navigation from one spot to another can be done quickly and well. Unfortunately inside a shopping mall, where GPS signal is hardly available, navigation becomes troublesome. In this paper, we propose an indoor navigation system to address the problem. Unlike most existing indoor navigation systems, which relies heavily on infrastructures and pre-labelled maps, our system uses only photos taken by cellphone cameras as input. We utilize multiple image processing techniques to parse photos of a mall's shopping instruction and a construct topological map of the mall. Moreover, we propose a new feature fusion method to help automatically identifying shops in a photo. During navigation, we make use of this method to find out the realtime position of the user."
"Understanding expressions of emotions in support forums has considerable value and NLP methods are key to automating this. Many approaches understandably use subjective categories which are more fine-grained than a straightforward polarity-based spectrum. However, the definition of such categories is non-trivial and, in fact, we argue for a need to incorporate communicative elements even beyond subjectivity. To support our position, we report experiments on a sentiment-labelled corpus of posts taken from a medical support forum. We argue that not only is a more fine-grained approach to text analysis important, but simultaneously recognising the social function behind affective expressions enable a more accurate and valuable level of understanding."
"\emph{Simon's problem} is one of the most important problems demonstrating the power of quantum computers, which achieves a large separation between quantum and classical query complexities. However, Simon's discussion on his problem was limited to \emph{bounded-error} setting, which means his algorithm can not always get the correct answer. \emph{Exact} quantum algorithms for Simon's problem have also been proposed, which deterministically solve the problem with $O(n)$ queries. Although these algorithms are either complicated or specialized, their results give a $O(n)$ versus $\Omega(\sqrt{2^{n}})$ separation in exact query complexities for Simon's problem. Later results indicate the the quantum lower bound for Simon's problem, but it has not been proved whether this separation is optimal. In this paper, we propose another exact quantum algorithm for solving Simon's problem with $O(n)$ queries, which is simple, concrete and does not rely on some special query oracles. Our algorithm combines Simon's algorithm with the quantum amplitude amplification technique to ensure its determinism. In particular, we show that Simon's problem can be solved by a classically \emph{deterministic} algorithm with $O(\sqrt{2^{n}})$ queries (as we are aware, there were no classically deterministic algorithms for solving Simon's problem with $O(\sqrt{2^{n}})$ queries). Combining some previous results, we prove the optimal separation in exact query complexities for Simon's problem: $\Theta({n})$ versus $\Theta({\sqrt{2^{n}}})$."
"A machine learning method needs to adapt to over time changes in the environment. Such changes are known as concept drift. In this paper, we propose concept drift tackling method as an enhancement of Online Sequential Extreme Learning Machine (OS-ELM) and Constructive Enhancement OS-ELM (CEOS-ELM) by adding adaptive capability for classification and regression problem. The scheme is named as adaptive OS-ELM (AOS-ELM). It is a single classifier scheme that works well to handle real drift, virtual drift, and hybrid drift. The AOS-ELM also works well for sudden drift and recurrent context change type. The scheme is a simple unified method implemented in simple lines of code. We evaluated AOS-ELM on regression and classification problem by using concept drift public data set (SEA and STAGGER) and other public data sets such as MNIST, USPS, and IDS. Experiments show that our method gives higher kappa value compared to the multiclassifier ELM ensemble. Even though AOS-ELM in practice does not need hidden nodes increase, we address some issues related to the increasing of the hidden nodes such as error condition and rank values. We propose taking the rank of the pseudoinverse matrix as an indicator parameter to detect underfitting condition."
"A typical modern optimization technique is usually either heuristic or metaheuristic. This technique has managed to solve some optimization problems in the research area of science, engineering, and industry. However, implementation strategy of metaheuristic for accuracy improvement on convolution neural networks (CNN), a famous deep learning method, is still rarely investigated. Deep learning relates to a type of machine learning technique, where its aim is to move closer to the goal of artificial intelligence of creating a machine that could successfully perform any intellectual tasks that can be carried out by a human. In this paper, we propose the implementation strategy of three popular metaheuristic approaches, that is, simulated annealing, differential evolution, and harmony search, to optimize CNN. The performances of these metaheuristic methods in optimizing CNN on classifying MNIST and CIFAR dataset were evaluated and compared. Furthermore, the proposed methods are also compared with the original CNN. Although the proposed methods show an increase in the computation time, their accuracy has also been improved (up to 7.14 percent)."
"Trial-offer markets, where customers can sample a product before deciding whether to buy it, are ubiquitous in the online experience. Their static and dynamic properties are often studied by assuming that consumers follow a multinomial logit model and try exactly one product. In this paper, we study how to generalize existing results to a more realistic setting where consumers can try multiple products. We show that a multinomial logit model with continuation can be reduced to a standard multinomial logit model with different appeal and product qualities. We examine the consequences of this reduction on the performance and predictability of the market, the role of social influence, and the ranking policies."
"Deep neural networks have proven to be quite effective in a wide variety of machine learning tasks, ranging from improved speech recognition systems to advancing the development of autonomous vehicles. However, despite their superior performance in many applications, these models have been recently shown to be susceptible to a particular type of attack possible through the generation of particular synthetic examples referred to as adversarial samples. These samples are constructed by manipulating real examples from the training data distribution in order to ""fool"" the original neural model, resulting in misclassification (with high confidence) of previously correctly classified samples. Addressing this weakness is of utmost importance if deep neural architectures are to be applied to critical applications, such as those in the domain of cybersecurity. In this paper, we present an analysis of this fundamental flaw lurking in all neural architectures to uncover limitations of previously proposed defense mechanisms. More importantly, we present a unifying framework for protecting deep neural models using a non-invertible data transformation--developing two adversary-resilient architectures utilizing both linear and nonlinear dimensionality reduction. Empirical results indicate that our framework provides better robustness compared to state-of-art solutions while having negligible degradation in accuracy."
"Sleep signals from a polysomnographic database are sequences in nature. Commonly employed analysis and classification methods, however, ignored this fact and treated the sleep signals as non-sequence data. Treating the sleep signals as sequences, this paper compared two powerful unsupervised feature extractors and three sequence-based classifiers regarding accuracy and computational (training and testing) time after 10-folds cross-validation. The compared feature extractors are Deep Belief Networks (DBN) and Fuzzy C-Means (FCM) clustering. Whereas the compared sequence-based classifiers are Hidden Markov Models (HMM), Conditional Random Fields (CRF) and its variants, i.e., Hidden-state CRF (HCRF) and Latent-Dynamic CRF (LDCRF); and Conditional Neural Fields (CNF) and its variant (LDCNF). In this study, we use two datasets. The first dataset is an open (public) polysomnographic dataset downloadable from the Internet, while the second dataset is our polysomnographic dataset (also available for download). For the first dataset, the combination of FCM and CNF gives the highest accuracy (96.75\%) with relatively short training time (0.33 hours). For the second dataset, the combination of DBN and CRF gives the accuracy of 99.96\% but with 1.02 hours training time, whereas the combination of DBN and CNF gives slightly less accuracy (99.69\%) but also less computation time (0.89 hours)."
"Ancient rock engravings (so called petroglyphs) represent one of the earliest surviving artifacts describing life of our ancestors. Recently, modern 3D scanning techniques found their application in the domain of rock art documentation by providing high-resolution reconstructions of rock surfaces. Reconstruction results demonstrate the strengths of novel 3D techniques and have the potential to replace the traditional (manual) documentation techniques of archaeologists.   An important analysis task in rock art documentation is the segmentation of petroglyphs. To foster automation of this tedious step, we present a high-resolution 3D surface dataset of natural rock surfaces which exhibit different petroglyphs together with accurate expert ground-truth annotations. To our knowledge, this dataset is the first public 3D surface dataset which allows for surface segmentation at sub-millimeter scale. We conduct experiments with state-of-the-art methods to generate a baseline for the dataset and verify that the size and variability of the data is sufficient to successfully adopt even recent data-hungry Convolutional Neural Networks (CNNs). Furthermore, we experimentally demonstrate that the provided geometric information is key to successful automatic segmentation and strongly outperforms color-based segmentation. The introduced dataset represents a novel benchmark for 3D surface segmentation methods in general and is intended to foster comparability among different approaches in future."
"Both generative adversarial networks (GAN) in unsupervised learning and actor-critic methods in reinforcement learning (RL) have gained a reputation for being difficult to optimize. Practitioners in both fields have amassed a large number of strategies to mitigate these instabilities and improve training. Here we show that GANs can be viewed as actor-critic methods in an environment where the actor cannot affect the reward. We review the strategies for stabilizing training for each class of models, both those that generalize between the two and those that are particular to that model. We also review a number of extensions to GANs and RL algorithms with even more complicated information flow. We hope that by highlighting this formal connection we will encourage both GAN and RL communities to develop general, scalable, and stable algorithms for multilevel optimization with deep networks, and to draw inspiration across communities."
"We consider the future cyber security of industrial control systems. As best as we can see, much of this future unfolds in the context of the Internet of Things (IoT). In fact, we envision that all industrial and infrastructure environments, and cyber-physical systems in general, will take the form reminiscent of what today is referred to as the IoT. IoT is envisioned as multitude of heterogeneous devices densely interconnected and communicating with the objective of accomplishing a diverse range of objectives, often collaboratively. One can argue that in the relatively near future, the IoT construct will subsume industrial plants, infrastructures, housing and other systems that today are controlled by ICS and SCADA systems. In the IoT environments, cybersecurity will derive largely from system agility, moving-target defenses, cybermaneuvering, and other autonomous or semi-autonomous behaviors. Cyber security of IoT may also benefit from new design methods for mixed-trusted systems; and from big data analytics -- predictive and autonomous."
This article presents a new method for detecting a source point of time based network steganography - MoveSteg. A steganography carrier could be an example of multimedia stream made with packets. These packets are then delayed intentionally to send hidden information using time based steganography methods. The presented analysis describes a method that allows finding the source of steganography stream in network that is under our management.
"It was shown recently that the $K$ L1-norm principal components (L1-PCs) of a real-valued data matrix $\mathbf X \in \mathbb R^{D \times N}$ ($N$ data samples of $D$ dimensions) can be exactly calculated with cost $\mathcal{O}(2^{NK})$ or, when advantageous, $\mathcal{O}(N^{dK - K + 1})$ where $d=\mathrm{rank}(\mathbf X)$, $K<d$ [1],[2]. In applications where $\mathbf X$ is large (e.g., ""big"" data of large $N$ and/or ""heavy"" data of large $d$), these costs are prohibitive. In this work, we present a novel suboptimal algorithm for the calculation of the $K < d$ L1-PCs of $\mathbf X$ of cost $\mathcal O(ND \mathrm{min} \{ N,D\} + N^2(K^4 + dK^2) + dNK^3)$, which is comparable to that of standard (L2-norm) PC analysis. Our theoretical and experimental studies show that the proposed algorithm calculates the exact optimal L1-PCs with high frequency and achieves higher value in the L1-PC optimization metric than any known alternative algorithm of comparable computational cost. The superiority of the calculated L1-PCs over standard L2-PCs (singular vectors) in characterizing potentially faulty data/measurements is demonstrated with experiments on data dimensionality reduction and disease diagnosis from genomic data."
"Discovering dense subgraphs and understanding the relations among them is a fundamental problem in graph mining. We want to not only identify dense subgraphs, but also build a hierarchy among them (e.g., larger but sparser subgraphs formed by two smaller dense subgraphs). Peeling algorithms (k-core, k-truss, and nucleus decomposition) have been effective to locate many dense subgraphs. However, constructing a hierarchical representation of density structure, even correctly computing the connected k-cores and k-trusses, have been mostly overlooked. Keeping track of connected components during peeling requires an additional traversal operation, which is as expensive as the peeling process. In this paper, we start with a thorough survey and point to nuances in problem formulations that lead to significant differences in runtimes. We then propose efficient and generic algorithms to construct the hierarchy of dense subgraphs for k-core, k-truss, or any nucleus decomposition. Our algorithms leverage the disjoint-set forest data structure to efficiently construct the hierarchy during traversal. Furthermore, we introduce a new idea to avoid traversal. We construct the subgraphs while visiting neighborhoods in the peeling process, and build the relations to previously constructed subgraphs. We also consider an existing idea to find the k-core hierarchy and adapt for our objectives efficiently. Experiments on different types of large scale real-world networks show significant speedups over naive algorithms and existing alternatives. Our algorithms also outperform the hypothetical limits of any possible traversal-based solution."
"In this paper a mesoscopic hybrid model, i.e. a microscopic hybrid model that takes into account macroscopic parameters, is introduced for designing a human-inspired Adaptive Cruise Control. A control law is proposed with the design goal of replacing and imitating the behaviour of a human driver in a car-following situation where lane changes are possible. First, a microscopic hybrid automaton model is presented, based on human psycho-physical behavior, for both longitudinal and lateral vehicle control. Then a rule for changing time headway on the basis of macroscopic quantities is used to describe the interaction among next vehicles and their impact on driver performance. Simulation results show the advantages of the mesoscopic model. A feasibility analysis of the needed communication network is also presented."
"We investigate the ability of a homogeneous collection of deferrable energy loads to behave as a battery; that is, to absorb and release energy in a controllable fashion up to fixed and predetermined limits on volume, charge rate and discharge rate. We derive bounds on the battery capacity that can be realized and show that there are fundamental trade-offs between battery parameters. By characterizing the state trajectories under scheduling policies that emulate two illustrative batteries, we show that the trade-offs occur because the states that allow the loads to absorb and release energy at high aggregate rates are conflicting."
"We give new algorithms based on the sum-of-squares method for tensor decomposition. Our results improve the best known running times from quasi-polynomial to polynomial for several problems, including decomposing random overcomplete 3-tensors and learning overcomplete dictionaries with constant relative sparsity. We also give the first robust analysis for decomposing overcomplete 4-tensors in the smoothed analysis model. A key ingredient of our analysis is to establish small spectral gaps in moment matrices derived from solutions to sum-of-squares relaxations. To enable this analysis we augment sum-of-squares relaxations with spectral analogs of maximum entropy constraints."
"In this paper, the problem of optimized beam alignment for wearable device-to-device (D2D) communications over millimeter wave (mmW) frequencies is studied. In particular, a noncooperative game is formulated between wearable communication pairs that engage in D2D communications. In this game, wearable devices acting as transmitters autonomously select the directions of their beams so as to maximize the data rate to their receivers. To solve the game, an algorithm based on best response dynamics is proposed that allows the transmitters to reach a Nash equilibrium in a distributed manner. To further improve the performance of mmW D2D communications, a novel quantum game model is formulated to enable the wearable devices to exploit new quantum directions during their beam alignment so as to further enhance their data rate. Simulation results show that the proposed game-theoretic approach improves the performance, in terms of data rate, of about 75% compared to a uniform beam alignment. The results also show that the quantum game model can further yield up to 20% improvement in data rates, relative to the classical game approach."
"Deep learning has rapidly transformed the state of the art algorithms used to address a variety of problems in computer vision and robotics. These breakthroughs have however relied upon massive amounts of human annotated training data. This time-consuming process has begun impeding the progress of these deep learning efforts. This paper describes a method to incorporate photo-realistic computer images from a simulation engine to rapidly generate annotated data that can be used for training of machine learning algorithms. We demonstrate that a state of the art architecture, which is trained only using these synthetic annotations, performs better than the identical architecture trained on human annotated real-world data, when tested on the KITTI data set for vehicle detection. By training machine learning algorithms on a rich virtual world, this paper illustrates that real objects in real scenes can be learned and classified using synthetic data. This approach offers the possibility of accelerating deep learning's application to sensor based classification problems like those that appear in self-driving cars."
"Online model-free reinforcement learning (RL) methods with continuous actions are playing a prominent role when dealing with real-world applications such as Robotics. However, when confronted to non-stationary environments, these methods crucially rely on an exploration-exploitation trade-off which is rarely dynamically and automatically adjusted to changes in the environment. Here we propose an active exploration algorithm for RL in structured (parameterized) continuous action space. This framework deals with a set of discrete actions, each of which is parameterized with continuous variables. Discrete exploration is controlled through a Boltzmann softmax function with an inverse temperature $\beta$ parameter. In parallel, a Gaussian exploration is applied to the continuous action parameters. We apply a meta-learning algorithm based on the comparison between variations of short-term and long-term reward running averages to simultaneously tune $\beta$ and the width of the Gaussian distribution from which continuous action parameters are drawn. When applied to a simple virtual human-robot interaction task, we show that this algorithm outperforms continuous parameterized RL both without active exploration and with active exploration based on uncertainty variations measured by a Kalman-Q-learning algorithm."
"We introduce a unified framework for the study of utility maximization problems in interference-coupled wireless networks. The framework can be applied to a large class of utilities, but in this study special attention is devoted to the rate. In more detail, we resort to results from concave Perron-Frobenius theory to show that, within the class of problems we consider here, each problem has a unique solution. As a consequence, given any network utility maximization problem belonging to this class, we can define two functions that relate the power budget $\bar{p}$ of a network to the network utility and to the energy efficiency achieved by the solution to the given problem. Among many interesting properties, we prove that these functions are continuous and monotonic. In addition, we derive bounds revealing that the solution is characterized by a low and a high power regime. In the low power regime, the energy efficiency can decrease slowly as the power budget increases, and the network utility grows linearly at best. In contrast, in the high power regime, the energy efficiency typically scales as $\mathcal{O}(1/\bar{p})$ as $\bar{p}\to\infty$, and the network utility scales as $\mathcal{O}(1)$. We apply the theoretical findings to a novel weighted rate maximization problem involving the joint optimization of the uplink power and the base station assignment. For this novel problem formulation, we also propose a simple and practical iterative solver."
"We introduce Delay Pruning, a simple yet powerful technique to regularize dynamic Boltzmann machines (DyBM). The recently introduced DyBM provides a particularly structured Boltzmann machine, as a generative model of a multi-dimensional time-series. This Boltzmann machine can have infinitely many layers of units but allows exact inference and learning based on its biologically motivated structure. DyBM uses the idea of conduction delays in the form of fixed length first-in first-out (FIFO) queues, with a neuron connected to another via this FIFO queue, and spikes from a pre-synaptic neuron travel along the queue to the post-synaptic neuron with a constant period of delay. Here, we present Delay Pruning as a mechanism to prune the lengths of the FIFO queues (making them zero) by setting some delay lengths to one with a fixed probability, and finally selecting the best performing model with fixed delays. The uniqueness of structure and a non-sampling based learning rule in DyBM, make the application of previously proposed regularization techniques like Dropout or DropConnect difficult, leading to poor generalization. First, we evaluate the performance of Delay Pruning to let DyBM learn a multidimensional temporal sequence generated by a Markov chain. Finally, we show the effectiveness of delay pruning in learning high dimensional sequences using the moving MNIST dataset, and compare it with Dropout and DropConnect methods."
"Machine translation is the discipline concerned with developing automated tools for translating from one human language to another. Statistical machine translation (SMT) is the dominant paradigm in this field. In SMT, translations are generated by means of statistical models whose parameters are learned from bilingual data. Scalability is a key concern in SMT, as one would like to make use of as much data as possible to train better translation systems.   In recent years, mobile devices with adequate computing power have become widely available. Despite being very successful, mobile applications relying on NLP systems continue to follow a client-server architecture, which is of limited use because access to internet is often limited and expensive. The goal of this dissertation is to show how to construct a scalable machine translation system that can operate with the limited resources available on a mobile device.   The main challenge for porting translation systems on mobile devices is memory usage. The amount of memory available on a mobile device is far less than what is typically available on the server side of a client-server application. In this thesis, we investigate alternatives for the two components which prevent standard translation systems from working on mobile devices due to high memory usage. We show that once these standard components are replaced with our proposed alternatives, we obtain a scalable translation system that can work on a device with limited memory."
"This article is about a decoding algorithm for error-correcting subspace codes. A version of this algorithm was previously described by Rosenthal, Silberstein and Trautmann. The decoding algorithm requires the code to be defined as the intersection of the Pl\""ucker embedding of the Grassmannian and an algebraic variety. We call such codes \emph{geometric subspace codes}. Complexity is substantially improved compared to the algorithm by Rosenthal, Silberstein and Trautmann and connections to finite geometry are given. The decoding algorithm is applied to Desarguesian spread codes, which are known to be defined as the intersection of the Pl\""ucker embedding of the Grassmannian with a linear space."
In this paper the use of artificial neural network in power system stability is studied. A predictive controller based on two neural networks is designed and tested on a single machine infinite bus system which is used to replace conventional power system stabilizers. They have been used for decades in power system to dampen small amplitude low frequency oscillation in power systems. The increases in size and complexity of power systems have cast a shadow on efficiency of conventional method. New control strategies have been proposed in many researches. Artificial Neural Networks have been studied in many publications but lack of assurance of their functionality has hindered the practical usage of them in utilities. The proposed control structure is modelled using a novel data exchange established between MATLAB and
"The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification at tasks such as object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories and attributes, comprising a quasi-exhaustive list of the types of environments encountered in the world. Using state of the art Convolutional Neural Networks, we provide impressive baseline performances at scene classification. With its high-coverage and high-diversity of exemplars, the Places Database offers an ecosystem to guide future progress on currently intractable visual recognition problems."
"We study the non-uniform capacitated multi-item lot-sizing (\lotsizing) problem. In this problem, there is a set of demands over a planning horizon of $T$ time periods and all demands must be satisfied on time. We can place an order at the beginning of each period $s$, incurring an ordering cost $K_s$. The total quantity of all products ordered at time $s$ can not exceed a given capacity $C_s$. On the other hand, carrying inventory from time to time incurs inventory holding cost. The goal of the problem is to find a feasible solution that minimizes the sum of ordering and holding costs.   Levi et al.\ (Levi, Lodi and Sviridenko, Mathmatics of Operations Research 33(2), 2008) gave a 2-approximation for the problem when the capacities $C_s$ are the same. In this paper, we extend their result to the case of non-uniform capacities. That is, we give a constant approximation algorithm for the capacitated multi-item lot-sizing problem with general capacities.   The constant approximation is achieved by adding an exponentially large set of new covering inequalities to the natural facility-location type linear programming relaxation for the problem. Along the way of our algorithm, we reduce the \lotsizing problem to two generalizations of the classic knapsack covering problem. We give LP-based constant approximation algorithms for both generalizations, via the iterative rounding technique."
"The mass shooting at Sandy Hook elementary school on December 14, 2012 catalyzed a year of active debate and legislation on gun control in the United States. Social media hosted an active public discussion where people expressed their support and opposition to a variety of issues surrounding gun legislation. In this paper, we show how a content-based analysis of Twitter data can provide insights and understanding into this debate. We estimate the relative support and opposition to gun control measures, along with a topic analysis of each camp by analyzing over 70 million gun-related tweets from 2013. We focus on spikes in conversation surrounding major events related to guns throughout the year. Our general approach can be applied to other important public health and political issues to analyze the prevalence and nature of public opinion."
"After carefully considering the scalability problem in Tor and exhaustively evaluating related works on AS-level adversaries, the author proposes ASmoniTor, which is an autonomous system monitor for mitigating correlation attacks in the Tor network. In contrast to prior works, which often released offline packets, including the source code of a modified Tor client and a snapshot of the Internet topology, ASmoniTor is an online system that assists end users with mitigating the threat of AS-level adversaries in a near real-time fashion. For Tor clients proposed in previous works, users need to compile the source code on their machine and continually update the snapshot of the Internet topology in order to obtain accurate AS-path inferences. On the contrary, ASmoniTor is an online platform that can be utilized easily by not only technical users, but also by users without a technical background, because they only need to access it via Tor and input two parameters to execute an AS-aware path selection algorithm. With ASmoniTor, the author makes three key technical contributions to the research against AS-level adversaries in the Tor network. First, ASmoniTor does not require the users to initiate complicated source code compilations. Second, it helps to reduce errors in AS-path inferences by letting users input a set of suspected ASes obtained directly from their own traceroute measurements. Third, the Internet topology database at the back-end of ASmoniTor is periodically updated to assure near real-time AS-path inferences between Tor exit nodes and the most likely visited websites. Finally, in addition to its convenience, ASmoniTor gives users full control over the information they want to input, thus preserving their privacy."
"We study the problem of estimating the number of triangles in a graph stream. No streaming algorithm can get sublinear space on all graphs, so methods in this area bound the space in terms of parameters of the input graph such as the maximum number of triangles sharing a single edge. We give a sampling algorithm that is additionally parameterized by the maximum number of triangles sharing a single vertex. Our bound matches the best known turnstile results in all graphs, and gets better performance on simple graphs like $G(n, p)$ or a set of independent triangles.   We complement the upper bound with a lower bound showing that no sampling algorithm can do better on those graphs by more than a log factor. In particular, any insertion stream algorithm must use $\sqrt{T}$ space when all the triangles share a common vertex, and any sampling algorithm must take $T^\frac{1}{3}$ samples when all the triangles are independent. We add another lower bound, also matching our algorithm's performance, which applies to all graph classes. This lower bound covers ""triangle-dependent"" sampling algorithms, a subclass that includes our algorithm and all previous sampling algorithms for the problem.   Finally, we show how to generalize our algorithm to count arbitrary subgraphs of constant size."
"In this paper, the problem of smart grid energy management under stochastic dynamics is investigated. In the considered model, at the demand side, it is assumed that customers can act as prosumers who own renewable energy sources and can both produce and consume energy. Due to the coupling between the prosumers' decisions and the stochastic nature of renewable energy, the interaction among prosumers is formulated as a stochastic game, in which each prosumer seeks to maximize its payoff, in terms of revenues, by controlling its energy consumption and demand. In particular, the subjective behavior of prosumers is explicitly reflected into their payoff functions using prospect theory, a powerful framework that allows modeling real-life human choices. For this prospect-based stochastic game, it is shown that there always exists a stationary Nash equilibrium where the prosumers' trading policies in the equilibrium are independent of the time and their histories of the play. Moreover, a novel distributed algorithm with no information sharing among prosumers is proposed and shown to converge to an $\epsilon$-Nash equilibrium. On the other hand, at the supply side, the interaction between the utility company and the prosumers is formulated as an online optimization problem in which the utility company's goal is to learn its optimal energy allocation rules. For this case, it is shown that such an optimization problem admits a no-regret algorithm meaning that regardless of the actual outcome of the game among the prosumers, the utility company can follow a strategy that mitigates its allocation costs as if it knew the entire demand market a priori. Simulation results show the convergence of the proposed algorithms to their predicted outcomes and present new insights resulting from prospect theory that contribute toward more efficient energy management in the smart grids."
"For the linear bandit problem, we extend the analysis of algorithm CombEXP from [R. Combes, M. S. Talebi Mazraeh Shahi, A. Proutiere, and M. Lelarge. Combinatorial bandits revisited. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2116--2124. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5831-combinatorial-bandits-revisited.pdf] to the high-probability case against adaptive adversaries, allowing actions to come from an arbitrary polytope. We prove a high-probability regret of \(O(T^{2/3})\) for time horizon \(T\). While this bound is weaker than the optimal \(O(\sqrt{T})\) bound achieved by GeometricHedge in [P. L. Bartlett, V. Dani, T. Hayes, S. Kakade, A. Rakhlin, and A. Tewari. High-probability regret bounds for bandit online linear optimization. In 21th Annual Conference on Learning Theory (COLT 2008), July 2008. http://eprints.qut.edu.au/45706/1/30-Bartlett.pdf], CombEXP is computationally efficient, requiring only an efficient linear optimization oracle over the convex hull of the actions."
"We initiate a line of investigation into biological neural networks from an algorithmic perspective. We develop a simplified but biologically plausible model for distributed computation in stochastic spiking neural networks and study tradeoffs between computation time and network complexity in this model. Our aim is to abstract real neural networks in a way that, while not capturing all interesting features, preserves high-level behavior and allows us to make biologically relevant conclusions.   In this paper, we focus on the important `winner-take-all' (WTA) problem, which is analogous to a neural leader election unit: a network consisting of $n$ input neurons and $n$ corresponding output neurons must converge to a state in which a single output corresponding to a firing input (the `winner') fires, while all other outputs remain silent. Neural circuits for WTA rely on inhibitory neurons, which suppress the activity of competing outputs and drive the network towards a converged state with a single firing winner. We attempt to understand how the number of inhibitors used affects network convergence time.   We show that it is possible to significantly outperform naive WTA constructions through a more refined use of inhibition, solving the problem in $O(\theta)$ rounds in expectation with just $O(\log^{1/\theta} n)$ inhibitors for any $\theta$. An alternative construction gives convergence in $O(\log^{1/\theta} n)$ rounds with $O(\theta)$ inhibitors. We compliment these upper bounds with our main technical contribution, a nearly matching lower bound for networks using $\ge \log\log n$ inhibitors. Our lower bound uses familiar indistinguishability and locality arguments from distributed computing theory. It lets us derive a number of interesting conclusions about the structure of any network solving WTA with good probability, and the use of randomness and inhibition within such a network."
"We have designed, fabricated, and successfully tested a prototype mixed-signal, 28x28-binary-input, 10-output, 3-layer neuromorphic network (""MLP perceptron""). It is based on embedded nonvolatile floating-gate cell arrays redesigned from a commercial 180-nm NOR flash memory. The arrays allow precise (~1%) individual tuning of all memory cells, having long-term analog-level retention and low noise. Each array performs a very fast and energy-efficient analog vector-by-matrix multiplication, which is the bottleneck for signal propagation in most neuromorphic networks. All functional components of the prototype circuit, including 2 synaptic arrays with 101,780 floating-gate synaptic cells, 74 analog neurons, and the peripheral circuitry for weight adjustment and I/O operations, have a total area below 1 mm^2. Its testing on the common MNIST benchmark set (at this stage, with a relatively low weight import precision) has shown a classification fidelity of 94.65%, close to the 96.2% obtained in simulation. The classification of one pattern takes less than 1 us time and ~20 nJ energy - both numbers much better than for digital implementations of the same task. Estimates show that this performance may be further improved using a better neuron design and a more advanced memory technology, leading to a >10^2 advantage in speed and a >10^4 advantage in energy efficiency over the state-of-the-art purely digital (GPU and custom) circuits, at classification of large, complex patterns."
"Energy efficiency is essential for Wireless Body Area Network (WBAN) applications because of the battery-operated nodes. Other requirements such as throughput, delay, quality of service, and security levels also need to be considered in optimizing the network design. In this paper, we study the case in which the nodes access the medium probabilistically and we formulate an energy efficiency optimization problem under the rate and access probability constraints for IEEE 802.15.6 Impulse Radio Ultra-wideband (IR-UWB) WBANs. The proposed algorithm, dubbed Energy Efficiency Optimization of Channel Access Probabilities (EECAP), determines the optimal access probability and payload frame size for each node. The simulation results show that our algorithm rapidly converges to the optimal solution. We also provide detailed insights on the relationship between the optimal access probabilities and other network parameters such as the link distance, the number of nodes, and the minimum rate constraints."
"Cycle-accurate software simulation of multicores with complex microarchitectures is often excruciatingly slow. People use simplified core models to gain simulation speed. However, a persistent question is to what extent the results derived from a simplified core model can be used to characterize the behavior of a real machine.   We propose a new methodology of validating simplified simulation models, which focuses on the trends of metric values across benchmarks and architectures, instead of errors of absolute metric values. To illustrate this methodology, we conduct a case study using an FPGA-accelerated cycle-accurate full system simulator. We evaluated three cache replacement polices on a 10-stage in-order core model, and then re-conducted all the experiments by substituting a 1-IPC core model for the 10-stage core model. We found that the 1-IPC core model generally produces qualitatively the same results as the accurate core model except for a few mismatches. We argue that most observed mismatches were either indistinguishable from experimental noise or corresponded to the cases where the policy differences even in the accurate model showed inconclusive results. We think it is fair to use simplified core models to study a feature once the influence of the simplification is understood. Additional studies on branch predictors and scaling properties of multithread benchmarks reinforce our argument. However, the validation of a simplified model requires a detailed cycle-accurate model!"
"A large number of web applications is based on a relational database together with a program, typically a script, that enables the user to interact with the database through embedded SQL queries and commands. In this paper, we introduce a method for formal automated verification of such systems which connects database theory to mainstream program analysis. We identify a fragment of SQL which captures the behavior of the queries in our case studies, is algorithmically decidable, and facilitates the construction of weakest preconditions. Thus, we can integrate the analysis of SQL queries into a program analysis tool chain. To this end, we implement a new decision procedure for the SQL fragment that we introduce. We demonstrate practical applicability of our results with three case studies, a web administrator, a simple firewall, and a conference management system."
"The proliferation of distributed generation and storage units is leading to the development of local, small-scale distribution grids, known as microgrids (MGs). In this paper, the problem of optimizing the energy trading decisions of MG operators (MGOs) is studied using game theory. In the formulated game, each MGO chooses the amount of energy that must be sold immediately or stored for future emergencies, given the prospective market prices which are influenced by other MGOs' decisions. The problem is modeled using a Bayesian game to account for the incomplete information that MGOs have about each others' levels of surplus. The proposed game explicitly accounts for each MGO's subjective decision when faced with the uncertainty of its opponents' energy surplus. In particular, the so-called framing effect, from the framework of prospect theory (PT), is used to account for each MGO's valuation of its gains and losses with respect to an individual utility reference point. The reference point is typically different for each individual and originates from its past experiences and future aspirations. A closed-form expression for the Bayesian Nash equilibrium is derived for the standard game formulation. Under PT, a best response algorithm is proposed to find the equilibrium. Simulation results show that, depending on their individual reference points, MGOs can tend to store more or less energy under PT compared to classical game theory. In addition, the impact of the reference point is found to be more prominent as the emergency price set by the power company increases."
"In order to reduce the computational cost of the simulation of electromagnetic responses in geophysical settings that involve highly heterogeneous media, we develop a multiscale finite volume method with oversampling for the quasi-static Maxwell's equations in the frequency domain. We assume a coarse mesh nested within a fine mesh that accurately discretizes the problem. For each coarse cell, we independently solve a local version of the original Maxwell's system subject to linear boundary conditions on an extended domain, which includes the coarse cell and a neighborhood of fine cells around it. The local Maxwell's system is solved using the fine mesh contained in the extended domain and the mimetic finite volume method. Next, these local solutions (basis functions) together with a weak-continuity condition are used to construct a coarse-mesh version of the global problem. The basis functions can be used to obtain the fine-mesh details from the solution of the coarse-mesh problem. Our approach leads to a significant reduction in the size of the final system of equations and the computational time, while accurately approximating the behavior of the fine-mesh solutions. We demonstrate the performance of our method using a synthetic 3D example of a mineral deposit."
"In this paper, a general model for cyber-physical systems (CPSs), that captures the diffusion of attacks from the cyber layer to the physical system, is studied. In particular, a game-theoretic approach is proposed to analyze the interactions between one defender and one attacker over a CPS. In this game, the attacker launches cyber attacks on a number of cyber components of the CPS to maximize the potential harm to the physical system while the system operator chooses to defend a number of cyber nodes to thwart the attacks and minimize potential damage to the physical side. The proposed game explicitly accounts for the fact that both attacker and defender can have different computational capabilities and disparate levels of knowledge of the system. To capture such bounded rationality of attacker and defender, a novel approach inspired from the behavioral framework of cognitive hierarchy theory is developed. In this framework, the defender is assumed to be faced with an attacker that can have different possible thinking levels reflecting its knowledge of the system and computational capabilities. To solve the game, the optimal strategies of each attacker type are characterized and the optimal response of the defender facing these different types is computed. This general approach is applied to smart grid security considering wide area protection with energy markets implications. Numerical results show that a deviation from the Nash equilibrium strategy is beneficial when the bounded rationality of the attacker is considered. Moreover, the results show that the defender's incentive to deviate from the Nash equilibrium decreases when faced with an attacker that has high computational ability."
"Current methods for automatically evaluating grammatical error correction (GEC) systems rely on gold-standard references. However, these methods suffer from penalizing grammatical edits that are correct but not in the gold standard. We show that reference-less grammaticality metrics correlate very strongly with human judgments and are competitive with the leading reference-based evaluation metrics. By interpolating both methods, we achieve state-of-the-art correlation with human judgments. Finally, we show that GEC metrics are much more reliable when they are calculated at the sentence level instead of the corpus level. We have set up a CodaLab site for benchmarking GEC output using a common dataset and different evaluation metrics."
"In the last few years it has been seen that many software vendors have started delivering projects incrementally with very short release cycles. Best examples of success of this approach has been Ubuntu Operating system that has a 6 months release cycle and popular web browsers such as Google Chrome, Opera, Mozilla Firefox. However there is very little knowledge available to the project managers to validate the chosen release cycle length. We propose a decision support system that helps to validate and estimate release cycle length in the early development phase by assuming that release cycle length is directly affected by three factors, (i) choosing right requirements for current cycle, (ii) estimating proximal time for each requirement, (iii) requirement wise feedback from last iteration based on product reception, model accuracy and failed requirements. We have altered and used the EVOLVE technique proposed by G. Ruhe to select best requirements for current cycle and map it to time domain using UCP (Use Case Points) based estimation and feedback factors. The model has been evaluated on both in-house as well as industry projects."
"Fifth generation (5G) dense small cell networks (SCNs) are expected to meet the thousand-fold mobile traffic challenge within the next few years. When developing solution schemes for resource allocation problems in such networks, conventional centralized control is no longer viable due to excessive computational complexity and large signaling overhead caused by the large number of users and network nodes in such a network. Instead, distributed resource allocation (or decision making) methods with low complexity would be desirable to make the network self-organizing and autonomous. Minority game (MG) has recently gained attention of the research community as a tool to model and solve distributed resource allocation problems. The main objective of this article is to study the applicability of the MG to solve the distributed decision making problems in future wireless networks. We present the fundamental theoretical aspects of basic MG, some variants of MG, and the notion of equilibrium. We also study the current state-of-the-art on the applications of MGs in communication networks. Furthermore, we describe an example application of MG to SCNs, where the problem of computation offloading by users in an SCN is modeled and analyzed using MG."
"Parallel implementations of stochastic gradient descent (SGD) have received significant research attention, thanks to excellent scalability properties of this algorithm, and to its efficiency in the context of training deep neural networks. A fundamental barrier for parallelizing large-scale SGD is the fact that the cost of communicating the gradient updates between nodes can be very large. Consequently, lossy compresion heuristics have been proposed, by which nodes only communicate quantized gradients. Although effective in practice, these heuristics do not always provably converge, and it is not clear whether they are optimal.   In this paper, we propose Quantized SGD (QSGD), a family of compression schemes which allow the compression of gradient updates at each node, while guaranteeing convergence under standard assumptions. QSGD allows the user to trade off compression and convergence time: it can communicate a sublinear number of bits per iteration in the model dimension, and can achieve asymptotically optimal communication cost. We complement our theoretical results with empirical data, showing that QSGD can significantly reduce communication cost, while being competitive with standard uncompressed techniques on a variety of real tasks."
"We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks."
"Existing approaches to resource allocation for nowadays stochastic networks are challenged to meet fast convergence and tolerable delay requirements. In the era of data deluge and information explosion, the present paper leverages online learning advances to facilitate stochastic resource allocation tasks. By recognizing the central role of Lagrange multipliers, the underlying constrained optimization problem is formulated as a machine learning task involving both training and operational modes, with the goal of learning the sought multipliers in a fast and efficient manner. To this end, an order-optimal offline learning approach is developed first for batch training, and it is then generalized to the online setting with a procedure termed learn-and-adapt. The novel resource allocation protocol permeates benefits of stochastic approximation and statistical learning to obtain low-complexity online updates with learning errors close to the statistical accuracy limits, while still preserving adaptation performance, which in the stochastic network optimization context guarantees queue stability. Analysis and simulated tests demonstrate that the proposed data-driven approach improves the delay and convergence performance of existing resource allocation schemes."
"The ability to record and replay program executions with low overhead enables many applications, such as reverse-execution debugging, debugging of hard-to-reproduce test failures, and ""black box"" forensic analysis of failures in deployed systems. Existing record-and-replay approaches rely on recording an entire virtual machine (which is heavyweight), modifying the OS kernel (which adds deployment and maintenance costs), or pervasive code instrumentation (which imposes significant performance and complexity overhead). We investigated whether it is possible to build a practical record-and-replay system avoiding all these issues. The answer turns out to be yes --- if the CPU and operating system meet certain non-obvious constraints. Fortunately modern Intel CPUs, Linux kernels and user-space frameworks meet these constraints, although this has only become true recently. With some novel optimizations, our system RR records and replays real-world workloads with low overhead with an entirely user-space implementation running on stock hardware and operating systems. RR forms the basis of an open-source reverse-execution debugger seeing significant use in practice. We present the design and implementation of RR, describe its performance on a variety of workloads, and identify constraints on hardware and operating system design required to support our approach."
"Using current reinforcement learning methods, it has recently become possible to learn to play unknown 3D games from raw pixels. In this work, we study the challenges that arise in such complex environments, and summarize current methods to approach these. We choose a task within the Doom game, that has not been approached yet. The goal for the agent is to fight enemies in a 3D world consisting of five rooms. We train the DQN and LSTM-A3C algorithms on this task. Results show that both algorithms learn sensible policies, but fail to achieve high scores given the amount of training. We provide insights into the learned behavior, which can serve as a valuable starting point for further research in the Doom domain."
"In this study, the explicit expressions for F-index and coindex of derived graphs such as a line graph, subdivision graph, vertex-semitotal graph, edge-semitotal graph, total graph and paraline graph (line graph of the subdivision graph) are obtained."
"Automatic segmentation of the liver and its lesion is an important step towards deriving quantitative biomarkers for accurate clinical diagnosis and computer-aided decision support systems. This paper presents a method to automatically segment liver and lesions in CT abdomen images using cascaded fully convolutional neural networks (CFCNs) and dense 3D conditional random fields (CRFs). We train and cascade two FCNs for a combined segmentation of the liver and its lesions. In the first step, we train a FCN to segment the liver as ROI input for a second FCN. The second FCN solely segments lesions from the predicted liver ROIs of step 1. We refine the segmentations of the CFCN using a dense 3D CRF that accounts for both spatial coherence and appearance. CFCN models were trained in a 2-fold cross-validation on the abdominal CT dataset 3DIRCAD comprising 15 hepatic tumor volumes. Our results show that CFCN-based semantic liver and lesion segmentation achieves Dice scores over 94% for liver with computation times below 100s per volume. We experimentally demonstrate the robustness of the proposed method as a decision support system with a high accuracy and speed for usage in daily clinical routine."
"A new approach to solving a class of rankconstrained semi-definite programming (SDP) problems, which appear in many signal processing applications such as transmit beamspace design in multiple-input multiple-output (MIMO) radar, downlink beamforming design in MIMO communications, generalized sidelobe canceller design, phase retrieval, etc., is presented. The essence of the approach is the use of underlying algebraic structure enforced in such problems by other practical constraints such as, for example, null shaping constraint. According to this approach, instead of relaxing the non-convex rankconstrained SDP problem to a feasible set of positive semidefinite matrices, we restrict it to a space of polynomials whose dimension is equal to the desired rank. The resulting optimization problem is then convex as its solution is required to be full rank, and can be efficiently and exactly solved. A simple matrix decomposition is needed to recover the solution of the original problem from the solution of the restricted one. We show how this approach can be applied to solving some important signal processing problems that contain null-shaping constraints. As a byproduct of our study, the conjugacy of beamfoming and parameter estimation problems leads us to formulation of a new and rigorous criterion for signal/noise subspace identification. Simulation results are performed for the problem of rank-constrained beamforming design and show an exact agreement of the solution with the proposed algebraic structure, as well as significant performance improvements in terms of sidelobe suppression compared to the existing methods."
"Morphology unbalanced languages remains a big challenge in the context of machine translation. In this paper, we propose to de-couple machine translation from morphology generation in order to better deal with the problem. We investigate the morphology simplification with a reasonable trade-off between expected gain and generation complexity.   For the Chinese-Spanish task, optimum morphological simplification is in gender and number. For this purpose, we design a new classification architecture which, compared to other standard machine learning techniques, obtains the best results. This proposed neural-based architecture consists of several layers: an embedding, a convolutional followed by a recurrent neural network and, finally, ends with sigmoid and softmax layers. We obtain classification results over 98% accuracy in gender classification, over 93% in number classification, and an overall translation improvement of 0.7 METEOR."
"This paper addresses challenges of Natural Language Processing (NLP) on non-canonical multilingual data in which two or more languages are mixed. It refers to code-switching which has become more popular in our daily life and therefore obtains an increasing amount of attention from the research community. We report our experience that cov- ers not only core NLP tasks such as normalisation, language identification, language modelling, part-of-speech tagging and dependency parsing but also more downstream ones such as machine translation and automatic speech recognition. We highlight and discuss the key problems for each of the tasks with supporting examples from different language pairs and relevant previous work."
"In large-scale emergencies social media has become a key source of information for public awareness, government authorities and relief agencies. However, the sheer volume of data and the low signal-to- noise ratio limit the effectiveness and the efficiency of using social media as an intelligence resource. We describe Australian Crisis Tracker (ACT), a tool designed for agencies responding to large- scale emergency events, to facilitate the understanding of critical information in Twitter. ACT was piloted by the Australian Red Cross (ARC) during the 2013-2014 Australian bushfires season.   Video is available at: https://www.youtube.com/watch?v=Y-1rtNFqQbE"
"We present an approach for weakly supervised learning of human actions from video transcriptions. Our system is based on the idea that, given a sequence of input data and a transcript, i.e. a list of the order the actions occur in the video, it is possible to infer the actions within the video stream, and thus, learn the related action models without the need for any frame-based annotation. Starting from the transcript information at hand, we split the given data sequences uniformly based on the number of expected actions. We then learn action models for each class by maximizing the probability that the training video sequences are generated by the action models given the sequence order as defined by the transcripts. The learned model can be used to temporally segment an unseen video with or without transcript. We evaluate our approach on four distinct activity datasets, namely Hollywood Extended, MPII Cooking, Breakfast and CRIM13. We show that our system is able to align the scripted actions with the video data and that the learned models localize and classify actions competitively in comparison to models trained with full supervision, i.e. with frame level annotations, and that they outperform any current state-of-the-art approach for aligning transcripts with video data."
"We present an algorithm for deriving a spatial-behavioral type system from a formal presentation of a computational calculus. Given a 2-monad Calc: Catv$\to$ Cat for the free calculus on a category of terms and rewrites and a 2-monad BoolAlg for the free Boolean algebra on a category, we get a 2-monad Form = BoolAlg + Calc for the free category of formulae and proofs. We also get the 2-monad BoolAlg $\circ$ Calc for subsets of terms. The interpretation of formulae is a natural transformation $\interp{-}$: Form $\Rightarrow$ BoolAlg $\circ$ Calc defined by the units and multiplications of the monads and a distributive law transformation $\delta$: Calc $\circ$ BoolAlg $\Rightarrow$ BoolAlg $\circ$ Calc. This interpretation is consistent both with the Curry-Howard isomorphism and with realizability. We give an implementation of the ""possibly"" modal operator parametrized by a two-hole term context and show that, surprisingly, the arrow type constructor in the $\lambda$-calculus is a specific case. We also exhibit nontrivial formulae encoding confinement and liveness properties for a reflective higher-order variant of the $\pi$-calculus."
"This document is meant to help individuals use the Cerebral Signal Phase Analysis toolbox which implements different methods for estimating the instantaneous phase and frequency of a signal and calculating some related popular quantities.The toolbox -- which is distributed under the terms of the GNU GENERAL PUBLIC LICENSE as a set of MATLAB routines -- can be downloaded at the address http://oset.ir/category.php?dir=Tools.The purpose of this toolbox is to calculate the instantaneous phase and frequency sequences of cerebral signals (EEG, MEG, etc.) and some related popular features and quantities in brain studies and Neuroscience such as Phase Shift, Phase Resetting, Phase Locking Value (PLV), Phase Difference and more, to help researchers in these fields."
"In mammography, the efficacy of computer-aided detection methods depends, in part, on the robust localisation of micro-calcifications ($\mu$C). Currently, the most effective methods are based on three steps: 1) detection of individual $\mu$C candidates, 2) clustering of individual $\mu$C candidates, and 3) classification of $\mu$C clusters. Where the second step is motivated both to reduce the number of false positive detections from the first step and on the evidence that malignancy depends on a relatively large number of $\mu$C detections within a certain area. In this paper, we propose a novel approach to $\mu$C detection, consisting of the detection \emph{and} classification of individual $\mu$C candidates, using shape and appearance features, using a cascade of boosting classifiers. The final step in our approach then clusters the remaining individual $\mu$C candidates. The main advantage of this approach lies in its ability to reject a significant number of false positive $\mu$C candidates compared to previously proposed methods. Specifically, on the INbreast dataset, we show that our approach has a true positive rate (TPR) for individual $\mu$Cs of 40\% at one false positive per image (FPI) and a TPR of 80\% at 10 FPI. These results are significantly more accurate than the current state of the art, which has a TPR of less than 1\% at one FPI and a TPR of 10\% at 10 FPI. Our results are competitive with the state of the art at the subsequent stage of detecting clusters of $\mu$Cs."
"Spatial smoothing is a widely used preprocessing scheme to improve the performance of high-resolution parameter estimation algorithms in case of coherent signals or if only a small number of snapshots is available. In this paper, we present a first-order performance analysis of the spatially smoothed versions of R-D Standard ESPRIT and R-D Unitary ESPRIT for sources with arbitrary signal constellations as well as R-D NC Standard ESPRIT and R-D NC Unitary ESPRIT for strictly second-order (SO) non-circular (NC) sources. The derived expressions are asymptotic in the effective signal-to-noise ratio (SNR), i.e., the approximations become exact for either high SNRs or a large sample size. Moreover, no assumptions on the noise statistics are required apart from a zero-mean and finite SO moments. We show that both R-D NC ESPRIT-type algorithms with spatial smoothing perform asymptotically identical in the high effective SNR regime. Generally, the performance of spatial smoothing based algorithms depends on the number of subarrays, which is a design parameter and needs to be chosen beforehand. In order to gain more insights into the optimal choice of the number of subarrays, we simplify the derived analytical R-D mean square error (MSE) expressions for the special case of a single source. The obtained MSE expression explicitly depends on the number of subarrays in each dimension, which allows us to analytically find the optimal number of subarrays for spatial smoothing. Based on this result, we additionally derive the maximum asymptotic gain from spatial smoothing and explicitly compute the asymptotic efficiency for this special case. All the analytical results are verified by simulations."
"Differently from computer vision systems which require explicit supervision, humans can learn facial expressions by observing people in their environment. In this paper, we look at how similar capabilities could be developed in machine vision. As a starting point, we consider the problem of relating facial expressions to objectively measurable events occurring in videos. In particular, we consider a gameshow in which contestants play to win significant sums of money. We extract events affecting the game and corresponding facial expressions objectively and automatically from the videos, obtaining large quantities of labelled data for our study. We also develop, using benchmarks such as FER and SFEW 2.0, state-of-the-art deep neural networks for facial expression recognition, showing that pre-training on face verification data can be highly beneficial for this task. Then, we extend these models to use facial expressions to predict events in videos and learn nameable expressions from them. The dataset and emotion recognition models are available at http://www.robots.ox.ac.uk/~vgg/data/facevalue"
"In this paper we investigate the image aesthetics classification problem, aka, automatically classifying an image into low or high aesthetic quality, which is quite a challenging problem beyond image recognition. Deep convolutional neural network (DCNN) methods have recently shown promising results for image aesthetics assessment. Currently, a powerful inception module is proposed which shows very high performance in object classification. However, the inception module has not been taken into consideration for the image aesthetics assessment problem. In this paper, we propose a novel DCNN structure codenamed ILGNet for image aesthetics classification, which introduces the Inception module and connects intermediate Local layers to the Global layer for the output. Besides, we use a pre-trained image classification CNN called GoogLeNet on the ImageNet dataset and fine tune our connected local and global layer on the large scale aesthetics assessment AVA dataset. The experimental results show that the proposed ILGNet outperforms the state of the art results in image aesthetics assessment in the AVA benchmark."
"Stochastic, spatial reaction-diffusion simulations have been widely used in systems biology and computational neuroscience. However, the increasing scale and complexity of simulated models and morphologies have exceeded the capacity of any serial implementation. This led to development of parallel solutions that benefit from the boost in performance of modern large-scale supercomputers. In this paper, we describe an MPI-based, parallel Operator-Splitting implementation for stochastic spatial reaction-diffusion simulations with irregular tetrahedral meshes. The performance of our implementation is first examined and analyzed with simulations of a simple model. We then demonstrate its usage in real-world research by simulating the reaction-diffusion components of a published calcium burst model in both Purkinje neuron sub-branch and full dendrite morphologies. Simulation results indicate that our implementation is capable of achieving super-linear speedup for balanced loading simulations with reasonable molecule density and mesh quality. In the best scenario a parallel simulation with 2000 processes achieves more than 3600 times of speedup relative to its serial SSA counterpart and more than 20 times of speedup relative to parallel simulation with 100 processes. While simulation performance is affected by unbalanced loading, a substantial speedup can still be observed without any special treatment."
"A generalisation of Scott's information systems~\cite{sco82} is presented that captures exactly all L-domains. The global consistency predicate in Scott's definition is relativised in such a way that there is a consistency predicate for each atomic proposition (token) saying which finite sets of such statements express information that is consistent with the given statement.   It is shown that the states of such generalised information systems form an L-domain, and that each L-domain can be generated in this way, up to isomorphism. Moreover, the equivalence of the category of generalised information systems with the category of L-domains is derived. In addition, it will be seen that from every generalised information system capturing an algebraic bounded-complete domain a corresponding Scott information system can be obtained in an easy and natural way; similarly for Hoofman's continuous information systems~\cite{ho93} and the continuous bounded-complete domains captured by them."
"The main task of HTTP Adaptive Streaming is to adapt video quality dynamically under variable network conditions. This is a key feature for multimedia delivery especially when quality of service cannot be granted network-wide and, e.g., throughput may suffer short term fluctuations.   Hence, robust bitrate adaptation schemes become crucial in order to improve video quality. The objective, in this context, is to control the filling level of the playback buffer and maximize the quality of the video, while avoiding unnecessary video quality variations.   In this paper we study bitrate adaptation algorithms based on Backward-Shifted Coding (BSC), a scalable video coding scheme able to greatly improve video quality. We design bitrate adaptation algorithms that balance video rate smoothness and high network capacity utilization, leveraging both on throughput-based and buffer-based adaptation mechanisms.   Extensive simulations using synthetic and real-world video traffic traces show that the proposed scheme performs remarkably well even under challenging network conditions."
"Accurate estimation of the confidence of an indoor localization system is crucial for a number of applications including crowd-sensing applications, map-matching services, and probabilistic location fusion techniques; all of which lead to an enhanced user experience. Current approaches for quantifying the output accuracy of a localization system in real-time either do not provide a distance metric, require an extensive training process, and/or are tailored to a specific localization system. In this paper, we present the design, implementation, and evaluation of CONE: a novel calibration-free accurate confidence estimation system that can work in real-time with any location determination system. CONE builds on a sound theoretical model that allows it to trade the required user confidence with tight bound on the estimated confidence radius. We also introduce a new metric for evaluating confidence estimation systems that can capture new aspects of their performance. Evaluation of CONE on Android phones in a typical testbed using the iBeacons BLE technology with a side-by-side comparison with traditional confidence estimation techniques shows that CONE can achieve a consistent median absolute error difference accuracy of less than 2.7m while estimating the user position more than 80% of the time within the confidence circle. This is significantly better than the state-of-the-art confidence estimation systems that are tailored to the specific localization system in use. Moreover, CONE does not require any calibration and therefore provides a scalable and ubiquitous confidence estimation system for pervasive applications."
"Consider unsupervised clustering of objects drawn from a discrete set, through the use of human intelligence available in crowdsourcing platforms. This paper defines and studies the problem of universal clustering using responses of crowd workers, without knowledge of worker reliability or task difficulty. We model stochastic worker response distributions by incorporating traits of memory for similar objects and traits of distance among differing objects. We are particularly interested in two limiting worker types---temporary workers who retain no memory of responses and long-term workers with memory. We first define clustering algorithms for these limiting cases and then integrate them into an algorithm for the unified worker model. We prove asymptotic consistency of the algorithms and establish sufficient conditions on the sample complexity of the algorithm. Converse arguments establish necessary conditions on sample complexity, proving that the defined algorithms are asymptotically order-optimal in cost."
"We present methodology, algorithms and software for evaluating flow and view for architectural settlement layouts. For a given settlement layout consisting of a number of buildings arbitrarily positioned on a piece of land, in the present study an island situated on the west coast of Sweden, the methodology allows for evaluation of flow patterns and for evaluating the view experienced from the buildings. The computation of flow is based on a multimesh finite element method, which allows each building to be embedded in a boundary-fitted mesh which can be moved around freely in a fixed background mesh. The computation of view is based on a novel and objective measure of the view which can be efficiently computed by rasterization."
"MicroRNAs (miRNAs) are non-coding RNAs with approximately 22 nucleotides (nt) that are derived from precursor molecules. These precursor molecules or pre-miRNAs often fold into stem-loop hairpin structures. However, a large number of sequences with pre-miRNA-like hairpins can be found in genomes. It is a challenge to distinguish the real pre-miRNAs from other hairpin sequences with similar stem-loops (referred to as pseudo pre-miRNAs). Several computational methods have been developed to tackle this challenge. In this paper we propose a new method, called MirID, for identifying and classifying microRNA precursors. We collect 74 features from the sequences and secondary structures of pre-miRNAs; some of these features are taken from our previous studies on non-coding RNA prediction while others were suggested in the literature. We develop a combinatorial feature mining algorithm to identify suitable feature sets. These feature sets are then used to train support vector machines to obtain classification models, based on which classifier ensemble is constructed. Finally we use an AdaBoost algorithm to further enhance the accuracy of the classifier ensemble. Experimental results on a variety of species demonstrate the good performance of the proposed method, and its superiority over existing tools."
"This document describes a class of C functions implementing a distributed software voting mechanism for EPX or similar message passing multi-threaded environments. Such a tool may be used for example, to set up a restoring organ, i.e., an NMR (i.e., N-module redundant) system with N voters. In order to describe the tool we start defining its basic building block, the voter. A voter is defined as a software module connected to one user module and to a farm of fellow voters arranged into a clique. By means of the functions in the class the user module is able: to create a static ""picture"" of the voting farm, needed for the set up of the clique; to instantiate the local voter; to send input or control messages to that voter. No interlocutor is needed other than the local voter. The other user modules are supposed to create coherent pictures and instances of voters on other nodes of the machine and to manage consistently the task of their local intermediary. All technicalities concerning the set up of the clique and the exchange of messages between the voters are completely transparent to the user module. In the following the basic functionalities of the VotingFarm class will be discussed, namely how to set up a ""passive farm"", or a non-alive topological representation of a yet-to-be-activated voting farm; how to initiate the voting farm; how to control the farm."
"This paper proposes a Bayesian modeling approach to address the problem of online fault-tolerant dynamic event region detection in wireless sensor networks. In our model every network node is associated with a virtual community and a trust index, which quantitatively measures the trustworthiness of this node in its community. If a sensor node's trust value is smaller than a threshold, it suggests that this node encounters a fault and thus its sensor reading can not be trusted at this moment. This concept of sensor node trust discriminates our model with the other alternatives, e.g.,the Markov random fields. The practical issues, including spatiotemporal correlations of neighbor nodes' sensor readings, the presence of sensor faults and the requirement of online processing are linked together by the concept trust and are all taken into account in the modeling stage. Based on the proposed model, the trust value of each node is updated online by a particle filter algorithm upon the arrival of new observations. The decision on whether a node is located in the event region is made based upon the current estimate of this node's trust value. Experimental results demonstrate that the proposed solution can provide striking better performance than existent methods in terms of error rate in detecting the event region."
"Planning has achieved significant progress in recent years. Among the various approaches to scale up plan synthesis, the use of macro-actions has been widely explored. As a first stage towards the development of a solution to learn on-line macro-actions, we propose an algorithm to identify useful macro-actions based on data mining techniques. The integration in the planning search of these learned macro-actions shows significant improvements over four classical planning benchmarks."
"Convolutional neural network (CNN) is one of the most prominent architectures and algorithm in Deep Learning. It shows a remarkable improvement in the recognition and classification of objects. This method has also been proven to be very effective in a variety of computer vision and machine learning problems. As in other deep learning, however, training the CNN is interesting yet challenging. Recently, some metaheuristic algorithms have been used to optimize CNN using Genetic Algorithm, Particle Swarm Optimization, Simulated Annealing and Harmony Search. In this paper, another type of metaheuristic algorithms with different strategy has been proposed, i.e. Microcanonical Annealing to optimize Convolutional Neural Network. The performance of the proposed method is tested using the MNIST and CIFAR-10 datasets. Although experiment results of MNIST dataset indicate the increase in computation time (1.02x - 1.38x), nevertheless this proposed method can considerably enhance the performance of the original CNN (up to 4.60\%). On the CIFAR10 dataset, currently, state of the art is 96.53\% using fractional pooling, while this proposed method achieves 99.14\%."
"This paper proposes energy-efficient coordinated beamforming strategies for multi-cell multi-user multiple-input single-output system. We consider a practical power consumption model, where part of the consumed power depends on the base station or user specific data rates due to coding, decoding and backhaul. This is different from the existing approaches where the base station power consumption has been assumed to be a convex or linear function. Two optimization criteria are considered, namely network energy efficiency maximization and weighted sum energy efficiency maximization. We develop successive convex approximation based algorithms to tackle these difficult nonconvex problems. We further propose decentralized implementations for the considered problems, in which base stations perform parallel and distributed computation based on local channel state information and limited backhaul information exchange. The decentralized approaches admit closed-form solutions and can be implemented without invoking a generic external convex solver. The effect of pilot contamination caused by pilot reuse is also taken into account in the energy efficiency problems. To achieve energy efficiency improvements with a limited number of pilot resources, we propose a heuristic energy-efficient pilot allocation strategy to mitigate the pilot contamination effect. The numerical results are provided to demonstrate that the rate dependent power consumption has a large impact on the system energy efficiency, and, thus, has to be taken into account when devising energy-efficient transmission strategies. We also investigate the effect of pilot contamination and show that the proposed pilot allocation strategy achieve significant performance improvements when a limited number of pilot resources is available."
"We develop Gibbs sampling based techniques for learning the optimal content placement in a cellular network. A collection of base stations are scattered on the space, each having a cell (possibly overlapping with other cells). Mobile users request for downloads from a finite set of contents according to some popularity distribution. Each base station can store only a strict subset of the contents at a time; if a requested content is not available at any serving base station, it has to be downloaded from the backhaul. Thus, there arises the problem of optimal content placement which can minimize the download rate from the backhaul, or equivalently maximize the cache hit rate. Using similar ideas as Gibbs sampling, we propose simple sequential content update rules that decide whether to store a content at a base station based on the knowledge of contents in neighbouring base stations. The update rule is shown to be asymptotically converging to the optimal content placement for all nodes. Next, we extend the algorithm to address the situation where content popularities and cell topology are initially unknown, but are estimated as new requests arrive to the base stations. Finally, improvement in cache hit rate is demonstrated numerically."
"Small-gain conditions used in analysis of feedback interconnections are contraction conditions which imply certain stability properties. Such conditions are applied to a finite or infinite interval. In this paper we consider the case, when a small-gain condition is applied to several disjunct intervals and use the density propagation condition in the gaps between these intervals to derive global stability properties for an interconnection. This extends and improves recent results from [1]."
"We offer a lattice-theoretic account of dynamic slicing for {\pi}-calculus, building on prior work in the sequential setting. For any run of a concurrent program, we exhibit a Galois connection relating forward slices of the start configuration to backward slices of the end configuration. We prove that, up to lattice isomorphism, the same Galois connection arises for any causally equivalent execution, allowing an efficient concurrent implementation of slicing via a standard interleaving semantics. Our approach has been formalised in the dependently-typed language Agda."
"This paper presents a systematic survey on existing literature and seminal works relevant to the application of ontologies in different aspects of Cloud computing. Our hypothesis is that ontologies along with their reasoning capabilities can have significant impact on improving various aspects of the Cloud computing phenomena. Ontologies can promote intelligent decision support mechanisms for various Cloud based services. They can also provide effective interoperability among the Cloud based systems and resources. This survey can promote a comprehensive understanding on the roles and significance of ontologies within the overall domain of Cloud Computing. Also, this project can potentially form the basis of new research area and possibilities for both ontology and Cloud computing communities."
"In this paper, we present approximation algorithms for the directed multi-multiway cut and directed multicut problems. The so called region growing paradigm \cite{1} is modified and used for these two cut problems on directed graphs By this paradigm, we give for each problem an approximation algorithm such that both algorithms have an approximate factor. The work previously done on these problems need to solve k linear programming, whereas our algorithms require only one linear programming for obtaining a good approximate factor."
"With the rapid increase in computing, storage and networking resources, data is not only collected and stored, but also analyzed. This creates a serious privacy problem which often inhibits the use of this data. In this chapter, we investigate and resolve the privacy issues in a fundamental optimization problem -- linear programming (LP) which is formulated by data collected from different parties. We first consider the case where the objective function and constraints of the linear programming problem are not partitioned between two parties where one party privately holds the objective function while the other party privately holds the constraints. Second, we present a privacy preserving technique for the case that objective function and constraints are arbitrarily partitioned between two parties where each party privately holds a share of objective function and constraints. Finally, we extend the technique for securely solving two-party arbitrarily partitioned linear programming problems to a multi-party scenario. In summary, we propose a set of efficient and secure transformation based techniques that create significant value-added benefits of being independent of the specific algorithms used for solving the linear programming problem."
"In big data era, the data continuously generated and its distribution may keep changes overtime. These challenges in online stream of data are known as concept drift. In this paper, we proposed the Adaptive Convolutional ELM method (ACNNELM) as enhancement of Convolutional Neural Network (CNN) with a hybrid Extreme Learning Machine (ELM) model plus adaptive capability. This method is aimed for concept drift handling. We enhanced the CNN as convolutional hiererchical features representation learner combined with Elastic ELM (E$^2$LM) as a parallel supervised classifier. We propose an Adaptive OS-ELM (AOS-ELM) for concept drift adaptability in classifier level (named ACNNELM-1) and matrices concatenation ensembles for concept drift adaptability in ensemble level (named ACNNELM-2). Our proposed Adaptive CNNELM is flexible that works well in classifier level and ensemble level while most current methods only proposed to work on either one of the levels.   We verified our method in extended MNIST data set and not MNIST data set. We set the experiment to simulate virtual drift, real drift, and hybrid drift event and we demonstrated how our CNNELM adaptability works. Our proposed method works well and gives better accuracy, computation scalability, and concept drifts adaptability compared to the regular ELM and CNN. Further researches are still required to study the optimum parameters and to use more varied image data set."
"We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the \textit{depthwise separable convolution} operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameter as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters."
"For many European countries, in 2015 the refugee situation developed from a remote tragedy reported upon in the news to a situation they have to deal with in their own neighborhood. Driven by this observation, we investigated the development of the perception of the refugee situation during 2015 in Twitter. Starting from a dataset of 1.7 Million tweets covering refugee-related topics from May to December 2015, we investigated how the discussion on refugees changed over time, in different countries as well as in relationship with the evolution of the actual situation. In this paper we report and discuss our findings from checking a set of hypotheses, such as that the closeness to the actual situation would influence the intensity and polarity of discussions and that news media takes a mediating role between the actual and perceived refugee situation."
"We consider the problem of delivering $m$ messages between specified source-target pairs in a weighted undirected graph, by $k$ mobile agents initially located at distinct nodes of the graph. Each agent consumes energy proportional to the distance it travels in the graph and we are interested in optimizing the total energy consumption for the team of agents. Unlike previous related work, we consider heterogeneous agents with different rates of energy consumption (weights~$w_i$). To solve the delivery problem, agents face three major challenges: \emph{Collaboration} (how to work together on each message), \emph{Planning} (which route to take) and \emph{Coordination} (how to assign agents to messages).   We first show that the delivery problem can be 2-approximated \emph{without} collaborating and that this is best possible, i.e., we show that the \emph{benefit of collaboration} is 2 in general. We also show that the benefit of collaboration for a single message is~$1/\ln 2 \approx 1.44$. Planning turns out to be \NP-hard to approximate even for a single agent, but can be 2-approximated in polynomial time if agents have unit capacities and do not collaborate. We further show that coordination is \NP-hard even for agents with unit capacity, but can be efficiently solved exactly if they have uniform weights. Finally, we give a polynomial-time $(4\max\tfrac{w_i}{w_j})$-approximation for message delivery with unit capacities."
"We present a coordination language for the modeling of distributed database applications. The language, baptized Klaim-DB, borrows the concepts of localities and nets of the coordination language Klaim but re-incarnates the tuple spaces of Klaim as databases. It provides high-level abstractions and primitives for the access and manipulation of structured data, with integrity and atomicity considerations. We present the formal semantics of Klaim-DB and develop a type system that avoids potential runtime errors such as certain evaluation errors and mismatches of data format in tables, which are monitored in the semantics. The use of the language is illustrated in a scenario where the sales from different branches of a chain of department stores are aggregated from their local databases. Raising the abstraction level and encapsulating integrity checks in the language primitives have benefited the modeling task considerably."
"Increasing the scalability of machine learning to handle big volume of data is a challenging task. The scale up approach has some limitations. In this paper, we proposed a scale out approach for CNN-ELM based on MapReduce on classifier level. Map process is the CNN-ELM training for certain partition of data. It involves many CNN-ELM models that can be trained asynchronously. Reduce process is the averaging of all CNN-ELM weights as final training result. This approach can save a lot of training time than single CNN-ELM models trained alone. This approach also increased the scalability of machine learning by combining scale out and scale up approaches. We verified our method in extended MNIST data set and not-MNIST data set experiment. However, it has some drawbacks by additional iteration learning parameters that need to be carefully taken and training data distribution that need to be carefully selected. Further researches to use more complex image data set are required."
"More often than not, there is a need to understand the structure of complex computer code: what functions and in what order they are called, how information travels around static, input, and output variables, what depends on what. As a rule, executable code and data are scattered among multiple files and even multiple modules. Information is transmitted among variables which often change names. These tangled relations greatly complicate the development, maintenance, and redevelopment of code, its analysis for complexity and its robustness. As of now, there is no tool which is capable of presenting the real-life, useful diagram of actual code. Conventional flowcharts fail. Proposed is the method which overcomes these difficulties. The main idea is that functionality of software can be described through flows of control, which is essentially flows of time, and flows of data. These are inseparable. The second idea is to follow very strict system boundaries and distinctions with respect to modules, functions, blocks, and operators, as well as data holders, showing them all as subsystems, in other words, by clearly expressing the system structure when every piece of executable code and every variable may have its own graphical representation. The third is defining timelines as the entities clearly separated from the connected blocks of code. Timelines allow presentation of nesting of the control flow as deep as necessary. As a proof of concept, the same methods successfully describe production systems. Keywords: flowchart, UML, software diagram, visual programming, extreme programming, extreme modeling, control flow, data flow."
"Step adjustment for humanoid robots has been shown to improve robustness in gaits. However, step duration adaptation is often neglected in control strategies. In this paper, we propose an approach that combines both step location and timing adjustment for generating robust gaits. In this approach, step location and step timing are decided, based on feedback from the current state of the robot. The proposed approach is comprised of two stages. In the first stage, the nominal step location and step duration for the next step or a previewed number of steps are specified. In this stage which is done at the start of each step, the main goal is to specify the best step length and step duration for a desired walking speed. The second stage deals with finding the best landing point and landing time of the swing foot at each control cycle. In this stage, stability of the gaits is preserved by specifying a desired offset between the swing foot landing point and the Divergent Component of Motion (DCM) at the end of current step. After specifying the landing point of the swing foot at a desired time, the swing foot trajectory is regenerated at each control cycle to realize desired landing properties. Simulation on different scenarios shows the robustness of the generated gaits from our proposed approach compared to the case where no timing adjustment is employed."
"In this paper, we investigate the synthesis of piecewise affine feedback controllers to address the problem of safe and robust controller design in robotics based on high-level controls specifications. The methodology is based on formulating the problem as a collection of reach control problems on a polytopic state space. Reach control has so far only been developed in theory and has not been tested experimentally on a real system before. Using a quadrocopter as our experimental platform, we show that these theoretical tools can achieve fast, albeit safe and robust maneuvers. In contrast to most traditional control techniques, the reach control approach does not require a predefined open-loop reference trajectory or spacial path. Experimental results on a quadrocopter show the effectiveness and robustness of this control approach. In a proof-of-concept demonstration, the reach controller is implemented in one translational direction while the other degrees of freedom are stabilized by separate controllers."
"This report presents the results of the UNIFY Service Provider DevOps activities. First, we present the final definition and assessment of the concept. SP-DevOps is realized by a combination of various functional components facilitating integrated service verification, efficient and programmable observability, and automated troubleshooting processes. Our assessment shows that SP-DevOps can help providers to reach a medium level of DevOps maturity and allows significant reduction in OPEX. Second, we focus on the evaluation of the proposed SP-DevOps components. The set of tools proposed supports ops and devs across all stages, with a focus on the deployment, operation and debugging phases, and allows to activate automated processes for operating NFV environments. Finally, we present use-cases and our demonstrators for selected process implementions, which allowed the functional validation of SP-DevOps."
"We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing the regions of input that are ""important"" for predictions from these models - or visual explanations.   Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses the class-specific gradient information flowing into the final convolutional layer of a CNN to produce a coarse localization map of the important regions in the image. Grad-CAM is a strict generalization of the Class Activation Mapping. Unlike CAM, Grad-CAM requires no re-training and is broadly applicable to any CNN-based architectures. We also show how Grad-CAM may be combined with existing pixel-space visualizations to create a high-resolution class-discriminative visualization (Guided Grad-CAM). We generate Grad-CAM and Guided Grad-CAM visual explanations to better understand image classification, image captioning, and visual question answering (VQA) models. In the context of image classification models, our visualizations (a) lend insight into their failure modes showing that seemingly unreasonable predictions have reasonable explanations, and (b) outperform pixel-space gradient visualizations (Guided Backpropagation and Deconvolution) on the ILSVRC-15 weakly supervised localization task. For image captioning and VQA, our visualizations expose the somewhat surprising insight that common CNN + LSTM models can often be good at localizing discriminative input image regions despite not being trained on grounded image-text pairs.   Finally, we design and conduct human studies to measure if Guided Grad-CAM explanations help users establish trust in the predictions made by deep networks. Interestingly, we show that Guided Grad-CAM helps untrained users successfully discern a ""stronger"" deep network from a ""weaker"" one even when both networks make identical predictions."
"In this paper, robust detection, tracking and geometry estimation methods are developed and combined into a system for estimating time-difference estimates, microphone localization and sound source movement. No assumptions on the 3D locations of the microphones and sound sources are made. The system is capable of tracking continuously moving sound sources in an reverberant environment. The multi-path components are explicitly tracked and used in the geometry estimation parts. The system is based on matching between pairs of channels using GCC-PHAT. Instead of taking a single maximum at each time instant from each such pair, we select the four strongest local maxima. This produce a set of hypothesis to work with in the subsequent steps, where consistency constraints between the channels and time-continuity constraints are exploited. In the paper it demonstrated how such detections can be used to estimate microphone positions, sound source movement and room geometry. The methods are tested and verified using real data from several reverberant environments. The evaluation demonstrated accuracy in the order of few millimeters."
"We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.   In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests.   We illustrate our notion using a case study of FICO credit scores."
"In this paper, we propose a robust and parsimonious approach using Deep Convolutional Neural Network (DCNN) to recognize and interpret interior space. DCNN has achieved incredible success in object and scene recognition. In this study we design and train a DCNN to classify a pre-zoning indoor space, and from a single phone photo to recognize the learned space features, with no need of additional assistive technology. We collect more than 600,000 images inside MIT campus buildings to train our DCNN model, and achieved 97.9% accuracy in validation dataset and 81.7% accuracy in test dataset based on spatial-scale fixed model. Furthermore, the recognition accuracy and spatial resolution can be potentially improved through multiscale classification model. We identify the discriminative image regions through Class Activating Mapping (CAM) technique, to observe the model's behavior in how to recognize space and interpret it in an abstract way. By evaluating the results with misclassification matrix, we investigate the visual spatial feature of interior space by looking into its visual similarity and visual distinctiveness, giving insights into interior design and human indoor perception and wayfinding research. The contribution of this paper is threefold. First, we propose a robust and parsimonious approach for indoor navigation using DCNN. Second, we demonstrate that DCNN also has a potential capability in space feature learning and recognition, even under severe appearance changes. Third, we introduce a DCNN based approach to look into the visual similarity and visual distinctiveness of interior space."
"We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This generative model allows efficient search and optimization through open-ended spaces of chemical compounds. We train deep neural networks on hundreds of thousands of existing chemical structures to construct two coupled functions: an encoder and a decoder. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to the discrete representation from this latent space. Continuous representations allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. Continuous representations also allow the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds. We demonstrate our method in the design of drug-like molecules as well as organic light-emitting diodes."
"Neural sequence models are widely used to model time-series data in many fields. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top-$B$ candidates -- resulting in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose \emph{Diverse Beam Search} (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing for a diversity-augmented objective. We observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space -- implying that DBS is a \emph{better search algorithm}. Moreover, these gains are achieved with minimal computational or memory overhead as compared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation and visual question generation using both standard quantitative metrics and qualitative human studies. Our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models."
"In this short note we introduce ResearchDoom, an implementation of the Doom first-person shooter that can extract detailed metadata from the game. We also introduce the CocoDoom dataset, a collection of pre-recorded data extracted from Doom gaming sessions along with annotations in the MS Coco format. ResearchDoom and CocoDoom can be used to train and evaluate a variety of computer vision methods such as object recognition, detection and segmentation at the level of instances and categories, tracking, ego-motion estimation, monocular depth estimation and scene segmentation. The code and data are available at http://www.robots.ox.ac.uk/~vgg/research/researchdoom."
"In this paper, we study a model reduction technique for leader-follower networked multi-agent systems defined on weighted, undirected graphs with arbitrary linear multivariable agent dynamics. In the network graph of this network, nodes represent the agents and edges represent communication links between the agents. Only the leaders in the network receive an external input, the followers only exchange information with their neighbors. The reduced network is obtained by partitioning the set of nodes into disjoint sets, called clusters, and associating with each cluster a single, new, node in a reduced network graph. The resulting reduced network has a weighted, symmetric, directed network graph, and inherits some of the structure of the original network. We establish a priori upper bounds on the $\mathcal{H}_2$ and $\mathcal{H}_\infty$ model reduction error for the special case that the graph partition is almost equitable. These upper bounds depend on the Laplacian eigenvalues of the original and reduced network, an auxiliary system associated with the agent dynamics, and the number of nodes that belong to the same clusters as the leaders in the network. Finally, we consider the problem of obtaining a priori upper bounds if we cluster using arbitrary, possibly non almost equitable, partitions."
"Lecture notes are important for students to review and understand the key points in the class. Unfortunately, the students often miss or lose part of the lecture notes. In this paper, we design and implement an infrared sensor based system, InfraNotes, to automatically record the notes on the board by sensing and analyzing hand gestures of the lecturer. Compared with existing techniques, our system does not require special accessories with lecturers such as sensor-facilitated pens, writing surfaces or the video-taping infrastructure. Instead, it only has an infrared-sensor module on the eraser holder of black/white board to capture handwritten trajectories. With a lightweight framework for handwritten trajectory processing, clear lecture notes can be generated automatically. We evaluate the quality of lecture notes by three standard character recognition techniques. The results indicate that InfraNotes is a promising solution to create clear and complete lectures to promote the education."
"The advancement in digital technologies have made it possible to produce perfect copies of digital content. In this environment, malicious users reproduce the digital content and share it without compensation to the content owner. Content owners are concerned about the potential loss of revenue and reputation from piracy, especially when the content is available over the Internet. Digital watermarking has emerged as a deterrent measure towards such malicious activities. Several methods have been proposed for copyright protection and fingerprinting of digital images. However, these methods are not applicable to text documents as these documents lack rich texture information which is abundantly available in digital images. In this paper, a framework (mPDF) is proposed which facilitates the usage of digital image watermarking algorithms on text documents. The proposed method divides a text document into texture and non-texture blocks using an energy-based approach. After classification, a watermark is embedded inside the texture blocks in a content adaptive manner. The proposed method is integrated with five known image watermarking methods and its performance is studied in terms of quality and robustness. Experiments are conducted on documents in 11 different languages. Experimental results clearly show that the proposed method facilitates the usage of image watermarking algorithms on text documents and is robust against attacks such as print & scan, print screen, and skew. Also, the proposed method overcomes the drawbacks of existing text watermarking methods such as manual inspection and language dependency."
"Because of the spread of the Internet, social platforms become big data pools. From there we can learn about the trends, culture and hot topics. This project focuses on analyzing the data from Instagram. It shows the relationship of Instagram filter data with location and number of likes to give users filter suggestion on achieving more likes based on their location. It also analyzes the popular hashtags in different locations to show visual culture differences between different cities."
"Generative Adversarial Networks (GANs) have recently demonstrated the capability to synthesize compelling real-world images, such as room interiors, album covers, manga, faces, birds, and flowers. While existing models can synthesize images based on global constraints such as a class label or caption, they do not provide control over pose or object location. We propose a new model, the Generative Adversarial What-Where Network (GAWWN), that synthesizes images given instructions describing what content to draw in which location. We show high-quality 128 x 128 image synthesis on the Caltech-UCSD Birds dataset, conditioned on both informal text descriptions and also object location. Our system exposes control over both the bounding box around the bird and its constituent parts. By modeling the conditional distributions over part locations, our system also enables conditioning on arbitrary subsets of parts (e.g. only the beak and tail), yielding an efficient interface for picking part locations. We also show preliminary results on the more challenging domain of text- and location-controllable synthesis of images of human actions on the MPII Human Pose dataset."
"Approximate Nearest neighbor search (ANNS) is fundamental and essential operation in applications from many domains, such as databases, machine learning, multimedia, and computer vision. Although many algorithms have been continuously proposed in the literature in the above domains each year, there is no comprehensive evaluation and analysis of their performances.   In this paper, we conduct a comprehensive experimental evaluation of many state-of-the-art methods for approximate nearest neighbor search. Our study (1) is cross-disciplinary (i.e., including 16 algorithms in different domains, and from practitioners) and (2) has evaluated a diverse range of settings, including 20 datasets, several evaluation metrics, and different query workloads. The experimental results are carefully reported and analyzed to understand the performance results. Furthermore, we propose a new method that achieves both high query efficiency and high recall empirically on majority of the datasets under a wide range of settings."
"The supervisory control theory of fuzzy discrete event systems (FDESs) for fuzzy language equivalence has been developed. However, in a way, language equivalence has limited expressiveness. So if the given specification can not be expressed by language equivalence, then the control for language equivalence does not work. In this paper, we further establish the supervisory control theory of FDESs for fuzzy simulation equivalence whose expressiveness is stronger than that of fuzzy language equivalence. First, we formalize the notions of fuzzy simulation and fuzzy simulation equivalence between two FDESs. Then we present a method for deciding whether there is a fuzzy simulation or not. In addition, we also show several basic properties of fuzzy simulation relations. Afterwards, we put forward the notion of fuzzy simulation-based controllability, and particularly show that it serves as a necessary and sufficient condition for the existence of the fuzzy supervisors of FDESs. Moreover, we study the ""range"" control problem of FDESs. Some examples are given to illustrate the main results obtained."
"Small variance asymptotics is emerging as a useful technique for inference in large scale Bayesian non-parametric mixture models. This paper analyses the online learning of robot manipulation tasks with Bayesian non-parametric mixture models under small variance asymptotics. The analysis yields a scalable online sequence clustering (SOSC) algorithm that is non-parametric in the number of clusters and the subspace dimension of each cluster. SOSC groups the new datapoint in its low dimensional subspace by online inference in a non-parametric mixture of probabilistic principal component analyzers (MPPCA) based on Dirichlet process, and captures the state transition and state duration information online in a hidden semi-Markov model (HSMM) based on hierarchical Dirichlet process. A task-parameterized formulation of our approach autonomously adapts the model to changing environmental situations during manipulation. We apply the algorithm in a teleoperation setting to recognize the intention of the operator and remotely adjust the movement of the robot using the learned model. The generative model is used to synthesize both time-independent and time-dependent behaviours by relying on the principles of shared and autonomous control. Experiments with the Baxter robot yield parsimonious clusters that adapt online with new demonstrations and assist the operator in performing remote manipulation tasks."
"It is well known that type-1 fuzzy sets (T1 FSs) have limited capabilities to handle some data uncertainties directly, and type-2 fuzzy sets (T2 FSs) can cover the shortcoming of T1 FSs to a certain extent. Fuzzy discrete event systems (FDESs) were proposed based on T1 FSs theory. Hence, FDES may not be a satisfactory model to characterize some high-uncertainty systems. In this paper, we propose a new model, called as bi-fuzzy discrete event systems (BFDESs), by combining classical DESs theory and T2 FSs theory. Then, we consider the supervisory control problem of BFDESs. The bi-fuzzy controllability theorem and nonblocking bi-fuzzy controllability theorem are demonstrated. Also, an algorithm for checking the bi-fuzzy controllability condition is presented. In addition, two controllable approximations to an uncontrollable language are investigated in detail. An illustrative example is provided to show the applicability and the advantages of BFDESs model."
"This paper presents NetWorks (NW), an interactive music generation system that uses a hierarchically clustered scale free network to generate music that ranges from orderly to chaotic. NW was inspired by the Honing Theory of creativity, according to which human-like creativity hinges on (1) the ability to self-organize and maintain dynamics at the 'edge of chaos' using something akin to 'psychological entropy', and (2) the capacity to shift between analytic and associative processing modes. At the 'edge of chaos', NW generates patterns that exhibit emergent complexity through coherent development at low, mid, and high levels of musical organization, and often suggests goal seeking behaviour. The architecture consists of four 16-node modules: one each for pitch, velocity, duration, and entry delay. The Core allows users to define how nodes are connected, and rules that determine when and how nodes respond to their inputs. The Mapping Layer allows users to map node output values to MIDI data that is routed to software instruments in a digital audio workstation. By shifting between bottom-up and top-down NW shifts between analytic and associative processing modes."
"We examine two recent artificial intelligence (AI) based deep learning algorithms for visual blending in convolutional neural networks (Mordvintsev et al. 2015, Gatys et al. 2015). To investigate the potential value of these algorithms as tools for computational creativity research, we explain and schematize the essential aspects of the algorithms' operation and give visual examples of their output. We discuss the relationship of the two algorithms to human cognitive science theories of creativity such as conceptual blending theory and honing theory, and characterize the algorithms with respect to generation of novelty and aesthetic quality."
"Frequency estimation of multiple sinusoids is significant in both theory and application. In some application scenarios, only sub-Nyquist samples are available to estimate the frequencies. A conventional approach is to sample the signals at several lower rates. In this paper, we propose a novel method based on subspace techniques using three-channel undersampled data. We analyze the impact of undersampling and demonstrate that three sub-Nyquist channels are general enough to estimate the frequencies provided the undersampling ratios are coprime. The ambiguous frequencies obtained from one channel are identified and the correct frequencies are screened out by using three-channel samples jointly. Numerical experiments verify the correctness of our analysis and conclusion. Simulations show that the proposed method is valid and with high accuracy."
"Autonomous crop monitoring at high spatial and temporal resolution is a critical problem in precision agriculture. While Structure from Motion and Multi-View Stereo algorithms can finely reconstruct the 3D structure of a field with low-cost image sensors, these algorithms fail to capture the dynamic nature of continuously growing crops. In this paper we propose a 4D reconstruction approach to crop monitoring, which employs a spatio-temporal model of dynamic scenes that is useful for precision agriculture applications. Additionally, we provide a robust data association algorithm to address the problem of large appearance changes due to scenes being viewed from different angles at different points in time, which is critical to achieving 4D reconstruction. Finally, we collected a high quality dataset with ground truth statistics to evaluate the performance of our method. We demonstrate that our 4D reconstruction approach provides models that are qualitatively correct with respect to visual appearance and quantitatively accurate when measured against the ground truth geometric properties of the monitored crops."
"Due to its simplicity and versatility, k-means remains popular since it was proposed three decades ago. Since then, continuous efforts have been taken to enhance its performance. Unfortunately, a good trade-off between quality and efficiency is hardly reached. In this paper, a novel k-means variant is presented. Different from most of k-means variants, the clustering procedure is explicitly driven by an objective function, which is feasible for the whole l2-space. The classic egg-chicken loop in k-means has been simplified to a pure stochastic optimization procedure. K-means therefore becomes simpler, faster and better. The effectiveness of this new variant has been studied extensively in different contexts, such as document clustering, nearest neighbor search and image clustering. Superior performance is observed across different scenarios."
"The Daala project is a royalty-free video codec that attempts to compete with the best patent-encumbered codecs. Part of our strategy is to replace core tools of traditional video codecs with alternative approaches, many of them designed to take perceptual aspects into account, rather than optimizing for simple metrics like PSNR. This paper documents some of our experiences with these tools, which ones worked and which did not. We evaluate which tools are easy to integrate into a more traditional codec design, and show results in the context of the codec being developed by the Alliance for Open Media."
"This work is part of a large research project entitled ""Or\'eodule"" aimed at developing tools for automatic speech recognition, translation, and synthesis for Arabic language. Our attention has mainly been focused on an attempt to improve the probabilistic model on which our semantic decoder is based. To achieve this goal, we have decided to test the influence of the pertinent context use, and of the contextual data integration of different types, on the effectiveness of the semantic decoder. The findings are quite satisfactory."
"Latent Dirichlet Allocation (LDA) is a popular tool for analyzing discrete count data such as text and images. Applications require LDA to handle both large datasets and a large number of topics. Though distributed CPU systems have been used, GPU-based systems have emerged as a promising alternative because of the high computational power and memory bandwidth of GPUs. However, existing GPU-based LDA systems cannot support a large number of topics because they use algorithms on dense data structures whose time and space complexity is linear to the number of topics. In this paper, we propose SaberLDA, a GPU-based LDA system that implements a sparsity-aware algorithm to achieve sublinear time complexity and scales well to learn a large number of topics. To address the challenges introduced by sparsity, we propose a novel data layout, a new warp-based sampling kernel, and an efficient sparse count matrix updating algorithm that improves locality, makes efficient utilization of GPU warps, and reduces memory consumption. Experiments show that SaberLDA can learn from billions-token-scale data with up to 10,000 topics, which is almost two orders of magnitude larger than that of the previous GPU-based systems. With a single GPU card, SaberLDA is able to learn 10,000 topics from a dataset of billions of tokens in a few hours, which is only achievable with clusters with tens of machines before."
"In uplink block-fading multiple access channel (BF-MAC), the advantage of multiuser diversity (MUD) can be taken to achieve higher system throughput. In this letter, with rate constraints for all users and only channel state information available at the receiver assumed, we demonstrate that non-orthogonal multiple access (NOMA) outperforms the counterpart of orthogonal multiple access (OMA) via exploiting the MUD from the superposition of multiple users. The MUD gain achieved by NOMA compared with OMA is quantitatively analyzed for finite users, and the closed-form upper bound of MUD gain from the superposition of infinite users is also derived. Numerical results show that the potential MUD gain from superposition of infinite users can be well approached with limited superposed users, which indicates that multiple access schemes with limited superposed users can provide a good tradeoff between system performance and decoding complexity."
"We have unified quantum and classical computing in open quantum systems called qACP which is a quantum generalization of process algebra ACP. But, an axiomatization for quantum and classical processes with an assumption of closed quantum systems is still missing. For closed quantum systems, unitary operator, quantum measurement and quantum entanglement are three basic components for quantum computing. This leads to probability unavoidable. Along the solution of qACP to unify quantum and classical computing in open quantum systems, we unify quantum and classical computing with an assumption of closed systems under the framework of ACP-like probabilistic process algebra. This unification make it can be used widely in verification for quantum and classical computing mixed systems, such as most quantum communication protocols."
"Recently neural networks and multiple instance learning are both attractive topics in Artificial Intelligence related research fields. Deep neural networks have achieved great success in supervised learning problems, and multiple instance learning as a typical weakly-supervised learning method is effective for many applications in computer vision, biometrics, nature language processing, etc. In this paper, we revisit the problem of solving multiple instance learning problems using neural networks. Neural networks are appealing for solving multiple instance learning problem. The multiple instance neural networks perform multiple instance learning in an end-to-end way, which take a bag with various number of instances as input and directly output bag label. All of the parameters in a multiple instance network are able to be optimized via back-propagation. We propose a new multiple instance neural network to learn bag representations, which is different from the existing multiple instance neural networks that focus on estimating instance label. In addition, recent tricks developed in deep learning have been studied in multiple instance networks, we find deep supervision is effective for boosting bag classification accuracy. In the experiments, the proposed multiple instance networks achieve state-of-the-art or competitive performance on several MIL benchmarks. Moreover, it is extremely fast for both testing and training, e.g., it takes only 0.0003 second to predict a bag and a few seconds to train on a MIL datasets on a moderate CPU."
"Modern multi-stage retrieval systems are comprised of a candidate generation stage followed by one or more reranking stages. In such an architecture, the quality of the final ranked list may not be sensitive to the quality of initial candidate pool, especially in terms of early precision. This provides several opportunities to increase retrieval efficiency without significantly sacrificing effectiveness. In this paper, we explore a new approach to dynamically predicting two different parameters in the candidate generation stage which can directly affect the overall efficiency and effectiveness of the entire system. Previous work exploring this tradeoff has focused on global parameter settings that apply to all queries, even though optimal settings vary across queries. In contrast, we propose a technique which makes a parameter prediction that maximizes efficiency within a effectiveness envelope on a per query basis, using only static pre-retrieval features. The query-specific tradeoff point between effectiveness and efficiency is decided using a classifier cascade that weighs possible efficiency gains against effectiveness losses over a range of possible parameter cutoffs to make the prediction. The interesting twist in our new approach is to train classifiers without requiring explicit relevance judgments. We show that our framework is generalizable by applying it to two different retrieval parameters - selecting k in common top-k query retrieval algorithms, and setting a quality threshold, $\rho$, for score-at-a-time approximate query evaluation algorithms. Experimental results show that substantial efficiency gains are achievable depending on the dynamic parameter choice. In addition, our framework provides a versatile tool that can be used to estimate the effectiveness-efficiency tradeoffs that are possible before selecting and tuning algorithms to make machine learned predictions."
"Content-Based Image Retrieval (CBIR) systems have been widely used for a wide range of applications such as Art collections, Crime prevention and Intellectual property. In this paper, a novel CBIR system, which utilizes visual contents (color, texture and shape) of an image to retrieve images, is proposed. The proposed system builds three feature vectors and stores them into MySQL database. The first feature vector uses descriptive statistics to describe the distribution of data in each channel of RGB channels of the image. The second feature vector describes the texture using eigenvalues of the 39 sub-bands that are generated after applying four levels 2D DWT in each channel (red, green and blue channels) of the image. These wavelets sub-bands perfectly describes the horizontal, vertical and diagonal edges that exist in the multi-resolution analysis of the image. The third feature vector describes the basic shapes that exist in the skeletonization version of the black and white representation of the image. Experimental results on a private MYSQL database that consists of 10000 images, using color, texture, shape and stored relevance feedbacks, showed 96.4% average correct retrieval rate in an efficient recovery time."
"Millimeter wave (mmWave) communication is a promising technology for the fifth-generation (5G) wireless system. However, the large number of antennas used and the wide signal bandwidth in mmWave systems render the conventional multi-antenna techniques increasingly costly in terms of signal processing complexity, hardware implementation, and power consumption. In this article, we investigate cost-effective mmWave communications by first providing an overview of the main existing techniques that offer different trade-offs between performance and cost, and then focusing our discussion on a promising new technique based on the advanced lens antenna array. It is revealed that by exploiting the angle-dependent energy focusing property of lens arrays, together with the angular sparsity of the mmWave channels, mmWave lens-antenna system is able to achieve the capacity-optimal performance with very few radio-frequency (RF) chains and using the low-complexity single-carrier transmission, even for wide-band frequency-selective channels. Numerical results show that the lens-based system significantly outperforms the state-of-the-art designs for mmWave systems in both spectrum efficiency and energy efficiency."
"One of the key limitation of massive MIMO systems is pilot contamination, which is defined as the interference during uplink channel estimation due to re-use of the same pilots in surrounding cells. In this paper, we propose a location-based approach to the pilot contamination problem for uplink MIMO systems. Our approach makes use of the approximate locations of mobile devices to provide good estimates of the channel statistics between the mobile devices and their corresponding base stations (BSs). We aim at minimizing the pilot contamination even when the number of BS antennas is not very large, and when multiple users from different cells, or even the same cell, are assigned the same pilot sequence. First, we characterize a desired angular region of the target user at the target BS in which interference is very low or zero, based on the number of BS antennas and the location of the target user. Second, based on this observation, we propose various pilot coordination methods for multi-user multi-cell scenarios to eliminate pilot contamination."
"The latest High Efficiency Video Coding (HEVC) standard significantly improves coding efficiency over its previous video coding standards. The expense of such improvement is enormous computational complexity, from both encoding and decoding perspectives. Since the capability and capacity of power are diverse across portable devices, it is necessary to reduce decoding complexity to a target with tolerable quality loss, so called complexity control. This paper proposes a Saliency-Guided Complexity Control (SGCC) approach for HEVC decoding, which reduces the decoding complexity to the target with minimal perceptual quality loss. First, an HEVC domain method is developed to detect video saliency from HEVC bitstreams, as the preliminary for assessing perceptual quality. Based on detected saliency, we establish the SGCC formulation to minimize perceptual quality loss at the constraint on reduced decoding complexity, which is achieved via disabling Deblocking Filter (DF) and simplifying Motion Compensation (MC) of some non-salient Largest Coding Units (LCUs). One important component in this formulation is the modelled relationship between decoding complexity reduction and DF disabling/MC simplification, which determines the control accuracy of our approach. Another component is the modelled relationship between quality loss and DF disabling/MC simplification, responsible for optimizing perceptual quality. By solving the SGCC formulation, we can obtain the DF and MC states of each LCU given a target complexity, and then the decoding complexity can be reduced to the target. Finally, the experimental results show the effectiveness of our SGCC approach, from the aspects of control performance, complexity-distortion performance and subjective quality."
"Early software effort estimation is a hallmark of successful software project management. Building a reliable effort estimation model usually requires historical data. Unfortunately, since the information available at early stages of software development is scarce, it is recommended to use software size metrics as key cost factor of effort estimation. Use Case Points (UCP) is a prominent size measure designed mainly for object-oriented projects. Nevertheless, there are no established models that can translate UCP into its corresponding effort, therefore, most models use productivity as a second cost driver. The productivity in those models is usually guessed by experts and does not depend on historical data, which makes it subject to uncertainty. Thus, these models were not well examined using a large number of historical data. In this paper, we designed a hybrid model that consists of classification and prediction stages using a support vector machine and radial basis neural networks. The proposed model was constructed over a large number of observations collected from industrial and student projects. The proposed model was compared against previous UCP prediction models. The validation and empirical results demonstrated that the proposed model significantly surpasses these models on all datasets. The main conclusion is that the environmental factors of UCP can be used to classify and estimate productivity"
"Let $m < n$ be non-negative integers. An oracle chooses a permutation $\pi$ of $\{0, 1\}^{n}$ uniformly at random. When queried with an $n$-bit string $w$, it truncates the last $m$ bits of $\pi (w)$, and returns the remaining first $n-m$ bits. Such truncated random permutations were suggested by Hall et al., in 1998, as a construction of a Pseudo Random Function. They conjectured that the distinguishing advantage of this PRF, given a budget of $q$ queries, ${\bf Adv}_{n, m} (q)$, is small if $q = o (2^{(m+n)/2})$. They established a general upper bound on ${\bf Adv}_{n, m} (q)$, which confirms the conjecture only for $m < n/7$. The conjecture was essentialy confirmed by Bellare and Impagliazzo in 1999. Nevertheless, the problem of estimating ${\bf Adv}_{n, m} (q)$ remained open.   Combining the trivial bound $1$, the birthday bound, and a result that Stam had published much earlier in 1978, in a different context, leads to the following upper bound: $${\bf Adv}_{n,m}(q)=O\left(\min\left\{\frac{q^2}{2^n},\,\frac{q}{2^{\frac{n+m}{2}}},\,1\right\}\right)$$ This paper settles the open problem by showing that this bound is tight."
"In this paper, we coin the term Policy Enforcement as a Service (PEPS), which enables the provision of innovative inter-layer and inter-domain Access Control. We leverage the architecture of Software-Defined-Network (SDN) to introduce a common network-level enforcement point, which is made available to a range of access control systems. With our PEPS model, it is possible to have a `defense in depth' protection model and drop unsuccessful access requests before engaging the data provider (e.g. a database system). Moreover, the current implementation of access control within the `trusted' perimeter of an organization is no longer a restriction so that the potential for novel, distributed and cooperative security services can be realized. We conduct an analysis of the security requirements and technical challenges for implementing Policy Enforcement as a Service. To illustrate the benefits of our proposal in practice, we include a report on our prototype PEPS-enabled location-based access control."
"We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are unevenly distributed over an extremely large number of nodes. The goal is to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of the utmost importance and minimizing the number of rounds of communication is the principal goal.   A motivating example arises when we keep the training data locally on users' mobile devices instead of logging it to a data center for training. In federated optimziation, the devices are used as compute nodes performing computation on their local data in order to update a global model. We suppose that we have extremely large number of devices in the network --- as many as the number of users of a given service, each of which has only a tiny fraction of the total data available. In particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, it is reasonable to assume that no device has a representative sample of the overall distribution.   We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results for sparse convex problems. This work also sets a path for future research needed in the context of \federated optimization."
"This paper studies the security of a recently-proposed chaos-based image encryption scheme, and points out the following problems: 1) there exist a number of invalid keys and weak keys, and some keys are partially equivalent for encryption/decryption; 2) given one chosen plain-image, a subkey $K_{10}$ can be guessed with a smaller computational complexity than that of the simple brute-force attack; 3) given at most 128 chosen plain-images, a chosen-plaintext attack can possibly break the following part of the secret key: $\{K_i\bmod 128\}_{i=4}^{10}$, which works very well when $K_{10}$ is not too large; 4) when $K_{10}$ is relatively small, a known-plaintext attack can be carried out with only one known plain-image to recover some visual information of any other plain-images encrypted by the same key."
"A linking theory explains how verbs' semantic arguments are mapped to their syntactic arguments---the inverse of the Semantic Role Labeling task from the shallow semantic parsing literature. In this paper, we develop the Computational Linking Theory framework as a method for implementing and testing linking theories proposed in the theoretical literature. We deploy this framework to assess two cross-cutting types of linking theory: local v. global models and categorical v. featural models. To further investigate the behavior of these models, we develop a measurement model in the spirit of previous work in semantic role induction: the Semantic Proto-Role Linking Model. We use this model, which implements a generalization of Dowty's seminal Proto-Role Theory, to induce semantic proto-roles, which we compare to those Dowty proposes."
"Due to the increasing number of authors per publication, a change in counting method for a publication or citation indicator will often change the result of the indicator. Therefore it is important to know why a specific counting method has been applied. I have analyzed arguments for counting methods in a sample of 13 recent bibliometric studies and compared the result with discussions of arguments for counting methods in three older studies. Based on the arguments in the studies I formed argument categories which were grouped based on the underlying logics of the arguments. It resulted in four groups which can be used to describe and discuss how bibliometric studies with publication and citation indicators argue for counting methods. The first group focuses on arguments related to what an indicator measures, the next group on avoiding double counting of publications and/or citations, the third on pragmatic reasons for the choice of counting method, and the fourth group on an indicators influence on the research community or how it is influenced by the research community."
"This paper indicates two errors in the formulation of the main optimization model in the article ""Dynamic power management in energy-aware computer networks and data intensive computing systems"" by Niewiadomska-Szynkiewicz et al. [FGCS, vol.37 (2014), pp.284-296] and shows how to fix them."
"Remote estimation with an energy harvesting sensor with a limited data and energy buffer is considered. The sensor node observes an unknown Gaussian field and communicates its observations to a remote fusion center using the energy it harvested. The fusion center employs minimum mean-square error (MMSE) estimation to reconstruct the unknown field. The distortion minimization problem under the online scheme, where the sensor has access to only the statistical information for the future energy packets is considered. We provide performance bounds on the achievable distortion under a slotted block transmission scheme, where at each transmission time slot, the data and the energy buffer are completely emptied. Our bounds provide insights to the trade-offs between the buffer sizes, the statistical properties of the energy harvesting process and the achievable distortion. In particular, these trade-offs illustrate the insensitivity of the performance to the buffer sizes for signals with low degree of freedom and suggest performance improvements with increasing buffer size for signals with relatively higher degree of freedom. Depending only on the mean, variance and finite support of the energy arrival process, these results provide practical insights for the battery and buffer sizes for deployment in future energy harvesting wireless sensing systems."
"Adverse reactions caused by drugs following their release into the market are among the leading causes of death in many countries. The rapid growth of electronically available health related information, and the ability to process large volumes of them automatically, using natural language processing (NLP) and machine learning algorithms, have opened new opportunities for pharmacovigilance. Survey found that more than 70% of US Internet users consult the Internet when they require medical information. In recent years, research in this area has addressed for Adverse Drug Reaction (ADR) pharmacovigilance using social media, mainly Twitter and medical forums and websites. This paper will show the information which can be collected from a variety of Internet data sources and search engines, mainly Google Trends and Google Correlate. While considering the case study of two popular Major depressive Disorder (MDD) drugs, Duloxetine and Venlafaxine, we will provide a comparative analysis for their reactions using publicly-available alternative data sources."
"This paper presents the first real-time discretization-error-driven adaptive finite element approach for corotational elasticity problems involving strain localization. We propose a hexahedron-based finite element method, combined with a posteriori error estimation driven local h-refinement, for simulating soft tissue deformation. This enables to control the local error and global error level in the mechanical fields (e.g. displacement or gradient) during the simulation. The local error level is used to refine the mesh only where it is needed, while maintaining a coarser mesh elsewhere. We investigate the convergence of the algorithm on academic examples, and demonstrate its practical usability on a percutaneous procedure involving needle insertion in a liver. For the latter case, we compare the force displacement curves obtained from the proposed adaptive algorithm with that obtained from a uniform refinement approach."
Proceedings of the 1st International Workshop on Robot Learning and Planning (RLP 2016)
"The atom-bond connectivity (ABC) index is a degree-based molecular structure descriptor that can be used for modelling thermodynamic properties of organic chemical compounds. Motivated by its applicable potential, a series of investigations have been carried out in the past several years. In this note we first consider graphs with given edge-connectivity that attain the maximum ABC index. In particular, we give an affirmative answer to the conjecture about the structure of graphs with edge-connectivity equal to one that maximize the ABC index, which was recently raised by Zhang, Yang, Wang and Zhang~\cite{zywz mabciggp-2016}. In addition, we provide supporting evidence for another conjecture posed by the same authors which concerns graphs that maximize the ABC index among all graphs with chromatic number equal to some fixed $\chi \geq 3$. Specifically, we confirm this conjecture in the case where the order of the graph is divisible by $\chi$."
"Network models of healthcare systems can be used to examine how providers collaborate, communicate, refer patients to each other. Most healthcare service network models have been constructed from patient claims data, using billing claims to link patients with providers. The data sets can be quite large, making standard methods for network construction computationally challenging and thus requiring the use of alternate construction algorithms. While these alternate methods have seen increasing use in generating healthcare networks, there is little to no literature comparing the differences in the structural properties of the generated networks. To address this issue, we compared the properties of healthcare networks constructed using different algorithms and the 2013 Medicare Part B outpatient claims data. Three different algorithms were compared: binning, sliding frame, and trace-route. Unipartite networks linking either providers or healthcare organizations by shared patients were built using each method. We found that each algorithm produced networks with substantially different topological properties. Provider networks adhered to a power law, and organization networks to a power law with exponential cutoff. Censoring networks to exclude edges with less than 11 shared patients, a common de-identification practice for healthcare network data, markedly reduced edge numbers and greatly altered measures of vertex prominence such as the betweenness centrality. We identified patterns in the distance patients travel between network providers, and most strikingly between providers in the Northeast United States and Florida. We conclude that the choice of network construction algorithm is critical for healthcare network analysis, and discuss the implications for selecting the algorithm best suited to the type of analysis to be performed."
"This paper addresses the problem of adding redundancy to a collection of physical objects so that the overall system is more robust to failures. Physical redundancy can (generally) be achieved by employing copy/substitute procedures. This is fundamentally different from information redundancy, where a single parity check simultaneously protects a large number of data bits against a single erasure. We propose a bipartite graph model of designing defect-tolerant systems where defective objects are repaired by reconnecting them to strategically placed redundant objects. The fundamental limits of this model are characterized under various asymptotic settings and both asymptotic and finite-size optimal systems are constructed.   Mathematically, we say that a $k$ by $m$ bipartite graph corrects $t$ defects over an alphabet of size $q$ if for every $q$-coloring of $k$ left vertices there exists a $q$-coloring of $m$ right vertices such that every left vertex is connected to at least $t$ same-colored right vertices. We study the trade-off between redundancy $m / k$ and the total number of edges in the graph divided by $k$. The question is trivial when $q\ge k$: the optimal solution is a simple $t$-fold replication. However, when $q<k$ non-trivial savings are possible by leveraging the inherent repetition of colors."
"The visual cues from multiple support regions of different sizes and resolutions are complementary in classifying a candidate box in object detection. Effective integration of local and contextual visual cues from these regions has become a fundamental problem in object detection.   In this paper, we propose a gated bi-directional CNN (GBD-Net) to pass messages among features from different support regions during both feature learning and feature extraction. Such message passing can be implemented through convolution between neighboring support regions in two directions and can be conducted in various layers. Therefore, local and contextual visual patterns can validate the existence of each other by learning their nonlinear relationships and their close interactions are modeled in a more complex way. It is also shown that message passing is not always helpful but dependent on individual samples. Gated functions are therefore needed to control message transmission, whose on-or-offs are controlled by extra visual evidence from the input sample. The effectiveness of GBD-Net is shown through experiments on three object detection datasets, ImageNet, Pascal VOC2007 and Microsoft COCO. This paper also shows the details of our approach in wining the ImageNet object detection challenge of 2016, with source code provided on \url{https://github.com/craftGBD/craftGBD}."
"We describe recurrent neural networks (RNNs), which have attracted great attention on sequential tasks, such as handwriting recognition, speech recognition and image to text. However, compared to general feedforward neural networks, RNNs have feedback loops, which makes it a little hard to understand the backpropagation step. Thus, we focus on basics, especially the error backpropagation to compute gradients with respect to model parameters. Further, we go into detail on how error backpropagation algorithm is applied on long short-term memory (LSTM) by unfolding the memory unit."
"We propose in this paper a simulation implementation of self-organizing Networks optimization related to mobility load balancing (MLB) for lte systems using ns-3. the implementation is achieved toward two MLB algorithms dynamically adjusting handover (HO) parameters based on the reference signal received power measurements. such adjustments are done with respect to loads of both an overloaded cell and its cell neighbours having enough available resources enabling to achieve load balancing. numerical investigations through selected key performance indicators of the proposed MLB algorithms when compared with another HO algorithm (already implemented in ns3) based on A3 event highlight the significant MLB gains provided in terms global network throughput, packet loss rate and the number of successful HO without incurring significant overhead."
"Arising from many applications at the intersection of decision making and machine learning, Marginal Maximum A Posteriori (Marginal MAP) Problems unify the two main classes of inference, namely maximization (optimization) and marginal inference (counting), and are believed to have higher complexity than both of them. We propose XOR_MMAP, a novel approach to solve the Marginal MAP Problem, which represents the intractable counting subproblem with queries to NP oracles, subject to additional parity constraints. XOR_MMAP provides a constant factor approximation to the Marginal MAP Problem, by encoding it as a single optimization in polynomial size of the original problem. We evaluate our approach in several machine learning and decision making applications, and show that our approach outperforms several state-of-the-art Marginal MAP solvers."
"Energy and power consumption are major limitations to continued scaling of computing systems. Inexactness, where the quality of the solution can be traded for energy savings, has been proposed as an approach to overcoming those limitations. In the past, however, inexactness necessitated the need for highly customized or specialized hardware. The current evolution of commercial off-the-shelf(COTS) processors facilitates the use of lower-precision arithmetic in ways that reduce energy consumption. We study these new opportunities in this paper, using the example of an inexact Newton algorithm for solving nonlinear equations. Moreover, we have begun developing a set of techniques we call reinvestment that, paradoxically, use reduced precision to improve the quality of the computed result: They do so by reinvesting the energy saved by reduced precision."
"Over the past two decades the field of computational science and engineering (CSE) has penetrated both basic and applied research in academia, industry, and laboratories to advance discovery, optimize systems, support decision-makers, and educate the scientific and engineering workforce. Informed by centuries of theory and experiment, CSE performs computational experiments to answer questions that neither theory nor experiment alone is equipped to answer. CSE provides scientists and engineers of all persuasions with algorithmic inventions and software systems that transcend disciplines and scales. Carried on a wave of digital technology, CSE brings the power of parallelism to bear on troves of data. Mathematics-based advanced computing has become a prevalent means of discovery and innovation in essentially all areas of science, engineering, technology, and society; and the CSE community is at the core of this transformation. However, a combination of disruptive developments---including the architectural complexity of extreme-scale computing, the data revolution that engulfs the planet, and the specialization required to follow the applications to new frontiers---is redefining the scope and reach of the CSE endeavor. This report describes the rapid expansion of CSE and the challenges to sustaining its bold advances. The report also presents strategies and directions for CSE research and education for the next decade."
"Human-robot handovers are characterized by high uncertainty and poor structure of the problem that make them difficult tasks. While machine learning methods have shown promising results, their application to problems with large state dimensionality, such as in the case of humanoid robots, is still limited. Additionally, by using these methods and during the interaction with the human operator, no guarantees can be obtained on the correct interpretation of spatial constraints (e.g., from social rules). In this paper, we present Policy Improvement with Spatio-Temporal Affordance Maps -- $\pi$-STAM, a novel iterative algorithm to learn spatial affordances and generate robot behaviors. Our goal consists in generating a policy that adapts to the unknown action semantics by using affordances. In this way, while learning to perform a human-robot handover task, we can (1) efficiently generate good policies with few training episodes, and (2) easily encode action semantics and, if available, enforce prior knowledge in it. We experimentally validate our approach both in simulation and on a real NAO robot whose task consists in taking an object from the hands of a human. The obtained results show that our algorithm obtains a good policy while reducing the computational load and time duration of the learning process."
"Pouring a specific amount of liquid is a challenging task. In this paper we develop methods for robots to use visual feedback to perform closed-loop control for pouring liquids. We propose both a model-based and a model-free method utilizing deep learning for estimating the volume of liquid in a container. Our results show that the model-free method is better able to estimate the volume. We combine this with a simple PID controller to pour specific amounts of liquid, and show that the robot is able to achieve an average 38ml deviation from the target amount. To our knowledge, this is the first use of raw visual feedback to pour liquids in robotics."
"Online handwritten Chinese text recognition (OHCTR) is a challenging problem as it involves a large-scale character set, ambiguous segmentation, and variable-length input sequences. In this paper, we exploit the outstanding capability of path signature to translate online pen-tip trajectories into informative signature feature maps using a sliding window-based method, successfully capturing the analytic and geometric properties of pen strokes with strong local invariance and robustness. A multi-spatial-context fully convolutional recurrent network (MCFCRN) is proposed to exploit the multiple spatial contexts from the signature feature maps and generate a prediction sequence while completely avoiding the difficult segmentation problem. Furthermore, an implicit language model is developed to make predictions based on semantic context within a predicting feature sequence, providing a new perspective for incorporating lexicon constraints and prior knowledge about a certain language in the recognition procedure. Experiments on two standard benchmarks, Dataset-CASIA and Dataset-ICDAR, yielded outstanding results, with correct rates of 97.10% and 97.15%, respectively, which are significantly better than the best result reported thus far in the literature."
"We consider the problem of reconstructing two signals from the autocorrelation and cross-correlation measurements. This inverse problem is a fundamental one in signal processing, and arises in many applications, including phase retrieval and blind channel estimation. In a typical phase retrieval setup, only the autocorrelation measurements are obtainable. We show that, when the measurements are obtained using three simple ""masks"", phase retrieval reduces to the aforementioned reconstruction problem.   The classic solution to this problem is based on finding common factors between the $z$-transforms of the autocorrelation and cross-correlation vectors. This solution has enjoyed limited practical success, mainly due to the fact that it is not sufficiently stable in the noisy setting. In this work, inspired by the success of convex programming in provably and stably solving various quadratic constrained problems, we develop a semidefinite programming-based algorithm and provide theoretical guarantees. In particular, we show that almost all signals can be uniquely recovered by this algorithm (up to a global phase). Comparative numerical studies demonstrate that the proposed method significantly outperforms the classic method in the noisy setting."
"We propose a new probabilistic framework that allows mobile robots to autonomously learn deep generative models of their environments that span multiple levels of abstraction. Unlike traditional approaches that attempt to integrate separately engineered components for low-level features, geometry, and semantic representations, our approach leverages recent advances in sum-product networks (SPNs) and deep learning to learn a unified deep model of a robot's spatial environment, from low-level representations to semantic interpretations. Our results, based on laser range finder data from a mobile robot, demonstrate that the proposed approach can learn the geometry of places and is a versatile platform for solving tasks ranging from semantic classification of places, uncertainty estimation and novelty detection to generation of place appearances based on semantic information and prediction of missing data in partial observations."
"We present research towards bridging the language gap between migrant workers in Qatar and medical staff. In particular, we present the first steps towards the development of a real-world Hindi-English machine translation system for doctor-patient communication. As this is a low-resource language pair, especially for speech and for the medical domain, our initial focus has been on gathering suitable training data from various sources. We applied a variety of methods ranging from fully automatic extraction from the Web to manual annotation of test data. Moreover, we developed a method for automatically augmenting the training data with synthetically generated variants, which yielded a very sizable improvement of more than 3 BLEU points absolute."
"Antichain based semantics for general rough sets were introduced recently by the present author. In her paper two different semantics, one for general rough sets and another for general approximation spaces over quasi-equivalence relations, were developed. These semantics are improved and studied further from a lateral algebraic logic perspective in this research. The main results concern the structure of the algebras and deductive systems in the context."
"This research introduces a new strategy in cluster ensemble selection by using Independency and Diversity metrics. In recent years, Diversity and Quality, which are two metrics in evaluation procedure, have been used for selecting basic clustering results in the cluster ensemble selection. Although quality can improve the final results in cluster ensemble, it cannot control the procedures of generating basic results, which causes a gap in prediction of the generated basic results' accuracy. Instead of quality, this paper introduces Independency as a supplementary method to be used in conjunction with Diversity. Therefore, this paper uses a heuristic metric, which is based on the procedure of converting code to graph in Software Testing, in order to calculate the Independency of two basic clustering algorithms. Moreover, a new modeling language, which we called as ""Clustering Algorithms Independency Language"" (CAIL), is introduced in order to generate graphs which depict Independency of algorithms. Also, Uniformity, which is a new similarity metric, has been introduced for evaluating the diversity of basic results. As a credential, our experimental results on varied different standard data sets show that the proposed framework improves the accuracy of final results dramatically in comparison with other cluster ensemble methods."
"This paper provides a framework to hash images containing instances of unknown object classes. In many object recognition problems, we might have access to huge amount of data. It may so happen that even this huge data doesn't cover the objects belonging to classes that we see in our day to day life. Zero shot learning exploits auxiliary information (also called as signatures) in order to predict the labels corresponding to unknown classes. In this work, we attempt to generate the hash codes for images belonging to unseen classes, information of which is available only through the textual corpus. We formulate this as an unsupervised hashing formulation as the exact labels are not available for the instances of unseen classes. We show that the proposed solution is able to generate hash codes which can predict labels corresponding to unseen classes with appreciably good precision."
"This paper proposes to exploit the so-called {\it reciprocity parameters} (modelling non-reciprocal communication hardware) to use them as decision metric for binary hypothesis testing based authentication framework at a receiver node Bob. Specifically, Bob first learns the reciprocity parameters of the legitimate sender Alice via initial training. Then, during the test phase, Bob first obtains a measurement of reciprocity parameters of channel occupier (Alice, or, the intruder Eve). Then, with ground truth and current measurement both in hand, Bob carries out the hypothesis testing to automatically accept (reject) the packets sent by Alice (Eve). For the proposed scheme, we provide its success rate (the detection probability of Eve), and its performance comparison with other schemes."
"In this work we study the coupled dynamics of social balance and opinion formation. We propose a model where agents form opinions under bounded confidence, but only considering the opinions of their friends. The signs of social ties -friendships and enmities- evolve seeking for social balance, taking into account how similar agents' opinions are. We consider both the case where opinions have one and two dimensions. We find that our dynamics produces the segregation of agents into two cliques, with the opinions of agents in one clique differing from those in the other. Depending on the level of bounded confidence, the dynamics can produce either consensus of opinions within each clique or the coexistence of several opinion clusters in a clique. For the uni-dimensional case, the opinions in one clique are all below the opinions in the other clique, hence defining a ""left clique"" and a ""right clique"". In the two-dimensional case, our numerical results suggest that the two cliques are separated by a hyperplane in the opinion space. We also show that the phenomenon of unidimensional opinions identified by DeMarzo, Vayanos and Zwiebel (Q J Econ 2003) extends partially to our dynamics. Finally, in the context of politics, we comment about the possible relation of our results to the fragmentation of an ideology and the emergence of new political parties."
"Knowing where the vehicles are, what the drivers doing and monitoring every event in real time is the key parameters for a well-managed decision-making process. In this paper, a novel approach for control and monitoring of a fleet management system using three elements including GPS/GLONASS-based automatic vehicle locators (called Rad100), GPRS/SMS GSM cellular network and web-based software (called PayaRadyab) is proposed to show exact position of the desired vehicle on different maps and take detailed reports of the mission, travelled path, fuel consumption rate, speed limits, and other necessary information according to the customers requests. The most significant features of the proposed system are its global covering, high accuracy of positioning, easy operation by the user at any location, and easy energy management. In this study, I have designed and fabricated more than 50 Rad100 trackers and also programmed a web-based PayaRadyab software in which their performance and accuracy have been confirmed by the practical results in different conditions."
"Graph partitioning problems emerge in a wide variety of complex systems, ranging from biology to finance, but can be rigorously analyzed and solved only for a few graph ensembles. Here, an ensemble of random graphs with regular block structure is introduced, for which analytical results can be obtained. In particular, the spectral density of such random regular blockmodels is computed exactly for a modular, bipartite and core-periphery structure. McKay's law for random regular graphs is found analytically to apply also for regular modular and bipartite structures when blocks are homogeneous. In core-periphery structures, where blocks are intrinsically heterogeneous, a new law is found to apply for the spectral density. Exact solution to the inference problem is provided for the models discussed. All analytical results show perfect agreement with numerical experiments. Final discussion summarizes results and outlines the relevance of the results for the solution of graph partitioning problems in other graph ensembles, in particular for the study of detectability thresholds and resolution limits."
"We consider the remote estimation of a time-correlated signal using an energy harvesting (EH) sensor. The sensor observes the unknown signal and communicates its observations to a remote fusion center using an amplify-and-forward strategy. We consider the design of optimal power allocation strategies in order to minimize the mean-square error at the fusion center. Contrary to the traditional approaches, the degree of correlation between the signal values constitutes an important aspect of our formulation. We provide the optimal power allocation strategies for a number of illustrative scenarios. We show that the most majorized power allocation strategy, i.e. the power allocation as balanced as possible, is optimal for the cases of circularly wide-sense stationary (c.w.s.s.) signals with a static correlation coefficient, and sampled low-pass c.w.s.s. signals for a static channel. We show that the optimal strategy can be characterized as a water-filling type solution for sampled low-pass c.w.s.s. signals for a fading channel. Motivated by the high-complexity of the numerical solution of the optimization problem, we propose low-complexity policies for the general scenario. Numerical evaluations illustrate the close performance of these low-complexity policies to that of the optimal policies, and demonstrate the effect of the EH constraints and the degree of freedom of the signal."
"We stratify intuitionistic first-order logic over $(\forall,\to)$ into fragments determined by the alternation of positive and negative occurrences of quantifiers (Mints hierarchy).   We study the decidability and complexity of these fragments. We prove that even the $\Delta_2$ level is undecidable and that $\Sigma_1$ is Expspace-complete. We also prove that the arity-bounded fragment of $\Sigma_1$ is complete for co-Nexptime."
The problem of detecting a change in the drift of a Brownian motion is considered. The change point is assumed to have a modified exponential prior distribution with unknown parameters. A worst-case analysis with respect to these parameters is adopted leading to a min-max problem formulation. Analytical and numerical justifications are provided towards establishing that the Shiryaev-Roberts procedure with a specially designed starting point is exactly optimal for the proposed mathematical setup.
"We present an interpretable neural network approach to predicting and understanding politeness in natural language requests. Our models are based on simple convolutional neural networks directly on raw text, avoiding any manual identification of complex sentiment or syntactic features, while performing better than such feature-based models from previous work. More importantly, we use the challenging task of politeness prediction as a testbed to next present a much-needed understanding of what these successful networks are actually learning. For this, we present several network visualizations based on activation clusters, first derivative saliency, and embedding space transformations, helping us automatically identify several subtle linguistics markers of politeness theories. Further, this analysis reveals multiple novel, high-scoring politeness strategies which, when added back as new features, reduce the accuracy gap between the original featurized system and the neural model, thus providing a clear quantitative interpretation of the success of these neural networks."
"We study the following computational problem: for which values of $k$, the majority of $n$ bits $\text{MAJ}_n$ can be computed with a depth two formula whose each gate computes a majority function of at most $k$ bits? The corresponding computational model is denoted by $\text{MAJ}_k \circ \text{MAJ}_k$. We observe that the minimum value of $k$ for which there exists a $\text{MAJ}_k \circ \text{MAJ}_k$ circuit that has high correlation with the majority of $n$ bits is equal to $\Theta(n^{1/2})$. We then show that for a randomized $\text{MAJ}_k \circ \text{MAJ}_k$ circuit computing the majority of $n$ input bits with high probability for every input, the minimum value of $k$ is equal to $n^{2/3+o(1)}$. We show a worst case lower bound: if a $\text{MAJ}_k \circ \text{MAJ}_k$ circuit computes the majority of $n$ bits correctly on all inputs, then $k\geq n^{13/19+o(1)}$. This lower bound exceeds the optimal value for randomized circuits and thus is unreachable for pure randomized techniques. For depth $3$ circuits we show that a circuit with $k= O(n^{2/3})$ can compute $\text{MAJ}_n$ correctly on all inputs."
"This thesis report studies methods to solve Visual Question-Answering (VQA) tasks with a Deep Learning framework. As a preliminary step, we explore Long Short-Term Memory (LSTM) networks used in Natural Language Processing (NLP) to tackle Question-Answering (text based). We then modify the previous model to accept an image as an input in addition to the question. For this purpose, we explore the VGG-16 and K-CNN convolutional neural networks to extract visual features from the image. These are merged with the word embedding or with a sentence embedding of the question to predict the answer. This work was successfully submitted to the Visual Question Answering Challenge 2016, where it achieved a 53,62% of accuracy in the test dataset. The developed software has followed the best programming practices and Python code style, providing a consistent baseline in Keras for different configurations."
"In this thesis, we provide an initial investigation into bounds for topological entropy of switched linear systems. Entropy measures, roughly, the information needed to describe the behavior of a system with finite precision on finite time horizons, in the limit. After working out entropy computations in detail for the scalar switched case, we review the linear time-invariant nonscalar case, and extend to the nonscalar switched case. We assume some commutation relations among the matrices of the switched system, namely solvability, define an upper average time of activation quantity and use it to provide an upper bound on the entropy of the switched system in terms of the eigenvalues of each subsystem."
"We show that sub-exponential size ($\exp(n^{c})$ for constants $0 < c < 1$) linear programming relaxations cannot beat random guessing for approximately satisfying constraint satisfaction problems (CSPs). In fact, we show that for such problems, sub-exponential size relaxations are as powerful as $n^{\Omega(1)}$-rounds of the Sherali-Adams hierarchy. This is a nearly-exponential improvement over previous results, previously, it was only known that linear programs of size $n^{o(\log n)}$ cannot beat random guessing for any CSP ([CLRS13]).   Our bounds are obtained by exploiting and extending the recent progress in communication complexity for ""lifting"" query lower bounds to communication problems. The main ingredient in our results is a new structural result on ""high-entropy rectangles"" that may of independent interest in communication complexity."
"We propose Deep Optimistic Linear Support Learning (DOL) to solve high-dimensional multi-objective decision problems where the relative importances of the objectives are not known a priori. Using features from the high-dimensional inputs, DOL computes the convex coverage set containing all potential optimal solutions of the convex combinations of the objectives. To our knowledge, this is the first time that deep reinforcement learning has succeeded in learning multi-objective policies. In addition, we provide a testbed with two experiments to be used as a benchmark for deep multi-objective reinforcement learning."
"Egocentric, or first-person vision which became popular in recent years with an emerge in wearable technology, is different than exocentric (third-person) vision in some distinguishable ways, one of which being that the camera wearer is generally not visible in the video frames. Recent work has been done on action and object recognition in egocentric videos, as well as work on biometric extraction from first-person videos. Height estimation can be a useful feature for both soft-biometrics and object tracking. Here, we propose a method of estimating the height of an egocentric camera without any calibration or reference points. We used both traditional computer vision approaches and deep learning in order to determine the visual cues that results in best height estimation. Here, we introduce a framework inspired by two stream networks comprising of two Convolutional Neural Networks, one based on spatial information, and one based on information given by optical flow in a frame. Given an egocentric video as an input to the framework, our model yields a height estimate as an output. We also incorporate late fusion to learn a combination of temporal and spatial cues. Comparing our model with other methods we used as baselines, we achieve height estimates for videos with a Mean Average Error of 14.04 cm over a range of 103 cm of data, and classification accuracy for relative height (tall, medium or short) up to 93.75% where chance level is 33%."
"A gambler moves between the vertices $1, \ldots, n$ of a graph using the probability distribution $p_{1}, \ldots, p_{n}$. Multiple cops pursue the gambler on the graph, only being able to move between adjacent vertices. We investigate the expected capture time for the gambler against $k$ cops as a function of $n$ and $k$ for three versions of the game: (1) known gambler: the cops know the gambler's distribution (2) unknown gambler: the cops do not know the gambler's distribution (3) known changing gambler: the gambler's distribution can change every turn, but the cops know all of the gambler's distributions from the beginning. We show for $n > k$ that if the cops are allowed to choose their initial positions before the game starts and before they know the gambler's distribution(s), and if both the gambler and the cops play optimally, then the expected capture time is $\Theta(n/k)$ for the known gambler, the unknown gambler, and the known changing gambler."
"Evolutionary algorithms have been successfully applied to a variety of optimisation problems in stationary environments. However, many real world optimisation problems are set in dynamic environments where the success criteria shifts regularly. Population diversity affects algorithmic performance, particularly on multiobjective and dynamic problems. Diversity mechanisms are methods of altering evolutionary algorithms in a way that promotes the maintenance of population diversity. This project intends to measure and compare the performance effect a variety of diversity mechanisms have on an evolutionary algorithm when facing an assortment of dynamic problems."
"We develop a broadband channel estimation algorithm for millimeter wave (mmWave) multiple input multiple output (MIMO) systems with few-bit analog-to-digital converters (ADCs). The mmWave MIMO channel is approximately sparse in the joint angle-delay domain since there are relatively fewer paths in the mmWave channel. We formulate the estimation problem as a noisy quantized compressed sensing problem. Then the Expectation-Maximization Generalized Approximate Message Passing (EM-GAMP) algorithm is used to estimate the channel. The angle-delay domain channel coefficients are modeled by a Bernoulli-Gaussian-Mixture distribution with unknown parameters, in which case the EM-GAMP algorithm can adaptively estimate the parameters. Furthermore, training sequences are designed to accelerate the algorithm and minimize the estimation error. Our simulation results show that with one-bit ADCs, the proposed approach yields relatively low MSE in the important low and medium SNR regions. Furthermore, with 3 or 4-bit ADCs, it yields MSE and achievable rate that are only slightly worse than with infinite-bit ADCs in terms of estimation error and achievable rate at low and medium SNR."
"Linguistic laws constitute one of the quantitative cornerstones of modern cognitive sciences and have been routinely investigated in written corpora, or in the equivalent transcription of oral corpora. This means that inferences of statistical patterns of language in acoustics are biased by the arbitrary, language-dependent segmentation of the signal, and virtually precludes the possibility of making comparative studies between human voice and other animal communication systems. Here we bridge this gap by proposing a method that allows to measure such patterns in acoustic signals of arbitrary origin, without needs to have access to the language corpus underneath. The method has been applied to six different human languages, recovering successfully some well-known laws of human communication at timescales even below the phoneme and finding yet another link between complexity and criticality in a biological system. These methods further pave the way for new comparative studies in animal communication or the analysis of signals of unknown code."
"Providing users of HPC systems with a wide variety of up to date software packages is a challenging task. Large software stacks built from source are difficult to manage, requiring powerful package management tools. The Portage package manager from Gentoo is a highly flexible tool that offers a mature solution to this otherwise daunting task. The Gentoo Prefix project develops and maintains a way of installing Gentoo systems in non-standard locations, bringing the virtues of Gentoo to other operating systems. Here we demonstrate how a Gentoo Prefix installation can be used to cross compile software packages for the Intel Xeon Phi known as Knights Corner, as well as to manage large software stacks in HPC environments."
"Combinatory Category Grammar (CCG) supertagging is a task to assign lexical categories to each word in a sentence. Almost all previous methods use fixed context window sizes as input features. However, it is obvious that different tags usually rely on different context window sizes. These motivate us to build a supertagger with a dynamic window approach, which can be treated as an attention mechanism on the local contexts. Applying dropout on the dynamic filters can be seen as drop on words directly, which is superior to the regular dropout on word embeddings. We use this approach to demonstrate the state-of-the-art CCG supertagging performance on the standard test set."
"Imprecise-information processing will play an indispensable role in intelligent systems, especially in the anthropomorphic intelligent systems (as intelligent robots). A new theoretical and technological system of imprecise-information processing has been founded in Principles of Imprecise-Information Processing: A New Theoretical and Technological System[1] which is different from fuzzy technology. The system has clear hierarchy and rigorous structure, which results from the formation principle of imprecise information and has solid mathematical and logical bases, and which has many advantages beyond fuzzy technology. The system provides a technological platform for relevant applications and lays a theoretical foundation for further research."
"Our team won the second prize of the Safe Aging with SPHERE Challenge organized by SPHERE, in conjunction with ECML-PKDD and Driven Data. The goal of the competition was to recognize activities performed by humans, using sensor data. This paper presents our solution. It is based on a rich pre-processing and state of the art machine learning methods. From the raw train data, we generate a synthetic train set with the same statistical characteristics as the test set. We then perform feature engineering. The machine learning modeling part is based on stacking weak learners through a grid searched XGBoost algorithm. Finally, we use post-processing to smooth our predictions over time."
"In this paper, a novel model of 3D elastic mesh is presented for image segmentation. The model is inspired by stress and strain in physical elastic objects, while the repulsive force and elastic force in the model are defined slightly different from the physical force to suit the segmentation problem well. The self-balancing mechanism in the model guarantees the stability of the method in segmentation. The shape of the elastic mesh at balance state is used for region segmentation, in which the sign distribution of the points'z coordinate values is taken as the basis for segmentation. The effectiveness of the proposed method is proved by analysis and experimental results for both test images and real world images."
"A novel approach of image matching for rotating transformation is presented and studied. The approach is inspired by electromagnetic interaction force between physical currents. The virtual current in images is proposed based on the significant edge lines extracted as the fundamental structural feature of images. The virtual electromagnetic force and the corresponding moment is studied between two images after the extraction of the virtual currents in the images. Then image matching for rotating transformation is implemented by exploiting the interaction between the virtual currents in the two images to be matched. The experimental results prove the effectiveness of the novel idea, which indicates the promising application of the proposed method in image registration."
"Vehicle-to-vehicle (V2V) communication can improve road safety and traffic efficiency, particularly around critical areas such as intersections. We analytically derive V2V success probability near an urban intersection, based on empirically supported line-of-sight (LOS), weak-line-of-sight (WLOS), and non-line-of-sight (NLOS) channel models. The analysis can serve as a preliminary design tool for performance assessment over different system parameters and target performance requirements."
"The inverse diffusion curve problem focuses on automatic creation of diffusion curve images that resemble user provided color fields. This problem is challenging since the 1D curves have a nonlinear and global impact on resulting color fields via a partial differential equation (PDE). We introduce a new approach complementary to previous methods by optimizing curve geometry. In particular, we propose a novel iterative algorithm based on the theory of shape derivatives. The resulting diffusion curves are clean and well-shaped, and the final image closely approximates the input. Our method provides a user-controlled parameter to regularize curve complexity, and generalizes to handle input color fields represented in a variety of formats."
"We characterise multi-candidate pure-strategy equilibria in the Hotelling-Downs spatial election model for the class of best-worst voting rules, in which each voter is endowed with both a positive and a negative vote, i.e., each voter can vote in favour of one candidate and against another one. The weights attached to positive and negative votes in calculating a candidate's net score may be different, so that a negative vote and a positive vote need not cancel out exactly. These rules combine the first-place seeking incentives of plurality with the incentives to avoid being ranked last of anti-plurality. We show that these rules generally admit equilibria, which are nonconvergent if and only if the importance of a positive vote exceeds that of a negative vote. The set of equilibria in the latter case is very similar to that of plurality, except that the platforms are less extreme due to the moderating effect of negative votes. Moreover, any degree of dispersion between plurality, at one extreme, and full convergence, at the other, can be attained for the correct choice of the weights."
"In this paper a computationally efficient approach is suggested for the stochastic modeling of an inhomogeneous reluctivity of magnetic materials. These materials can be part of electrical machines, such as a single phase transformer (a benchmark example that is considered in this paper). The approach is based on the Karhunen-Lo\`{e}ve expansion. The stochastic model is further used to study the statistics of the self inductance of the primary coil as a quantity of interest."
"This paper presents an algorithm for solving the problem of tracking smooth curves by a fixed wing unmanned aerial vehicle travelling with a constant airspeed and under a constant wind disturbance. The algorithm is based on the idea of following a guiding vector field which is constructed from the implicit function that describes the desired (possibly time-varying) trajectory. The output of the algorithm can be directly expressed in terms of the bank angle of the UAV in order to achieve coordinated turns. Furthermore, the algorithm can be tuned offline such that physical constrain of the UAV, e.g. the maximum bank angle, will not be violated in a neighborhood of the desired trajectory. We provide the corresponding theoretical convergence analysis and performance results from actual flights."
"To ensure the low end-to-end (E2E) delay for tactile internet, short frame structures will be used in 5G systems. As such, transmission errors with finite blocklength channel codes should be considered to guarantee the high reliability requirement. In this paper, we study cross-layer transmission optimization for tactile internet, where both queueing delay and transmission delay are accounted for in the E2E delay, and different packet loss/error probabilities are considered to characterize the reliability. We show that the required transmit power becomes unbounded when the allowed maximal queueing delay is shorter than the channel coherence time. To satisfy quality-of-service requirement with finite transmit power, we introduce a proactive packet dropping mechanism, and optimize a queue state information and channel state information dependent transmission policy. Since the resource and policy for transmission and the packet dropping policy are related to the packet error probability, queueing delay violation probability, and packet dropping probability, we optimize the three probabilities and obtain the policies related to these probabilities. We start from single-user scenario and then extend our framework to the multi-user scenario. Simulation results show that the optimized three probabilities are in the same order of magnitude. Therefore, we have to take into account all these factors when we design systems for tactile internet applications."
"Transparent authentication (TA) schemes are those in which a user is authenticated by a verifier without requiring explicit user interaction. Those schemes promise high usability and security simultaneously. Many TA schemes rely on the received signal strength as an indicator for the proximity of a user device (prover). However, such implicit proximity verification is not secure against an adversary who can relay messages. In this paper, we propose a novel approach for thwarting relay attacks in TA schemes: the prover permits access to authentication credentials only if it can confirm that it is near the verifier. We present TRec, a system for relay-resilient transparent authentication in which the prover does proximity verification by identifying its approach trajectory to the intended verifier and comparing it with known authorized reference trajectories. Trajectories are measured using low-cost sensors commonly available on personal devices. We demonstrate the security of TRec against a class of adversaries and its ease-of-use by analyzing empirical data, collected using a TRec prototype. TRec is efficient and can be easily integrated into existing TA schemes."
"We describe an attentive encoder that combines tree-structured recursive neural networks and sequential recurrent neural networks for modelling sentence pairs. Since existing attentive models exert attention on the sequential structure, we propose a way to incorporate attention into the tree topology. Specially, given a pair of sentences, our attentive encoder uses the representation of one sentence, which generated via an RNN, to guide the structural encoding of the other sentence on the dependency parse tree. We evaluate the proposed attentive encoder on three tasks: semantic similarity, paraphrase identification and true-false question selection. Experimental results show that our encoder outperforms all baselines and achieves state-of-the-art results on two tasks."
"We consider the problem of robust compressed sensing whose objective is to recover a high-dimensional sparse signal from compressed measurements corrupted by outliers. A new sparse Bayesian learning method is developed for robust compressed sensing. The basic idea of the proposed method is to identify and remove the outliers from sparse signal recovery. To automatically identify the outliers, we employ a set of binary indicator hyperparameters to indicate which observations are outliers. These indicator hyperparameters are treated as random variables and assigned a beta process prior such that their values are confined to be binary. In addition, a Gaussian-inverse Gamma prior is imposed on the sparse signal to promote sparsity. Based on this hierarchical prior model, we develop a variational Bayesian method to estimate the indicator hyperparameters as well as the sparse signal. Simulation results show that the proposed method achieves a substantial performance improvement over existing robust compressed sensing techniques."
"Ensuring the ultra-low end-to-end latency and ultrahigh reliability required by tactile internet is challenging. This is especially true when the stringent Quality-of-Service (QoS) requirement is expected to be satisfied not at the cost of significantly reducing spectral efficiency and energy efficiency (EE). In this paper, we study how to maximize the EE for tactile internet under the stringent QoS constraint, where both queueing delay and transmission delay are taken into account. We first validate that the upper bound of queueing delay violation probability derived from the effective bandwidth can be used to characterize the queueing delay violation probability in the short delay regime for Poisson arrival process. However, the upper bound is not tight for short delay, which leads to conservative designs and hence leads to wasting energy. To avoid this, we optimize resource allocation that depends on the queue state information and channel state information. Analytical results show that with a large number of transmit antennas the EE achieved by the proposed policy approaches to the EE limit achieved for infinite delay bound, which implies that the policy does not lead to any EE loss. Simulation and numerical results show that even for not-so-large number of antennas, the EE achieved by the proposed policy is still close to the EE limit."
"This paper presents a novel approach to distinguish driving styles with respect to their energy efficiency. A distinct property of our method is that it relies exclusively on Global Positioning System (GPS) logs of drivers. This setting is highly relevant in practice as these data can easily be acquired.   Relying on positional data alone means that all derived features will be correlated, so we strive to find a single quantity that allows us to perform the driving style analysis. To this end we consider a robust variation of the so called jerk of a movement. We show that our feature choice outperforms other more commonly used jerk-based formulations and we discuss the handling of noisy, inconsistent, and incomplete data as this is a notorious problem when dealing with real-world GPS logs.   Our solving strategy relies on an agglomerative hierarchical clustering combined with an L-term heuristic to determine the relevant number of clusters. It can easily be implemented and performs fast, even on very large, real-world data sets. Experiments show that our approach is robust against noise and able to discern different driving styles."
"In this work, we study how to design uplink transmission with massive machine type devices in tactile internet, where ultra-short delay and ultra-high reliability are required. To characterize the transmission reliability constraint, we employ a two-state transmission model based on the achievable rate with finite blocklength channel codes. If the channel gain exceeds a threshold, a short packet can be transmitted with a small error probability; otherwise there is a packet loss. To exploit frequency diversity, we assign multiple subchannels to each active device, from which the device selects a subchannel with channel gain exceeding the threshold for transmission. To show the total bandwidth required to ensure the reliability, we optimize the number of subchannels and bandwidth of each subchannel and the threshold for each device to minimize the total bandwidth of the system with a given number of antennas at the base station. Numerical results show that with 1000 devices in one cell, the required bandwidth of the optimized policy is acceptable even for prevalent cellular systems. Furthermore, we show that by increasing antennas at the BS, frequency diversity becomes unnecessary, and the required bandwidth is reduced."
"The problem of detecting and removing redundant constraints is fundamental in optimization. We focus on the case of linear programs (LPs), given by $d$ variables with $n$ inequality constraints. A constraint is called \emph{redundant}, if after its removal, the LP still has the same feasible region. The currently fastest method to detect all redundancies is due to Clarkson: it solves $n$ linear programs, but each of them has at most $s$ constraints, where $s$ is the number of nonredundant constraints.   In this paper, we study the special case where every constraint has at most two variables with nonzero coefficients. This family, denoted by $LI(2)$, has some nice properties. Namely, as shown by Aspvall and Shiloach, given a variable $x_i$ and a value $\lambda$, we can test in time $O(nd)$ whether there is a feasible solution with $x_i = \lambda$. Hochbaum and Naor present an $O(d^2 n \log n)$ algorithm for solving the feasibility problem in $LI(2)$. Their technique makes use of the Fourier-Motzkin elimination method and the earlier mentioned result by Aspvall and Shiloach.   We present a strongly polynomial algorithm that solves redundancy detection in time $O(n d^2 s \log s)$. It uses a modification of Clarkson's algorithm, together with a revised version of Hochbaum and Naor's technique. Finally we show that dimensionality testing can be done with the same running time as solving feasibility."
We introduce a diversity extraction for multicarrier continuous-variable (CV) quantum key distribution (QKD). The diversity extraction utilizes the resources that are injected into the transmission by the additional degrees of freedom of the multicarrier modulation. The multicarrier scheme granulates the information into Gaussian subcarrier CVs and divides the physical link into several Gaussian sub-channels for the transmission. We prove that the exploitable extra degree of freedom in a multicarrier CVQKD scenario significantly extends the possibilities of single-carrier CVQKD. The diversity extraction allows for the parties to reach decreased error probabilities by utilizing those extra resources of a multicarrier transmission that are not available in a single-carrier CVQKD setting. The additional resources of multicarrier CVQKD allow the achievement of significant performance improvements that are particularly crucial in an experimental scenario.
"We define an iterative error-minimizing secret key adapting method for multicarrier CVQKD. A multicarrier CVQKD protocol uses Gaussian subcarrier quantum continuous variables (CVs) for the transmission. The proposed method allows for the parties to reach a given target secret key rate with minimized error rate through the Gaussian sub-channels by a sub-channel adaption procedure. The adaption algorithm iteratively determines the optimal transmit conditions to achieve the target secret key rate and the minimal error rate over the sub-channels. The solution requires no complex calculations or computational tools, allowing for easy implementation for experimental CVQKD scenarios."
"In multi-hop secondary networks, bidding strategies for spectrum auction, route selection and relaying incentives should be jointly considered to establish multi-hop communication. In this paper, a framework for joint resource bidding and tipping is developed where users iteratively revise their strategies, which include bidding and incentivizing relays, to achieve their Quality of Service (QoS) requirements. A bidding language is designed to generalize secondary users' heterogeneous demands for multiple resources and willingness to pay. Then, group partitioning-based auction mechanisms are presented to exploit the heterogeneity of SU demands in multi-hop secondary networks. These mechanisms include primary operator (PO) strategies based on static and dynamic partition schemes combined with new payment mechanisms to obtain high revenue and fairly allocate the resources. The proposed auction schemes stimulate the participation of SUs and provide high revenue for the PO while maximizing the social welfare. Besides, they satisfy the properties of truthfulness, individual rationality and computational tractability. Simulation results have shown that for highly demanding users the static group scheme achieves 150% more winners and 3 times higher revenue for the PO compared to a scheme without grouping. For lowly demanding users, the PO may keep similar revenue with the dynamic scheme by lowering 50% the price per channel as the number of winners will increase proportionally."
"The crux of the problem in KDD Cup 2016 involves developing data mining techniques to rank research institutions based on publications. Rank importance of research institutions are derived from predictions on the number of full research papers that would potentially get accepted in upcoming top-tier conferences, utilizing public information on the web. This paper describes our solution to KDD Cup 2016. We used a two step approach in which we first identify full research papers corresponding to each conference of interest and then train two variants of exponential smoothing models to make predictions. Our solution achieves an overall score of 0.7508, while the winning submission scored 0.7656 in the overall results."
"This paper explores how the in- and out-domain probabilistic linear discriminant analysis (PLDA) speaker verification behave when enrolment and verification lengths are reduced. Experiment studies have found that when full-length utterance is used for evaluation, in-domain PLDA approach shows more than 28% improvement in EER and DCF values over out-domain PLDA approach and when short utterances are used for evaluation, the performance gain of in-domain speaker verification reduces at an increasing rate. Novel modified inter dataset variability (IDV) compensation is used to compensate the mismatch between in- and out-domain data and IDV-compensated out-domain PLDA shows respectively 26% and 14% improvement over out-domain PLDA speaker verification when SWB and NIST data are respectively used for S normalization. When the evaluation utterance length is reduced, the performance gain by IDV also reduces as short utterance evaluation data i-vectors have more variations due to phonetic variations when compared to the dataset mismatch between in- and out-domain data."
"In this paper we study a family of algorithms, introduced by Chan [SODA 1999] and called LR-algorithms, for drawing ordered rooted binary trees. In particular, we are interested in constructing LR-drawings (that are drawings obtained via LR-algorithms) with small width. Chan showed three different LR-algorithms that achieve, for an ordered rooted binary tree with $n$ nodes, width $O(n^{0.695})$, width $O(n^{0.5})$, and width $O(n^{0.48})$.   We prove that, for every $n$-node ordered rooted binary tree, an LR-drawing with minimum width can be constructed in $O(n^{1.48})$ time. Further, we show an infinite family of $n$-node ordered rooted binary trees requiring $\Omega(n^{0.418})$ width in any LR-drawing; no lower bound better than $\Omega(\log n)$ was previously known. Finally, we present the results of an experimental evaluation that allowed us to determine the minimum width of all the ordered rooted binary trees with up to $451$ nodes.   Our interest in LR-drawings is mainly motivated by a result of Di Battista and Frati [Algorithmica 2009], who proved that $n$-vertex outerplanar graphs have outerplanar straight-line drawings in $O(n^{1.48})$ area by means of a drawing algorithm which resembles an LR-algorithm.   We deepen the connection between LR-drawings and outerplanar straight-line drawings by proving that, if $n$-node ordered rooted binary trees have LR-drawings with $f(n)$ width, for any function $f(n)$, then $n$-vertex outerplanar graphs have outerplanar straight-line drawings in $O(f(n))$ area.   Finally, we exploit a structural decomposition for ordered rooted binary trees introduced by Chan in order to prove that every $n$-vertex outerplanar graph has an outerplanar straight-line drawing in $O(n\cdot 2^{\sqrt{2 \log_2 n}} \sqrt{\log n})$ area."
"Hierarchical Reinforcement Learning has been previously shown to speed up the convergence rate of RL planning algorithms as well as mitigate feature-based model misspecification (Mankowitz et. al. 2016a,b, Bacon 2015). To do so, it utilizes hierarchical abstractions, also known as skills -- a type of temporally extended action (Sutton et. al. 1999) to plan at a higher level, abstracting away from the lower-level details. We incorporate risk sensitivity, also referred to as Situational Awareness (SA), into hierarchical RL for the first time by defining and learning risk aware skills in a Probabilistic Goal Semi-Markov Decision Process (PG-SMDP). This is achieved using our novel Situational Awareness by Risk-Conscious Skills (SARiCoS) algorithm which comes with a theoretical convergence guarantee. We show in a RoboCup soccer domain that the learned risk aware skills exhibit complex human behaviors such as `time-wasting' in a soccer game. In addition, the learned risk aware skills are able to mitigate reward-based model misspecification."
"This paper proposes a technique for automatic gain tuning of a momentum based balancing controller for humanoid robots. The controller ensures the stabilization of the centroidal dynamics and the associated zero dynamics. Then, the closed-loop, constrained joint space dynamics is linearized and the controller's gains are chosen so as to obtain desired properties of the linearized system. Symmetry and positive definiteness constraints of gain matrices are enforced by proposing a tracker for symmetric positive definite matrices. Simulation results are carried out on the humanoid robot iCub."
"We propose Impatient Deep Neural Networks (DNNs) which deal with dynamic time budgets during application. They allow for individual budgets given a priori for each test example and for anytime prediction, i.e., a possible interruption at multiple stages during inference while still providing output estimates. Our approach can therefore tackle the computational costs and energy demands of DNNs in an adaptive manner, a property essential for real-time applications. Our Impatient DNNs are based on a new general framework of learning dynamic budget predictors using risk minimization, which can be applied to current DNN architectures by adding early prediction and additional loss layers. A key aspect of our method is that all of the intermediate predictors are learned jointly. In experiments, we evaluate our approach for different budget distributions, architectures, and datasets. Our results show a significant gain in expected accuracy compared to common baselines."
"The realisation of sensing modalities based on the principles of compressed sensing is often hindered by discrepancies between the mathematical model of its sensing operator, which is necessary during signal recovery, and its actual physical implementation, whose values may differ significantly from the assumed model. In this paper we tackle the bilinear inverse problem of recovering a sparse input signal and some unknown, unstructured multiplicative factors affecting the sensors that capture each compressive measurement. Our methodology relies on collecting a few snapshots under new draws of the sensing operator, and applying a greedy algorithm based on projected gradient descent and the principles of iterative hard thresholding. We explore empirically the sample complexity requirements of this algorithm by testing the phase transition of our algorithm, and show in a practically relevant instance of compressive imaging that the exact solution can be obtained with only a few snapshots."
"The availability heuristic is a strategy that people use to make quick decisions but often lead to systematic errors. We propose three ways that visualization could facilitate unbiased decision-making. First, visualizations can alter the way our memory stores the events for later recall, so as to improve users' long-term intuitions. Second, the known biases could lead to new visualization guidelines. Third, we suggest the design of decision-making tools that are inspired by heuristics, e.g. suggesting intuitive approximations, rather than target to present exhaustive comparisons of all possible outcomes, or automated solutions for choosing decisions."
"Although introduced only a few years ago, the problem of order-preserving pattern matching has already attracted significant attention. In this paper we introduce the first index for this problem that uses sublinear space in addition to the data, which is useful when we want to search repeatedly in datasets to which we have read-only access and which are too large to copy conveniently. Specifically, we show how, given a string $S [1..n]$ and a constant $c \geq 1$, we can build an $O (n \log \log n)$-bit index such that later, given a pattern $P [1..m]$ with $m \leq \lg^c n$ and fast random access to $S$, we can return the starting position of some order-preserving match for $P$ in $S$ in $O (m \log^4 n)$ time if one exists."
"The importance of timely response to natural disasters and evacuating affected people to safe areas is paramount to save lives. Emergency services are often handicapped by the amount of rescue resources at their disposal. We present a system that leverages the power of a social network forming new connections among people based on \textit{real-time location} and expands the rescue resources pool by adding private sector cars. We also introduce a car-sharing algorithm to identify safe routes in an emergency with the aim of minimizing evacuation time, maximizing pick-up of people without cars, and avoiding traffic congestion."
"We perform a special number field sieve discrete logarithm computation in a 1024-bit prime field. To our knowledge, this is the first kilobit-sized discrete logarithm computation ever reported for prime fields. This computation took a little over two months of calendar time on an academic cluster using the open-source CADO-NFS software. Our chosen prime $p$ looks random, and $p--1$ has a 160-bit prime factor, in line with recommended parameters for the Digital Signature Algorithm. However, our p has been trapdoored in such a way that the special number field sieve can be used to compute discrete logarithms in $\mathbb{F}\_p^*$ , yet detecting that p has this trapdoor seems out of reach. Twenty-five years ago, there was considerable controversy around the possibility of back-doored parameters for DSA. Our computations show that trapdoored primes are entirely feasible with current computing technology. We also describe special number field sieve discrete log computations carried out for multiple weak primes found in use in the wild."
"Local Process Model (LPM) discovery is focused on the mining of a set of process models where each model describes the behavior represented in the event log only partially, i.e. subsets of possible events are taken into account to create so-called local process models. Often such smaller models provide valuable insights into the behavior of the process, especially when no adequate and comprehensible single overall process model exists that is able to describe the traces of the process from start to end. The practical application of LPM discovery is however hindered by computational issues in the case of logs with many activities (problems may already occur when there are more than 17 unique activities). In this paper, we explore three heuristics to discover subsets of activities that lead to useful log projections with the goal of speeding up LPM discovery considerably while still finding high-quality LPMs. We found that a Markov clustering approach to create projection sets results in the largest improvement of execution time, with discovered LPMs still being better than with the use of randomly generated activity sets of the same size. Another heuristic, based on log entropy, yields a more moderate speedup, but enables the discovery of higher quality LPMs. The third heuristic, based on the relative information gain, shows unstable performance: for some data sets the speedup and LPM quality are higher than with the log entropy based method, while for other data sets there is no speedup at all."
"Anomalous random walks having long-range jumps are a critical branch of dynamical processes on networks, which can model a number of search and transport processes. However, traditional measurements based on mean first passage time are not useful as they fail to characterize the cost associated with each jump. Here we introduce a new concept of mean first traverse distance (MFTD) to characterize anomalous random walks that represents the expected traverse distance taken by walkers searching from source node to target node, and we provide a procedure for calculating the MFTD between two nodes. We use Levy walks on networks as an example, and demonstrate that the proposed approach can unravel the interplay between diffusion dynamics of Levy walks and the underlying network structure. Interestingly, applying our framework to the famous PageRank search, we can explain why its damping factor empirically chosen to be around 0.85. The framework for analyzing anomalous random walks on complex networks offers a new useful paradigm to understand the dynamics of anomalous diffusion processes, and provides a unified scheme to characterize search and transport processes on networks."
"This letter summarizes some known properties and also presents several new properties of the Numerical Integration (NI) method for time-optimal trajectory planning along a specified path. The contribution is that rigorous mathematical proofs of these properties are presented, most of which cannot be found in existing literatures. We first give some properties regarding switch points and accelerating/decelerating curves of the NI method. Then, for the fact that when kinematic constraints are considered, the original version of NI which only considers torque constraints may result in failure of trajectory planning, we give the concrete failure conditions with rigorous mathematical proof. Accordingly, a failure detection algorithm is given in a run-and-test manner. Some simulation results on a unicycle vehicle are provided to verify those presented properties. Note that though those known properties are not discovered first, their mathematical proofs are given first in this letter. The detailed proofs make the theory of NI more complete and help interested readers to gain a thorough understanding of the method."
"Cassandra is one of the most widely used distributed data stores these days. Cassandra supports flexible consistency guarantees over a wide-column data access model and provides almost linear scale-out performance. This enables application developers to tailor the performance and availability of Cassandra to their exact application's needs and required semantics. Yet, Cassandra is designed to withstand benign failures, and cannot cope with most forms of Byzantine attacks.   In this work, we present an analysis of Cassandra's vulnerabilities and propose protocols for hardening Cassandra against Byzantine failures. We examine several alternative design choices and compare between them both qualitatively and empirically by using the Yahoo! Cloud Serving Benchmark (YCSB) performance benchmark. We include incremental performance analysis for our algorithmic and cryptographic adjustments, supporting our design choices."
"The randomized version of the Kaczmarz method for the solution of linear systems is known to converge linearly in expectation. In this work we extend this result and show that the recently proposed Randomized Sparse Kaczmarz method for recovery of sparse solutions, as well as many variants, also converges linearly in expectation. The result is achieved in the framework of split feasibility problems and their solution by randomized Bregman projections with respect to strongly convex functions. To obtain the expected convergence rates we prove extensions of error bounds for projections. The convergence result is shown to hold in more general settings involving smooth convex functions, piecewise linear-quadratic functions and also the regularized nuclear norm, which is used in the area of low rank matrix problems. Numerical experiments indicate that the Randomized Sparse Kaczmarz method provides advantages over both the non-randomized and the non-sparse Kaczmarz methods for the solution of over- and under-determined linear systems."
"It is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient. Personalized dialogue systems trained on a small dataset can overfit and make it difficult to adapt to different user needs. One way to solve this problem is to consider a collection of multiple users' data as a source domain and an individual user's data as a target domain, and to perform a transfer learning from the source to the target domain. By following this idea, we propose ""PETAL""(PErsonalized Task-oriented diALogue), a transfer-learning framework based on POMDP to learn a personalized dialogue system. The system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. This framework can avoid the negative transfer problem by considering differences between source and target users. The policy in the personalized POMDP can learn to choose different actions appropriately for different users. Experimental results on a real-world coffee-shopping data and simulation data show that our personalized dialogue system can choose different optimal actions for different users, and thus effectively improve the dialogue quality under the personalized setting."
"Increasing mobile data demands in current cellular networks and proliferation of advanced handheld devices have given rise to a new generation of dynamic network architectures (DNAs). In a DNA, users share their connectivities and act as access points providing Internet connections for others without additional network infrastructure cost. A large number of users and their dynamic connections make DNA highly adaptive to variations in the network and suitable for low cost ubiquitous Internet connectivity. In this article, we propose a novel collaborative cognitive dynamic network architecture (CDNA) which incorporates cognitive capabilities to exploit underutilized spectrum in a more flexible and intelligent way. The design principles of CDNA are perfectly aligned to the functionality requirements of future 5G wireless networks such as energy and spectrum efficiency, scalability, dynamic reconfigurability, support for multi-hop communications, infrastructure sharing, and multi-operator cooperation. A case study with a new resource allocation problem enabled by CDNA is conducted using matching theory with pricing to illustrate the potential benefits of CDNA for users and operators, tackle user associations for data and spectrum trading with low complexity, and enable self-organizing capabilities. Finally, possible challenges and future research directions are given."
"Content-Based Image Retrieval (CBIR) locates, retrieves and displays images alike to one given as a query, using a set of features. It demands accessible data in medical archives and from medical equipment, to infer meaning after some processing. A problem similar in some sense to the target image can aid clinicians. CBIR complements text-based retrieval and improves evidence-based diagnosis, administration, teaching, and research in healthcare. It facilitates visual/automatic diagnosis and decision-making in real-time remote consultation/screening, store-and-forward tests, home care assistance and overall patient surveillance. Metrics help comparing visual data and improve diagnostic. Specially designed architectures can benefit from the application scenario. CBIR use calls for file storage standardization, querying procedures, efficient image transmission, realistic databases, global availability, access simplicity, and Internet-based structures. This chapter recommends important and complex aspects required to handle visual content in healthcare."
"This paper investigates the problem of network embedding, which aims at learning low-dimensional vector representation of nodes in networks. Most existing network embedding methods rely solely on the network structure, i.e., the linkage relationships between nodes, but ignore the rich content information associated with it, which is common in real world networks and beneficial to describing the characteristics of a node. In this paper, we propose content-enhanced network embedding (CENE), which is capable of jointly leveraging the network structure and the content information. Our approach integrates text modeling and structure modeling in a general framework by treating the content information as a special kind of node. Experiments on several real world net- works with application to node classification show that our models outperform all existing network embedding methods, demonstrating the merits of content information and joint learning."
"Two very important problems regarding spreading phenomena in complex topologies are the optimal selection of node sets either to minimize or maximize the extent of outbreaks. Both problems are nontrivial when a small fraction of the nodes in the network can be used to achieve the desired goal. The minimization problem is equivalent to a structural optimization. The ""superblockers"", i.e., the nodes that should be removed from the network to minimize the size of outbreaks, are those nodes that make connected components as small as possible. ""Superspreaders"" are instead the nodes such that, if chosen as initiators, they maximize the average size of outbreaks. The identity of superspreaders is expected to depend not just on the topology, but also on the specific dynamics considered. Recently, it has been conjectured that the two optimization problems might be equivalent, in the sense that superblockers act also as superspreaders. In spite of its potential groundbreaking importance, no empirical study has been performed to validate this conjecture. In this paper, we perform an extensive analysis over a large set of real-world networks to test the similarity between sets of superblockers and of superspreaders. We show that the two optimization problems are not equivalent: superblockers do not act as optimal spreaders."
"For 5G it will be important to leverage the available millimeter wave spectrum. To achieve an approximately omni- directional coverage with a similar effective antenna aperture compared to state of the art cellular systems, an antenna array is required at both the mobile and basestation. Due to the large bandwidth and inefficient amplifiers available in CMOS for mmWave, the analog front-end of the receiver with a large number of antennas becomes especially power hungry. Two main solutions exist to reduce the power consumption: Hybrid BeamForming (HBF) and Digital BeamForming (DBF) with low resolution ADC. Hybrid beamforming can also be combined with low resolution ADCs. This paper compares the spectral and energy efficiency based on the chosen RF-frontend configuration. A channel with multipath propagation is used. In contrast to previous publication, we take the spatial correlation of the quantization noise into account. We show that the low resolution ADC are robust to small Automatic Gain Control (AGC) imperfections. We showed that in the low SNR regime the performance of DBF even with 1-2 bit resolution outperforms HBF. If we consider the relationship of spectral and energy efficiency then DBF with 3-5 bits resolution achieves the best ratio of spectral efficiency per power consumption of the RF receiver frontend over a very wide SNR region. The power consumption model is based on components reported in literature."
"Deep convolutional neural networks (DCNNs) have shown remarkable performance in image classification tasks in recent years. Generally, deep neural network architectures are stacks consisting of a large number of convolution layers, and they perform downsampling along the spatial dimension via pooling to reduce memory usage. At the same time, the feature map dimension (i.e., the number of channels) is sharply increased at downsampling locations, which is essential to ensure effective performance because it increases the capability of high-level attributes. Moreover, this also applies to residual networks and is very closely related to their performance. In this research, instead of using downsampling to achieve a sharp increase at each residual unit, we gradually increase the feature map dimension at all the units to involve as many locations as possible. This is discussed in depth together with our new insights as it has proven to be an effective design to improve the generalization ability. Furthermore, we propose a novel residual unit capable of further improving the classification accuracy with our new network architecture. Experiments on benchmark CIFAR datasets have shown that our network architecture has a superior generalization ability compared to the original residual networks. Code is available at https://github.com/jhkim89/PyramidNet"
"We consider the problem of Gaussian mixture clustering in the high-dimensional limit where the data consists of $m$ points in $n$ dimensions, $n,m \rightarrow \infty$ and $\alpha = m/n$ stays finite. Using exact but non-rigorous methods from statistical physics, we determine the critical value of $\alpha$ and the distance between the clusters at which it becomes information-theoretically possible to reconstruct the membership into clusters better than chance. We also determine the accuracy achievable by the Bayes-optimal estimation algorithm. In particular, we find that when the number of clusters is sufficiently large, $r > 4 + 2 \sqrt{\alpha}$, there is a gap between the threshold for information-theoretically optimal performance and the threshold at which known algorithms succeed."
"Decoupling uplink (UL) and downlink (DL) is a new architectural paradigm where DL and UL are not constrained to be associated to the same base station (BS). Building upon this paradigm, the goal of the present paper is to provide lower, albeit tight bounds for the ergodic UL capacity of a decoupled cellular network. The analysis is performed for a scenario consisting of a macro BS and a set of small cells (SCs) whose positions are selected randomly according to a Poisson point process of a given spatial density. Based on this analysis simple bounds in closed form expressions are defined. The devised bounds are employed to compare the performance of the decoupled case versus a set of benchmark cases, namely the coupled case, and the situations of having either a single macro BS or only SCs. This comparison provides valuable insights regarding the behavior and performance of such networks, providing simpler expressions for the ergodic UL capacity as a function of the distances to the macro BS and the density of SCs. These expressions constitute a simple guide to the minimum degree of densification that guarantees the Quality of Service (QoS) objectives of the network, thus, providing a valuable tool to the network operator of significant practical and commercial value."
"An interesting research problem in our age of Big Data is that of determining provenance. Granular evaluation of provenance of physical goods--e.g. tracking ingredients of a pharmaceutical or demonstrating authenticity of luxury goods--has often not been possible with today's items that are produced and transported in complex, inter-organizational, often internationally-spanning supply chains. Recent adoption of Internet of Things and Blockchain technologies give promise at better supply chain provenance. We are particularly interested in the blockchain as many favoured use cases of blockchain are for provenance tracking. We are also interested in applying ontologies as there has been some work done on knowledge provenance, traceability, and food provenance using ontologies. In this paper, we make a case for why ontologies can contribute to blockchain design. To support this case, we analyze a traceability ontology and translate some of its representations to smart contracts that execute a provenance trace and enforce traceability constraints on the Ethereum blockchain platform."
"Surveillance system (SS) development requires hi-tech support to prevail over the shortcomings related to the massive quantity of visual information from SSs. Anything but reduced human monitoring became impossible by means of its physical and economic implications, and an advance towards an automated surveillance becomes the only way out. When it comes to a computer vision system, automatic video event comprehension is a challenging task due to motion clutter, event understanding under complex scenes, multilevel semantic event inference, contextualization of events and views obtained from multiple cameras, unevenness of motion scales, shape changes, occlusions and object interactions among lots of other impairments. In recent years, state-of-the-art models for video event classification and recognition include modeling events to discern context, detecting incidents with only one camera, low-level feature extraction and description, high-level semantic event classification, and recognition. Even so, it is still very burdensome to recuperate or label a specific video part relying solely on its content. Principal component analysis (PCA) has been widely known and used, but when combined with other techniques such as the expectation-maximization (EM) algorithm its computation becomes more efficient. This chapter introduces advances associated with the concept of Probabilistic PCA (PPCA) analysis of video event and it also aims at looking closely to ways and metrics to evaluate these less intensive EM implementations of PCA and KPCA."
"We continue the recent line of research studying information dissemination problems in adversarial dynamic radio networks. We give two generic algorithms which allow to transform generalized version of single-message broadcast algorithms into multi-message broadcast algorithms. Based on these generic algorithms, we obtain multi-message broadcast algorithms for dynamic radio networks for a number of different dynamic network settings. For one of the modeling assumptions, our algorithms are complemented by a lower bound which shows that the upper bound is close to optimal."
"We consider distributed multitask learning problems over a network of agents where each agent is interested in estimating its own parameter vector, also called task, and where the tasks at neighboring agents are related according to a set of linear equality constraints. Each agent possesses its own convex cost function of its parameter vector and a set of linear equality constraints involving its own parameter vector and the parameter vectors of its neighboring agents. We propose an adaptive stochastic algorithm based on the projection gradient method and diffusion strategies in order to allow the network to optimize the individual costs subject to all constraints. Although the derivation is carried out for linear equality constraints, the technique can be applied to other forms of convex constraints. We conduct a detailed mean-square-error analysis of the proposed algorithm and derive closed-form expressions to predict its learning behavior. We provide simulations to illustrate the theoretical findings. Finally, the algorithm is employed for solving two problems in a distributed manner: a minimum-cost flow problem over a network and a space-time varying field reconstruction problem."
"We present the approaches for the four video-to-language tasks of LSMDC 2016, including movie description, fill-in-the-blank, multiple-choice test, and movie retrieval. Our key idea is to adopt the semantic attention mechanism; we first build a set of attribute words that are consistently discovered on video frames, and then selectively fuse them with input words for more semantic representation and with output words for more accurate prediction. We show that our implementation of semantic attention indeed improves the performance of multiple video-to-language tasks. Specifically, the presented approaches participated in all the four tasks of the LSMDC 2016, and have won three of them, including fill-in-the-blank, multiple-choice test, and movie retrieval."
"Electromagnetic simulations of complex geologic settings are computationally expensive. One reason for this is the fact that a fine mesh is required to accurately discretize the electrical conductivity model of a given setting. This conductivity model may vary over several orders of magnitude and these variations can occur over a large range of length scales. Using a very fine mesh for the discretization of this setting leads to the necessity to solve a large system of equations that is often difficult to deal with. To keep the simulations computationally tractable, coarse meshes are often employed for the discretization of the model. Such coarse meshes typically fail to capture the fine-scale variations in the conductivity model resulting in inaccuracies in the predicted data. In this work, we introduce a framework for constructing a coarse-mesh or upscaled conductivity model based on a prescribed fine-mesh model. Rather than using analytical expressions, we opt to pose upscaling as a parameter estimation problem. By solving an optimization problem, we obtain a coarse-mesh conductivity model. The optimization criterion can be tailored to the survey setting in order to produce coarse models that accurately reproduce the predicted data generated on the fine mesh. This allows us to upscale arbitrary conductivity structures, as well as to better understand the meaning of the upscaled quantity. We use 1D and 3D examples to demonstrate that the proposed framework is able to emulate the behavior of the heterogeneity in the fine-mesh conductivity model, and to produce an accurate description of the desired predicted data obtained by using a coarse mesh in the simulation process."
"The octagon abstract domain is a widely used numeric abstract domain expressing relational information between variables whilst being both computationally efficient and simple to implement. Each element of the domain is a system of constraints where each constraint takes the restricted form $\pm x_i \pm x_j \leq d$. A key family of operations for the octagon domain are closure algorithms, which check satisfiability and provide a normal form for octagonal constraint systems. We present new quadratic incremental algorithms for closure, strong closure and integer closure and proofs of their correctness. We highlight the benefits and measure the performance of these new algorithms."
"Dumitrescu [arXiv:1607.07673] describes a data structure referred to as a Selectable Sloppy Heap. We present a simplified approach, and also point out aspects of Dumitrescu's exposition that require scrutiny."
"Dynamic Mode Decomposition (DMD) has emerged as a powerful tool for analyzing the dynamics of non-linear systems from experimental datasets. Recently, several attempts have extended DMD to the context of low-rank approximations. This extension is of particular interest for reduced-order modeling in various applicative domains, e.g. for climate prediction, to study molecular dynamics or micro-electromechanical devices. This low-rank extension takes the form of a nonconvex optimization problem. To the best of our knowledge, only sub-optimal algorithms have been proposed in the literature to compute the solution of this problem. In this paper, we prove that there exists a closed-form optimal solution to this problem and design an effective algorithm to compute it based on Singular Value Decomposition (SVD). Based on this solution, we then propose efficient procedures for reduced-order modeling and for the identification of the the low-rank DMD modes and amplitudes. Experiments illustrates the gain in performance of the proposed algorithm compared to state-of-the-art techniques."
"We address the problem of solving convex optimization problems with many convex constraints in a distributed setting. Our approach is based on an extension of the alternating direction method of multipliers (ADMM) that recently gained a lot of attention in the Big Data context. Although it has been invented decades ago, ADMM so far can be applied only to unconstrained problems and problems with linear equality or inequality constraints. Our extension can handle arbitrary inequality constraints directly. It combines the ability of ADMM to solve convex optimization problems in a distributed setting with the ability of the Augmented Lagrangian method to solve constrained optimization problems, and as we show, it inherits the convergence guarantees of ADMM and the Augmented Lagrangian method."
"Person re-identification (re-ID) has become increasingly popular in the community due to its application and research significance. It aims at spotting a person of interest in other cameras. In the early days, hand-crafted algorithms and small-scale evaluation were predominantly reported. Recent years have witnessed the emergence of large-scale datasets and deep learning systems which make use of large data volumes. Considering different tasks, we classify most current re-ID methods into two classes, i.e., image-based and video-based; in both tasks, hand-crafted and deep learning systems will be reviewed. Moreover, two new re-ID tasks which are much closer to real-world applications are described and discussed, i.e., end-to-end re-ID and fast re-ID in very large galleries. This paper: 1) introduces the history of person re-ID and its relationship with image classification and instance retrieval; 2) surveys a broad selection of the hand-crafted systems and the large-scale methods in both image- and video-based re-ID; 3) describes critical future directions in end-to-end re-ID and fast retrieval in large galleries; and 4) finally briefs some important yet under-developed issues."
"In classical machine learning, regression is treated as a black box process of identifying a suitable function from a hypothesis set without attempting to gain insight into the mechanism connecting inputs and outputs. In the natural sciences, however, finding an interpretable function for a phenomenon is the prime goal as it allows to understand and generalize results. This paper proposes a novel type of function learning network, called equation learner (EQL), that can learn analytical expressions and is able to extrapolate to unseen domains. It is implemented as an end-to-end differentiable feed-forward network and allows for efficient gradient based training. Due to sparsity regularization concise interpretable expressions can be obtained. Often the true underlying source expression is identified."
"In graph coloring problems, the goal is to assign a positive integer color to each vertex of an input graph such that adjacent vertices do not receive the same color assignment. For classic graph coloring, the goal is to minimize the maximum color used, and for the sum coloring problem, the goal is to minimize the sum of colors assigned to all input vertices. In the offline variant, the entire graph is presented at once, and in online problems, one vertex is presented for coloring at each time, and the only information is the identity of its neighbors among previously known vertices. In batched graph coloring, vertices are presented in k batches, for a fixed integer k > 1, such that the vertices of a batch are presented as a set, and must be colored before the vertices of the next batch are presented. This last model is an intermediate model, which bridges between the two extreme scenarios of the online and offline models. We provide several results, including a general result for sum coloring and results for the classic graph coloring problem on restricted graph classes: We show tight bounds for any graph class containing trees as a subclass (e.g., forests, bipartite graphs, planar graphs, and perfect graphs), and a surprising result for interval graphs and k = 2, where the value of the (strict and asymptotic) competitive ratio depends on whether the graph is presented with its interval representation or not."
"Demand response management has become one of the key enabling technologies for smart grids. Motivated by the increasing demand response incentives offered by service operators, more customers are subscribing to various demand response programs. However, with growing customer participation, the problem of determining the optimal loads to be curtailed in a microgrid during contingencies within a feasible time frame becomes computationally hard. This paper proposes an efficient approximation algorithm for event-based demand response management in microgrids. In event-based management, it is important to curtail loads as fast as possible to maintain the stability of a microgrid during the islanded mode in a scalable manner. A simple greedy approach is presented that can rapidly determine a close-to-optimal load curtailment scheme to maximize the aggregate customer utility in milliseconds for a large number of customers. This paper further derives a novel theoretical guarantee of the gap between the proposed efficient algorithm and the optimal solution (that may be computationally hard to obtain). The performance of algorithm is corroborated extensively by simulations with up to thousands of customers. For the sake of practicality, the proposed event-based demand response management algorithm is applied to a feeder from the Canadian benchmark distribution system. The simulation results demonstrate that the proposed approach efficiently optimizes microgrid operation during islanded mode while maintaining appropriate voltage levels and network constrains."
"The suffix array is the key to efficient solutions for myriads of string processing problems in different applications domains, like data compression, data mining, or Bioinformatics. With the rapid growth of available data, suffix array construction algorithms had to be adapted to advanced computational models such as external memory and distributed computing. In this article, we present five suffix array construction algorithms utilizing the new algorithmic big data batch processing framework Thrill, which allows us to process input sizes in orders of magnitude that have not been considered before."
"Speaker verification systems are vulnerable to spoofing attacks which presents a major problem in their real-life deployment. To date, most of the proposed synthetic speech detectors (SSDs) have weighted the importance of different segments of speech equally. However, different attack methods have different strengths and weaknesses and the traces that they leave may be short or long term acoustic artifacts. Moreover, those may occur for only particular phonemes or sounds. Here, we propose three algorithms that weigh likelihood-ratio scores of individual frames, phonemes, and sound-classes depending on their importance for the SSD. Significant improvement over the baseline system has been obtained for known attack methods that were used in training the SSDs. However, improvement with unknown attack types was not substantial. Thus, the type of distortions that were caused by the unknown systems were different and could not be captured better with the proposed SSD compared to the baseline SSD."
"In display and mobile advertising, the most significant progress in recent years is the employment of the so-called Real-Time Bidding (RTB) mechanism to buy and sell ads. RTB essentially facilitates buying an individual ad impression in real time while it is still being generated from a user's visit. RTB not only scales up the buying process by aggregating a large amount of available inventories across publishers, but more importantly, enables directly targeting individual users. As such, RTB has fundamentally changed the landscape of the digital marketing. Scientifically, the demand for automation, integration, and optimization in RTB also brings new research opportunities in information retrieval, data mining, machine learning, and other related fields. In this monograph, we provide an overview of the fundamental infrastructure, algorithms, and technical challenges and their solutions of this new frontier of computational advertising. The topics we have covered include user response prediction, bid landscape forecasting, bidding algorithms, revenue optimisation, statistical arbitrage, dynamic pricing, and ad fraud detection."
"We study a two-tiered wireless sensor network (WSN) consisting of $N$ access points (APs) and $M$ base stations (BSs). The sensing data, which is distributed on the sensing field according to a density function $f$, is first transmitted to the APs and then forwarded to the BSs. Our goal is to find an optimal deployment of APs and BSs to minimize the average weighted total, or Lagrangian, of sensor and AP powers. For $M=1$, we show that the optimal deployment of APs is simply a linear transformation of the optimal $N$-level quantizer for density $f$, and the sole BS should be located at the geometric centroid of the sensing field. Also, for a one-dimensional network and uniform $f$, we determine the optimal deployment of APs and BSs for any $N$ and $M$. Moreover, to numerically optimize node deployment for general scenarios, we propose one- and two-tiered Lloyd algorithms and analyze their convergence properties. Simulation results show that, when compared to random deployment, our algorithms can save up to 79\% of the power on average."
"Sequence-to-sequence models have shown success in end-to-end speech recognition. However these models have only used shallow acoustic encoder networks. In our work, we successively train very deep convolutional networks to add more expressive power and better generalization for end-to-end ASR models. We apply network-in-network principles, batch normalization, residual connections and convolutional LSTMs to build very deep recurrent and convolutional structures. Our models exploit the spectral structure in the feature space and add computational depth without overfitting issues. We experiment with the WSJ ASR task and achieve 10.5\% word error rate without any dictionary or language using a 15 layer deep network."
"In this letter, we investigate how to extract deep feature representations based on convolutional neural networks (CNN) for high-resolution remote-sensing imagery retrieval (HRRSIR). Two effective schemes are proposed to generate the final feature rep-resentations for similarity measure. In the first scheme, the deep features are extracted from the fully-connected and convolutional layers of the pre-trained CNN models, respectively; in the second scheme, we fine-tune the pre-trained CNN model using the target remote sensing dataset to learn dataset-specific features. The deep feature representations generated by the two schemes are evalu-ated on two public and challenging datasets. The experimental results indicate that the proposed schemes are able to achieve state-of-the-art performance due to the good transferability of the CNN models."
"We present ABA+, a new approach to handling preferences in a well known structured argumentation formalism, Assumption-Based Argumentation (ABA). In ABA+, preference information given over assumptions is incorporated directly into the attack relation, thus resulting in attack reversal. ABA+ conservatively extends ABA and exhibits various desirable features regarding relationship among argumentation semantics as well as preference handling. We also introduce Weak Contraposition, a principle concerning reasoning with rules and preferences that relaxes the standard principle of contraposition, while guaranteeing additional desirable features for ABA+."
"We present a generalized, 3 dimensional version of the Purcell's swimmer which is a planar mechanism locomoting at low Reynlods number regime. We use Cox theory and resistive force theory to come up with the forces acting on the system. We finally come up with a purely kinematic form of the system's equations."
"For a $k$-ary predicate $P$, a random instance of CSP$(P)$ with $n$ variables and $m$ constraints is unsatisfiable with high probability when $m \gg n$. The natural algorithmic task in this regime is \emph{refutation}: finding a proof that a given random instance is unsatisfiable. Recent work of Allen et al. suggests that the difficulty of refuting CSP$(P)$ using an SDP is determined by a parameter $\mathrm{cmplx}(P)$, the smallest $t$ for which there does not exist a $t$-wise uniform distribution over satisfying assignments to $P$. In particular they show that random instances of CSP$(P)$ with $m \gg n^{\mathrm{cmplx(P)}/2}$ can be refuted efficiently using an SDP.   In this work, we give evidence that $n^{\mathrm{cmplx}(P)/2}$ constraints are also \emph{necessary} for refutation using SDPs. Specifically, we show that if $P$ supports a $(t-1)$-wise uniform distribution over satisfying assignments, then the Sherali-Adams$_+$ and Lov\'{a}sz-Schrijver$_+$ SDP hierarchies cannot refute a random instance of CSP$(P)$ in polynomial time for any $m \leq n^{t/2-\epsilon}$."
"We present the Macaulay2 package NumericalImplicitization, which allows for user-friendly computation of the basic invariants of the image of a polynomial map, such as dimension, degree, and Hilbert function values. This package relies on methods of numerical algebraic geometry, such as homotopy continuation and monodromy."
"We present a brief summary of current approaches and challenges in the network traffic management area. In this contribution, we well cover the most recent experiments on the network traffic and behavior of applications under various network conditions. Quality of the users' experience is the most important factor being considered in this group of experiments."
"Sketching techniques have become popular for scaling up machine learning algorithms by reducing the sample size or dimensionality of massive data sets, while still maintaining the statistical power of big data. In this paper, we study sketching from an optimization point of view: we first show that the iterative Hessian sketch is an optimization process with preconditioning, and develop accelerated iterative Hessian sketch via the searching the conjugate direction; we then establish primal-dual connections between the Hessian sketch and dual random projection, and apply the preconditioned conjugate gradient approach on the dual problem, which leads to the accelerated iterative dual random projection methods. Finally to tackle the challenges from both large sample size and high-dimensionality, we propose the primal-dual sketch, which iteratively sketches the primal and dual formulations. We show that using a logarithmic number of calls to solvers of small scale problem, primal-dual sketch is able to recover the optimum of the original problem up to arbitrary precision. The proposed algorithms are validated via extensive experiments on synthetic and real data sets which complements our theoretical results."
"A private information retrieval scheme is a mechanism that allows a user to retrieve any one out of $K$ messages from $N$ non-communicating replicated databases, each of which stores all $K$ messages, without revealing anything about the identity of the desired message index to any individual database. If the size of each message is $L$ bits and the total download required by a PIR scheme from all $N$ databases is $D$ bits, then $D$ is called the download cost and the ratio $L/D$ is called an achievable rate. For fixed $K,N\in\mathbb{N}$, the capacity of PIR, denoted by $C$, is the supremum of achievable rates over all PIR schemes and over all message sizes, and was recently shown to be $C=(1+1/N+1/N^2+\cdots+1/N^{K-1})^{-1}$. In this work, for arbitrary $K, N$, we explore the minimum download cost $D_L$ across all PIR schemes (not restricted to linear schemes) for arbitrary message lengths $L$ under arbitrary choices of alphabet (not restricted to finite fields) for the message and download symbols. If the same $M$-ary alphabet is used for the message and download symbols, then we show that the optimal download cost in $M$-ary symbols is $D_L=\lceil\frac{L}{C}\rceil$. If the message symbols are in $M$-ary alphabet and the downloaded symbols are in $M'$-ary alphabet, then we show that the optimal download cost in $M'$-ary symbols, $D_L\in\left\{\left\lceil \frac{L'}{C}\right\rceil,\left\lceil \frac{L'}{C}\right\rceil-1,\left\lceil \frac{L'}{C}\right\rceil-2\right\}$, where $L'= \lceil L \log_{M'} M\rceil$."
"Read-Copy Update (RCU) is a scalable, high-performance Linux-kernel synchronization mechanism that runs low-overhead readers concurrently with updaters. Production-quality RCU implementations for multi-core systems are decidedly non-trivial. Giving the ubiquity of Linux, a rare ""million-year"" bug can occur several times per day across the installed base. Stringent validation of RCU's complex behaviors is thus critically important. Exhaustive testing is infeasible due to the exponential number of possible executions, which suggests use of formal verification.   Previous verification efforts on RCU either focus on simple implementations or use modeling languages, the latter requiring error-prone manual translation that must be repeated frequently due to regular changes in the Linux kernel's RCU implementation. In this paper, we first describe the implementation of Tree RCU in the Linux kernel. We then discuss how to construct a model directly from Tree RCU's source code in C, and use the CBMC model checker to verify its safety and liveness properties. To our best knowledge, this is the first verification of a significant part of RCU's source code, and is an important step towards integration of formal verification into the Linux kernel's regression test suite."
"Flexible numerologies are being considered as part of designs for 5G systems to support vertical services with diverse requirements such as enhanced mobile broadband, ultra-reliable low-latency communications, and massive machine type communication. Different vertical services can be multiplexed in either frequency domain, time domain, or both. In this paper, we investigate the use of spatial multiplexing of services using MU-MIMO where the numerologies for different users may be different. The users are grouped according to the chosen numerology and a separate pre-coder and FFT size is used per numerology at the transmitter. The pre-coded signals for the multiple numerologies are added in the time domain before transmission. We analyze the performance gains of this approach using capacity analysis and link level simulations using conjugate beamforming and signal-to-leakage noise ratio maximization techniques. We show that the MU interference between users with different numerologies can be suppressed efficiently with reasonable number of antennas at the base-station. This feature enables MU-MIMO techniques to be applied for 5G across different numerologies."
"The standard linear regression (SLR) problem is to recover a vector $\mathbf{x}^0$ from noisy linear observations $\mathbf{y}=\mathbf{Ax}^0+\mathbf{w}$. The approximate message passing (AMP) algorithm recently proposed by Donoho, Maleki, and Montanari is a computationally efficient iterative approach to SLR that has a remarkable property: for large i.i.d.\ sub-Gaussian matrices $\mathbf{A}$, its per-iteration behavior is rigorously characterized by a scalar state-evolution whose fixed points, when unique, are Bayes optimal. AMP, however, is fragile in that even small deviations from the i.i.d.\ sub-Gaussian model can cause the algorithm to diverge. This paper considers a ""vector AMP"" (VAMP) algorithm and shows that VAMP has a rigorous scalar state-evolution that holds under a much broader class of large random matrices $\mathbf{A}$: those that are right-rotationally invariant. After performing an initial singular value decomposition (SVD) of $\mathbf{A}$, the per-iteration complexity of VAMP can be made similar to that of AMP. In addition, the fixed points of VAMP's state evolution are consistent with the replica prediction of the minimum mean-squared error recently derived by Tulino, Caire, Verd\'u, and Shamai. The effectiveness and state evolution predictions of VAMP are confirmed in numerical experiments."
"A b-coloring of the vertices of a graph is a proper coloring where each color class contains a vertex which is adjacent to each other color class. The b-chromatic number of $G$ is the maximum integer $\chi_b(G)$ for which $G$ has a b-coloring with $\chi_b(G)$ colors. A graph $G$ is b-continuous if $G$ has a b-coloring with $k$ colors, for every integer $k$ in the interval $[\chi(G),\chi_b(G)]$. It is known that not all graphs are b-continuous. Here, we investigate whether the lexicographic product $G[H]$ of b-continuous graphs $G$ and $H$ is also b-continuous. Using homomorphisms, we provide a new lower bound for $\chi_b(G[H])$, namely $\chi_b(G[K_t])$, where $t=\chi_b(H)$, and prove that if $G[K_\ell]$ is b-continuous for every positive integer $\ell$, then $G[H]$ admits a b-coloring with $k$ colors, for every $k$ in the interval $[\chi(G[H]),\chi_b(G[K_t])]$. We also prove that $G[K_\ell]$ is b-continuous, for every positive integer $\ell$, whenever $G$ is a $P_4$-sparse graph, and we give further results on the b-spectrum of $G[K_\ell]$, when $G$ is chordal."
"In this paper, we study the performance of the downlink of a cellular network with automatic repeat-request (ARQ) and a half duplex decode-and-forward shared relay. In this system, two multiple-input-multiple-output (MIMO) base stations serve two single antenna users. A MIMO shared relay retransmits the lost packets to the target users. First, we study the system with direct retransmission from the base station and derive a closed form expression for the outage probability of the system.We show that the direct retransmission can overcome the fading, however, it cannot overcome the interference. After that, we invoke the shared relay and design the relay beamforming matrices such that the signal-to-interference-and-noise ratio (SINR) is improved at the users subject to power constraints on the relay. In the case when the transmission of only one user fails, we derive a closed form solution for the relay beamformers. On the other hand when both transmissions fail, we pose the beamforming problem as a sequence of non-convex feasibility problems. We use semidefinite relaxation (SDR) to convert each feasibility problem into a convex optimization problem. We ensure a rank one solution, and hence, there is no loss of optimality in SDR. Simulation results are presented showing the superior performance of the proposed relay beamforming strategy compared to direct ARQ system in terms of the outage probability."
"Recent work in distance metric learning has focused on learning transformations of data that best align with specified pairwise similarity and dissimilarity constraints, often supplied by a human observer. The learned transformations lead to improved retrieval, classification, and clustering algorithms due to the better adapted distance or similarity measures. Here, we address the problem of learning these transformations when the underlying constraint generation process is nonstationary. This nonstationarity can be due to changes in either the ground-truth clustering used to generate constraints or changes in the feature subspaces in which the class structure is apparent. We propose Online Convex Ensemble StrongLy Adaptive Dynamic Learning (OCELAD), a general adaptive, online approach for learning and tracking optimal metrics as they change over time that is highly robust to a variety of nonstationary behaviors in the changing metric. We apply the OCELAD framework to an ensemble of online learners. Specifically, we create a retro-initialized composite objective mirror descent (COMID) ensemble (RICE) consisting of a set of parallel COMID learners with different learning rates, demonstrate RICE-OCELAD on both real and synthetic data sets and show significant performance improvements relative to previously proposed batch and online distance metric learning algorithms."
"In this paper, we propose a novel neural approach for paraphrase generation. Conventional para- phrase generation methods either leverage hand-written rules and thesauri-based alignments, or use statistical machine learning principles. To the best of our knowledge, this work is the first to explore deep learning models for paraphrase generation. Our primary contribution is a stacked residual LSTM network, where we add residual connections between LSTM layers. This allows for efficient training of deep LSTMs. We evaluate our model and other state-of-the-art deep learning models on three different datasets: PPDB, WikiAnswers and MSCOCO. Evaluation results demonstrate that our model outperforms sequence to sequence, attention-based and bi- directional LSTM models on BLEU, METEOR, TER and an embedding-based sentence similarity metric."
"Data-driven research is increasingly ubiquitous and data itself is a defining asset for researchers, particularly in the computational social sciences and humanities. Entire careers and research communities are built around valuable, proprietary or sensitive datasets. However, many existing computation resources fail to support secure and cost-effective storage of data while also enabling secure and flexible analysis of the data. To address these needs we present CLOUD KOTTA, a cloud-based architecture for the secure management and analysis of social science data. CLOUD KOTTA leverages reliable, secure, and scalable cloud resources to deliver capabilities to users, and removes the need for users to manage complicated infrastructure. CLOUD KOTTA implements automated, cost-aware models for efficiently provisioning tiered storage and automatically scaled compute resources. CLOUD KOTTA has been used in production for several months and currently manages approximately 10TB of data and has been used to process more than 5TB of data with over 75,000 CPU hours. It has been used for a broad variety of text analysis workflows, matrix factorization, and various machine learning algorithms, and more broadly, it supports fast, secure and cost-effective research."
"Term weighting metrics assign weights to terms in order to discriminate the important terms from the less crucial ones. Due to this characteristic, these metrics have attracted growing attention in text classification and recently in sentiment analysis. Using the weights given by such metrics could lead to more accurate document representation which may improve the performance of the classification. While previous studies have focused on proposing or comparing different weighting metrics at two-classes document level sentiment analysis, this study propose to analyse the results given by each metric in order to find out the characteristics of good and bad weighting metrics. Therefore we present an empirical study of fifteen global supervised weighting metrics with four local weighting metrics adopted from information retrieval, we also give an analysis to understand the behavior of each metric by observing and analysing how each metric distributes the terms and deduce some characteristics which may distinguish the good and bad metrics. The evaluation has been done using Support Vector Machine on three different datasets: Twitter, restaurant and laptop reviews."
"Distributed communities of researchers rely increasingly on valuable, proprietary, or sensitive datasets. Given the growth of such data, especially in fields new to data-driven, computationally intensive research like the social sciences and humanities, coupled with what are often strict and complex data-use agreements, many research communities now require methods that allow secure, scalable and cost-effective storage and analysis. Here we present CLOUD KOTTA: a cloud-based data management and analytics framework. CLOUD KOTTA delivers an end-to-end solution for coordinating secure access to large datasets, and an execution model that provides both automated infrastructure scaling and support for executing analytics near to the data. CLOUD KOTTA implements a fine-grained security model ensuring that only authorized users may access, analyze, and download protected data. It also implements automated methods for acquiring and configuring low-cost storage and compute resources as they are needed. We present the architecture and implementation of CLOUD KOTTA and demonstrate the advantages it provides in terms of increased performance and flexibility. We show that CLOUD KOTTA's elastic provisioning model can reduce costs by up to 16x when compared with statically provisioned models."
"Social norms are shared rules that govern and facilitate social interaction. Violating such social norms via teasing and insults may serve to upend power imbalances or, on the contrary reinforce solidarity and rapport in conversation, rapport which is highly situated and context-dependent. In this work, we investigate the task of automatically identifying the phenomena of social norm violation in discourse. Towards this goal, we leverage the power of recurrent neural networks and multimodal information present in the interaction, and propose a predictive model to recognize social norm violation. Using long-term temporal and contextual information, our model achieves an F1 score of 0.705. Implications of our work regarding developing a social-aware agent are discussed."
"The classic supervised classification algorithms are efficient, but time-consuming, complicated and not interpretable, which makes it difficult to analyze their results that limits the possibility to improve them based on real observations. In this paper, we propose a new and a simple classifier to predict a sentiment label of a short text. This model keeps the capacity of human interpret-ability and can be extended to integrate NLP techniques in a more interpretable way. Our model is based on a correlation metric which measures the degree of association between a sentiment label and a word. Ten correlation metrics are proposed and evaluated intrinsically. And then a classifier based on each metric is proposed, evaluated and compared to the classic classification algorithms which have proved their performance in many studies. Our model outperforms these algorithms with several correlation metrics."
"We consider computing a longest palindrome in the streaming model, where the symbols arrive one-by-one and we do not have random access to the input. While computing the answer exactly using sublinear space is not possible in such a setting, one can still hope for a good approximation guarantee. Our contribution is twofold. First, we provide lower bounds on the space requirements for randomized approximation algorithms processing inputs of length $n$. We rule out Las Vegas algorithms, as they cannot achieve sublinear space complexity. For Monte Carlo algorithms, we prove a lower bounds of $\Omega( M \log\min\{|\Sigma|,M\})$ bits of memory; here $M=n/E$ for approximating the answer with additive error $E$, and $M= \frac{\log n}{\log (1+\varepsilon)}$ for approximating the answer with multiplicative error $(1 + \varepsilon)$. Second, we design three real-time algorithms for this problem. Our Monte Carlo approximation algorithms for both additive and multiplicative versions of the problem use $O(M)$ words of memory. Thus the obtained lower bounds are asymptotically tight up to a logarithmic factor. The third algorithm is deterministic and finds a longest palindrome exactly if it is short. This algorithm can be run in parallel with a Monte Carlo algorithm to obtain better results in practice. Overall, both the time and space complexity of finding a longest palindrome in a stream are essentially settled."
"Extracting shape information from object bound- aries is a well studied problem in vision, and has found tremen- dous use in applications like object recognition. Conversely, studying the space of shapes represented by curves satisfying certain constraints is also intriguing. In this paper, we model and analyze the space of shapes represented by a 3D curve (space curve) formed by connecting n pieces of quarter of a unit circle. Such a space curve is what we call a Tangle, the name coming from a toy built on the same principle. We provide two models for the shape space of n-link open and closed tangles, and we show that tangles are a subset of trigonometric splines of a certain order. We give algorithms for curve approximation using open/closed tangles, computing geodesics on these shape spaces, and to find the deformation that takes one given tangle to another given tangle, i.e., the Log map. The algorithms provided yield tangles upto a small and acceptable tolerance, as shown by the results given in the paper."
"We present a protocol that transforms any quantum multi-prover interactive proof into a nonlocal game in which questions consist of logarithmic number of bits and answers of constant number of bits. As a corollary, this proves that the promise problem corresponding to the approximation of the nonlocal value to inverse polynomial accuracy is complete for QMIP*, and therefore NEXP-hard. This establishes that nonlocal games are provably harder than classical games without any complexity theory assumptions. Our result also indicates that gap amplification for nonlocal games may be impossible in general and provides a negative evidence for the possibility of the gap amplification approach to the multi-prover variant of the quantum PCP conjecture."
"People enjoy encounters with generative software, but rarely are they encouraged to interact with, understand or engage with it. In this paper we define the term 'PCG-based game', and explain how this concept follows on from the idea of an AI-based game. We look at existing examples of games which foreground their AI, put forward a methodology for designing PCG-based games, describe some example case study designs for PCG-based games, and describe lessons learned during this process of sketching and developing ideas."
"The Massive Open Online Course (MOOC) has expanded significantly in recent years. With the widespread of MOOC, the opportunity to study the fascinating courses for free has attracted numerous people of diverse educational backgrounds all over the world. In the big data era, a key research topic for MOOC is how to mine the needed courses in the massive course databases in cloud for each individual student accurately and rapidly as the number of courses is increasing fleetly. In this respect, the key challenge is how to realize personalized course recommendation as well as to reduce the computing and storage costs for the tremendous course data. In this paper, we propose a big data-supported, context-aware online learning-based course recommender system that could handle the dynamic and infinitely massive datasets, which recommends courses by using personalized context information and historical statistics. The context-awareness takes the personal preferences into consideration, making the recommendation suitable for people with different backgrounds. Besides, the algorithm achieves the sublinear regret performance, which means it can gradually recommend the mostly preferred and matched courses to students. In addition, our storage module is expanded to the distributed-connected storage nodes, where the devised algorithm can handle massive course storage problems from heterogeneous sources of course datasets. Comparing to existing algorithms, our proposed algorithms achieve the linear time complexity and space complexity. Experiment results verify the superiority of our algorithms when comparing with existing ones in the MOOC big data setting."
"A program can be viewed as a syntactic structure P (syntactic skeleton) parameterized by a collection of the identifiers V (variable names). This paper introduces the skeletal program enumeration (SPE) problem: Given a fixed syntactic skeleton P and a set of variables V , enumerate a set of programs P exhibiting all possible variable usage patterns within P. It proposes an effective realization of SPE for systematic, rigorous compiler testing by leveraging three important observations: (1) Programs with different variable usage patterns exhibit diverse control- and data-dependence information, and help exploit different compiler optimizations and stress-test compilers; (2) most real compiler bugs were revealed by small tests (i.e., small-sized P) --- this ""small-scope"" observation opens up SPE for practical compiler validation; and (3) SPE is exhaustive w.r.t. a given syntactic skeleton and variable set, and thus can offer a level of guarantee that is absent from all existing compiler testing techniques.   The key challenge of SPE is how to eliminate the enormous amount of equivalent programs w.r.t. {\alpha}-conversion. Our main technical contribution is a novel algorithm for computing the canonical (and smallest) set of all non-{\alpha}-equivalent programs. We have realized our SPE technique and evaluated it using syntactic skeletons derived from GCC's testsuite. Our evaluation results on testing GCC and Clang are extremely promising. In less than six months, our approach has led to 217 confirmed bug reports, 104 of which have already been fixed, and the majority are long latent bugs despite the extensive prior efforts of automatically testing both compilers (e.g., Csmith and EMI). The results also show that our algorithm for enumerating non-{\alpha}-equivalent programs provides six orders of magnitude reduction, enabling processing the GCC test-suite in under a month."
"We introduce FaceVR, a novel method for gaze-aware facial reenactment in the Virtual Reality (VR) context. The key component of FaceVR is a robust algorithm to perform real-time facial motion capture of an actor who is wearing a head-mounted display (HMD), as well as a new data-driven approach for eye tracking from monocular videos. In addition to these face reconstruction components, FaceVR incorporates photo-realistic re-rendering in real time, thus allowing artificial modifications of face and eye appearances. For instance, we can alter facial expressions, change gaze directions, or remove the VR goggles in realistic re-renderings. In a live setup with a source and a target actor, we apply these newly-introduced algorithmic components. We assume that the source actor is wearing a VR device, and we capture his facial expressions and eye movement in real-time. For the target video, we mimic a similar tracking process; however, we use the source input to drive the animations of the target video, thus enabling gaze-aware facial reenactment. To render the modified target video on a stereo display, we augment our capture and reconstruction process with stereo data. In the end, FaceVR produces compelling results for a variety of applications, such as gaze-aware facial reenactment, reenactment in virtual reality, removal of VR goggles, and re-targeting of somebody's gaze direction in a video conferencing call."
"Convolutional Neural Networks (CNN) have demon- strated its successful applications in computer vision, speech recognition, and natural language processing. For object recog- nition, CNNs might be limited by its strict label requirement and an implicit assumption that images are supposed to be target- object-dominated for optimal solutions. However, the labeling procedure, necessitating laying out the locations of target ob- jects, is very tedious, making high-quality large-scale dataset prohibitively expensive. Data augmentation schemes are widely used when deep networks suffer the insufficient training data problem. All the images produced through data augmentation share the same label, which may be problematic since not all data augmentation methods are label-preserving. In this paper, we propose a weakly supervised CNN framework named Multiple Instance Learning Convolutional Neural Networks (MILCNN) to solve this problem. We apply MILCNN framework to object recognition and report state-of-the-art performance on three benchmark datasets: CIFAR10, CIFAR100 and ILSVRC2015 classification dataset."
"Modern robotics applications that involve human-robot interaction require robots to be able to communicate with humans seamlessly and effectively. Natural language provides a flexible and efficient medium through which robots can exchange information with their human partners. Significant advancements have been made in developing robots capable of interpreting free-form instructions, but less attention has been devoted to endowing robots with the ability to generate natural language. We propose a navigational guide model that enables robots to generate natural language instructions that allow humans to navigate a priori unknown environments. We first decide which information to share with the user according to their preferences, using a policy trained from human demonstrations via inverse reinforcement learning. We then ""translate"" this information into a natural language instruction using a neural sequence-to-sequence model that learns to generate free-form instructions from natural language corpora. We evaluate our method on a benchmark route instruction dataset and achieve a BLEU score of 72.18% when compared to human-generated reference instructions. We additionally conduct navigation experiments with human participants that demonstrate that our method generates instructions that people follow as accurately and easily as those produced by humans."
"Long short-term memory (LSTM) recurrent neural networks (RNNs) have been shown to give state-of-the-art performance on many speech recognition tasks, as they are able to provide the learned dynamically changing contextual window of all sequence history. On the other hand, the convolutional neural networks (CNNs) have brought significant improvements to deep feed-forward neural networks (FFNNs), as they are able to better reduce spectral variation in the input signal. In this paper, a network architecture called as convolutional recurrent neural network (CRNN) is proposed by combining the CNN and LSTM RNN. In the proposed CRNNs, each speech frame, without adjacent context frames, is organized as a number of local feature patches along the frequency axis, and then a LSTM network is performed on each feature patch along the time axis. We train and compare FFNNs, LSTM RNNs and the proposed LSTM CRNNs at various number of configurations. Experimental results show that the LSTM CRNNs can exceed state-of-the-art speech recognition performance."
"In this paper, we empirically explore the effects of various kinds of skip connections in stacked bidirectional LSTMs for sequential tagging. We investigate three kinds of skip connections connecting to LSTM cells: (a) skip connections to the gates, (b) skip connections to the internal states and (c) skip connections to the cell outputs. We present comprehensive experiments showing that skip connections to cell outputs outperform the remaining two. Furthermore, we observe that using gated identity functions as skip mappings works pretty well. Based on this novel skip connections, we successfully train deep stacked bidirectional LSTM models and obtain state-of-the-art results on CCG supertagging and comparable results on POS tagging."
"Nowadays, asynchronous motors have wide range use in many industrial applications. Field oriented control (FOC) and direct torque control (DTC) are commonly used methods in high performance vector control for asynchronous motors. Therefore, it is very important to identify clearly advantages and disadvantages of both systems in the selection of appropriate control methods for many industrial applications. This paper aims to present a new and different perspective regarding the comparison of the switching behaviours on the FOC and the DTC drivers. For this purpose, the experimental studies have been carried out to compare the inverter switching frequencies and torque responses of the asynchronous motor in the FOC and the DTC systems under different working conditions. The dSPACE 1103 controller board was programmed with Matlab/Simulink software. As expected, the experimental studies showed that the FOC controlled motors has a lessened torque ripple. On the other hand, the FOC controlled motor switching frequency has about 65-75% more than the DTC controlled under both loaded and unloaded working conditions."
"There are several metrics (Modularity, Mutual Information, Conductance, etc.) to evaluate the strength of graph clustering in large graphs. These metrics have great significance to measure the effectiveness and they are often used to find the strongly connected clusters with respect to the whole graph. In this paper, we propose a new metric to evaluate the strength of graph clustering and also study its applications. We show that our proposed metric has great consistency which is similar to other metrics and easy to calculate. Our proposed metric also shows consistency where other metrics fail in some special cases. We demonstrate that our metric has reasonable strength while extracting strongly connected communities in both simulated (in silico) data and real data networks. We also show some comparative results of our proposed metric with other popular metric(s) for Online Social Networks (OSN) and Gene Regulatory Networks (GRN)."
"This study concerns the security challenges that the people face in the usage and implementation of cloud computing. Despite its growth in the past few decades, this platform has experienced different challenges. They all arise from the concern of data safety that the nature of sharing in the cloud presents. This paper looks to identify the benefits of using a cloud computing platform and the issue of information security. The paper also reviews the concept of information hiding and its relevance to the cloud. This technique has two ways about it that impact how people use cloud computing in their organizations and even for personal implementations. First it presents the potential to circulate harmful information and files that can adversely affect the data those users upload on those platforms. It is also the basis of the strategies such as steganalysis and cryptographic storage architecture that are essential for data security."
"Agile development gets more appreciation from the market due to the flexible nature and more productivity. Among the Agile processes, Scrum gives better management of the processes, which are practiced in an organization. Though Scrum process model have several features and strengths but it still lacks in management and quality. This research deals with the improvement of Scrum processes for better management and to improve the quality of the software using the infusion of different practices from internationally renowned quality standards. Survey is used to validate the proposed framework. Statistical analysis shows that the proposed research has a profound effect on Scrum model to develop high quality software."
"The busy beaver problem is a well-known example of a non-computable function. In order to determine a particular value of this function, it is necessary to generate and classify a large number of Turing machines. Previous work on this problem has described the processes used for the generation and classification of these machines, but unfortunately has generally not provided details of the machines considered. While there is no reason to doubt the veracity of the results known so far, it is difficult to accept such results as scientifically proven without being able to inspect the appropriate evidence. In addition, a list of machines and their classifications can be used for other results, such as variations on the busy beaver problem and related problems such as the placid platypus problem. In this paper we investigate how to generate classes of machines to be considered for the busy beaver problem. We discuss the relationship between quadruple and quintuple variants of Turing machines, and show that the latter are more general than the former. We give some formal results to justify our strategy for minimising the number of machines generated, and define a process reflecting this strategy for generating machines. We describe our implementation, and the results of generating various classes of machines with up to 5 states or up to 5 symbols, all of which (together with our code) are available on the author's website."
"This paper investigates the effects of limited speech data in the context of speaker verification using deep neural network (DNN) approach. Being able to reduce the length of required speech data is important to the development of speaker verification system in real world applications. The experimental studies have found that DNN-senone-based Gaussian probabilistic linear discriminant analysis (GPLDA) system respectively achieves above 50% and 18% improvements in EER values over GMM-UBM GPLDA system on NIST 2010 coreext-coreext and truncated 15sec-15sec evaluation conditions. Further when GPLDA model is trained on short-length utterances (30sec) rather than full-length utterances (2min), DNN-senone GPLDA system achieves above 7% improvement in EER values on truncated 15sec-15sec condition. This is because short length development i-vectors have speaker, session and phonetic variation and GPLDA is able to robustly model those variations. For several real world applications, longer utterances (2min) can be used for enrollment and shorter utterances (15sec) are required for verification, and in those conditions, DNN-senone GPLDA system achieves above 26% improvement in EER values over GMM-UBM GPLDA systems."
"The effort to extend cellular technologies to unlicensed spectrum has been gaining high momentum. Listen-before-talk (LBT) is enforced in the regions such as European Union and Japan to harmonize coexistence of cellular and incumbent systems in unlicensed spectrum. In this paper, we study throughput optimal LBT transmission strategy for load based equipment (LBE). We find that the optimal rule is a pure threshold policy: The LBE should stop listening and transmit once the channel quality exceeds an optimized threshold. We also reveal the optimal set of LBT parameters that are compliant with regulatory requirements. Our results shed light on how the regulatory LBT requirements can affect the transmission strategies of radio equipment in unlicensed spectrum."
"As the world becomes increasingly dependent on technology, researchers in both industry and academia endeavor to understand how technology is used, the impact it has on everyday life, the artifact life-cycle and overall integrations of digital information. In doing so, researchers are increasingly gathering 'real-world' or 'in-the-wild' residual data, obtained from a variety of sources, without the explicit consent of the original owners. This data gathering raises significant concerns regarding privacy, ethics and legislation, as well as practical considerations concerning investigator training, data storage, overall security and data disposal. This research surveys recent studies of residual data gathered in-the-wild and analyzes the challenges that were confronted. Amalgamating these insights, the research presents a compendium of practices for addressing the issues that can arise in-the-wild when conducting residual data research. The practices identified in this research can be used to critique current projects and assess the feasibility of proposed future research."
"Performing a stability analysis during the design of any electronic circuit is critical to guarantee its correct operation. A closed-loop stability analysis can be performed by analysing the impedance presented by the circuit at a well-chosen node without internal access to the simulator. If any of the poles of this impedance lie in the complex right half-plane, the circuit is unstable. The classic way to detect unstable poles is to fit a rational model on the impedance. This rational approximation has to deal with model order selection, which is difficult in circuits with transmission lines.   In this paper, a projection-based method is proposed which splits the impedance into a stable and an unstable part by projecting on an orthogonal basis of stable and unstable functions. Working with a projection instead of a rational approximation greatly simplifies the stability analysis. When the projection is mapped from the complex plane to the unit disc, the projection boils down to calculating a Fourier series. If a significant part of the impedance is projected on the unstable part, a low-order rational approximation is fitted on this unstable part to find the location of the unstable poles."
"Due to the requirement of hosting tens of thousands of hosts in today's data centers, data center networks strive for scalability and high throughput on the one hand. On the other hand, the cost for networking hardware should be minimized. Consequently, the number and complexity (e.g. TCAM size) of switches has to be minimized. These requirements led to network topologies like Clos and Leaf-Spine networks only requiring a shallow hierarchy of switches---two levels for Leaf-Spine networks. The drawback of these topologies is that switches at higher levels like Spine switches need a high port density and, thus, are expensive and limit the scalability of the network.   In this paper, we propose a data center network topology based on De Bruijn graphs completely avoiding a switch hierarchy and implementing a flat network topology of top-of-rack switches instead. This topology guarantees logarithmic (short) path length. We show that the required routing logic can be implemented by standard prefix matching operations in hardware (TCAM) allowing for using commodity switches without any modification. Moreover, forwarding requires only a very small number of forwarding table entries, saving costly and energy-intensive TCAM."
We are developing the method to start new instances of NELL in various languages and develop then NELL multilingualism. We base our method on our experience on NELL Portuguese and NELL French. This reports explain our method and develops some research perspectives.
"Recently, attempts have been made to remove Gaussian mixture models (GMM) from the training process of deep neural network-based hidden Markov models (HMM/DNN). For the GMM-free training of a HMM/DNN hybrid we have to solve two problems, namely the initial alignment of the frame-level state labels and the creation of context-dependent states. Although flat-start training via iteratively realigning and retraining the DNN using a frame-level error function is viable, it is quite cumbersome. Here, we propose to use a sequence-discriminative training criterion for flat start. While sequence-discriminative training is routinely applied only in the final phase of model training, we show that with proper caution it is also suitable for getting an alignment of context-independent DNN models. For the construction of tied states we apply a recently proposed KL-divergence-based state clustering method, hence our whole training process is GMM-free. In the experimental evaluation we found that the sequence-discriminative flat start training method is not only significantly faster than the straightforward approach of iterative retraining and realignment, but the word error rates attained are slightly better as well."
"A network of analog repeaters, each fed by a wireless fronthaul link and powered by e.g., solar energy, is a promising candidate for a flexible small cell deployment. A key challenge is the acquisition of accurate channel state information by the fronthaul hub (FH), which is needed for the spatial multiplexing of multiple fronthaul links over the same time/frequency resource. For frequency division duplex channels, a simple pilot loop-back procedure has been proposed that allows the estimation of the UL & DL channels at the FH without relying on any digital signal processing at the repeater side. For this scheme, we derive the maximum likelihood (ML) estimators for the UL & DL channel subspaces, formulate the corresponding Cram\'er-Rao bounds and show the asymptotic efficiency of both (SVD-based) estimators by means of Monte Carlo simulations. In addition, we illustrate how to compute the underlying (rank-1) SVD with quadratic time complexity by employing the power iteration method. To enable power control for the fronthaul links, knowledge of the channel gains is needed. Assuming that the UL & DL channels have on average the same gain, we formulate the ML estimator for the UL channel gain, and illustrate its robustness against strong noise by means of simulations."
"We consider the model reduction problem for linear time-invariant dynamical systems having nonzero (but otherwise indeterminate) initial conditions. Building upon the observation that the full system response is decomposable as a superposition of the response map for an unforced system having nontrivial initial conditions and the response map for a forced system having null initial conditions, we develop a new approach that involves reducing these component responses independently and then combining the reduced responses into an aggregate reduced system response. This approach allows greater flexibility and offers better approximation properties than other comparable methods."
"It is generally difficult to make any statements about the expected prediction error in an univariate setting without further knowledge about how the data were generated. Recent work showed that knowledge about the real underlying causal structure of a data generation process has implications for various machine learning settings. Assuming an additive noise and an independence between data generating mechanism and its input, we draw a novel connection between the intrinsic causal relationship of two variables and the expected prediction error. We formulate the theorem that the expected error of the true data generating function as prediction model is generally smaller when the effect is predicted from its cause and, on the contrary, greater when the cause is predicted from its effect. The theorem implies an asymmetry in the error depending on the prediction direction. This is further corroborated with empirical evaluations in artificial and real-world data sets."
"The problem of merging sorted lists in the least number of pairwise comparisons has been solved completely only for a few special cases. Graham and Karp \cite{taocp} independently discovered that the tape merge algorithm is optimal in the worst case when the two lists have the same size. In the seminal papers, Stockmeyer and Yao\cite{yao}, Murphy and Paull\cite{3k3}, and Christen\cite{christen1978optimality} independently showed when the lists to be merged are of size $m$ and $n$ satisfying $m\leq n\leq\lfloor\frac{3}{2}m\rfloor+1$, the tape merge algorithm is optimal in the worst case. This paper extends this result by showing that the tape merge algorithm is optimal in the worst case whenever the size of one list is no larger than 1.52 times the size of the other. The main tool we used to prove lower bounds is Knuth's adversary methods \cite{taocp}. In addition, we show that the lower bound cannot be improved to 1.8 via Knuth's adversary methods. We also develop a new inequality about Knuth's adversary methods, which might be interesting in its own right. Moreover, we design a simple procedure to achieve constant improvement of the upper bounds for $2m-2\leq n\leq 3m $."
"Autonomous driving is a multi-agent setting where the host vehicle must apply sophisticated negotiation skills with other road users when overtaking, giving way, merging, taking left and right turns and while pushing ahead in unstructured urban roadways. Since there are many possible scenarios, manually tackling all possible cases will likely yield a too simplistic policy. Moreover, one must balance between unexpected behavior of other drivers/pedestrians and at the same time not to be too defensive so that normal traffic flow is maintained.   In this paper we apply deep reinforcement learning to the problem of forming long term driving strategies. We note that there are two major challenges that make autonomous driving different from other robotic tasks. First, is the necessity for ensuring functional safety - something that machine learning has difficulty with given that performance is optimized at the level of an expectation over many instances. Second, the Markov Decision Process model often used in robotics is problematic in our case because of unpredictable behavior of other agents in this multi-agent scenario. We make three contributions in our work. First, we show how policy gradient iterations can be used without Markovian assumptions. Second, we decompose the problem into a composition of a Policy for Desires (which is to be learned) and trajectory planning with hard constraints (which is not learned). The goal of Desires is to enable comfort of driving, while hard constraints guarantees the safety of driving. Third, we introduce a hierarchical temporal abstraction we call an ""Option Graph"" with a gating mechanism that significantly reduces the effective horizon and thereby reducing the variance of the gradient estimation even further."
"We show that CSP is fixed-parameter tractable when parameterized by the treewidth of a backdoor into any tractable CSP problem over a finite constraint language. This result combines the two prominent approaches for achieving tractability for CSP: (i) by structural restrictions on the interaction between the variables and the constraints and (ii) by language restrictions on the relations that can be used inside the constraints. Apart from defining the notion of backdoor-treewidth and showing how backdoors of small treewidth can be used to efficiently solve CSP, our main technical contribution is a fixed-parameter algorithm that finds a backdoor of small treewidth."
"Dixon's famous theorem states that the group generated by two random permutations of a finite set is generically either the whole symmetric group or the alternating group. In the context of random generation of finite groups this means that it is hopeless to wish for a uniform distribution -- or even a non-trivial one -- by drawing random permutations and looking at the generated group. Mealy automata are a powerful tool to generate groups, including all finite groups and many interesting infinite ones, whence the idea of generating random finite groups by drawing random Mealy automata. In this paper we show that, for a special class of Mealy automata that generate only finite groups, the distribution is far from being uniform since the obtained groups are generically a semi-direct product between a direct product of alternating groups and a group generated by a tuple of transpositions."
"This paper proposes and validates an in situ calibration method to calibrate six axis force torque (F/T) sensors once they are mounted on the system. This procedure takes advantage of the knowledge of the model of the robot to generate the expected wrenches of the sensors during some arbitrary motions. It then uses this information to train and validate new calibration matrices, taking into account the calibration matrix obtained with a classical Workbench calibration. The proposed calibration algorithm is validated on the F/T sensors mounted on the iCub humanoid robot legs."
"Maximum Inner Product Search (MIPS) is an important task in many machine learning applications such as the prediction phase of a low-rank matrix factorization model for a recommender system. There have been some works on how to perform MIPS in sub-linear time recently. However, most of them do not have the flexibility to control the trade-off between search efficient and search quality. In this paper, we study the MIPS problem with a computational budget. By carefully studying the problem structure of MIPS, we develop a novel Greedy-MIPS algorithm, which can handle budgeted MIPS by design. While simple and intuitive, Greedy-MIPS yields surprisingly superior performance compared to state-of-the-art approaches. As a specific example, on a candidate set containing half a million vectors of dimension 200, Greedy-MIPS runs 200x faster than the naive approach while yielding search results with the top-5 precision greater than 75\%."
"Keystroke dynamics have been extensively used in psycholinguistic and writing research to gain insights into cognitive processing. But do keystroke logs contain actual signal that can be used to learn better natural language processing models?   We postulate that keystroke dynamics contain information about syntactic structure that can inform shallow syntactic parsing. To test this hypothesis, we explore labels derived from keystroke logs as auxiliary task in a multi-task bidirectional Long Short-Term Memory (bi-LSTM). Our results show promising results on two shallow syntactic parsing tasks, chunking and CCG supertagging. Our model is simple, has the advantage that data can come from distinct sources, and produces models that are significantly better than models trained on the text annotations alone."
"We present a distributed full-text index for big data applications in a distributed environment. The index can be used to answer different types of pattern matching queries (existential, counting and enumeration) and also be extended to answer document retrieval queries (counting, retrieve and top-$k$). We also show that succinct data structures are indeed useful for big data applications, as their low memory consumption allows us to build indices for larger slices of text in the main memory."
"Model driven development is an effective method due to its benefits such as code transformation, increasing productivity and reducing human based error possibilities. Meanwhile, agile software development increases the software flexibility and customer satisfaction by using iterative method. Can these two development approaches be combined to develop web applications efficiently? What are the challenges and what are the benefits of this approach? In this paper, we answer these two crucial problems; combining model driven development and agile software development results in not only fast development and easiness of the user interface design but also efficient job tracking. We also defined an agile model based approach for web applications whose implementation study has been carried out to support the answers we gave these two crucial problems."
"We say a string has a cadence if a certain character is repeated at regular intervals, possibly with intervening occurrences of that character. We call the cadence anchored if the first interval must be the same length as the others. We give a sub-quadratic algorithm for determining whether a string has any cadence consisting of at least three occurrences of a character, and a nearly linear algorithm for finding all anchored cadences."
"In this paper, we propose an automatic and mechanized license and number plate recognition (LNPR) system which can extract the license plate number of the vehicles passing through a given location using image processing algorithms. No additional devices such as GPS or radio frequency identification (RFID) need to be installed for implementing the proposed system. Using special cameras, the system takes pictures from each passing vehicle and forwards the image to the computer for being processed by the LPR software. Plate recognition software uses different algorithms such as localization, orientation, normalization, segmentation and finally optical character recognition (OCR). The resulting data is applied to compare with the records on a database. Experimental results reveal that the presented system successfully detects and recognizes the vehicle number plate on real images. This system can also be used for security and traffic control."
"We present a model of visually-grounded language learning based on stacked gated recurrent neural networks which learns to predict visual features given an image description in the form of a sequence of phonemes. The learning task resembles that faced by human language learners who need to discover both structure and meaning from noisy and ambiguous data across modalities. We show that our model indeed learns to predict features of the visual context given phonetically transcribed image descriptions, and show that it represents linguistic information in a hierarchy of levels: lower layers in the stack are comparatively more sensitive to form, whereas higher layers are more sensitive to meaning."
"Graphics processing units have been extensively used to accelerate classical molecular dynamics simulations. However, there is much less progress on the acceleration of force evaluations for many-body potentials compared to pairwise ones. In the conventional force evaluation algorithm for many-body potentials, the force, virial stress, and heat current for a given atom are accumulated within different loops, which could result in write conflict between different threads in a CUDA kernel. In this work, we provide a new force evaluation algorithm, which is based on an explicit pairwise force expression for many-body potentials derived recently [Phys. Rev. B 92 (2015) 094301]. In our algorithm, the force, virial stress, and heat current for a given atom can be accumulated within a single thread and is free of write conflicts. We discuss the formulations and algorithms and evaluate their performance. A new open-source code, GPUMD, is developed based on the proposed formulations. For the Tersoff many-body potential, the double precision performance of GPUMD using a Tesla K40 card is equivalent to that of the LAMMPS (Large-scale Atomic/Molecular Massively Parallel Simulator) molecular dynamics code running with about 100 CPU cores (Intel Xeon CPU X5670 @ 2.93 GHz) and the single-precision performance of GPUMD reaches $10^8$ atom $\times$ step / second."
"Visual attention is the cognitive process that allows humans to parse a large amount of sensory data by selecting relevant information and filtering out irrelevant stimuli. This papers develops a computational framework for visual attention in robots. We consider a Visual Inertial Navigation (VIN) problem in which a robot needs to estimate its state using an on-board camera and an inertial sensor. The robot can allocate limited resources to VIN, due to time and energy constraints. Therefore, we answer the following question: under limited resources, what are the most relevant visual cues to maximize the performance of visual-inertial navigation? Our approach has four key features. First, it is task-driven, in that the selection of the visual cues is guided by a metric quantifying the task performance. Second, it exploits the notion of anticipation, since it uses a simplified model for forward-simulation of robot dynamics, predicting the utility of a set of visual cues over a time horizon. Third, it is efficient and easy to implement, since it leads to a greedy algorithm for the selection of the most relevant visual cues. Fourth, it provides formal performance guarantees: we leverage submodularity to prove that the greedy selection cannot be far from the optimal (combinatorial) selection. Simulations and real experiments on agile drones show that our approach leads to dramatic improvements in the VIN performance. In the easy scenarios, our approach outperforms the state-of-the-art in terms of localization errors. In the most challenging scenarios, it enables accurate visual-inertial navigation while the state of the art fails to track robot's motion during aggressive maneuvers."
"As the groupoid model of Hofmann and Streicher proves, identity proofs in intensional Martin-L\""of type theory cannot generally be shown to be unique. Inspired by a theorem by Hedberg, we give some simple characterizations of types that do have unique identity proofs. A key ingredient in these constructions are weakly constant endofunctions on identity types. We study such endofunctions on arbitrary types and show that they always factor through a propositional type, the truncated or squashed domain. Such a factorization is impossible for weakly constant functions in general (a result by Shulman), but we present several non-trivial cases in which it can be done. Based on these results, we define a new notion of anonymous existence in type theory and compare different forms of existence carefully. In addition, we show possibly surprising consequences of the judgmental computation rule of the truncation, in particular in the context of homotopy type theory. All the results have been formalized and verified in the dependently typed programming language Agda."
"We consider the shortest path routing (SPR) of a network with stochastically time varying link metrics under potential adversarial attacks. Due to potential denial of service attacks, the distributions of link states could be stochastic (benign) or adversarial at different temporal and spatial locations. Without any \emph{a priori}, designing an adaptive SPR protocol to cope with all possible situations in practice optimally is a very challenging issue. In this paper, we present the first solution by formulating it as a multi-armed bandit (MAB) problem. By introducing a novel control parameter into the exploration phase for each link, a martingale inequality is applied in the our combinatorial adversarial MAB framework. As such, our proposed algorithms could automatically detect features of the environment within a unified framework and find the optimal SPR strategies with almost optimal learning performance in all possible cases over time. Moreover, we study important issues related to the practical implementation, such as decoupling route selection with multi-path route probing, cooperative learning among multiple sources, the ""cold-start"" issue and delayed feedback of our algorithm. Nonetheless, the proposed SPR algorithms can be implemented with low complexity and they are proved to scale very well with the network size. Comparing to existing approaches in a typical network scenario under jamming attacks, our algorithm has a 65.3\% improvement of network delay given a learning period and a 81.5\% improvement of learning duration under a specified network delay."
"In recent years linguistic typology, which classifies the world's languages according to their functional and structural properties, has been widely used to support multilingual NLP. While the growing importance of typological information in supporting multilingual tasks has been recognised, no systematic survey of existing typological resources and their use in NLP has been published. This paper provides such a survey as well as discussion which we hope will both inform and inspire future work in the area."
"Contemporary field-programmable gate arrays (FPGAs) are predestined for the application of finite impulse response (FIR) filters. Their embedded digital signal processing (DSP) blocks for multiply-accumulate operations enable efficient fixed-point computations, in cases where the filter structure is accurately mapped to the dedicated hardware architecture. This brief presents a generic systolic structure for high-order FIR filters, efficiently exploiting the hardware resources of an FPGA in terms of routability and timing. Although this seems to be an easily implementable task, the synthesizing tools require an adaptation of the straightforward digital filter implementation for an optimal mapping. Using the example of a symmetric FIR filter with 90 taps, we demonstrate the performance of the proposed structure with FPGAs from Xilinx and Altera. The implementation utilizes less than 1% of slice logic and runs at clock frequencies up to 526 MHz. Moreover, an enhancement of the structure ultimately provides an extended dynamic range for the quantized coefficients without the costs of additional slice logic."
"The problem of efficient modulation classification (MC) in multiple-input multiple-output (MIMO) systems is considered. Per-layer likelihood-based MC is proposed by employing subspace decomposition to partially decouple the transmitted streams. When detecting the modulation type of the stream of interest, a dense constellation is assumed on all remaining streams. The proposed classifier outperforms existing MC schemes at a lower complexity cost, and can be efficiently implemented in the context of joint MC and subspace data detection."
"The Wasserstein metric or earth mover's distance (EMD) is a useful tool in statistics, machine learning and computer science with many applications to biological or medical imaging, among others. Especially in the light of increasingly complex data, the computation of these distances via optimal transport is often the limiting factor. Inspired by this challenge, a variety of new approaches to optimal transport has been proposed in recent years and along with these new methods comes the need for a meaningful comparison.   In this paper, we introduce a benchmark for discrete optimal transport, called DOTmark, which is designed to serve as a neutral collection of problems, where discrete optimal transport methods can be tested, compared to one another, and brought to their limits on large-scale instances. It consists of a variety of grayscale images, in various resolutions and classes, such as several types of randomly generated images, classical test images and real data from microscopy.   Along with the DOTmark we present a survey and a performance test for a cross section of established methods ranging from more traditional algorithms, such as the transportation simplex, to recently developed approaches, such as the shielding neighborhood method, and including also a comparison with commercial solvers."
"This paper studies an attacker against a cyber-physical system (CPS) whose goal is to move the state of a CPS to a target state while ensuring that his or her probability of being detected does not exceed a given bound. The attacker's probability of being detected is related to the nonnegative bias induced by his or her attack on the CPS' detection statistic. We formulate a linear quadratic cost function that captures the attacker's control goal and establish constraints on the induced bias that reflect the attacker's detection-avoidance objectives. When the attacker is constrained to be detected at the false-alarm rate of the detector, we show that the optimal attack strategy reduces to a linear feedback of the attacker's state estimate. In the case that the attacker's bias is upper bounded by a positive constant, we provide two algorithms -- an optimal algorithm and a sub-optimal, less computationally intensive algorithm -- to find suitable attack sequences. Finally, we illustrate our attack strategies in numerical examples based on a remotely-controlled helicopter under attack."
"Distributed algorithms of multi-agent coordination have attracted substantial attention from the research community; the simplest and most thoroughly studied of them are consensus protocols in the form of differential or difference equations over general time-varying weighted graphs. These graphs are usually characterized algebraically by their associated Laplacian matrices. Network algorithms with similar algebraic graph theoretic structures, called being of Laplacian-type in this paper, also arise in other related multi-agent control problems, such as aggregation and containment control, target surrounding, distributed optimization and modeling of opinion evolution in social groups. In spite of their similarities, each of such algorithms has often been studied using separate mathematical techniques. In this paper, a novel approach is offered, allowing a unified and elegant way to examine many Laplacian-type algorithms for multi-agent coordination. This approach is based on the analysis of some differential or difference inequalities that have to be satisfied by the some ""outputs"" of the agents (e.g. the distances to the desired set in aggregation problems). Although such inequalities may have many unbounded solutions, under natural graphic connectivity conditions all their bounded solutions converge (and even reach consensus), entailing the convergence of the corresponding distributed algorithms. In the theory of differential equations the absence of bounded non-convergent solutions is referred to as the equation's dichotomy. In this paper, we establish the dichotomy criteria of Laplacian-type differential and difference inequalities and show that these criteria enable one to extend a number of recent results, concerned with Laplacian-type algorithms for multi-agent coordination and modeling opinion formation in social groups."
"Many randomized algorithms can be derandomized efficiently using either the method of conditional expectations or probability spaces with low (almost-) independence. A series of papers, beginning with work by Luby (1988) and continuing with Berger & Rompel (1991) and Chari et al. (1994), showed that these techniques can be combined to give deterministic parallel algorithms for combinatorial optimization problems involving sums of $w$-juntas. We improve these algorithms through derandomized variable partitioning. This reduces the processor complexity to essentially independent of $w$ while the running time is reduced from exponential in $w$ to linear in $w$. For example, we improve the time complexity of an algorithm of Berger & Rompel (1991) for rainbow hypergraph coloring by a factor of approximately $\log^2 n$ and the processor complexity by a factor of approximately $m^{\ln 2}$.   As a major application of this, we give an NC algorithm for the Lov\'{a}sz Local Lemma. Previous NC algorithms, including the seminal algorithm of Moser & Tardos (2010) and the work of Chandrasekaran et. al (2013), required that (essentially) the bad-events could span only $O(\log n)$ variables; we relax this to allowing $\text{polylog}(n)$ variables. As two applications of our new algorithm, we give algorithms for defective vertex coloring and domatic graph partition.   One main sub-problem encountered in these algorithms is to generate a probability space which can ""fool"" a given list of $GF(2)$ Fourier characters. Schulman (1992) gave an NC algorithm for this; we dramatically improve its efficiency to near-optimal time and processor complexity and code dimension. This leads to a new algorithm to solve the heavy-codeword problem, introduced by Naor & Naor (1993), with a near-linear processor complexity $(mn)^{1+o(1)}$."
"In the absence of pedestrian crossing lights, finding a safe moment to cross the road is often hazardous and challenging, especially for people with visual impairments. We present a reliable low-cost solution, an Android device attached to a traffic sign or lighting pole near the crossing, indicating whether it is safe to cross the road. The indication can be by sound, display, vibration, and various communication modalities provided by the Android device. The integral system camera is aimed at approaching traffic. Optical flow is computed from the incoming video stream, and projected onto an influx map, automatically acquired during a brief training period. The crossing safety is determined based on a 1-dimensional temporal signal derived from the projection. We implemented the complete system on a Samsung Galaxy K-Zoom Android smartphone, and obtained real-time operation. The system achieves promising experimental results, providing pedestrians with sufficiently early warning of approaching vehicles. The system can serve as a stand-alone safety device, that can be installed where pedestrian crossing lights are ruled out. Requiring no dedicated infrastructure, it can be powered by a solar panel and remotely maintained via the cellular network."
"We present a first algorithm for finding Euler tours in undirected graphs in the StrSort model. This model is a relaxation of the semi streaming model. The graph is given as a stream of its edges and can only be read sequentially, but while doing a pass over the stream we are allowed to write out another stream which will be the input for the next pass. In addition, items in the stream are sorted between passes. This model was introduced by Aggarwal et al. in 2004. Here we apply this model to the problem of finding an Euler tour in a graph (or to decide that the graph does not admit an Euler tour). The algorithm works in two steps. In the first step, a single pass is conducted while a linear (in the number of vertices $n$) amount of RAM is required. In the second step, $\mathcal O(\log(n))$ passes are conducted while only $\mathcal O(\log(n))$ RAM is required.   We use an alteration of the algorithm of Atallah and Vishkin from 1984 for finding Euler tours in parallel. It finds a partition of edge-disjoint circuits and arranges them in a tree expressing their connectivity. Then the circuits are merged according to this tree. In order to minimize the needed amount of RAM, we evade the need to store the entire tree and use techniques suggested by Aggarwal et al. to exchange information concerning the merging of circuits."
"We introduce a Maximum Entropy model able to capture the statistics of melodies in music. The model can be used to generate new melodies that emulate the style of the musical corpus which was used to train it. Instead of using the $n-$body interactions of $(n-1)-$order Markov models, traditionally used in automatic music generation, we use a $k-$nearest neighbour model with pairwise interactions only. In that way, we keep the number of parameters low and avoid over-fitting problems typical of Markov models. We show that long-range musical phrases don't need to be explicitly enforced using high-order Markov interactions, but can instead emerge from multiple, competing, pairwise interactions. We validate our Maximum Entropy model by contrasting how much the generated sequences capture the style of the original corpus without plagiarizing it. To this end we use a data-compression approach to discriminate the levels of borrowing and innovation featured by the artificial sequences. The results show that our modelling scheme outperforms both fixed-order and variable-order Markov models. This shows that, despite being based only on pairwise interactions, this Maximum Entropy scheme opens the possibility to generate musically sensible alterations of the original phrases, providing a way to generate innovation."
"Given a string on an integer alphabet, we present an algorithm that computes the set of all distinct squares belonging to this string in time linear to the string length. As an application, we show how to compute the tree topology of the minimal augmented suffix tree in linear time. Asides from that, we elaborate an algorithm computing the longest previous table in a succinct representation using compressed working space."
"We show that a family of quantum authentication protocols introduced in [Barnum et al., FOCS 2002] can be used to construct a secure quantum channel and additionally recycle all of the secret key if the message is successfully authenticated, and recycle part of the key if tampering is detected. We give a full security proof that constructs the secure channel given only insecure noisy channels and a shared secret key. We also prove that the number of recycled key bits is optimal for this family of protocols, i.e., there exists an adversarial strategy to obtain all non-recycled bits. Previous works recycled less key and only gave partial security proofs, since they did not consider all possible distinguishers (environments) that may be used to distinguish the real setting from the ideal secure quantum channel."
"In this article, we present a denoising algorithm to improve the interpretation and quality of scanning tunneling microscopy (STM) images. Given the high level of self-similarity of STM images, we propose a denoising algorithm by reformulating the true estimation problem as a sparse regression, often termed sparse coding. We introduce modifications to the algorithm to cope with the existence of artifacts, mainly dropouts, which appear in a structured way as consecutive line segments on the scanning direction. The resulting algorithm treats the artifacts as missing data, and the estimated values outperform those algorithms that substitute the outliers by a local filtering. We provide code implementations for both Matlab and Gwyddion."
"This work introduces a novel, modular, layered web based platform for managing machine learning experiments on grid-based High Performance Computing infrastructures. The coupling of the communication services offered by the grid, with an administration layer and conventional web server programming, via a data synchronization utility, leads to the straightforward development of a web-based user interface that allows the monitoring and managing of diverse online distributed computing applications. It also introduces an experiment generation and monitoring tool particularly suitable for investigating machine learning in game playing. The platform is demonstrated with experiments for two different games."
"We present deep variational canonical correlation analysis (VCCA), a deep multi-view learning model that extends the latent variable model interpretation of linear CCA~\citep{BachJordan05a} to nonlinear observation models parameterized by deep neural networks (DNNs). Marginal data likelihood as well as inference are intractable under this model. We derive a variational lower bound of the data likelihood by parameterizing the posterior density of the latent variables with another DNN, and approximate the lower bound via Monte Carlo sampling. Interestingly, the resulting model resembles that of multi-view autoencoders~\citep{Ngiam_11b}, with the key distinction of an additional sampling procedure at the bottleneck layer. We also propose a variant of VCCA called VCCA-private which can, in addition to the ""common variables"" underlying both views, extract the ""private variables"" within each view. We demonstrate that VCCA-private is able to disentangle the shared and private information for multi-view data without hard supervision."
"The phenomenon of so-called break-away forces, as maximal actuation forces at which a sticking system begins to slide and thus passes over to a steady (macro) motion, is well known from engineering practice but still less understood in its cause-effect relationship. This note analyzes the break-away behavior of systems with nonlinear friction, which is analytically well-described by combining the Coulomb friction law with rate-independent presliding transitions and, when necessary, Stribeck effect of the velocity-weakening steady-state curve. The break-away conditions are harmonized with analytic form of the system description and shown to be in accord with a relationship between the varying break-away force and actuation force rate -- well known from the experiments reported in several independently published works."
"We propose a deep neural network fusion architecture for fast and robust pedestrian detection. The proposed network fusion architecture allows for parallel processing of multiple networks for speed. A single shot deep convolutional network is trained as a object detector to generate all possible pedestrian candidates of different sizes and occlusions. This network outputs a large variety of pedestrian candidates to cover the majority of ground-truth pedestrians while also introducing a large number of false positives. Next, multiple deep neural networks are used in parallel for further refinement of these pedestrian candidates. We introduce a soft-rejection based network fusion method to fuse the soft metrics from all networks together to generate the final confidence scores. Our method performs better than existing state-of-the-arts, especially when detecting small-size and occluded pedestrians. Furthermore, we propose a method for integrating pixel-wise semantic segmentation network into the network fusion architecture as a reinforcement to the pedestrian detector. The approach outperforms state-of-the-art methods on most protocols on Caltech Pedestrian dataset, with significant boosts on several protocols. It is also faster than all other methods."
"Current analysis of tumor proliferation, the most salient prognostic biomarker for invasive breast cancer, is limited to subjective mitosis counting by pathologists in localized regions of tissue images. This study presents the first data-driven integrative approach to characterize the severity of tumor growth and spread on a categorical and molecular level, utilizing multiple biologically salient deep learning classifiers to develop a comprehensive prognostic model. Our approach achieves pathologist-level performance on three-class categorical tumor severity prediction. It additionally pioneers prediction of molecular expression data from a tissue image, obtaining a Spearman's rank correlation coefficient of 0.60 with ex vivo mean calculated RNA expression. Furthermore, our framework is applied to identify over two hundred unprecedented biomarkers critical to the accurate assessment of tumor proliferation, validating our proposed integrative pipeline as the first to holistically and objectively analyze histopathological images."
"We propose a method to efficiently compute the forward stochastic reach (FSR) set and its probability measure for nonlinear systems with an affine disturbance input, that is stochastic and bounded. This method is applicable to systems with an a priori known controller, or to uncontrolled systems, and often arises in problems in obstacle avoidance in mobile robotics. When used as a constraint in finite horizon controller synthesis, the FSR set, and its probability measure facilitates probabilistic collision avoidance, in contrast to methods which presume the obstacles act in a worst-case fashion and generate hard constraints that cannot be violated. We tailor our approach to accommodate rigid body constraints, and show convexity is assured so long as the rigid body shape of each obstacle is also convex. We extend methods for multi-obstacle avoidance through mixed integer linear programming (with linear robot and obstacle dynamics) to accommodate chance constraints that represent the FSR set probability measure. We demonstrate our method on a rigid-body obstacle avoidance scenario, in which a receding horizon controller is designed to avoid several stochastically moving obstacles while reaching the desired goal. Our approach can provide solutions when approaches that presume a worst-case action from the obstacle fail."
"In participatory budgeting, communities collectively decide on the allocation of public tax dollars for local public projects. In this work, we consider the question of fairly aggregating the preferences of community members to determine an allocation of funds to projects. This problem is different from standard fair resource allocation because of public goods: The allocated goods benefit all users simultaneously. Fairness is crucial in participatory decision making, since generating equitable outcomes is an important goal of these processes. We argue that the classic game theoretic notion of core captures fairness in the setting. To compute the core, we first develop a novel characterization of a public goods market equilibrium called the Lindahl equilibrium, which is always a core solution. We then provide the first (to our knowledge) polynomial time algorithm for computing such an equilibrium for a broad set of utility functions; our algorithm also generalizes (in a non-trivial way) the well-known concept of proportional fairness. We use our theoretical insights to perform experiments on real participatory budgeting voting data. We empirically show that the core can be efficiently computed for utility functions that naturally model our practical setting, and examine the relation of the core with the familiar welfare objective. Finally, we address concerns of incentives and mechanism design by developing a randomized approximately dominant-strategy truthful mechanism building on the exponential mechanism from differential privacy."
"We consider two fundamental multi-user channel models: the multiple-input multiple-output (MIMO) wiretap channel with one helper (WTH) and the MIMO multiple access wiretap channel (MAC-WT). In each case, the eavesdropper has $K$ antennas while the remaining terminals have $N$ antennas each. We consider a fast fading channel where the channel state information (CSI) of the legitimate receiver is available at the transmitters but no channel state information at the transmitters (CSIT) is available for the eavesdropper's channel. We determine the optimal sum secure degrees of freedom (s.d.o.f.) for each channel model for the regime $K\leq N$, and show that in this regime, the MAC-WT channel reduces to the WTH in the absence of eavesdropper CSIT. For the regime $N\leq K\leq 2N$, we obtain the optimal linear s.d.o.f., and show that the MAC-WT channel and the WTH have the same optimal s.d.o.f. when restricted to linear encoding strategies. In the absence of any such restrictions, we provide an upper bound for the sum s.d.o.f. of the MAC-WT chanel in the regime $N\leq K\leq 2N$. Our results show that unlike in the single-input single-output (SISO) case, there is loss of s.d.o.f. for even the WTH due to lack of eavesdropper CSIT when $K\geq N$."
"This paper presents stop conditions for solving General Assignment Problems (GAP), in particular for Travel Salesman Problem in an Euclidian 2D space the well known condition Jordan's simple curve and opposite condition for the Knight Tour Problem. The Jordan's simple curve condition means that a optimal trajectory must be simple curve, i.e., without crossing but for Knight Tour Problem we use the contrary, the feasible trajectory must have crossing in all cities of the tour. The paper presents the algorithms, examples and some results come from Concorde's Home page. Several problem are studied to depict their properties. A classical decision problem SAT is studied in detail."
"A coupled hygro-thermo-mechanical computational model is proposed for fibre reinforced polymers, formulated within the framework of Computational Homogenisation (CH). At each macrostructure Gauss point, constitutive matrices for thermal, moisture transport and mechanical responses are calculated from CH of the underlying representative volume element (RVE). A degradation model, developed from experimental data relating evolution of mechanical properties over time for a given exposure temperature and moisture concentration is also developed and incorporated in the proposed computational model. A unified approach is used to impose the RVE boundary conditions, which allows convenient switching between linear Dirichlet, uniform Neumann and periodic boundary conditions. A plain weave textile composite RVE consisting of yarns embedded in a matrix is considered in this case. Matrix and yarns are considered as isotropic and transversely isotropic materials respectively. Furthermore, the computational framework utilises hierarchic basis functions and designed to take advantage of distributed memory high-performance computing."
"In massive multiple-input multiple-output (MIMO) systems, acquisition of the channel state information at the transmitter side (CSIT) is crucial. In this paper, a practical CSIT estimation scheme is proposed for frequency division duplexing (FDD) massive MIMO systems. Specifically, each received pilot symbol is first quantized to one bit per dimension at the receiver side and then the quantized bits are fed back to the transmitter. A joint one-bit compressed sensing algorithm is implemented at the transmitter to recover the channel matrices. The algorithm leverages the hidden joint sparsity structure in the user channel matrices to minimize the training and feedback overhead, which is considered to be a major challenge for FDD systems. Moreover, the one-bit compressed sensing algorithm accurately recovers the channel directions for beamforming. The one-bit feedback mechanism can be implemented in practical systems using the uplink control channel. Simulation results show that the proposed scheme nearly achieves the maximum output signal-to-noise-ratio for beamforming based on the estimated CSIT."
"Millimeter wave (mmWave) vehicular communication systems will provide an abundance of bandwidth for the exchange of raw sensor data and support driver-assisted and safety-related functionalities. Lack of secure communication links, however, may lead to abuses and attacks that jeopardize the efficiency of transportation systems and the physical safety of drivers. In this paper, we propose two physical layer (PHY) security techniques for vehicular mmWave communication systems. The first technique uses multiple antennas with a single RF chain to transmit information symbols to a target receiver and noise-like signals in non-receiver directions. The second technique uses multiple antennas with a few RF chains to transmit information symbols to a target receiver and opportunistically inject artificial noise in controlled directions, thereby reducing interference in vehicular environments. Theoretical and numerical results show that the proposed techniques provide higher secrecy rate when compared to traditional PHY security techniques that require digital or more complex antenna architectures."
"Developing control policies in simulation is often more practical and safer than directly running experiments in the real world. This applies to policies obtained from planning and optimization, and even more so to policies obtained from reinforcement learning, which is often very data demanding. However, a policy that succeeds in simulation often doesn't work when deployed on a real robot. Nevertheless, often the overall gist of what the policy does in simulation remains valid in the real world. In this paper we investigate such settings, where the sequence of states traversed in simulation remains reasonable for the real world, even if the details of the controls are not, as could be the case when the key differences lie in detailed friction, contact, mass and geometry properties. During execution, at each time step our approach computes what the simulation-based control policy would do, but then, rather than executing these controls on the real robot, our approach computes what the simulation expects the resulting next state(s) will be, and then relies on a learned deep inverse dynamics model to decide which real-world action is most suitable to achieve those next states. Deep models are only as good as their training data, and we also propose an approach for data collection to (incrementally) learn the deep inverse dynamics model. Our experiments shows our approach compares favorably with various baselines that have been developed for dealing with simulation to real world model discrepancy, including output error control and Gaussian dynamics adaptation."
"Existing parallel algorithms for wavelet tree construction have a work complexity of $O(n\log\sigma)$. This paper presents parallel algorithms for the problem with improved work complexity. Our first algorithm is based on parallel integer sorting and has either $O(n\sqrt{\log\log n}\lceil\log\sigma/\sqrt{\log n}\rceil)$ work and polylogarithmic depth, or $O(n\lceil\log\sigma/\sqrt{\log n}\rceil)$ work and sub-linear depth. We also describe another algorithm that has $O(n\lceil\log\sigma/\sqrt{\log n} \rceil)$ work and $O(\sigma+\log n)$ depth. We then show how to use similar ideas to construct variants of wavelet trees (arbitrary-shaped binary trees and multiary trees) as well as wavelet matrices in parallel with lower work complexity than prior algorithms. Finally, we show that the rank and select structures on binary sequences and multiary sequences, which are stored on wavelet tree nodes, can be constructed in parallel with improved work bounds. In particular, we show that the rank and select structures can be constructed for binary sequences in $O(n/\log n)$ work and $O(\log n)$ depth, and for multiary sequences in $O(n\log\sigma/\log n)$ work and $O(\log n)$ depth. These work bounds match that of the best existing sequential algorithms for constructing rank and select structures."
"Radio frequency (RF) energy transfer and harvesting has been intensively studied recently as a promising approach to significantly extend the lifetime of energy-constrained wireless networks. This technique has a great potential to provide relatively stable and continuous RF energy to devices wirelessly, it thus opened a new research paradigm, termed wireless-powered communication (WPC), which has raised many new research opportunities with wide applications. Among these, the design and evaluation of cooperative schemes towards energy-efficient WPC have attracted tremendous research interests nowadays. This article provides an overview of various energy-efficient cooperative strategies for WPC, with particular emphasis on relaying protocols for wireless-powered cooperative communications, cooperative spectrum sharing schemes for wireless-powered cognitive radio networks, and cooperative jamming strategies towards wireless-powered secure communications. We also identify some valuable research directions in this area before concluding this article."
"This paper deals with the uniqueness of $L$-fuzzy sets in the representation of a given family of subsets of nonempty set. It first shows a formula of the number of $L$-fuzzy sets whose collection of cuts coincides with a given family of subsets of a nonempty set, and then provides a necessary and sufficient condition under which such $L$-fuzzy sets are unique."
"Transmission Control Protocol (TCP) has been profusely used by most of internet applications. Since 1970s, several TCP variants have been developed in order to cope with the fast increasing of network capacities especially in high Bandwidth Delay Product (high-BDP) networks. In these TCP variants, several approaches have been used, some of these approaches have the ability to estimate available bandwidths and some react based on network loss and/or delay changes. This variety of the used approaches arises many consequent problems with different levels of dependability and accuracy. Indeed, a particular TCP variant which is proper for wireless networks, may not fit for high-BDP wired networks and vice versa. Therefore, it is necessary to conduct a comparison between the high-speed TCP variants that have a high level of importance especially after the fast growth of networks bandwidths. In this paper, high-speed TCP variants, that are implemented in Linux and available for research, have been evaluated using NS2 network simulator. This performance evaluation presents the advantages and disadvantages of these TCP variants in terms of throughput, loss-ratio and fairness over high-BDP networks. The results reveal that, CUBIC and YeAH overcome the other highspeed TCP variants in different cases of buffer size. However, they still require more improvement to extend their ability to fully utilize the high-speed bandwidths, especially when the applied buffer is near-zero or less than the BDP of the link."
"The information capacity of a distributed storage system is the amount of source data that can be reliably stored for long durations. Storage nodes fail over time and are replaced, and thus data is erased at an erasure rate. To maintain recoverability of source data, a repairer generates redundant data from data read from nodes, and writes redundant data to nodes, where the repair rate is the rate at which the repairer reads and writes data. We prove the information capacity approaches $\left(1-\frac{1}{2 \cdot \sigma} \right) \cdot N \cdot s$ as $N$ and $\sigma$ grow, where $N$ is the number of nodes, $s$ is the amount of data each node can store, and $\sigma$ is the repair rate to erasure rate ratio."
"The association between light and psychological states has a long history and permeates our language. LIVEIA (Light-based Immersive Visualization Environment for Imaginative Actualization) is a new immersive, interactive technology that uses physical light as a metaphor for visualizing peoples' inner lives and relationships. This paper outlines its educational value, as a tool for understanding and explaining aspects of how people think and interact, and its potential therapeutic value as a form of art therapy in which the artwork has straightforwardly interpretable symbolic meanings."
"Aaronson and Drucker (2011) asked whether there exists a quantum finite automaton that can distinguish fair coin tosses from biased ones by spending significantly more time in accepting states, on average, given an infinite sequence of tosses. We answer this question negatively."
"We propose a novel scoring concept for visual place recognition based on nearest neighbor descriptor voting and demonstrate how the algorithm naturally emerges from the problem formulation. Based on the observation that the number of votes for matching places can be evaluated using a binomial distribution model, loop closures can be detected with high precision. By casting the problem into a probabilistic framework, we not only remove the need for commonly employed heuristic parameters but also provide a powerful score to classify matching and non-matching places. We present methods for both a 2D-2D pose-graph vertex matching and a 2D-3D landmark matching based on the above scoring. The approach maintains accuracy while being efficient enough for online application through the use of compact (low dimensional) descriptors and fast nearest neighbor retrieval techniques. The proposed methods are evaluated on several challenging datasets in varied environments, showing state-of-the-art results with high precision and high recall."
"This paper reviews literature from 2011 to 2013 on how Latent attributes like gender, political leaning etc. can be inferred from a person's twitter and neighborhood data. Prediction of demographic data can bring value to businesses, can prove instrumental in legal investigation. Moreover, political leanings can be inferred from the wide variety of user data available on-line. The motive of this review is to understand how large data sets can be made from available twitter data. The tweeting and re tweeting behavior of a user can be user to infer attributes like, gender, age etc. We explore in this text how this field can be expanded in future and possible avenues for future research."
"Given the rapid development of robotics, computer vision, and machine learning, increasingly more robots will enter human environments to assist with daily life tasks. In contrast to traditional well-structured factory settings, human environments are full of dynamic, unpredictable, and stochastic events which require rapid modification of the ongoing plan of the robot. Reactivity can be accomplished through model-based re-planning, e.g. model-predictive control, or through a reactive feedback policy that modifies on-going behavior in response to sensory events. In this paper, we investigate how to use machine learning to add reactivity to a previously learned nominal skilled behavior. We approach this by learning a reactive modification term for movement plans represented by nonlinear differential equations. In particular, we use dynamic movement primitives (DMPs) to represent a skill and neural networks to learn a reactive policy from human demonstrations. We use the well explored domain of obstacle avoidance for robot manipulation as a test bed. Our approach demonstrates how a general neural network can be combined with physical insights to ensure robust behavior of the robot. Given a rich set of obstacle avoidance demonstrations, the system learns how to robustly avoid obstacles in novel situations. In addition, we also suggest some modifications to DMPs using a specific local coordinate system that easily generalizes a learned task in Euclidean task space. Evaluations on an anthropomorphic robotic system demonstrate the effectiveness of our work."
"In complex combinatorial markets with complementary valuations, truthful auctions can yield low revenue. Core-selecting auctions attempt to boost revenue by setting prices so that no group of agents, including the auctioneer, can jointly improve their utilities by switching to a different allocation and payments. Among outcomes in the core, bidder-optimal core points have been the most widely studied due to their incentive properties, such as being implementable at equilibrium. Prior work in economics has studied heuristics and algorithms for computing approximate bidder-optimal core points given oracle access to the welfare optimization problem, but these solutions either lack performance guarantees or are based on prohibitively convex programs. Our main result is a combinatorial algorithm that finds an approximate bidder-optimal core point in a quasi-linear number of calls to the welfare maximization oracle. Our results have implication for the sale of space in online advertising."
"We describe a general method of detecting valid chains or links of pieces on a two-dimensional grid. Specifically, using the example of the chess variant known as Switch-Side Chain-Chess (SSCC). Presently, no foolproof method of detecting such chains in any given chess position is known and existing graph theory, to our knowledge, is unable to fully address this problem either. We therefore propose a solution implemented and tested using the C++ programming language. We have been unable to find an incorrect result and therefore offer it as the most viable solution thus far to the chain-detection problem in this chess variant. The algorithm is also scalable, in principle, to areas beyond two-dimensional grids such as 3D analysis and molecular chemistry."
"We introduce a simple two-player test which certifies that the players apply tensor products of Pauli $\sigma_X$ and $\sigma_Z$ observables on the tensor product of $n$ EPR pairs. The test has constant robustness: any strategy achieving success probability within an additive $\varepsilon$ of the optimal must be $\mathrm{poly}(\varepsilon)$-close, in the appropriate distance measure, to the honest $n$-qubit strategy. The test involves $2n$-bit questions and $2$-bit answers. The key technical ingredient is a quantum version of the classical linearity test of Blum, Luby, and Rubinfeld.   As applications of our result we give (i) the first robust self-test for $n$ EPR pairs; (ii) a quantum multiprover interactive proof system for the local Hamiltonian problem with a constant number of provers and classical questions and answers, and a constant completeness-soundness gap independent of system size; (iii) a robust protocol for delegated quantum computation."
"Preserving privacy of continuous and/or high-dimensional data such as images, videos and audios, can be challenging with syntactic anonymization methods which are designed for discrete attributes. Differential privacy, which provides a more formal definition of privacy, has shown more success in sanitizing continuous data. However, both syntactic and differential privacy are susceptible to inference attacks, i.e., an adversary can accurately infer sensitive attributes from sanitized data. The paper proposes a novel filter-based mechanism which preserves privacy of continuous and high-dimensional attributes against inference attacks. Finding the optimal utility-privacy tradeoff is formulated as a min-diff-max optimization problem. The paper provides an ERM-like analysis of the generalization error and also a practical algorithm to perform the optimization. In addition, the paper proposes an extension that combines minimax filter and differentially-private noisy mechanism. Advantages of the method over purely noisy mechanisms is explained and demonstrated with examples. Experiments with several real-world tasks including facial expression classification, speech emotion classification, and activity classification from motion, show that the minimax filter can simultaneously achieve similar or better target task accuracy and lower inference accuracy, often significantly lower than previous methods."
"In this paper, we propose and study the problem of top-m rank aggregation of spatial objects in streaming queries, where, given a set of objects O, a stream of spatial queries (kNN or range), the goal is to report m objects with the highest aggregate rank. The rank of an object w.r.t. an individual query is computed based on its distance from the query location, and the aggregate rank is computed from all of the individual rank orderings. Solutions to this fundamental problem can be used to monitor the popularity of spatial objects, which in turn can provide new analytical tools for spatial data. Our work draws inspiration from three different domains: rank aggregation, continuous queries and spatial databases. To the best of our knowledge, there is no prior work that considers all three problem domains in a single context. Our problem is different from the classical rank aggregation problem in the way that the rank of spatial objects are dependent on streaming queries whose locations are not known a priori, and is different from the problem of continuous spatial queries because new query locations can arrive in any region, but do not move. In order to solve this problem, we show how to upper and lower bound the rank of an object for any unseen query. Then we propose an approximation solution to continuously monitor the top-m objects efficiently, for which we design an Inverted Rank File (IRF) index to guarantee the error bound of the solution. In particular, we propose the notion of safe ranking to determine whether the current result is still valid or not when new queries arrive, and propose the notion of validation objects to limit the number of objects to update in the top-m results. We also propose an exact solution for applications where an approximate solution is not sufficient. Last, we conduct extensive experiments to verify the efficiency and effectiveness of our solutions."
"In this work we consider the ground space connectivity problem for commuting local Hamiltonians. The ground space connectivity problem asks whether it is possible to go from one (efficiently preparable) state to another by applying a polynomial length sequence of 2-qubit unitaries while remaining at all times in a state with low energy for a given Hamiltonian $H$. It was shown in [Gharibian and Sikora, ICALP'15] that this problem is QCMA-complete for general local Hamiltonians, where QCMA is defined as QMA with a classical witness and BQP verifier. Here we show that the commuting version of the problem is also QCMA-complete. This provides one of the first examples where commuting local Hamiltonians exhibit complexity theoretic hardness equivalent to general local Hamiltonians."
"A distinguishing property of human intelligence is the ability to flexibly use language in order to communicate complex ideas with other humans in a variety of contexts. Research in natural language dialogue should focus on designing communicative agents which can integrate themselves into these contexts and productively collaborate with humans. In this abstract, we propose a general situated language learning paradigm which is designed to bring about robust language agents able to cooperate productively with humans."
"Frequency hopping sequences (FHSs) with favorable partial Hamming correlation properties have important applications in many synchronization and multiple-access systems. In this paper, we investigate constructions of FHS sets with optimal partial Hamming correlation. We present several direct constructions for balanced nested cyclic difference packings (BNCDPs) and balanced nested cyclic relative difference packings (BNCRDPs) such that both of them have a special property by using trace functions and discrete logarithm. We also show three recursive constructions for FHS sets with partial Hamming correlation, which are based on cyclic difference matrices and discrete logarithm. Combing these BNCDPs, BNCRDPs and three recursive constructions, we obtain infinitely many new strictly optimal FHS sets with respect to the Peng-Fan bounds."
"This work continues the study of the relationship between sample compression schemes and statistical learning, which has been mostly investigated within the framework of binary classification. The central theme of this work is establishing equivalences between learnability and compressibility, and utilizing these equivalences in the study of statistical learning theory.   We begin with the setting of multiclass categorization (zero/one loss). We prove that in this case learnability is equivalent to compression of logarithmic sample size, and that uniform convergence implies compression of constant size.   We then consider Vapnik's general learning setting: we show that in order to extend the compressibility-learnability equivalence to this case, it is necessary to consider an approximate variant of compression.   Finally, we provide some applications of the compressibility-learnability equivalences:   (i) Agnostic-case learnability and realizable-case learnability are equivalent in multiclass categorization problems (in terms of sample complexity).   (ii) This equivalence between agnostic-case learnability and realizable-case learnability does not hold for general learning problems: There exists a learning problem whose loss function takes just three values, under which agnostic-case and realizable-case learnability are not equivalent.   (iii) Uniform convergence implies compression of constant size in multiclass categorization problems. Part of the argument includes an analysis of the uniform convergence rate in terms of the graph dimension, in which we improve upon previous bounds.   (iv) A dichotomy for sample compression in multiclass categorization problems: If a non-trivial compression exists then a compression of logarithmic size exists.   (v) A compactness theorem for multiclass categorization problems."
"Subspace clustering refers to the problem of segmenting a set of data points approximately drawn from a union of multiple linear subspaces. Aiming at the subspace clustering problem, various subspace clustering algorithms have been proposed and low rank representation based subspace clustering is a very promising and efficient subspace clustering algorithm. Low rank representation method seeks the lowest rank representation among all the candidates that can represent the data points as linear combinations of the bases in a given dictionary. Nuclear norm minimization is adopted to minimize the rank of the representation matrix. However, nuclear norm is not a very good approximation of the rank of a matrix and the representation matrix thus obtained can be of high rank which will affect the final clustering accuracy. Weighted nuclear norm (WNN) is a better approximation of the rank of a matrix and WNN is adopted in this paper to describe the rank of the representation matrix. The convex program is solved via conventional alternation direction method of multipliers (ADMM) and linearized alternating direction method of multipliers (LADMM) and they are respectively refer to as WNNM-LRR and WNNM-LRR(L). Experimental results show that, compared with low rank representation method and several other state-of-the-art subspace clustering methods, WNNM-LRR and WNNM-LRR(L) can get higher clustering accuracy."
"In the context of contemporary monophonic music, expression can be seen as the difference between a musical performance and its symbolic representation, i.e. a musical score. In this paper, we show how Maximum Entropy (MaxEnt) models can be used to generate musical expression in order to mimic a human performance. As a training corpus, we had a professional pianist play about 150 melodies of jazz, pop, and latin jazz. The results show a good predictive power, validating the choice of our model. Additionally, we set up a listening test whose results reveal that on average, people significantly prefer the melodies generated by the MaxEnt model than the ones without any expression, or with fully random expression. Furthermore, in some cases, MaxEnt melodies are almost as popular as the human performed ones."
"In order to analyze the moving and deforming of the objects in image sequence, a novel way is presented to analyze the local changes of object edges between two related images (such as two adjacent frames in a video sequence), which is inspired by the physical electromagnetic interaction. The changes of edge between adjacent frames in sequences are analyzed by simulation of virtual current interaction, which can reflect the change of the object's position or shape. The virtual current along the main edge line is proposed based on the significant edge extraction. Then the virtual interaction between the current elements in the two related images is studied by imitating the interaction between physical current-carrying wires. The experimental results prove that the distribution of magnetic forces on the current elements in one image applied by the other can reflect the local change of edge lines from one image to the other, which is important in further analysis."
"A novel model for image segmentation is proposed, which is inspired by the carrier immigration mechanism in physical P-N junction. The carrier diffusing and drifting are simulated in the proposed model, which imitates the physical self-balancing mechanism in P-N junction. The effect of virtual carrier immigration in digital images is analyzed and studied by experiments on test images and real world images. The sign distribution of net carrier at the model's balance state is exploited for region segmentation. The experimental results for both test images and real-world images demonstrate self-adaptive and meaningful gathering of pixels to suitable regions, which prove the effectiveness of the proposed method for image region segmentation."
"A novel way of matching two images with shifting transformation is studied. The approach is based on the presentation of the virtual edge current in images, and also the study of virtual electromagnetic interaction between two related images inspired by electromagnetism. The edge current in images is proposed as a discrete simulation of the physical current, which is based on the significant edge line extracted by Canny-like edge detection. Then the virtual interaction of the edge currents between related images is studied by imitating the electro-magnetic interaction between current-carrying wires. Based on the virtual interaction force between two related images, a novel method is presented and applied in image matching for shifting transformation. The preliminary experimental results indicate the effectiveness of the proposed method."
"Leveraging large data sets, deep Convolutional Neural Networks (CNNs) achieve state-of-the-art recognition accuracy. Due to the substantial compute and memory operations, however, they require significant execution time. The massive parallel computing capability of GPUs make them as one of the ideal platforms to accelerate CNNs and a number of GPU-based CNN libraries have been developed. While existing works mainly focus on the computational efficiency of CNNs, the memory efficiency of CNNs have been largely overlooked. Yet CNNs have intricate data structures and their memory behavior can have significant impact on the performance. In this work, we study the memory efficiency of various CNN layers and reveal the performance implication from both data layouts and memory access patterns. Experiments show the universal effect of our proposed optimizations on both single layers and various networks, with up to 27.9x for a single layer and up to 5.6x on the whole networks."
"Training deep Convolutional Neural Networks (CNN) is a time consuming task that may take weeks to complete. In this article we propose a novel, theoretically founded method for reducing CNN training time without incurring any loss in accuracy. The basic idea is to begin training with a pre-train network using lower-resolution kernels and input images, and then refine the results at the full resolution by exploiting the spatial scaling property of convolutions. We apply our method to the ImageNet winner OverFeat and to the more recent ResNet architecture and show a reduction in training time of nearly 20% while test set accuracy is preserved in both cases."
"Optical Coherence Tomography (OCT) provides a unique ability to image the eye retina in 3D at micrometer resolution and gives ophthalmologist the ability to visualize retinal diseases such as Age-Related Macular Degeneration (AMD). While visual inspection of OCT volumes remains the main method for AMD identification, doing so is time consuming as each cross-section within the volume must be inspected individually by the clinician. In much the same way, acquiring ground truth information for each cross-section is expensive and time consuming. This fact heavily limits the ability to acquire large amounts of ground truth, which subsequently impacts the performance of learning-based methods geared at automatic pathology identification. To avoid this burden, we propose a novel strategy for automatic analysis of OCT volumes where only volume labels are needed. That is, we train a classifier in a semi-supervised manner to conduct this task. Our approach uses a novel Convolutional Neural Network (CNN) architecture, that only needs volume-level labels to be trained to automatically asses whether an OCT volume is healthy or contains AMD. Our architecture involves first learning a cross-section pathology classifier using pseudo-labels that could be corrupted and then leverage these towards a more accurate volume-level classification. We then show that our approach provides excellent performances on a publicly available dataset and outperforms a number of existing automatic techniques."
"Millions of images on the web enable us to explore images from social events such as a family party, thus it is of interest to understand and model the affect exhibited by a group of people in images. But analysis of the affect expressed by multiple people is challenging due to varied indoor and outdoor settings, and interactions taking place between various numbers of people. A few existing works on Group-level Emotion Recognition (GER) have investigated on face-level information. Due to the challenging environments, face may not provide enough information to GER. Relatively few studies have investigated multi-modal GER. Therefore, we propose a novel multi-modal approach based on a new feature description for understanding emotional state of a group of people in an image. In this paper, we firstly exploit three kinds of rich information containing face, upperbody and scene in a group-level image. Furthermore, in order to integrate multiple person's information in a group-level image, we propose an information aggregation method to generate three features for face, upperbody and scene, respectively. We fuse face, upperbody and scene information for robustness of GER against the challenging environments. Intensive experiments are performed on two challenging group-level emotion databases to investigate the role of face, upperbody and scene as well as multi-modal framework. Experimental results demonstrate that our framework achieves very promising performance for GER."
"With advancing technologies, robotic manipulators and visual environment sensors are becoming cheaper and more widespread. However, robot control can be still a limiting factor for better adaptation of these technologies. Robotic manipulators are performing very well in structured workspaces, but do not adapt well to unexpected changes, like people entering the workspace. We present a method combining 3D Camera based workspace mapping, and a predictive and reflexive robot manipulator trajectory estimation to allow more efficient and safer operation in dynamic workspaces. In experiments on a real UR5 robot our method has proven to provide shorter and smoother trajectories compared to a reactive trajectory planner in the same conditions. Furthermore, the robot has successfully avoided any contact by initialising the reflexive movement even when an obstacle got unexpectedly close to the robot. The main goal of our work is to make the operation more flexible in unstructured dynamic workspaces and not just avoid obstacles, but also adapt when performing collaborative tasks with humans in the near future."
"Authenticating websites is an ongoing problem for users. Recent proposals have suggested strengthening current server authentication methods by incorporating website location as an additional authentication factor. In this work, we explore how location information affects users' decision-making for security and privacy. We conducted a series of qualitative interviews to learn how users relate location to their online security, and we designed a security indicator to alert the user to changes in website locations. We evaluated our tool in a 44-participant user study and found that users were less likely to perform security-sensitive tasks when alerted to location changes. Our results suggest that website location can be used as an effective indicator for users' security awareness."
"We present new algorithms to perform fast probabilistic collision queries between convex as well as non-convex objects. Our approach is applicable to general shapes, where one or more objects are represented using Gaussian probability distributions. We present a fast new algorithm for a pair of convex objects, and extend the approach to non-convex models using hierarchical representations. We highlight the performance of our algorithms with various convex and non-convex shapes on complex synthetic benchmarks and trajectory planning benchmarks for a 7-DOF Fetch robot arm."
"Recently, virtual reality, augmented reality, robotics, self-driving cars et al attractive much attention of industrial community, in which image based camera localization is a key task. It is urgent to give an overview of image based camera localization. In this paper, an overview of image based camera localization is presented. It will be useful to not only researchers but also engineers."
"Considering a full-duplex network comprised of a full-duplex (FD) base station and two half-duplex (HD) users, one user transmits on the uplink channel and the other receives through the downlink channel on the same frequency. The uplink user will generate inter-user interference (IUI) on the downlink user through the interference channel. In this paper, we propose an IUI suppression approach when the base station knows the full channel station information. To evaluate the performance of the proposed approach, four cases are considered, i.e., the uplink, the downlink, and the interference channels are Gaussian; the downlink and the interference channels are Rayleigh fading and the uplink channel is Gaussian; the uplink, the downlink, and the interference channels are Rayleigh fading; the uplink, the downlink, and the interference channels are Rician fading. We derive the close-form expression of the achievable rate and the energy efficient for the former two cases and investigate the performance for the latter two cases through Monte Carlo simulations. Analytic and simulation results show that the achievable rate and the energy efficient of the proposed approach is significantly influenced by the signal-to-noise-ratio (SNR), the Rician factor, and the power ratio between the uplink and the interference channels."
"Recognising detailed clothing characteristics (fine-grained attributes) in unconstrained images of people in-the-wild is a challenging task for computer vision, especially when there is only limited training data from the wild whilst most data available for model learning are captured in well-controlled environments using fashion models (well lit, no background clutter, frontal view, high-resolution). In this work, we develop a deep learning framework capable of model transfer learning from well-controlled shop clothing images collected from web retailers to in-the-wild images from the street. Specifically, we formulate a novel Multi-Task Curriculum Transfer (MTCT) deep learning method to explore multiple sources of different types of web annotations with multi-labelled fine-grained attributes. Our multi-task loss function is designed to extract more discriminative representations in training by jointly learning all attributes, and our curriculum strategy exploits the staged easy-to-complex transfer learning motivated by cognitive studies. We demonstrate the advantages of the MTCT model over the state-of-the-art methods on the X-Domain benchmark, a large scale clothing attribute dataset. Moreover, we show that the MTCT model has a notable advantage over contemporary models when the training data size is small."
"Online social networks being extended to geographical space has resulted in large amount of user check-in data. Understanding check-ins can help to build appealing applications, such as location recommendation. In this paper, we propose DeepCity, a feature learning framework based on deep learning, to profile users and locations, with respect to user demographic and location category prediction. Both of the predictions are essential for social network companies to increase user engagement. The key contribution of DeepCity is the proposal of task-specific random walk which uses the location and user properties to guide the feature learning to be specific to each prediction task. Experiments conducted on 42M check-ins in three cities collected from Instagram have shown that DeepCity achieves a superior performance and outperforms other baseline models significantly."
"An accurate and reliable image based fruit detection system is critical for supporting higher level agriculture tasks such as yield mapping and robotic harvesting. This paper presents the use of a state-of-the-art object detection framework, Faster R-CNN, in the context of fruit detection in orchards, including mangoes, almonds and apples. Ablation studies are presented to better understand the practical deployment of the detection network, including how much training data is required to capture variability in the dataset. Data augmentation techniques are shown to yield significant performance gains, resulting in a greater than two-fold reduction in the number of training images required. In contrast, transferring knowledge between orchards contributed to negligible performance gain over initialising the Deep Convolutional Neural Network directly from ImageNet features. Finally, to operate over orchard data containing between 100-1000 fruit per image, a tiling approach is introduced for the Faster R-CNN framework. The study has resulted in the best yet detection performance for these orchards relative to previous works, with an F1-score of >0.9 achieved for apples and mangoes."
"Recent imaging technologies are rapidly evolving for sampling richer and more immersive representations of the 3D world. And one of the emerging technologies are light field (LF) cameras based on micro-lens arrays. To record the directional information of the light rays, a much larger storage space and transmission bandwidth are required by a LF image as compared to a conventional 2D image of similar spatial dimension, and the compression of LF data becomes a vital part of its application.   In this paper, we propose a LF codec that fully exploit the intrinsic geometry between the LF sub-views by first approximating the LF with disparity guided sparse coding over a perspective shifted light field dictionary. The sparse coding is only based on several optimized Structural Key Views (SKV), however the entire LF can be recovered from the coding coefficients. By keeping the approximation identical between encoder and decoder, only the sparse coding residual and the SKVs needs to be transmitted. An optimized SKV selection method is proposed such that most LF spatial information could be preserved. And to achieve optimum dictionary efficiency, the LF is divided into several Coding Regions (CR), over which the reconstruction works individually. Experiments and comparisons have been carried out over benchmark LF dataset which show that the proposed SC-SKV codec produces state-of-the-art compression results in terms of rate-distortion performance and visual quality compared with High Efficiency Video Coding (HEVC): with 37.79% BD rate reduction and 0.92 dB BD-PSNR improvement achieved on average, especially with up to 4 dB improvement for low bit rate scenarios."
"In the context of estimating material properties of porous walls based on in-site measurements and identification method, this paper presents the concept of Optimal Experiment Design (OED). It aims at searching the best experimental conditions in terms of quantity and position of sensors and boundary conditions imposed to the material. These optimal conditions ensure to provide the maximum accuracy of the identification method and thus the estimated parameters. The search of the OED is done by using the Fisher information matrix and a priori knowledge of the parameters. The methodology is applied for two case studies. The first one deals with purely conductive heat transfer. The concept of optimal experiment design is detailed and verified with 100 inverse problems for different experiment designs. The second case study combines a strong coupling between heat and moisture transfer through a porous building material. The methodology presented is based on a scientific formalism for efficient planning of experimental work that can be extended to the optimal design of experiments related to other problems in thermal and fluid sciences."
"In this paper, a method for low-cost system design oriented to indoor and outdoor autonomous navigation is illustrated. In order to provide a motivation for the solution here presented, a brief discussion of the typical drawbacks of state-of-the-art technologies is reported. Finally, an application of such a method for the design of a navigation system for blindfolded people is shown."
"We introduce an informative labeling algorithm for the vertices of a family of Koch networks. Each of the labels is consisted of two parts, the precise position and the time adding to Koch networks. The shortest path routing between any two vertices is determined only on the basis of their labels, and the routing is calculated only by few computations. The rigorous solutions of betweenness centrality for every node and edge are also derived by the help of their labels. Furthermore, the community structure in Koch networks is studied by the current and voltage characteristics of its resistor networks."
"Academic leadership is essential for research innovation and impact. Until now, there has been no dedicated measure of leadership by bibliometrics. Popular bibliometric indices are mainly based on academic output, such as the journal impact factor and the number of citations. Here we develop an academic leadership index based on readily available bibliometric data that is sensitive to not only academic output but also research efficiency. Our leadership index was tested in two studies on peer-reviewed journal papers by extramurally-funded principal investigators in the field of life sciences from China and the USA, respectively. The leadership performance of these principal investigators was quantified and compared relative to university rank and other factors. As a validation measure, we show that the highest average leadership index was achieved by principal investigators at top national universities in both countries. More interestingly, our results also indicate that on an individual basis, strong leadership and high efficiency are not necessarily associated with those at top-tier universities nor with the most funding. This leadership index may become the basis of a comprehensive merit system, facilitating academic evaluation and resource management."
"This paper explores new evaluation perspectives for image captioning and introduces a noun translation task that achieves comparative image caption generation performance by translating from a set of nouns to captions. This implies that in image captioning, all word categories other than nouns can be evoked by a powerful language model without sacrificing performance on n-gram precision. The paper also investigates lower and upper bounds of how much individual word categories in the captions contribute to the final BLEU score. A large possible improvement exists for nouns, verbs, and prepositions."
The goal of semi-supervised learning is to improve supervised classifiers by using additional unlabeled training examples. In this work we study a simple self-learning approach to semi-supervised learning applied to the least squares classifier. We show that a soft-label and a hard-label variant of self-learning can be derived by applying block coordinate descent to two related but slightly different objective functions. The resulting soft-label approach is related to an idea about dealing with missing data that dates back to the 1930s. We show that the soft-label variant typically outperforms the hard-label variant on benchmark datasets and partially explain this behaviour by studying the relative difficulty of finding good local minima for the corresponding objective functions.
"We study a model of dynamic two-stage R&D competition where the competing firms are uncertain about the difficulty of the first stage. Staying in the competition is costly and a firm can also choose whether and when to quit. When a firm solves the first stage, it can choose whether and when to disclose the solution. We find that there exists a unique symmetric equilibrium, in which each firm will disclose the solution of the first stage if it arrives early, withhold the solution if it arrives neither too soon nor too late, and exit the competition if it has not arrived after a sufficiently long time. From a social welfare perspective, a competition is surprisingly not always optimal: in certain scenarios, it is socially more desirable to assign the R&D project to a single firm."
"Given a network represented by a graph $G=(V,E)$, we consider a dynamical process of influence diffusion in $G$ that evolves as follows: Initially only the nodes of a given $S\subseteq V$ are influenced; subsequently, at each round, the set of influenced nodes is augmented by all the nodes in the network that have a sufficiently large number of already influenced neighbors. The question is to determine a small subset of nodes $S$ (\emph{a target set}) that can influence the whole network. This is a widely studied problem that abstracts many phenomena in the social, economic, biological, and physical sciences. It is known that the above optimization problem is hard to approximate within a factor of $2^{\log^{1-\epsilon}|V|}$, for any $\epsilon >0$. In this paper, we present a fast and surprisingly simple algorithm that exhibits the following features: 1) when applied to trees, cycles, or complete graphs, it always produces an optimal solution (i.e, a minimum size target set); 2) when applied to arbitrary networks, it always produces a solution of cardinality which improves on the previously known upper bound; 3) when applied to real-life networks, it always produces solutions that substantially outperform the ones obtained by previously published algorithms (for which no proof of optimality or performance guarantee is known in any class of graphs)."
"Caching of popular content on wireless nodes is recently proposed as a means to reduce congestion in the backbone of cellular networks and to improve Quality of Service. From a network point of view, the goal is to offload as many users as possible from the backbone network to the wireless caches. However, if not done appropriately, this can lead to an unbalanced association of users to caches. As a result, some nodes can be overloaded while the resources of others are underused. Given a fixed content placement, this work proposes an efficient distributed algorithm to balance the association of user traffic among cellular cache memories. The algorithm allows the network to achieve the globally optimal solution and can be executed on base stations using a limited amount of information exchange between them. It is based on a novel algorithm we call Bucket-filling. The solution guarantees the utilization of each cache in a fair way rendering all available caches useful. The improvement compared to common user assignment policies is highlighted for single- as well as for multi-tier random networks."
"We discuss the scheduling of a set of networked control systems implemented over a shared communication network. Each control loop is described by a linear-time-invariant (LTI) system with an event-triggered implementation. We assume the network can be used by at most one control loop at any time instant and after each controller update, a pre-defined channel occupancy time elapses before the network is available. In our framework we offer the scheduler two options to avoid conflicts: using the event-triggering mechanism, where the scheduler can choose the triggering coefficient; or forcing controller updates at an earlier pre-defined time. Our objective is avoiding communication conflict while guaranteeing stability of all control loops. We formulate the original scheduling problem as a control synthesis problem over a network of timed game automata (NTGA) with a safety objective. The NTGA is obtained by taking the parallel composition of the timed game automata (TGA) associated with the network and with all control loops. The construction of TGA associated with control loops leverages recent results on the abstraction of timing models of event-triggered LTI systems. In our problem, the safety objective is to avoid that update requests from a control loop happen while the network is in use by another task. We showcase the results in some examples."
"In digital communication systems different clock frequencies of transmitter and receiver usually is translated into cycle slips. Receivers might experience different sampling frequencies from transmitter due to manufacturing imperfection, Doppler Effect introduced by channel or wrong estimation of symbol rate. Timing synchronization in presence of cycle slip for a burst sequence of received information, leads to severe degradation in system performance that represents as shortening or prolonging of bit stream. Therefor the necessity of prior detection and elimination of cycle slip is unavoidable. Accordingly, the main idea introduced in this paper is to employ the Gardner Detector (GAD) not only to recover a fixed timing offset, its output is also processed in a way such that timing drifts can be estimated and corrected. Deriving a two steps algorithm, eliminates the cycle slips arising from wrong estimation of symbol rate firstly, and then iteratively synchronize symbol timing of a burst received signal by applying GAD to a feed forward structure with the additional benefits that convergence and stability problems are avoided, as they are typical for feedback schemes normally used by GAD. The proposed algorithm is able to compensate considerable symbol rate offsets at the receiver side. Considerable results in terms of BER confirm the algorithm proficiency."
We propose an algorithm for exploring the entire regularization path of asymmetric-cost linear support vector machines. Empirical evidence suggests the predictive power of support vector machines depends on the regularization parameters of the training algorithms. The algorithms exploring the entire regularization paths have been proposed for single-cost support vector machines thereby providing the complete knowledge on the behavior of the trained model over the hyperparameter space. Considering the problem in two-dimensional hyperparameter space though enables our algorithm to maintain greater flexibility in dealing with special cases and sheds light on problems encountered by algorithms building the paths in one-dimensional spaces. We demonstrate two-dimensional regularization paths for linear support vector machines that we train on synthetic and real data.
"When utilities are additive, we uncovered in our previous paper (Bogomolnaia et al. ""Dividing Goods or Bads under Additive Utilities"") many similarities but also surprising differences in the behavior of the familiar Competitive rule (with equal incomes), when we divide (private) goods or bads. The rule picks in both cases the critical points of the product of utilities (or disutilities) on the efficiency frontier, but there is only one such point if we share goods, while there can be exponentially many in the case of bads.   We extend this analysis to the fair division of mixed items: each item can be viewed by some participants as a good and by others as a bad, with corresponding positive or negative marginal utilities. We find that the division of mixed items boils down, normatively as well as computationally, to a variant of an all goods problem, or of an all bads problem: in particular the task of dividing the non disposable items must be either good news for everyone, or bad news for everyone.   If at least one feasible utility profile is positive, the Competitive rule picks the unique maximum of the product of (positive) utilities. If no feasible utility profile is positive, this rule picks all critical points of the product of disutilities on the efficient frontier."
"The first objective towards the effective use of microblogging services such as Twitter for situational awareness during the emerging disasters is discovery of the disaster-related postings. Given the wide range of possible disasters, using a pre-selected set of disaster-related keywords for the discovery is suboptimal. An alternative that we focus on in this work is to train a classifier using a small set of labeled postings that are becoming available as a disaster is emerging. Our hypothesis is that utilizing large quantities of historical microblogs could improve the quality of classification, as compared to training a classifier only on the labeled data. We propose to use unlabeled microblogs to cluster words into a limited number of clusters and use the word clusters as features for classification. To evaluate the proposed semi-supervised approach, we used Twitter data from 6 different disasters. Our results indicate that when the number of labeled tweets is 100 or less, the proposed approach is superior to the standard classification based on the bag or words feature representation. Our results also reveal that the choice of the unlabeled corpus, the choice of word clustering algorithm, and the choice of hyperparameters can have a significant impact on the classification accuracy."
"In this work we implement a training of a Language Model (LM), using Recurrent Neural Network (RNN) and GloVe word embeddings, introduced by Pennigton et al. in [1]. The implementation is following the general idea of training RNNs for LM tasks presented in [2], but is rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM [4]."
"A fall is an abnormal activity that occurs rarely, so it is hard to collect real data for falls. It is, therefore, difficult to use supervised learning methods to automatically detect falls. Another challenge in using machine learning methods to automatically detect falls is the choice of features. In this paper, we propose to use an ensemble of autoencoders to extract features from different channels of wearable sensor data trained only on normal activities. We show that choosing a threshold as maximum of the reconstruction error on the training normal data is not the right way to identify unseen falls. We propose two methods for automatic tightening of reconstruction error from only the normal activities for better identification of unseen falls. We present our results on two activity recognition datasets and show the efficacy of our proposed method against traditional autoencoder models and two standard one-class classification methods."
"Certain applications such as Magnetic Resonance Imaging (MRI) require the reconstruction of functions from Fourier spectral data. When the underlying functions are piecewise-smooth, standard Fourier approximation methods suffer from the Gibbs phenomenon - with associated oscillatory artifacts in the vicinity of edges and an overall reduced order of convergence in the approximation. This paper proposes an edge-augmented Fourier reconstruction procedure which uses only the first few Fourier coefficients of an underlying piecewise-smooth function to accurately estimate jump information and then incorporate it into a Fourier partial sum approximation. We provide both theoretical and empirical results showing the improved accuracy of the proposed method, as well as comparisons demonstrating superior performance over existing state-of-the-art sparse optimization-based methods. Extensions of the proposed techniques to functions of several variables are also addressed preliminarily. All code used to generate the results in this report are made publicly available."
"Traditionally, reconfiguration problems ask the question whether a given solution of an optimization problem can be transformed to a target solution in a sequence of small steps that preserve feasibility of the intermediate solutions. In this paper, rather than asking this question from an algorithmic perspective, we analyze the combinatorial structure behind it. We consider the problem of reconfiguring one independent set into another, using two different processes: (1) exchanging exactly $k$ vertices in each step, or (2) removing or adding one vertex in each step while ensuring the intermediate sets contain at most $k$ fewer vertices than the initial solution. We are interested in determining the minimum value of $k$ for which this reconfiguration is possible, and bound these threshold values in terms of several structural graph parameters. For hereditary graph classes we identify structures that cause the reconfiguration threshold to be large."
"In this paper, we introduce the task of targeted aspect-based sentiment analysis. The goal is to extract fine-grained information with respect to entities mentioned in user comments. This work extends both aspect-based sentiment analysis that assumes a single entity per document and targeted sentiment analysis that assumes a single sentiment towards a target entity. In particular, we identify the sentiment towards each aspect of one or more entities. As a testbed for this task, we introduce the SentiHood dataset, extracted from a question answering (QA) platform where urban neighbourhoods are discussed by users. In this context units of text often mention several aspects of one or more neighbourhoods. This is the first time that a generic social media platform in this case a QA platform, is used for fine-grained opinion mining. Text coming from QA platforms is far less constrained compared to text from review specific platforms which current datasets are based on. We develop several strong baselines, relying on logistic regression and state-of-the-art recurrent neural networks."
"Objective of this work is to integrate high performance computing (HPC) technologies and bioacoustics data-mining capabilities by offering a MATLAB-based toolbox called Raven-X. Raven-X will provide a hardware-independent solution, for processing large acoustic datasets - the toolkit will be available to the community at no cost. This goal will be achieved by leveraging prior work done which successfully deployed MATLAB based HPC tools within Cornell University's Bioacoustics Research Program (BRP). These tools enabled commonly available multi-core computers to process data at accelerated rates to detect and classify whale sounds in large multi-channel sound archives. Through this collaboration, we will expand on this effort which was featured through Mathworks research and industry forums incorporate new cutting-edge detectors and classifiers, and disseminate Raven-X to the broader bioacoustics community."
"This work characterizes the benefits of averaging techniques widely used in conjunction with stochastic gradient descent (SGD). In particular, this work sharply analyzes: (1) mini-batching, a method of averaging many samples of the gradient to both reduce the variance of a stochastic gradient estimate and for parallelizing SGD and (2) tail-averaging, a method involving averaging the final few iterates of SGD in order to decrease the variance in SGD's final iterate. This work presents the first tight non-asymptotic generalization error bounds for these schemes for the stochastic approximation problem of least squares regression.   Furthermore, this work establishes a precise problem-dependent extent to which mini-batching can be used to yield provable near-linear parallelization speedups over SGD with batch size one. These results are utilized in providing a highly parallelizable SGD algorithm that obtains the optimal statistical error rate with nearly the same number of serial updates as batch gradient descent, which improves significantly over existing SGD-style methods.   Finally, this work sheds light on some fundamental differences in SGD's behavior when dealing with agnostic noise in the (non-realizable) least squares regression problem. In particular, the work shows that the stepsizes that ensure optimal statistical error rates for the agnostic case must be a function of the noise properties.   The central analysis tools used by this paper are obtained through generalizing the operator view of averaged SGD, introduced by Defossez and Bach (2015) followed by developing a novel analysis in bounding these operators to characterize the generalization error. These techniques may be of broader interest in analyzing various computational aspects of stochastic approximation."
"We introduce a convolutional neural network for inferring a compact disentangled graphical description of objects from 2D images that can be used for volumetric reconstruction. The network comprises an encoder and a twin-tailed decoder. The encoder generates a disentangled graphics code. The first decoder generates a volume, and the second decoder reconstructs the input image using a novel training regime that allows the graphics code to learn a separate representation of the 3D object and a description of its lighting and pose conditions. We demonstrate this method by generating volumes and disentangled graphical descriptions from images and videos of faces and chairs."
"Journal maps and classifications for 11,359 journals listed in the combined Journal Citation Reports 2015 of the Science and Social Sciences Citation Indexes are provided at http://www.leydesdorff.net/jcr15. A routine using VOSviewer for integrating the journal mapping and their hierarchical clustering is also made available. In this short communication, we provide background on the journal mapping/clustering and an explanation and instructions about the routine. We compare 2015 journal maps with those for 2014 and show the delineations among fields and subfields to be sensitive to fluctuations. Labels for fields and sub-fields are not provided by the routine, but can be added by an analyst for pragmatic or intellectual reasons. The routine provides a means for testing one's assumptions against a baseline without claiming authority; clusters of related journals can be visualized to understand communities. The routine is generic and can be used for any 1-mode network."
"Many compelling video post-processing effects, in particular aesthetic focus editing and refocusing effects, are feasible if per-frame depth information is available. Existing computational methods to capture RGB and depth either purposefully modify the optics (coded aperture, light-field imaging), or employ active RGB-D cameras. Since these methods are less practical for users with normal cameras, we present an algorithm to capture all-in-focus RGB-D video of dynamic scenes with an unmodified commodity video camera. Our algorithm turns the often unwanted defocus blur into a valuable signal. The input to our method is a video in which the focus plane is continuously moving back and forth during capture, and thus defocus blur is provoked and strongly visible. This can be achieved by manually turning the focus ring of the lens during recording. The core algorithmic ingredient is a new video-based depth-from-defocus algorithm that computes space-time-coherent depth maps, deblurred all-in-focus video, and the focus distance for each frame. We extensively evaluate our approach, and show that it enables compelling video post-processing effects, such as different types of refocusing."
"Let P be a set of points in R^d, and let M be a function that maps any subset of P to a positive real number. We examine the problem of computing the exact mean and variance of M when a subset of points in P is selected according to a well-defined random distribution. We consider two distributions; in the first distribution (which we call the Bernoulli distribution), each point p in P is included in the random subset independently, with probability pi(p). In the second distribution (the fixed-size distribution), a subset of exactly s points is selected uniformly at random among all possible subsets of s points in P.   This problem is a crucial part of modern ecological analyses; each point in P represents a species in d-dimensional trait space, and the goal is to compute the statistics of a geometric measure on this trait space, when subsets of species are selected under random processes.   We present efficient exact algorithms for computing the mean and variance of several geometric measures when point sets are selected under one of the described random distributions. More specifically, we provide algorithms for the following measures: the bounding box volume, the convex hull volume, the mean pairwise distance (MPD), the squared Euclidean distance from the centroid, and the diameter of the minimum enclosing disk. We also describe an efficient (1-e)-approximation algorithm for computing the mean and variance of the mean pairwise distance. We implemented three of our algorithms and we show that our implementations can provide major speedups compared to the existing imprecise approaches."
"Decentralized coded caching is studied for a content server with $N$ files, each of size $F$ bits, serving $K$ active users, each equipped with a cache of distinct capacity. It is assumed that the users' caches are filled in advance during the off-peak traffic period without the knowledge of the number of active users, their identities, or the particular demands. User demands are revealed during the peak traffic period, and are served simultaneously through an error-free shared link. A new decentralized coded caching scheme is proposed for this scenario, and it is shown to improve upon the state-of-the-art in terms of the required delivery rate over the shared link, when there are more users in the system than the number of files. Numerical results indicate that the improvement becomes more significant as the cache capacities of the users become more skewed."
"A novel reinforcement learning benchmark, called Industrial Benchmark, is introduced. The Industrial Benchmark aims at being be realistic in the sense, that it includes a variety of aspects that we found to be vital in industrial applications. It is not designed to be an approximation of any real system, but to pose the same hardness and complexity."
"We present the first constructions of single-prover proof systems that achieve perfect zero knowledge (PZK) for languages beyond NP, under no intractability assumptions:   1. The complexity class #P has PZK proofs in the model of Interactive PCPs (IPCPs) [KR08], where the verifier first receives from the prover a PCP and then engages with the prover in an Interactive Proof (IP).   2. The complexity class NEXP has PZK proofs in the model of Interactive Oracle Proofs (IOPs) [BCS16,RRR16], where the verifier, in every round of interaction, receives a PCP from the prover.   Our constructions rely on succinct simulators that enable us to ""simulate beyond NP"", achieving exponential savings in efficiency over [BCGV16]. These simulators crucially rely on solving a problem that lies at the intersection of coding theory, linear algebra, and computational complexity, which we call the succinct constraint detection problem, and consists of detecting dual constraints with polynomial support size for codes of exponential block length. Our two results rely on solutions to this problem for fundamental classes of linear codes:   * An algorithm to detect constraints for Reed--Muller codes of exponential length.   * An algorithm to detect constraints for PCPs of Proximity of Reed--Solomon codes [BS08] of exponential degree.   The first algorithm exploits the Raz--Shpilka [RS05] deterministic polynomial identity testing algorithm, and shows, to our knowledge, a first connection of algebraic complexity theory with zero knowledge. Along the way, we give a perfect zero knowledge analogue of the celebrated sumcheck protocol [LFKN92], by leveraging both succinct constraint detection and low-degree testing. The second algorithm exploits the recursive structure of the PCPs of Proximity to show that small-support constraints are ""locally"" spanned by a small number of small-support constraints."
"Modern processing networks often consist of heterogeneous servers with widely varying capabilities, and process job flows with complex structure and requirements. A major challenge in designing efficient scheduling policies in these networks is the lack of reliable estimates of system parameters, and an attractive approach for addressing this challenge is to design robust policies, i.e., policies that do not use system parameters such as arrival and/or service rates for making scheduling decisions. In this paper, we propose a general framework for the design of robust policies. The main technical novelty is the use of a stochastic gradient projection method that reacts to queue-length changes in order to find a balanced allocation of service resources to incoming tasks. We illustrate our approach on two broad classes of processing systems, namely the flexible fork-join networks and the flexible queueing networks, and prove the rate stability of our proposed policies for these networks under non-restrictive assumptions."
"Question generation has been a research topic for a long time, where a big challenge is how to generate deep and natural questions. To tackle this challenge, we propose a system to generate natural language questions from a domain-specific knowledge base (KB) by utilizing rich web information. A small number of question templates are first created based on the KB and instantiated into questions, which are used as seed set and further expanded through the web to get more question candidates. A filtering model is then applied to select candidates with high grammaticality and domain relevance. The system is able to generate large amount of in-domain natural language questions with considerable semantic diversity and is easily applicable to other domains. We evaluate the quality of the generated questions by human judgments and the results show the effectiveness of our proposed system."
"We present a continuous model for structural brain connectivity based on the Poisson point process. The model treats each streamline curve in a tractography as an observed event in connectome space, here a product space of cortical white matter boundaries. We approximate the model parameter via kernel density estimation. To deal with the heavy computational burden, we develop a fast parameter estimation method by pre-computing associated Legendre products of the data, leveraging properties of the spherical heat kernel. We show how our approach can be used to assess the quality of cortical parcellations with respect to connectivty. We further present empirical results that suggest the discrete connectomes derived from our model have substantially higher test-retest reliability compared to standard methods."
"We consider the problem of finding a small hitting set in an {\it infinite} range space $\cF=(Q,\cR)$ of bounded VC-dimension. We show that, under reasonably general assumptions, the infinite dimensional convex relaxation can be solved (approximately) efficiently by multiplicative weight updates. As a consequence, we get an algorithm that finds, for any $\delta>0$, a set of size $O(s_{\cF}(z^*_\cF))$ that hits $(1-\delta)$-fraction of $\cR$ (with respect to a given measure) in time proportional to $\log(\frac{1}{\delta})$, where $s_{\cF}(\frac{1}{\epsilon})$ is the size of the smallest $\epsilon$-net the range space admits, and $z^*_{\cF}$ is the value of the {\it fractional} optimal solution. These assumptions hold, for instance, in the case when the range space represents the {\it visibility} regions of a polygon in $\RR^2$, giving thus a deterministic polynomial time $O(\log z^*_{\cF})$-approximation algorithm for guarding $(1-\delta)$-fraction of the area of any given simple polygon, with running time proportional to $\polylog(\frac{1}{\delta})$."
"This paper proposes a recursive diffeomorphism based regression method for one-dimensional generalized mode decomposition problem that aims at extracting generalized modes $\alpha_k(t)s_k(2\pi N_k\phi_k(t))$ from their superposition $\sum_{k=1}^K \alpha_k(t)s_k(2\pi N_k\phi_k(t))$. First, a one-dimensional synchrosqueezed transform is applied to estimate instantaneous information, e.g., $\alpha_k(t)$ and $N_k\phi_k(t)$. Second, a novel approach based on diffeomorphisms and nonparametric regression is proposed to estimate wave shape functions $s_k(t)$. These two methods lead to a framework for the generalized mode decomposition problem under a weak well-separation condition. Numerical examples of synthetic and real data are provided to demonstrate the fruitful applications of these methods."
"Vehicular WiFi is different from conventional WiFi access. Firstly, as the connections arise opportunistically, they are short lived and intermittent. Secondly, at vehicular speeds channel conditions change rapidly. Under these conditions, the MAC bit rate adaptation plays a critical role to ensure that devices can maximize throughput performance. The goal of this work is to gain a deeper understanding of current MAC bit rate adaptation algorithms and their interaction with higher layer protocols under vehicular settings. Towards this end, we evaluate the performance of four MAC bit rate adaptation algorithms; SampleRate, AMRR, Minstrel, and RRAA, and their interactions with transport layer protocols such as TCP in real world vehicular settings. Our experimental study reveals that in general these algorithm perform poorly in vehicular settings, and can have an extremely adverse impact on TCP performance."
"In the traditional setup of public goods game all players are involved in every available groups and the mutual benefit is shared among competing cooperator and defector strategies. But in real life situations the group formation of players could be more sophisticated because not all players are attractive enough for others to participate in a joint venture. What if only those players can initiate a group formation and establish a game who are successful enough to the neighbors? To elaborate this idea we employ a modified protocol and demonstrate that a carefully chosen threshold to establish joint venture could efficiently improve the cooperation level even if the synergy factor would suggest a full defector state otherwise. The microscopic mechanism which is responsible for this effect is based on the asymmetric consequences of competing strategies: while the success of a cooperator provides a long-time well-being for the neighborhood, the temporary advantage of defection cannot be maintained if the protocol is based on the success of leaders."
"The temperature developed in bondwires of integrated circuits (ICs) is a possible source of malfunction, and has to be taken into account during the design phase of an IC. Due to manufacturing tolerances, a bondwire's geometrical characteristics are uncertain parameters, and as such their impact has to be examined with the use of uncertainty quantification (UQ) methods. Sampling methods, like the Monte Carlo (MC), converge slowly, while efficient alternatives scale badly with respect to the number of considered uncertainties. Possible remedies to this, so-called, curse of dimensionality are sought in the application of stochastic collocation (SC) on sparse grids (SGs) and of the recently emerged low-rank tensor decomposition methods, with emphasis on the tensor train (TT) decomposition."
"For the Lucas sequence $\{U_{k}(P,Q)\}$ we discuss the identities such as the well-known Fibonacci identities. For example, the generalizations of $F_{2k}=F_{k+1}^2-F_{k-1}^2$ and $F_{2k+1}=F_{k+1}^2+F_{k}^2$ are $P U_{2k}=U_{k+1}^2- Q^2 U_{k-1}^2$ and $U_{2k+1}=U_{k+1}^2-Q U_{k}^2$, respectively. We propose a new simple method for obtaining identities involving any recurrences and we use it to obtain new identities involving the Fibonacci numbers such as $F_{k+3}^5-F_{k-3}^5+60F_{k}^5=8(F_{k+2}^5+F_{k-2}^5)+40(F_{k+1}^5-F_{k-1}^5)$."
"Various spectrum sensing approaches have been shown to suffer from a so-called SNR-wall, an SNR value below which a detector cannot perform robustly no matter how many observations are used. Up to now, the eigenvalue-based maximum-minimum-eigenvalue (MME) detector has been a notable exception. For instance, the model uncertainty of imperfect knowledge of the receiver noise power, which is known to be responsible for the energy detector's fundamental limits, does not adversely affect the MME detector's performance. While additive white Gaussian noise (AWGN) is a standard assumption in wireless communications, it is not a reasonable one for the MME detector. In fact, in this work we prove that uncertainty in the amount of noise coloring does lead to an SNR-wall for the MME detector. We derive a lower bound on this SNR-wall and evaluate it for example scenarios. The findings are supported by numerical simulations."
"In a sequence of recent results (PODC 2015 and PODC 2016), the running time of the fastest algorithm for the \emph{minimum spanning tree (MST)} problem in the \emph{Congested Clique} model was first improved to $O(\log \log \log n)$ from $O(\log \log n)$ (Hegeman et al., PODC 2015) and then to $O(\log^* n)$ (Ghaffari and Parter, PODC 2016). All of these algorithms use $\Theta(n^2)$ messages independent of the number of edges in the input graph.   This paper positively answers a question raised in Hegeman et al., and presents the first ""super-fast"" MST algorithm with $o(m)$ message complexity for input graphs with $m$ edges. Specifically, we present an algorithm running in $O(\log^* n)$ rounds, with message complexity $\tilde{O}(\sqrt{m \cdot n})$ and then build on this algorithm to derive a family of algorithms, containing for any $\varepsilon$, $0 < \varepsilon \le 1$, an algorithm running in $O(\log^* n/\varepsilon)$ rounds, using $\tilde{O}(n^{1 + \varepsilon}/\varepsilon)$ messages. Setting $\varepsilon = \log\log n/\log n$ leads to the first sub-logarithmic round Congested Clique MST algorithm that uses only $\tilde{O}(n)$ messages.   Our primary tools in achieving these results are (i) a component-wise bound on the number of candidates for MST edges, extending the sampling lemma of Karger, Klein, and Tarjan (Karger, Klein, and Tarjan, JACM 1995) and (ii) $\Theta(\log n)$-wise-independent linear graph sketches (Cormode and Firmani, Dist.~Par.~Databases, 2014) for generating MST candidate edges."
"Deep convolutional neural networks (ConvNets) have been recently shown to attain state-of-the-art performance for action recognition on standard-resolution videos. However, less attention has been paid to recognition performance at extremely low resolutions (eLR) (e.g., 16 x 12 pixels). Reliable action recognition using eLR cameras would address privacy concerns in various application environments such as private homes, hospitals, nursing/rehabilitation facilities, etc. In this paper, we propose a semi-coupled filter-sharing network that leverages high resolution (HR) videos during training in order to assist an eLR ConvNet. We also study methods for fusing spatial and temporal ConvNets customized for eLR videos in order to take advantage of appearance and motion information. Our method outperforms state-of-the-art methods at extremely low resolutions on IXMAS (93.7%) and HMDB (29.2%) datasets."
"Similarity learning has received a large amount of interest and is an important tool for many scientific and industrial applications. In this framework, we wish to infer the distance (similarity) between points with respect to an arbitrary distance function $d$. Here, we formulate the problem as a regression from a feature space $\mathcal{X}$ to an arbitrary vector space $\mathcal{Y}$, where the Euclidean distance is proportional to $d$. We then give Rademacher complexity bounds on the generalization error. We find that with high probability, the complexity is bounded by the maximum of the radius of $\mathcal{X}$ and the radius of $\mathcal{Y}$."
"We conjecture that bounded generalised polynomial functions cannot be generated by finite automata, except for the trivial case when they are periodic away from a finite set. Using methods from ergodic theory, we are able to partially resolve this conjecture, proving that any hypothetical counterexample is periodic away from a very sparse and structured set. In particular, we show that for a polynomial $p(n)$ with at least one irrational coefficient (except for the constant one) and integer $m$, the sequence $\lfloor p(n) \rfloor \bmod{m}$ is never automatic. We also obtain a conditional result, where we prove the conjecture under the assumption that the characteristic sequence of the set of powers of an integer $k\geq 2$ is not given by a generalised polynomial."
"Friendship is a fundamental characteristic of human beings and usually assumed to be reciprocal in nature. Despite this common expectation, in reality, not all friendships by default are reciprocal nor created equal. Here, we show that reciprocated friendships are more intimate and they are substantially different from those that are not. We examine the role of reciprocal ties in inducing more effective peer pressure in a cooperative arrangements setting and find that the directionality of friendship ties can significantly limit the ability to persuade others to act. Specifically, we observe a higher behavioral change and more effective peer-influence when subjects shared reciprocal ties with their peers compared to sharing unilateral ones. Moreover, through spreading process simulation, we find that although unilateral ties diffuse behaviors across communities, reciprocal ties play more important role at the early stages of the diffusion process."
"Straintronic magneto-tunneling junction (s-MTJ) switches, whose resistances are controlled with voltage-generated strain in the magnetostrictive free layer of the MTJ, are extremely energy-efficient switches that would dissipate a few aJ of energy during switching. Unfortunately, they are also relatively error-prone and have low resistance on/off ratio. This suggests that as computing elements, they are best suited for non-Boolean architectures. Here, we propose and analyze a ternary content addressable memory implemented with s-MTJs and some transistors. It overcomes challenges encountered by traditional all-transistor implementations, resulting in exceptionally high cell density."
"Moving target defense (MTD) techniques that enable a system to randomize its configuration to thwart prospective attacks are an effective security solution for tomorrow's wireless networks. However, there is a lack of analytical techniques that enable one to quantify the benefits and tradeoffs of MTDs. In this paper, a novel approach for implementing MTD techniques that can be used to randomize cryptographic techniques and keys in wireless networks is proposed. In particular, the problem is formulated as a stochastic game in which a base station (BS), acting as a defender seeks to strategically change its cryptographic techniques and keys in an effort to deter an attacker that is trying to eavesdrop on the data. The game is shown to exhibit a single-controller property in which only one player, the defender, controls the state of the game. For this game, the existence and properties of the Nash equilibrium are studied, in the presence of a defense cost for using MTD. Then, a practical algorithm for deriving the equilibrium MTD strategies is derived. Simulation results show that the proposed game-theoretic MTD framework can significantly improve the overall utility of the defender, while enabling effective randomization over cryptographic techniques."
"This work provides support for the notion that distributional methods of representing word meaning from computational linguistics are useful for capturing neural correlates of real life multi-sensory stimuli, where the stimuli ---in this case, a movie being watched by the human subjects--- have been given text annotations. We present an approach to combining sequences of word vectors into a single vector. We also identify a semantically-relevant low-dimensional shared representation of fMRI response in an unsupervised fashion by using views of multiple subjects watching the same natural movie stimulus. Learned orthogonal linear maps between the fMRI and semantic representations allow us to successfully transform fMRI data generated by a natural movie stimulus into semantic vectors representing textual descriptions of the movie. We succeed at a scene classification task with 76% accuracy, over a 20% chance rate. When we selected five brain regions-of-interest (ROIs) and learned distinct maps from these ROIs to the text representations, the Default Mode Network (DMN) supported the highest level of decoding performance."
"As the number and variety of smart devices increase, users may use myriad devices in their daily lives and the online activities become highly fragmented. Building an accurate user identity becomes a difficult and important problem for advertising companies. The task for the CIKM Cup 2016 Track 1 was to find the same user cross multiple devices. This paper discusses our solution to the challenge. It is mainly comprised of three parts: comprehensive feature engineering, negative sampling, and model selection. For each part we describe our special steps and demonstrate how the performance is boosted. We took the second prize of the competition with an F1-score of 0.41669."
"We present some generalizations of quantum information inequalities involving tracial positive linear maps between $C^*$-algebras. Among several results, we establish a noncommutative Heisenberg uncertainty relation. More precisely, we show that if $\Phi: \mathcal{A} \to \mathcal{B}$ is a tracial positive linear map between $C^*$-algebras , $\rho \in \mathcal{A}$ is a $\Phi$-density element and $A,B$ are self-adjoint operators of $\mathcal{A}$ such that $ {\rm sp}(\mbox{-i}\rho^\frac{1}{2}[A,B]\rho^\frac{1}{2}) \subseteq [m,M] $ for some scalers $0<m<M$, then under some conditions \begin{eqnarray}\label{inemain1} V_{\rho,\Phi}(A)\sharp V_{\rho,\Phi}(B)\geq \frac{1}{2\sqrt{K_{m,M}(\rho[A,B])}} \left|\Phi(\rho [A,B])\right|, \end{eqnarray} where $K_{m,M}(\rho[A,B])$ is the Kantorovich constant of the operator $\mbox{-i}\rho^\frac{1}{2}[A,B]\rho^\frac{1}{2}$ and $V_{\rho,\Phi}(X)$ is the generalized variance of $X$.\\ In addition, we use some arguments differing from the scalar theory to present some inequalities related to the generalized correlation and the generalized Wigner--Yanase--Dyson skew information."
"The Closest Point method, initially developed by Ruuth and Merriman, allows for the numerical solution of surface partial differential equations without the need for a parameterization of the surface itself. Surface quantities are embedded into the surrounding domain by assigning each value at a given spatial location to the corresponding value at the closest point on the surface. This embedding allows for surface derivatives to be replaced by their Cartesian counterparts (e.g. $\nabla_s = \nabla$). This equivalence is only valid on the surface, and thus, interpolation is used to enforce what is known as the side condition away from the surface. To improve upon the method, this work derives an operator embedding that incorporates curvature information, making it valid in a neighborhood of the surface. With this, direct enforcement of the side condition is no longer needed. Comparisons in $\mathbb{R}^2$ and $\mathbb{R}^3$ show that the resulting Curvature-Augmented Closest Point method has better accuracy and requires less memory, through increased matrix sparsity, than the Closest Point method. To demonstrate the utility of the method in a physical application, simulations of inextensible, bi-lipid vesicles evolving toward equilibrium shapes are also included."
"Speech Translation has always been about giving source text or audio input and waiting for system to give translated output in desired form. In this paper, we present the Acoustic Dialect Decoder (ADD) - a voice to voice ear-piece translation device. We introduce and survey the recent advances made in the field of Speech Engineering, to employ in the ADD, particularly focusing on the three major processing steps of Recognition, Translation and Synthesis. We tackle the problem of machine understanding of natural language by designing a recognition unit for source audio to text, a translation unit for source language text to target language text, and a synthesis unit for target language text to target language speech. Speech from the surroundings will be recorded by the recognition unit present on the ear-piece and translation will start as soon as one sentence is successfully read. This way, we hope to give translated output as and when input is being read. The recognition unit will use Hidden Markov Models (HMMs) Based Tool-Kit (HTK), hybrid RNN systems with gated memory cells, and the synthesis unit, HMM based speech synthesis system HTS. This system will initially be built as an English to Tamil translation device."
"Conditional independence reasoning has been shown to be helpful in the context of Bayesian nets to optimize probabilistic inference, and related techniques have been applied to speed up a number of logical reasoning tasks in boolean logic by eliminating irrelevant parts of the formulas. This paper shows that conditional independence reasoning can also be applied to optimize epistemic model checking, in which one verifies that a model for a number of agents operating with imperfect information satisfies a formula expressed in a modal multi-agent logic of knowledge. An optimization technique is developed that precedes the use of a model checking algorithm with an analysis that applies conditional independence reasoning to reduce the size of the model. The optimization has been implemented in the epistemic model checker MCK. The paper reports experimental results demonstrating that it can yield multiple orders of magnitude performance improvements."
"We introduce a probabilistic framework that represents stylized banking networks with the aim of predicting the size of contagion events. Most previous work on random financial networks assumes independent connections between banks, whereas our framework explicitly allows for (dis)assortative edge probabilities (e.g., a tendency for small banks to link to large banks). We analyze default cascades triggered by shocking the network and find that the cascade can be understood as an explicit iterated mapping on a set of edge probabilities that converges to a fixed point. We derive a cascade condition that characterizes whether or not an infinitesimal shock to the network can grow to a finite size cascade, in analogy to the basic reproduction number $R_0$ in epidemic modelling. The cascade condition provides an easily computed measure of the systemic risk inherent in a given banking network topology. Using the percolation theory for random networks we also derive an analytic formula for the frequency of global cascades. Although the analytical methods are derived for infinite networks, we demonstrate using Monte Carlo simulations the applicability of the results to finite-sized networks. We show that edge-assortativity, the propensity of nodes to connect to similar nodes, can have a strong effect on the level of systemic risk as measured by the cascade condition. However, the effect of assortativity on systemic risk is subtle, and we propose a simple graph theoretic quantity, which we call the graph-assortativity coefficient, that can be used to assess systemic risk."
"We propose a novel method to test the existence of community structure of undirected real-valued edge-weighted graph. The method is based on Wigner semicircular law on the asymptotic behavior of the random distribution for eigenvalues of a real symmetric matrix. We provide a theoretical foundation for this method and report on its performance in synthetic and real data, suggesting that our method outperforms other state-of-the-art methods."
"We propose a neural-network based model for coordination boundary prediction. The network is designed to incorporate two signals: the similarity between conjuncts and the observation that replacing the whole coordination phrase with a conjunct tends to produce a coherent sentences. The modeling makes use of several LSTM networks. The model is trained solely on conjunction annotations in a Treebank, without using external resources. We show improvements on predicting coordination boundaries on the PTB compared to two state-of-the-art parsers; as well as improvement over previous coordination boundary prediction systems on the Genia corpus."
"Neural networks are among the state-of-the-art techniques for language modeling. Existing neural language models typically map discrete words to distributed, dense vector representations. After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word. Such approaches are time- and memory-intensive because of the large numbers of parameters for word embeddings and the output layer. In this paper, we propose to compress neural language models by sparse word representations. In the experiments, the number of parameters in our model increases very slowly with the growth of the vocabulary size, which is almost imperceptible. Moreover, our approach not only reduces the parameter space to a large extent, but also improves the performance in terms of the perplexity measure."
"In human-computer conversation systems, the context of a user-issued utterance is particularly important because it provides useful background information of the conversation. However, it is unwise to track all previous utterances in the current session as not all of them are equally important. In this paper, we address the problem of session segmentation. We propose an embedding-enhanced TextTiling approach, inspired by the observation that conversation utterances are highly noisy, and that word embeddings provide a robust way of capturing semantics. Experimental results show that our approach achieves better performance than the TextTiling, MMD approaches."
"We study the question of consistency of multi-view models in UML and OCL. We first critically survey the large amount of literature that already exists. We find that only limited subsets of the UML/OCL have been covered so far and that consistency checks mostly only cover structural aspects, whereas only few methods also address behaviour. We also give a classification of different techniques for multi-view UML/OCL consistency: consistency rules, the system model approach, dynamic meta-modelling, universal logic, and heterogeneous transformation. Finally, we elaborate cornerstones of a comprehensive distributed semantics approach to consistency using OMG's Distributed Ontology, Model and Specification Language (DOL)."
"In this paper, we prove that a class of regular sequences can be viewed as projections of fixed points of uniform morphisms on a countable alphabet, and also can be generated by countable states automata. Moreover, we prove that the regularity of some regular sequences is invariant under some codings."
"A graph is well-covered if all its maximal independent sets are of the same size (M. D. Plummer, 1970). A well-covered graph (with at least two vertices) is 1-well-covered if the deletion of every vertex leaves a graph which is well-covered as well (J. W. Staples, 1975). In this paper, we provide new characterizations of 1-well-covered graphs, which we further use to build 1-well-covered graphs by corona, join, and concatenation operations."
"In this paper, we propose a dictionary update method for Nonnegative Matrix Factorization (NMF) with high dimensional data in a spectral conversion (SC) task. Voice conversion has been widely studied due to its potential applications such as personalized speech synthesis and speech enhancement. Exemplar-based NMF (ENMF) emerges as an effective and probably the simplest choice among all techniques for SC, as long as a source-target parallel speech corpus is given. ENMF-based SC systems usually need a large amount of bases (exemplars) to ensure the quality of the converted speech. However, a small and effective dictionary is desirable but hard to obtain via dictionary update, in particular when high-dimensional features such as STRAIGHT spectra are used. Therefore, we propose a dictionary update framework for NMF by means of an encoder-decoder reformulation. Regarding NMF as an encoder-decoder network makes it possible to exploit the whole parallel corpus more effectively and efficiently when applied to SC. Our experiments demonstrate significant gains of the proposed system with small dictionaries over conventional ENMF-based systems with dictionaries of same or much larger size."
"We provide open-source software implemented in MATLAB, that performs Fourier-Motzkin elimination (FME) and removes constraints that are redundant due to Shannon-type inequalities (STIs). The FME is often used in information theoretic contexts to simplify rate regions, e.g., by eliminating auxiliary rates. Occasionally, however, the procedure becomes cumbersome, which makes an error-free hand-written derivation an elusive task. Some computer software have circumvented this difficulty by exploiting an automated FME process. However, the outputs of such software often include constraints that are inactive due to information theoretic properties. By incorporating the notion of STIs (a class of information inequalities provable via a computer program), our algorithm removes such redundant constraints based on non-negativity properties, chain-rules and probability mass function factorization. This newsletter first illustrates the program's abilities, and then reviews the contribution of STIs to the identification of redundant constraints."
"In our today's information society more and more data emerges, e.g.~in social networks, technical applications, or business applications. Companies try to commercialize these data using data mining or machine learning methods. For this purpose, the data are categorized or classified, but often at high (monetary or temporal) costs. An effective approach to reduce these costs is to apply any kind of active learning (AL) methods, as AL controls the training process of a classifier by specific querying individual data points (samples), which are then labeled (e.g., provided with class memberships) by a domain expert. However, an analysis of current AL research shows that AL still has some shortcomings. In particular, the structure information given by the spatial pattern of the (un)labeled data in the input space of a classification model (e.g.,~cluster information), is used in an insufficient way. In addition, many existing AL techniques pay too little attention to their practical applicability. To meet these challenges, this article presents several techniques that together build a new approach for combining AL and semi-supervised learning (SSL) for support vector machines (SVM) in classification tasks. Structure information is captured by means of probabilistic models that are iteratively improved at runtime when label information becomes available. The probabilistic models are considered in a selection strategy based on distance, density, diversity, and distribution (4DS strategy) information for AL and in a kernel function (Responsibility Weighted Mahalanobis kernel) for SVM. The approach fuses generative and discriminative modeling techniques. With 20 benchmark data sets and with the MNIST data set it is shown that our new solution yields significantly better results than state-of-the-art methods."
"We describe the solution of team ISMLL for the ECML-PKDD 2016 Discovery Challenge on Bank Card Usage for both tasks. Our solution is based on three pillars. Gradient boosted decision trees as a strong regression and classification model, an intensive search for good hyperparameter configurations and strong features that exploit geolocation information. This approach achieved the best performance on the public leaderboard for the first task and a decent fourth position for the second task."
"We present a necessary and sufficient condition for a graph of odd-girth $2k+1$ to bound the class of $K_4$-minor-free graphs of odd-girth (at least) $2k+1$, that is, to admit a homomorphism from any such $K_4$-minor-free graph. This yields a polynomial-time algorithm to recognize such bounds. Using this condition, we first prove that every $K_4$-minor free graph of odd-girth $2k+1$ admits a homomorphism to the projective hypercube of dimension $2k$. This supports a conjecture of the third author which generalizes the four-color theorem and relates to several outstanding conjectures such as Seymour's conjecture on edge-colorings of planar graphs. Strengthening this result, we show that the Kneser graph $K(2k+1,k)$ satisfies the conditions, thus implying that every $K_4$-minor free graph of odd-girth $2k+1$ has fractional chromatic number exactly $2+\frac{1}{k}$. Knowing that a smallest bound of odd-girth $2k+1$ must have at least ${k+2 \choose 2}$ vertices, we build nearly optimal bounds of order $4k^2$. Furthermore, we conjecture that the suprema of the fractional and circular chromatic numbers for $K_4$-minor-free graphs of odd-girth $2k+1$ are achieved by a same bound of odd-girth $2k+1$. If true, this improves, in the homomorphism order, earlier tight results on the circular chromatic number of $K_4$-minor-free graphs. We support our conjecture by proving it for the first few cases. Finally, as an application of our work, and after noting that Seymour provided a formula for calculating the edge-chromatic number of $K_4$-minor-free multigraphs, we show that stronger results can be obtained in the case of $K_4$-minor-free regular multigraphs."
"Social media platforms are now a key source of information for a large segment of the public. As such, these platforms have a great potential as a means to provide real-time information to emergency management agencies. Moreover, during an emergency, these agencies are very interested in social media as a means to find public-driven response efforts, as well as to track how their handling of that emergency is being perceived. However, there is currently a lack advanced tools designed for monitoring social media during emergencies. The Emergency Analysis Identification and Management System (EAIMS) is a prototype service that aims to fill this technology gap by providing richer analytic and exploration tools than current solutions. In particular, EAIMS provides real-time detection of emergency events, related information finding, information access and credibility analysis tools for use over social media during emergencies."
"Content-Centric Networking (CCN) research addresses the mismatch between the modern usage of the Internet and its outdated architecture. Importantly, CCN routers may locally cache frequently requested content in order to speed up delivery to end users. Thus, the issue of caching strategies arises, i.e., which content shall be stored and when it should be replaced. In this work, we employ novel techniques towards intelligent administration of CCN routers that autonomously switch between existing strategies in response to changing content request patterns. In particular, we present a router architecture for CCN networks that is controlled by rule-based stream reasoning, following the recent formal framework LARS which extends Answer Set Programming for streams. The obtained possibility for flexible router configuration at runtime allows for faster experimentation and may thus help to advance the further development of CCN. Moreover, the empirical evaluation of our feasibility study shows that the resulting caching agent may give significant performance gains."
"In this paper we study the problem of maintaining the strongly connected components of a graph in the presence of failures. In particular, we show that given a directed graph $G=(V,E)$ with $n=|V|$ and $m=|E|$, and an integer value $k\geq 1$, there is an algorithm that computes the strongly connected components of the graph $G\setminus F$ in $O(2^{k}n\log^2 n)$ time, where $F$ is any set of at most $k$ vertices or edges. Our algorithm uses a $O(2^{k} n^2)$ size data structure that is computed in a preprocessing phase in polynomial time.   Our result is obtained by a mixture of old and classical techniques such as the heavy path decomposition of Sleator and Tarjan and the Depth-First-Search algorithm with new insights on the structure of reachability. Interestingly, one of the building blocks in our algorithm is a restricted variant of the problem in which we only compute strongly connected components that intersect a path. Restricting our attention to a path allows us to implicitly compute reachability between the path vertices and the rest of the graph very efficiently. Therefore, we expect that our new insights and techniques will be of an independent interest."
"We explore here surprising links between the time-cost-tradeoff problem and the minimum cost flow problem that lead to faster, strongly polynomial, algorithms for both problems. One of the main results is a new algorithm for the unit capacity min cost flow that culminates decades of efforts to match the complexity of the fastest strongly polynomial algorithm known for the assignment problem.   The time cost tradeoff (TCT) problem in project management is to expedite the durations of activities, subject to precedence constraints, in order to achieve a target project completion time at minimum expediting costs, or, to maximize the net benefit from a reward associated with project completion time reduction. We devise here the all-min-cuts procedure, which for a given maximum flow, is capable of generating all minimum cuts of equal value very efficiently in average $O(m+n \log n)$ time.   The all-min-cuts procedure implies faster algorithms for TCT problems: For a project network on $n$ nodes and $m$ arcs, with $n'$ arcs of finite uniform expediting costs, the run time is $O((n+n')(m+n\log n))$; for projects with reward of $O(K)$ per unit reduction in the project completion time the run time is $O((n+K)(m+n\log n))$.   Using the primal-dual relationship between TCT and the minimum cost flow problem (MCF) we generate faster strongly polynomial algorithms for various cases of minimum cost flow: For the unit capacity MCF the complexity of our algorithm is $O(n(m+n\log n))$, which is faster than any other strongly polynomial known to date; For a minimum convex (or linear) cost $K$-flow problem our algorithm runs in $O((n+K)(m+n\log n))$. This complexity of the algorithm for any min cost $O(n)$-flow matches the best known complexity for the assignment problem, $O(n(m+n\log n))$, yet with a significantly different approach."
"We propose a flexible framework for spectral conversion (SC) that facilitates training with unaligned corpora. Many SC frameworks require parallel corpora, phonetic alignments, or explicit frame-wise correspondence for learning conversion functions or for synthesizing a target spectrum with the aid of alignments. However, these requirements gravely limit the scope of practical applications of SC due to scarcity or even unavailability of parallel corpora. We propose an SC framework based on variational auto-encoder which enables us to exploit non-parallel corpora. The framework comprises an encoder that learns speaker-independent phonetic representations and a decoder that learns to reconstruct the designated speaker. It removes the requirement of parallel corpora or phonetic alignments to train a spectral conversion system. We report objective and subjective evaluations to validate our proposed method and compare it to SC methods that have access to aligned corpora."
"The effective differential Nullstellensatz is a fundamental result in the computational theory of algebraic differential equations. It allows one to reduce problems about differential equations to problems about polynomial equations. In particular, it provides an algorithm for checking consistency of a system of algebraic differential equations and an algorithm for testing membership in radical differential ideals. This problem and related questions received much attention during the last decade. An upper bound for the effective differential Nullstellensatz was improved several times. For the case of one derivation, we present a new bound, which is asymptotically significantly better than the previously known bounds. Moreover, our bound is the first bound that has feasible numerical values from the computational point of view."
"Recently there has been much interest in performing search queries over encrypted data to enable functionality while protecting sensitive data. One particularly efficient mechanism for executing such queries is order-preserving encryption/encoding (OPE) which results in ciphertexts that preserve the relative order of the underlying plaintexts thus allowing range and comparison queries to be performed directly on ciphertexts. In this paper, we propose an alternative approach to range queries over encrypted data that is optimized to support insert-heavy workloads as are common in ""big data"" applications while still maintaining search functionality and achieving stronger security. Specifically, we propose a new primitive called partial order preserving encoding (POPE) that achieves ideal OPE security with frequency hiding and also leaves a sizable fraction of the data pairwise incomparable. Using only O(1) persistent and $O(n^\epsilon)$ non-persistent client storage for $0<\epsilon<1$, our POPE scheme provides extremely fast batch insertion consisting of a single round, and efficient search with O(1) amortized cost for up to $O(n^{1-\epsilon})$ search queries. This improved security and performance makes our scheme better suited for today's insert-heavy databases."
"By Kolmogorov Complexity,two number-theoretic problems are solved in different way than before,one problem is Maxim Kontsevich and Don Bernard Zagier's Problem 3 \emph{Exhibit at least one number which does not belong to} $ \mathcal{P}$ (period number) in their paper,another is the problem about existence of bounded coefficients of continued fraction expansion of transcendental number.Thus we show a new approach to mathematical problems in the non-logical discipline.Futhermore,we show that resource-bounded Kolmogorov Complexity and computational complexity can at least provide tips or principles to mathematical problems in the non-traditional or logical discipline."
"Spectrum sensing is a crucial component of opportunistic spectrum access schemes, which aim at improving spectrum utilization by allowing for the reuse of idle licensed spectrum. Sensing a spectral band before using it makes sure the legitimate users are not disturbed. Since information about these users' signals is not necessarily available, the sensor should be able to conduct so-called blind spectrum sensing. Historically, this has not been a feature of cyclostationarity-based algorithms. Indeed, in many application scenarios the information required for traditional cyclostationarity detection might not be available, hindering its practical applicability. In this work we propose two new cyclostationary spectrum sensing algorithms that make use of the inherent sparsity of the cyclic autocorrelation to make blind operation possible. Along with utilizing sparse recovery methods for estimating the cyclic autocorrelation, we take further advantage of its structure by introducing joint sparsity as well as general structure dictionaries into the recovery process. Furthermore, we extend a statistical test for cyclostationarity to accommodate sparse cyclic spectra. Our numerical results demonstrate that the new methods achieve a near constant false alarm rate behavior in contrast to earlier approaches from the literature."
"A methodology for the development of a fuzzy expert system (FES) with application to earthquake prediction is presented. The idea is to reproduce the performance of a human expert in earthquake prediction. To do this, at the first step, rules provided by the human expert are used to generate a fuzzy rule base. These rules are then fed into an inference engine to produce a fuzzy inference system (FIS) and to infer the results. In this paper, we have used a Sugeno type fuzzy inference system to build the FES. At the next step, the adaptive network-based fuzzy inference system (ANFIS) is used to refine the FES parameters and improve its performance. The proposed framework is then employed to attain the performance of a human expert used to predict earthquakes in the Zagros area based on the idea of coupled earthquakes. While the prediction results are promising in parts of the testing set, the general performance indicates that prediction methodology based on coupled earthquakes needs more investigation and more complicated reasoning procedure to yield satisfactory predictions."
"We investigate how a residual network can learn to predict the dynamics of interacting shapes purely as an image-to-image regression problem.   With a simple 2d physics simulator, we generate short sequences composed of rectangles put in motion by applying a pulling force at a point picked at random. The network is trained with a quadratic loss to predict the image of the resulting configuration, given the image of the starting configuration and an image indicating the point of grasping.   Experiments show that the network learns to predict accurately the resulting image, which implies in particular that (1) it segments rectangles as distinct components, (2) it infers which one contains the grasping point, (3) it models properly the dynamic of a single rectangle, including the torque, (4) it detects and handles collisions to some extent, and (5) it re-synthesizes properly the entire scene with displaced rectangles."
"This paper presents an online transfer learning framework for improving temperature predictions in residential buildings. In transfer learning, prediction models trained under a set of available data from a target domain (e.g., house with limited data) can be improved through the use of data generated from similar source domains (e.g., houses with rich data). Given also the need for prediction models that can be trained online (e.g., as part of a model-predictive-control implementation), this paper introduces the generalized online transfer learning algorithm (GOTL). It employs a weighted combination of the available predictors (i.e., the target and source predictors) and guarantees convergence to the best weighted predictor. Furthermore, the use of Transfer Component Analysis (TCA) allows for using more than a single source domains, since it may facilitate the fit of a single model on more than one source domains (houses). This allows GOTL to transfer knowledge from more than one source domains. We further validate our results through experiments in climate control for residential buildings and show that GOTL may lead to non-negligible energy savings for given comfort levels."
"McNamara, Trimmer, and Houston [2012] claim to provide an explanation of certain systematic deviation from rational behavior using a mechanism that could arise through natural selection. We provide an arguably much simpler explanation. To argue convincingly that the type of mechanism proposed by McNamara et al. is adaptive, and is likely to be selected for by natural selection, one must argue that, in some sense, it is a better approach than the simple strategies that we propose."
"We study the complexity of approximate counting Constraint Satisfaction Problems (#CSPs) in a bounded degree setting. Specifically, given a Boolean constraint language $\Gamma$ and a degree bound $\Delta$, we study the complexity of #CSP$_\Delta(\Gamma)$, which is the problem of counting satisfying assignments to CSP instances with constraints from $\Gamma$ and whose variables can appear at most $\Delta$ times. Our main result shows that: (i) if every function in $\Gamma$ is affine, then #CSP$_\Delta(\Gamma)$ is in FP for all $\Delta$, (ii) otherwise, if every function in $\Gamma$ is in a class called IM$_2$, then for all sufficiently large $\Delta$, #CSP$_\Delta(\Gamma)$ is equivalent under approximation-preserving (AP) reductions to the counting problem #BIS (the problem of counting independent sets in bipartite graphs) (iii) otherwise, for all sufficiently large $\Delta$, it is NP-hard to approximate the number of satisfying assignments of an instance of #CSP$_\Delta(\Gamma)$, even within an exponential factor. Our result extends previous results, which apply only in the so-called ""conservative"" case."
"We provide a new estimator of integral operators with smooth kernels, obtained from a set of scattered and noisy impulse responses. The proposed approach relies on the formalism of smoothing in reproducing kernel Hilbert spaces and on the choice of an appropriate regularization term that takes the smoothness of the operator into account. It is numerically tractable in very large dimensions. We study the estimator's robustness to noise and analyze its approximation properties with respect to the size and the geometry of the dataset. In addition, we show minimax optimality of the proposed estimator."
"In this paper, we propose a novel model, named Stroke Sequence-dependent Deep Convolutional Neural Network (SSDCNN), using the stroke sequence information and eight-directional features for Online Handwritten Chinese Character Recognition (OLHCCR). On one hand, SSDCNN can learn the representation of Online Handwritten Chinese Character (OLHCC) by incorporating the natural sequence information of the strokes. On the other hand, SSDCNN can incorporate eight-directional features in a natural way. In order to train SSDCNN, we divide the process of training into two stages: 1) The training data is used to pre-train the whole architecture until the performance tends to converge. 2) Fully-connected neural network which is used to combine the stroke sequence-dependent representation with eight-directional features and softmax layer are further trained. Experiments were conducted on the OLHCCR competition tasks of ICDAR 2013. Results show that, SSDCNN can reduce the recognition error by 50\% (5.13\% vs 2.56\%) compared to the model which only use eight-directional features. The proposed SSDCNN achieves 97.44\% accuracy which reduces the recognition error by about 1.9\% compared with the best submitted system on ICDAR2013 competition. These results indicate that SSDCNN can exploit the stroke sequence information to learn high-quality representation of OLHCC. It also shows that the learnt representation and the classical eight-directional features complement each other within the SSDCNN architecture."
"Given a video and its incomplete textural description with missing words, the Video-Fill-in-the-Blank (ViFitB) task is to automatically find the missing word. The contextual information of the sentences are important to infer the missing words; the visual cues are even more crucial to get a more accurate inference. In this paper, we presents a new method which intuitively takes advantage of the structure of the sentences and employs merging LSTMs (to merge two LSTMs) to tackle the problem with embedded textural and visual cues. In the experiments, we have demonstrated the superior performance of the proposed method on the challenging ""Movie Fill-in-the-Blank"" dataset."
"Releasing connection data from social networking services can pose a significant threat to user privacy. In our work, we consider structural social network de-anonymization attacks, which are used when a malicious party uses connections in a public or other identified network to re-identify users in an anonymized social network release that he obtained previously.   In this paper we design and evaluate a novel social de-anonymization attack. In particular, we argue that the similarity function used to re-identify nodes is a key component of such attacks, and we design a novel measure tailored for social networks. We incorporate this measure in an attack called Bumblebee. We evaluate Bumblebee in depth, and show that it significantly outperforms the state-of-the-art, for example it has higher re-identification rates with high precision, robustness against noise, and also has better error control."
"We present a practical and highly secure method for the authentication of chips based on a new concept for implementing strong Physical Unclonable Function (PUF) on field programmable gate arrays (FPGA). Its qualitatively novel feature is a remote reconfiguration in which the delay stages of the PUF are arranged to a random pattern within a subset of the FPGA's gates. Before the reconfiguration is performed during authentication the PUF simply does not exist. Hence even if an attacker has the chip under control previously she can gain no useful information about the PUF. This feature, together with a strict renunciation of any error correction and challenge selection criteria that depend on individual properties of the PUF that goes into the field make our strong PUF construction immune to all machine learning attacks presented in the literature. More sophisticated attacks on our strong-PUF construction will be difficult, because they require the attacker to learn or directly measure the properties of the complete FPGA. A fully functional reference implementation for a secure ""chip biometrics"" is presented. We remotely configure ten 64-stage arbiter PUFs out of 1428 lookup tables within a time of 25 seconds and then receive one ""fingerprint"" from each PUF within 1 msec."
"We study truthful mechanisms for matching and related problems in a partial information setting, where the agents' true utilities are hidden, and the algorithm only has access to ordinal preference information. Our model is motivated by the fact that in many settings, agents cannot express the numerical values of their utility for different outcomes, but are still able to rank the outcomes in their order of preference. Specifically, we study problems where the ground truth exists in the form of a weighted graph of agent utilities, but the algorithm can only elicit the agents' private information in the form of a preference ordering for each agent induced by the underlying weights. Against this backdrop, we design truthful algorithms to approximate the true optimum solution with respect to the hidden weights. Our techniques yield universally truthful algorithms for a number of graph problems: a 1.76-approximation algorithm for Max-Weight Matching, 2-approximation algorithm for Max k-matching, a 6-approximation algorithm for Densest k-subgraph, and a 2-approximation algorithm for Max Traveling Salesman as long as the hidden weights constitute a metric. We also provide improved approximation algorithms for such problems when the agents are not able to lie about their preferences. Our results are the first non-trivial truthful approximation algorithms for these problems, and indicate that in many situations, we can design robust algorithms even when the agents may lie and only provide ordinal information instead of precise utilities."
"We study large markets with a single seller which can produce many types of goods, and many multi-minded buyers. The seller chooses posted prices for its many items, and the buyers purchase bundles to maximize their utility. For this setting, we consider the following questions: What fraction of the optimum social welfare does a revenue maximizing solution achieve? Are there pricing mechanisms which achieve both good revenue and good welfare simultaneously? To address these questions, we give efficient pricing schemes which are guaranteed to result in both good revenue and welfare, as long as the buyer valuations for the goods they desire have a nice (although reasonable) structure, e.g., that the aggregate buyer demand has a monotone hazard rate or is not too convex. We also show that our pricing schemes have implications for any pricing which achieves high revenue: specifically that even if the seller cares only about revenue, they can still ensure that their prices result in good social welfare without sacrificing profit. Our results holds for general multi-minded buyers in large markets; we also provide improved guarantees for the important special case of unit-demand buyers."
"Knowledge base completion aims to infer new relations from existing information. In this paper, we propose path-augmented TransR (PTransR) model to improve the accuracy of link prediction. In our approach, we base PTransR model on TransR, which is the best one-hop model at present. Then we regularize TransR with information of relation paths. In our experiment, we evaluate PTransR on the task of entity prediction. Experimental results show that PTransR outperforms previous models."
"Wireless Sensor Networks (WSNs) consist of many low cost and light sensors dispersed in an area to monitor the physical environment. Event detection in WSN area, especially detection of multi-events at the same time, is an important problem. This article is a new attempt for detection of two simultaneous events based on distributed data processing structure and Bayesian criteria. For accurate detection of two simultaneous events, we proposed new decision rules based on likelihood ratio test and also derived probability of detection error based on Bayesian criteria. In addition to multi-event detection, the proposed method is expanded to a fault-tolerant procedure if there are faults in decision making of sensors. Performance of the proposed approach is demonstrated for detection of events in different circumstances. Results show the effectiveness of the algorithm for fault-tolerant multi-event detection."
"The study of neurocognitive tasks requiring accurate localisation of activity often rely on functional Magnetic Resonance Imaging, a widely adopted technique that makes use of a pipeline of data processing modules, each involving a variety of parameters. These parameters are frequently set according to the local goal of each specific module, not accounting for the rest of the pipeline. Given recent success of neural network research in many different domains, we propose to convert the whole data pipeline into a deep neural network, where the parameters involved are jointly optimised by the network to best serve a common global goal. As a proof of concept, we develop a module able to adaptively apply the most suitable spatial smoothing to every brain volume for each specific neuroimaging task, and we validate its results in a standard brain decoding experiment."
"This chapter is dedicated to the so-called cuspidal robots, i.e. those robots that can move from one inverse geometric solution to another without meeting a singular confuguration. This feature was discovered quite recently and has then been fascinating a lot of researchers. After a brief history of cuspidal robots, the chapter provides the main features of cuspidal robots: explanation of the non-singular change of posture, uniqueness domains, regions of feasible paths, identification and classification of cuspidal robots. The chapter focuses on 3-R orthogonal serial robots. The case of 6-dof robots and parallel robots is discussed in the end of this chapter."
"Recommender system has attracted much attention during the past decade, and many attack detection algorithms have been developed for better recommendation. Most previous approaches focus on the shilling attacks, where the attack organizer fakes a large number of user profiles by the same strategy to promote or demote an item. In this paper, we study a different attack style: unorganized malicious attacks, where attackers respectively use a small number of user profiles to attack their own target items without any organizer. This attack style occurs in many real applications, yet relevant study remains open. In this paper, we formulate the unorganized malicious attacks detection as a variant of matrix completion problem, and prove that attackers can be detected theoretically. We propose the Unorganized Malicious Attacks detection (UMA) algorithm, which can be viewed as a proximal alternating splitting augmented Lagrangian method. We verify, both theoretically and empirically, the effectiveness of our proposed algorithm."
"Mobile agent networks, such as multi-UAV systems, are constrained by limited resources. In particular, limited energy affects system performance directly, such as system lifetime. It has been demonstrated in the wireless sensor network literature that the communication energy consumption dominates the computational and the sensing energy consumption. Hence, the lifetime of the multi-UAV systems can be extended significantly by optimizing the amount of communication data, at the expense of increasing computational cost. In this work, we aim at attaining an optimal trade-off between the communication and the computational energy. Specifically, we propose a mixed-integer optimization formulation for a multi-hop hierarchical clustering-based self-organizing UAV network incorporating data aggregation, to obtain an energy-efficient information routing scheme. The proposed framework is tested on two applications, namely target tracking and area mapping. Based on simulation results, our method can significantly save energy compared to a baseline strategy, where there is no data aggregation and clustering scheme."
"Esophageal adenocarcinoma arises from Barrett's esophagus, which is the most serious complication of gastroesophageal reflux disease. Strategies for screening involve periodic surveillance and tissue biopsies. A major challenge in such regular examinations is to record and track the disease evolution and re-localization of biopsied sites to provide targeted treatments. In this paper, we extend our original inter-operative relocalization framework to provide a constrained image based search for obtaining the best view-point match to the live view. Within this context we investigate the effect of: the choice of feature descriptors and color-space; filtering of uninformative frames and endoscopic modality, for view-point localization. Our experiments indicate an improvement in the best view-point retrieval rate to [92%,87%] from [73%,76%] (in our previous approach) for NBI and WL."
"Recent studies on cloud-radio access networks assume either signal-level or scheduling-level coordination. This paper considers a hybrid coordinated scheme as a means to benefit from both policies. Consider the downlink of a multi-cloud radio access network, where each cloud is connected to several base-stations (BSs) via high capacity links, and, therefore, allows for joint signal processing within the cloud transmission. Across the multiple clouds, however, only scheduling-level coordination is permitted, as low levels of backhaul communication are feasible. The frame structure of every BS is composed of various time/frequency blocks, called power-zones (PZs), which are maintained at a fixed power level. The paper addresses the problem of maximizing a network-wide utility by associating users to clouds and scheduling them to the PZs, under the practical constraints that each user is scheduled to a single cloud at most, but possibly to many BSs within the cloud, and can be served by one or more distinct PZs within the BSs' frame. The paper solves the problem using graph theory techniques by constructing the conflict graph. The considered scheduling problem is, then, shown to be equivalent to a maximum-weight independent set problem in the constructed graph, which can be solved using efficient techniques. The paper then proposes solving the problem using both optimal and heuristic algorithms that can be implemented in a distributed fashion across the network. The proposed distributed algorithms rely on the well-chosen structure of the constructed conflict graph utilized to solve the maximum-weight independent set problem. Simulation results suggest that the proposed optimal and heuristic hybrid scheduling strategies provide appreciable gain as compared to the scheduling-level coordinated networks, with a negligible degradation to signal-level coordination."
"This paper presents a deep learning architecture for the semantic decoder component of a Statistical Spoken Dialogue System. In a slot-filling dialogue, the semantic decoder predicts the dialogue act and a set of slot-value pairs from a set of n-best hypotheses returned by the Automatic Speech Recognition. Most current models for spoken language understanding assume (i) word-aligned semantic annotations as in sequence taggers and (ii) delexicalisation, or a mapping of input words to domain-specific concepts using heuristics that try to capture morphological variation but that do not scale to other domains nor to language variation (e.g., morphology, synonyms, paraphrasing ). In this work the semantic decoder is trained using unaligned semantic annotations and it uses distributed semantic representation learning to overcome the limitations of explicit delexicalisation. The proposed architecture uses a convolutional neural network for the sentence representation and a long-short term memory network for the context representation. Results are presented for the publicly available DSTC2 corpus and an In-car corpus which is similar to DSTC2 but has a significantly higher word error rate (WER)."
"Dense, robust and real-time computation of depth information from stereo-camera systems is a computationally demanding requirement for robotics, advanced driver assistance systems (ADAS) and autonomous vehicles. Semi-Global Matching (SGM) is a widely used algorithm that propagates consistency constraints along several paths across the image. This work presents a real-time system producing reliable disparity estimation results on the new embedded energy-efficient GPU devices. Our design runs on a Tegra X1 at 42 frames per second (fps) for an image size of 640x480, 128 disparity levels, and using 4 path directions for the SGM method."
"The Stixel World is a medium-level, compact representation of road scenes that abstracts millions of disparity pixels into hundreds or thousands of stixels. The goal of this work is to implement and evaluate a complete multi-stixel estimation pipeline on an embedded, energy-efficient, GPU-accelerated device. This work presents a full GPU-accelerated implementation of stixel estimation that produces reliable results at 26 frames per second (real-time) on the Tegra X1 for disparity images of 1024x440 pixels and stixel widths of 5 pixels, and achieves more than 400 frames per second on a high-end Titan X GPU card."
"Digital whole-slide images of pathological tissue samples have recently become feasible for use within routine diagnostic practice. These gigapixel sized images enable pathologists to perform reviews using computer workstations instead of microscopes. Existing workstations visualize scanned images by providing a zoomable image space that reproduces the capabilities of the microscope. This paper presents a novel visualization approach that enables filtering of the scale-space according to color preference. The visualization method reveals diagnostically important patterns that are otherwise not visible. The paper demonstrates how this approach has been implemented into a fully functional prototype that lets the user navigate the visualization parameter space in real time. The prototype was evaluated for two common clinical tasks with eight pathologists in a within-subjects study. The data reveal that task efficiency increased by 15% using the prototype, with maintained accuracy. By analyzing behavioral strategies, it was possible to conclude that efficiency gain was caused by a reduction of the panning needed to perform systematic search of the images. The prototype system was well received by the pathologists who did not detect any risks that would hinder use in clinical routine."
"Crowdsourced software development (CSD) offers a series of specified tasks to a large crowd of trustworthy software workers. Topcoder is a leading platform to manage the whole process of CSD. While increasingly accepted as a realistic option for software development, preliminary analysis on Topcoder's software crowd worker behaviors reveals an alarming task-quitting rate of 82.9%. In addition, a substantial number of tasks do not receive any successful submission.   In this paper, we report about a methodology to improve the efficiency of CSD. We apply massive data analytics and machine leaning to (i) perform comparative analysis on alternative technique analysis to predict likelihood of winners and quitters for each task, (ii) significantly reduce the amount of non-succeeding development effort in registered but inappropriate tasks, (iii) identify and rank the most qualified registered workers for each task, and (iv) provide reliable prediction of tasks risky to get any successful submission.   Our results and analysis show that Random Forest (RF) based predictive technique performs best among the alternative techniques studied. Applying RF, the tasks recommended to workers can reduce the amount of non-succeeding development effort to a great extent. On average, over a period of 30 days, the savings are 3.5 and 4.6 person-days per registered tasks for experienced resp. unexperienced workers. For the task-related recommendations of workers, we can accurately recommend at least 1 actual winner in the top ranked workers, particularly 94.07% of the time among the top-2 recommended workers for each task. Finally, we can predict, with more than 80% F-measure, the tasks likely not getting any submission, thus triggering timely corrective actions from CSD platforms or task requesters."
"Traditional studies of memristive devices have mainly focused on their applications in non-volatile information storage and information processing. Here, we demonstrate that the third fundamental component of information technologies { the transfer of information { can also be employed with memristive devices. For this purpose, we introduce a metastable memristive circuit. Combining metastable memristive circuits into a line, one obtains an architecture capable of transferring a signal edge from one space location to another. We emphasize that the suggested transmission lines employ only resistive components. Moreover, their networks (for example, Y-connected lines) have an information processing capability."
"With the advent of extremely high dimensional datasets, dimensionality reduction techniques are becoming mandatory. Among many techniques, feature selection has been growing in interest as an important tool to identify relevant features on huge datasets --both in number of instances and features--. The purpose of this work is to demonstrate that standard feature selection methods can be parallelized in Big Data platforms like Apache Spark, boosting both performance and accuracy. We thus propose a distributed implementation of a generic feature selection framework which includes a wide group of well-known Information Theoretic methods. Experimental results on a wide set of real-world datasets show that our distributed framework is capable of dealing with ultra-high dimensional datasets as well as those with a huge number of samples in a short period of time, outperforming the sequential version in all the cases studied."
"Basing on the theory for arbitrary oriented surfaces, we developed the theory of the moir\'e effect for cylindrical single-layer objects in the paraxial approximation. With using the dual grids, the moir\'e effect in the plane gratings is simulated, as well as the near-axis moir\'e effect in cylinders including the chiral layouts. The results can be applied to the graphene layers, to single-walled nanotubes, and to cylinders in general."
"Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. In this paper, we show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\varepsilon$) require $\Omega(\text{poly}(1/\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\varepsilon$) require $\mathcal{O}(\text{polylog}(1/\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on this simple observation that the binary approximation of a real number in the interval $[0,1]$ can be represented by a deep neural network which uses a ""small"" number of neurons."
"We introduce a generative model, we call Tensorial Mixture Models (TMMs) based on mixtures of basic component distributions over local structures (e.g. patches in an image) where the dependencies between the local-structures are represented by a ""priors tensor"" holding the prior probabilities of assigning a component distribution to each local-structure.   In their general form, TMMs are intractable as the prior tensor is typically of exponential size. However, when the priors tensor is decomposed it gives rise to an arithmetic circuit which in turn transforms the TMM into a Convolutional Arithmetic Circuit (ConvAC). A ConvAC corresponds to a shallow (single hidden layer) network when the priors tensor is decomposed by a CP (sum of rank-1) approach and corresponds to a deep network when the decomposition follows the Hierarchical Tucker (HT) model.   The ConvAC representation of a TMM possesses several attractive properties. First, the inference is tractable and is implemented by a forward pass through a deep network. Second, the architectural design of the model follows the deep networks community design, i.e., the structure of TMMs is determined by just two easily understood factors: size of pooling windows and number of channels. Finally, we demonstrate the effectiveness of our model when tackling the problem of classification with missing data, leveraging TMMs unique ability of tractable marginalization which leads to optimal classifiers regardless of the missingness distribution."
"Misinformation under the form of rumor, hoaxes, and conspiracy theories spreads on social media at alarming rates. One hypothesis is that, since social media are shaped by homophily, belief in misinformation may be more likely to thrive on those social circles that are segregated from the rest of the network. One possible antidote is fact checking which, in some cases, is known to stop rumors from spreading further. However, fact checking may also backfire and reinforce the belief in a hoax. Here we take into account the combination of network segregation, finite memory and attention, and fact-checking efforts. We consider a compartmental model of two interacting epidemic processes over a network that is segregated between gullible and skeptic users. Extensive simulation and mean-field analysis show that a more segregated network facilitates the spread of a hoax only at low forgetting rates, but has no effect when agents forget at faster rates. This finding may inform the development of mitigation techniques and overall inform on the risks of uncontrolled misinformation online."
"A simple proof is given for the monotonicity of entropy and Fisher information associated to sums of i.i.d. random variables. The proof relies on a characterization of maximal correlation for partial sums due to Dembo, Kagan and Shepp."
"Massive multiple-input multiple-output (MIMO) systems achieve high sum spectral efficiency by offering an order of magnitude increase in multiplexing gains. In time division duplexing systems, however, the reuse of uplink training pilots among cells results in additional channel estimation error, which causes downlink inter-cell interference, even when large numbers of antennas are employed. Handling this interference with conventional network MIMO techniques is challenging due to the large channel dimensionality. Further, the implementation of large antenna precoding/combining matrices is associated with high hardware complexity and power consumption. In this paper, we propose multi-layer precoding to enable efficient and low complexity operation in full-dimensional massive MIMO, where a large number of antennas is used in two dimensions. In multi-layer precoding, the precoding matrix of each base station is written as a product of a number of precoding matrices, each one called a layer. Multi-layer precoding (i) leverages the directional characteristics of large-scale MIMO channels to manage inter-cell interference with low channel knowledge requirements, and (ii) allows for an efficient implementation using low-complexity hybrid analog/digital architectures. We present a specific multi-layer precoding design for full-dimensional massive MIMO systems. The performance of this precoding design is analyzed and the per-user achievable rate is characterized for general channel models. The asymptotic optimality of the proposed multi-layer precoding design is then proved for some special yet important channels. Numerical simulations verify the analytical results and illustrate the potential gains of multi-layer precoding compared to traditional pilot-contaminated massive MIMO solutions."
"Wireless object tracking applications are gaining popularity and will soon utilize emerging ultra-low-power device-to-device communication. However, severe energy constraints require much more careful accounting of energy usage than what prior art provides. In particular, the available energy, the differing power consumption levels for listening, receiving, and transmitting, as well as the limited control band- width must all be considered. Therefore, we formulate the problem of maximizing the throughput among a set of heterogeneous broadcasting nodes with differing power consumption levels, each subject to a strict ultra-low-power budget. We obtain the oracle throughput (i.e., maximum achievable throughput) and use Lagrangian methods to design EconCast - a simple asynchronous distributed protocol in which nodes transition between sleep, listen, and transmit states, and dynamically change the transition rates. We also show that EconCast approaches the oracle throughput. The performance is evaluated numerically and via extensive simulations and it is shown that EconCast outperforms prior art by 6x - 17x under realistic assumptions. Finally, we implement EconCast using the TI eZ430-RF2500-SEH energy harvesting nodes and experimentally show that in realistic environments it obtains 57% - 77% of the analytically computed throughput."
"We propose a flexible convex relaxation for the phase retrieval problem that operates in the natural domain of the signal. Therefore, we avoid the prohibitive computational cost associated with ""lifting"" and semidefinite programming (SDP) in methods such as PhaseLift and compete with recently developed non-convex techniques for phase retrieval. We relax the quadratic equations for phaseless measurements to inequality constraints each of which representing a symmetric ""slab"". Through a simple convex program, our proposed estimator finds an extreme point of the intersection of these slabs that is best aligned with a given anchor vector. We characterize geometric conditions that certify success of the proposed estimator. Furthermore, using classic results in statistical learning theory, we show that for random measurements the geometric certificates hold with high probability at an optimal sample complexity. Phase transition of our estimator is evaluated through simulations. Our numerical experiments also suggest that the proposed method can solve phase retrieval problems with coded diffraction measurements as well."
"Machine reading using differentiable reasoning models has recently shown remarkable progress. In this context, End-to-End trainable Memory Networks, MemN2N, have demonstrated promising performance on simple natural language based reasoning tasks such as factual reasoning and basic deduction. However, other tasks, namely multi-fact question-answering, positional reasoning or dialog related tasks, remain challenging particularly due to the necessity of more complex interactions between the memory and controller modules composing this family of models. In this paper, we introduce a novel end-to-end memory access regulation mechanism inspired by the current progress on the connection short-cutting principle in the field of computer vision. Concretely, we develop a Gated End-to-End trainable Memory Network architecture, GMemN2N. From the machine learning perspective, this new capability is learned in an end-to-end fashion without the use of any additional supervision signal which is, as far as our knowledge goes, the first of its kind. Our experiments show significant improvements on the most challenging tasks in the 20 bAbI dataset, without the use of any domain knowledge. Then, we show improvements on the dialog bAbI tasks including the real human-bot conversion-based Dialog State Tracking Challenge (DSTC-2) dataset. On these two datasets, our model sets the new state of the art."
"The high probability of hardware failures prevents many advanced robots (e.g. legged robots) to be confidently deployed in real-world situations (e.g post-disaster rescue). Instead of attempting to diagnose the failure(s), robots could adapt by trial-and-error in order to be able to complete their tasks. However, the best trial-and-error algorithms for robotics are all episodic: between each trial, the robot needs to be put back in the same state, that is, the robot is not learning autonomously. In this paper, we introduce a novel learning algorithm called ""Reset-free Trial-and-Error"" (RTE) that allows robots to recover from damage while completing their tasks. We evaluate it on a hexapod robot that is damaged in several ways (e.g. a missing leg, a shortened leg, etc.) and whose objective is to reach a sequence of targets in an arena. Our experiments show that the robot can recover most of its locomotion abilities in a few minutes, in an environment with obstacles, and without any human intervention. Overall, this new algorithm makes it possible to contemplate sending robots to places that are truly too dangerous for humans and in which robots cannot be rescued."
"In the setting of encryption schemes, non-malleability is a highly desirable property: it ensures that no adversary can meaningfully manipulate the plaintext by acting on the ciphertext. In this work, we give a new definition of non-malleability for encryption of quantum data, improving on the previous definition of Ambainis Bouda and Winter. Our definition is expressed in terms of entropic quantities, considers stronger adversaries, and does not assume secrecy. We prove that quantum non-malleability implies secrecy, in stark contrast to the classical setting where the two properties are independent. In the case of non-unitary schemes, we also provide a separation example which demonstrates a weakness in the definition of Ambainis et al. In the case of unitary schemes, we establish that the two definitions are equivalent, by showing that our definition is equivalent to the notion of a unitary two-design. We also give new results regarding the closely-related task of quantum authentication. We show that the strong notion of ""total authentication"" recently proposed by Garg, Yuen and Zhandry can be satisfied with two-designs. This is a significant improvement over their construction, which required eight-designs. We also show that, under a mild adaptation of the rejection procedure, both total authentication and our notion of non-malleability yield quantum authentication as defined by Dupuis, Nielsen and Salvail."
"We prove the partial strong converse property for the discrete memoryless non-degraded wiretap channel, for which we require the leakage to the eavesdropper to vanish but allow an asymptotic error probability $\epsilon \in [0,1)$. We use a recently developed technique based on information spectrum and a recursive bounding method to evaluate the probability of correct decoding. We show that when the transmission rate is above the secrecy capacity, the probability of correct decoding decays to zero exponentially. Therefore, the maximum transmission rate is the same for $\epsilon \in [0,1)$, and the partial strong converse property holds."
"Large real-world networks typically follow a power-law degree distribution. To study such networks, numerous random graph models have been proposed. However, real-world networks are not drawn at random. Therefore, Brach, Cygan, {\L}acki, and Sankowski [SODA 2016] introduced two natural deterministic conditions: (1) a power-law upper bound on the degree distribution (PLB-U) and (2) power-law neighborhoods, that is, the degree distribution of neighbors of each vertex is also upper bounded by a power law (PLB-N). They showed that many real-world networks satisfy both deterministic properties and exploit them to design faster algorithms for a number of classical graph problems.   We complement the work of Brach et al. by showing that some well-studied random graph models exhibit both the mentioned PLB properties and additionally also a power-law lower bound on the degree distribution (PLB-L). All three properties hold with high probability for Chung-Lu Random Graphs and Geometric Inhomogeneous Random Graphs and almost surely for Hyperbolic Random Graphs. As a consequence, all results of Brach et al. also hold with high probability or almost surely for those random graph classes.   In the second part of this work we study three classical NP-hard combinatorial optimization problems on PLB networks. It is known that on general graphs with maximum degree {\Delta}, a greedy algorithm, which chooses nodes in the order of their degree, only achieves an {\Omega}(ln {\Delta})-approximation for Minimum Vertex Cover and Minimum Dominating Set, and an {\Omega}({\Delta})-approximation for Maximum Independent Set. We prove that the PLB-U property suffices for the greedy approach to achieve a constant-factor approximation for all three problems. We also show that all three combinatorial optimization problems are APX-complete, even if all PLB-properties hold."
"In this paper, we consider the problem of joint delay-Doppler estimation of moving targets in a passive radar that makes use of orthogonal frequency-division multiplexing (OFDM) communication signals. A compressed sensing algorithm is proposed to achieve supper-resolution and better accuracy, using both the atomic norm and the $\ell_1$-norm. The atomic norm is used to manifest the signal sparsity in the continuous domain. Unlike previous works which assume the demodulation to be error free, we explicitly introduce the demodulation error signal whose sparsity is imposed by the $\ell_1$-norm. On this basis, the delays and Doppler frequencies are estimated by solving a semidefinite program (SDP) which is convex. We also develop an iterative method for solving this SDP via the alternating direction method of multipliers (ADMM) where each iteration involves closed-form computation. Simulation results are presented to illustrate the high performance of the proposed algorithm."
"Deep neural networks are facing a potential security threat from adversarial examples, inputs that look normal but cause an incorrect classification by the deep neural network. For example, the proposed threat could result in hand-written digits on a scanned check being incorrectly classified but looking normal when humans see them. This research assesses the extent to which adversarial examples pose a security threat, when one considers the normal image acquisition process. This process is mimicked by simulating the transformations that normally occur in acquiring the image in a real world application, such as using a scanner to acquire digits for a check amount or using a camera in an autonomous car. These small transformations negate the effect of the carefully crafted perturbations of adversarial examples, resulting in a correct classification by the deep neural network. Thus just acquiring the image decreases the potential impact of the proposed security threat. We also show that the already widely used process of averaging over multiple crops neutralizes most adversarial examples. Normal preprocessing, such as text binarization, almost completely neutralizes adversarial examples. This is the first paper to show that for text driven classification, adversarial examples are an academic curiosity, not a security threat."
"Cloud computing technology provides the means to share physical resources among multiple users and data center tenants by exposing them as virtual resources. There is a strong industrial drive to use similar technology and concepts to provide timing sensitive services. One such is virtual networking services, so called services chains, which consist of several interconnected virtual network functions. This allows for the capacity to be scaled up and down by adding or removing virtual resources. In this work, we develop a model of a service chain and pose the dynamic allocation of resources as an optimization problem. We design and present a set of strategies to allot virtual network nodes in an optimal fashion subject to latency and buffer constraints."
"Conventional dual-frequency fringe projection algorithm often suffers from phase unwrapping failure when the frequency ratio between the high frequency and the low one is too large. Zhang et.al. proposed an enhanced two-frequency phase-shifting method to use geometric constraints of digital fringe projection(DFP) to reduce the noise impact due to the large frequency ratio. However, this method needs to calibrate the DFP system and calculate the minimum phase map at the nearest position from the camera perspective, these procedures are are relatively complex and more time-cosuming. In this paper, we proposed an improved method, which eliminates the system calibration and determination in Zhang's method,meanwhile does not need to use the low frequency fringe pattern. In the proposed method,we only need a set of high frequency fringe patterns to measure the object after the high frequency is directly estimated by the experiment. Thus the proposed method can simplify the procedure and improve the speed. Finally, the experimental evaluation is conducted to prove the validity of the proposed method.The results demonstrate that the proposed method can overcome the main disadvantages encountered by Zhang's method."
"The utilization of statistical machine translation (SMT) has grown enormously over the last decade, many using open-source software developed by the NLP community. As commercial use has increased, there is need for software that is optimized for commercial requirements, in particular, fast phrase-based decoding and more efficient utilization of modern multicore servers.   In this paper we re-examine the major components of phrase-based decoding and decoder implementation with particular emphasis on speed and scalability on multicore machines. The result is a drop-in replacement for the Moses decoder which is up to fifteen times faster and scales monotonically with the number of cores."
"Many critical EDA problems suffer from the curse of dimensionality, i.e. the very fast-scaling computational burden produced by large number of parameters and/or unknown variables. This phenomenon may be caused by multiple spatial or temporal factors (e.g. 3-D field solvers discretizations and multi-rate circuit simulation), nonlinearity of devices and circuits, large number of design or optimization parameters (e.g. full-chip routing/placement and circuit sizing), or extensive process variations (e.g. variability/reliability analysis and design for manufacturability). The computational challenges generated by such high dimensional problems are generally hard to handle efficiently with traditional EDA core algorithms that are based on matrix and vector computation. This paper presents ""tensor computation"" as an alternative general framework for the development of efficient EDA algorithms and tools. A tensor is a high-dimensional generalization of a matrix and a vector, and is a natural choice for both storing and solving efficiently high-dimensional EDA problems. This paper gives a basic tutorial on tensors, demonstrates some recent examples of EDA applications (e.g., nonlinear circuit modeling and high-dimensional uncertainty quantification), and suggests further open EDA problems where the use of tensor computation could be of advantage."
"Generalized Product (GPC) Codes, an unification of Product Codes and Integrated Interleaved (II) Codes, are presented. Applications for approaches requiring local and global parities are described. The more general problem of extending product codes by adding global parities is studied and an upper bound on the minimum distance of such codes is obtained. Codes with one, two and three global parities whose minimum distances meet the bound are presented. Tradeoffs between optimality and field size are discussed."
"This paper considers metric spaces where distances between a pair of nodes are represented by distance intervals. The goal is to study methods for the determination of hierarchical clusters, i.e., a family of nested partitions indexed by a resolution parameter, induced from the given distance intervals of the metric spaces. Our construction of hierarchical clustering methods is based on defining admissible methods to be those methods that abide to the axioms of value - nodes in a metric space with two nodes are clustered together at the convex combination of the distance bounds between them - and transformation - when both distance bounds are reduced, the output may become more clustered but not less. Two admissible methods are constructed and are shown to provide universal upper and lower bounds in the space of admissible methods. Practical implications are explored by clustering moving points via snapshots and by clustering networks representing brain structural connectivity using the lower and upper bounds of the network distance. The proposed clustering methods succeed in identifying underlying clustering structures via the maximum and minimum distances in all snapshots, as well as in differentiating brain connectivity networks of patients from those of healthy controls."
"The availability of large amounts of data together with advances in analytical techniques afford an opportunity to address difficult challenges in ensuring that healthcare is safe, effective, efficient, patient-centered, equitable, and timely. Surgical care and training stand to tremendously gain through surgical data science. Herein, we discuss a few perspectives on the scope and objectives for surgical data science."
"Early hands-on experiences with the Microsoft Hololens augmented/mixed reality device are reported and discussed, with a general aim of exploring basic 3D visualization. A range of usage cases are tested, including data visualization and immersive data spaces, in-situ visualization of 3D models and full scale architectural form visualization. Ultimately, the Hololens is found to provide a remarkable tool for moving from traditional visualization of 3D objects on a 2D screen, to fully experiential 3D visualizations embedded in the real world."
"Applying end-to-end learning to solve complex, interactive, pixel-driven control tasks on a robot is an unsolved problem. Deep Reinforcement Learning algorithms are too slow to achieve performance on a real robot, but their potential has been demonstrated in simulated environments. We propose using progressive networks to bridge the reality gap and transfer learned policies from simulation to the real world. The progressive net approach is a general framework that enables reuse of everything from low-level visual features to high-level policies for transfer to new tasks, enabling a compositional, yet simple, approach to building complex skills. We present an early demonstration of this approach with a number of experiments in the domain of robot manipulation that focus on bridging the reality gap. Unlike other proposed approaches, our real-world experiments demonstrate successful task learning from raw visual input on a fully actuated robot manipulator. Moreover, rather than relying on model-based trajectory optimisation, the task learning is accomplished using only deep reinforcement learning and sparse rewards."
"In this paper, electrothermal field phenomena in electronic components are considered. This coupling is tackled by multiphysical field simulations using the Finite Integration Technique (FIT). In particular, the design of bonding wires with respect to thermal degradation is investigated. Instead of resolving the wires by the computational grid, lumped element representations are introduced as point-to-point connections in the spatially distributed model. Fabrication tolerances lead to uncertainties of the wires' parameters and influence the operation and reliability of the final product. Based on geometric measurements, the resulting variability of the wire temperatures is determined using the stochastic electrothermal field-circuit model."
"Starting from a 3D electrothermal field problem discretized by the Finite Integration Technique, the equivalence to a circuit description is shown by exploiting the analogy to the Modified Nodal Analysis approach. Using this analogy, an algorithm for the automatic generation of a monolithic SPICE netlist is presented. Joule losses from the electrical circuit are included as heat sources in the thermal circuit. The thermal simulation yields nodal temperatures that influence the electrical conductivity. Apart from the used field discretization, this approach applies no further simplifications. An example 3D chip package is used to validate the algorithm."
"In this paper, we propose a new practical association rule mining algorithm for anomaly detection in Intrusion Detection System (IDS). First, with a view of anomaly cases being relatively rarely occurred in network packet database, we define a rare association rule among infrequent itemsets rather than the traditional association rule mining method. And then, we discuss an interest measure to catch differences between interesting relations and uninteresting ones, and what interest there is, and develop a hash based rare association rule mining algorithm for finding rare, but useful anomaly patterns to user. Finally, we define a quantitative association rule in relational database, propose a practical algorithm to mine rare association rules from network packet database, and show advantages of it giving a concrete example. Our algorithm can be applied to fields need to mine hidden patterns which are rare, but valuable, like IDS, and it is based on hashing method among infrequent itemsets, so that it has obvious advantages of speed and memory space limitation problems over the traditional association rule mining algorithms. Keywords: rare association mining algorithm, infrequent itemsets, quantitative association rule, network intrusion detection system, anomaly detection"
"Active vision is inherently attention-driven: The agent selects views of observation to best approach the vision task while improving its internal representation of the scene being observed. Inspired by the recent success of attention-based models in 2D vision tasks based on single RGB images, we propose to address the multi-view depth-based active object recognition using attention mechanism, through developing an end-to-end recurrent 3D attentional network. The architecture comprises of a recurrent neural network (RNN), storing and updating an internal representation, and two levels of spatial transformer units, guiding two-level attentions. Our model, trained with a 3D shape database, is able to iteratively attend to the best views targeting an object of interest for recognizing it, and focus on the object in each view for removing the background clutter. To realize 3D view selection, we derive a 3D spatial transformer network which is differentiable for training with back-propagation, achieving must faster convergence than the reinforcement learning employed by most existing attention-based models. Experiments show that our method outperforms state-of-the-art methods in cluttered scenes."
"Cross-application interference can affect drastically performance of HPC applications when running in clouds. This problem is caused by concurrent access performed by co-located applications to shared and non-sliceable resources such as cache and memory. In order to address this issue, some works adopted a qualitative approach that does not take into account the amount of access to shared resources. In addition, a few works, even considering the amount of access, evaluated just the SLLC access contention as the root of this problem. However, our experiments revealed that interference is intrinsically related to the amount of simultaneous access to shared resources, besides showing that another shared resources, apart from SLLC, can also influence the interference suffered by co-located applications. In this paper, we present a quantitative model for predicting cross-application interference in virtual environments. Our proposed model takes into account the amount of simultaneous access to SLLC, DRAM and virtual network, and the similarity of application's access burden to predict the level of interference suffered by applications when co-located in a same physical machine. Experiments considering a real petroleum reservoir simulator and applications from HPCC benchmark showed that our model reached an average and maximum prediction errors around 4\% and 12\%, besides achieving an error less than 10\% in approximately 96\% of all tested cases."
"In security games, the solution concept commonly used is that of a Stackelberg equilibrium where the defender gets to commit to a mixed strategy. The motivation for this is that the attacker can repeatedly observe the defender's actions and learn her distribution over actions, before acting himself. If the actions were not observable, Nash (or perhaps correlated) equilibrium would arguably be a more natural solution concept. But what if some, but not all, aspects of the defender's actions are observable? In this paper, we introduce solution concepts corresponding to this case, both with and without correlation. We study their basic properties, whether these solutions can be efficiently computed, and the impact of additional observability on the utility obtained."
"The paper determines the algebraic and logic structure of the multiset semantics of the core patterns of SPARQL. We prove that the fragment formed by AND, UNION, OPTIONAL, FILTER, MINUS and SELECT corresponds precisely to both, the intuitive multiset relational algebra (projection, selection, natural join, arithmetic union and except), and the multiset non-recursive Datalog with safe negation."
"In this paper we introduce a new approach for approximately counting in bounded degree systems with higher-order constraints. Our main result is an algorithm to approximately count the number of solutions to a CNF formula $\Phi$ with at least $k$ variables per clause and degree at most $d$ when $k$ is logarithmic in $d$. This closes an exponential gap between the known upper and lower bounds.   Moreover our algorithm extends straightforwardly to approximate sampling, which shows that under Lovasz Local Lemma-like conditions it is not only possible to find a satisfying assignment, it is also possible to generate one approximately uniformly at random from the set of all satisfying assignments. Our approach is a significant departure from earlier techniques in approximate counting, and is based on a framework to bootstrap an oracle for computing marginal probabilities on individual variables. Finally, we give an application of our results to show that it is algorithmically possible to sample from the posterior distribution in an interesting class of graphical models."
"We propose a deep learning-based feature fusion approach for facial computing including face recognition as well as gender, race and age detection. Instead of training a single classifier on face images to classify them based on the features of the person whose face appears in the image, we first train four different classifiers for classifying face images based on race, age, gender and identification (ID). Multi-task features are then extracted from the trained models and cross-task-feature training is conducted which shows the value of fusing multimodal features extracted from multi-tasks. We have found that features trained for one task can be used for other related tasks. More interestingly, the features trained for a task with more classes (e.g. ID) and then used in another task with fewer classes (e.g. race) outperforms the features trained for the other task itself. The final feature fusion is performed by combining the four types of features extracted from the images by the four classifiers. The feature fusion approach improves the classifications accuracy by a 7.2%, 20.1%, 22.2%, 21.8% margin, respectively, for ID, age, race and gender recognition, over the results of single classifiers trained only on their individual features. The proposed method can be applied to applications in which different types of data or features can be extracted."
"Minimum Message Length (MML) is a popular method for statistical inference, belonging to the Minimum Description Length (MDL) family. It is a general name for any of several computationally-feasible approximations to the generally NP-Hard Strict Minimum Message Length (SMML) estimator. One often-cited showcase for the power of MML is the Neyman-Scott estimation problem, where most popular estimation algorithms fail to produce a consistent result. MML's performance on Neyman-Scott was analysed by Dowe and Wallace (1997) and by Wallace (2005) and MML was shown to be consistent for the problem. However, this analysis was not performed on SMML, but rather on two SMML approximations: Wallace-Freeman and Ideal Group. As for most estimation problems, the exact SMML solution is not known for Neyman-Scott. We analyse the Dowe-Wallace solution, and show that it hinges critically on the use of an unnatural prior for the problem. We argue that the Jeffreys prior is a more natural prior to assume in this case. Re-analysing the problem over its Jeffreys prior, we show that both the Ideal Group and the Wallace-Freeman approximations converge to the (inconsistent) Maximum Likelihood (ML) solution. We develop novel techniques that enable determining properties of the SMML estimator for some general families of estimation problems without requiring a full construction of the estimator, and use these to show that for many problems, including Neyman-Scott, the SMML estimator is not a point-estimator at all. Rather, it maps each observation to an entire continuum of estimates. Furthermore, using the tools developed we show that for Neyman-Scott the SMML estimate is inconsistent for all parameter sets as well as asymptotically. We discuss methodological problems in the arguments put forward by previous authors, who argued that MML is consistent for Neyman-Scott and in general."
"In an era of unprecedented deluge of (mostly unstructured) data, graphs are proving more and more useful, across the sciences, as a flexible abstraction to capture complex relationships between complex objects. One of the main challenges arising in the study of such networks is the inference of macroscopic, large-scale properties affecting a large number of objects, based solely on the microscopic interactions between their elementary constituents. Statistical physics, precisely created to recover the macroscopic laws of thermodynamics from an idealized model of interacting particles, provides significant insight to tackle such complex networks.   In this dissertation, we use methods derived from the statistical physics of disordered systems to design and study new algorithms for inference on graphs. Our focus is on spectral methods, based on certain eigenvectors of carefully chosen matrices, and sparse graphs, containing only a small amount of information. We develop an original theory of spectral inference based on a relaxation of various mean-field free energy optimizations. Our approach is therefore fully probabilistic, and contrasts with more traditional motivations based on the optimization of a cost function. We illustrate the efficiency of our approach on various problems, including community detection, randomized similarity-based clustering, and matrix completion."
"Signal to Noise Ratio (SNR) is an important index for wireless communications. There are many methods for increasing SNR. In CDMA systems, spreading sequences are used. We consider the frequency-selective wide-sense-stationary uncorrelated-scattering (WSSUS) channel and evaluate the worst case of SNR. We construct the non-linear programing for maximizing the lower bound of the average of SNR. This problem becomes the convex programming if we did not take into account the norm constraint. We derive necessary conditions for optimal spreading sequences for the problem."
"Many methods have been used to recognize author personality traits from text, typically combining linguistic feature engineering with shallow learning models, e.g. linear regression or Support Vector Machines. This work uses deep-learning-based models and atomic features of text, the characters, to build hierarchical, vectorial word and sentence representations for trait inference. This method, applied to a corpus of tweets, shows state-of-the-art performance across five traits and three languages (English, Spanish and Italian) compared with prior work in author profiling. The results, supported by preliminary visualisation work, are encouraging for the ability to detect complex human traits."
"Steganography is the art of hiding data, in such a way that it is undetectable under traffic-pattern analysis and the data hidden is only known to the receiver and the sender. In this paper new method of text steganography over the silence interval of audio in a video file, is presented. In the proposed method first the audio signal is extracted from the video. After doing audio enhancement, the data on the audio signal is steganographed using new technique and then audio signal is rewritten in video file again. http://www.learnrnd.com/All_latest_research_findings.php   To enhance the security level we apply chaotic maps on arbitrary text. Furthermore, the algorithm in this paper, gives a technique which states that undetectable stegotext and cover-text has same probability distribution and no statistical test can detect the presence of the hidden message. http://www.learnrnd.com/detail.php?id=Biohack_Eyes_through_Chlorin_e6_eye_drop_:Stanford_University_Research   Moreover, hidden message does not affect the transmission rate of video file at all."
"We propose a simple discrete time semi-supervised graph embedding approach to link prediction in dynamic networks. The learned embedding reflects information from both the temporal and cross-sectional network structures, which is performed by defining the loss function as a weighted sum of the supervised loss from past dynamics and the unsupervised loss of predicting the neighborhood context in the current network. Our model is also capable of learning different embeddings for both formation and dissolution dynamics. These key aspects contributes to the predictive performance of our model and we provide experiments with three real--world dynamic networks showing that our method is comparable to state of the art methods in link formation prediction and outperforms state of the art baseline methods in link dissolution prediction."
"In this paper, we introduce a lab trial that is conducted to study the feasibility of ultra-low latency for 5G. Using a novel frame structure and signaling procedure, a stabilized 1.5 ms hybrid automatic repeat request (HARQ) round trip time (RTT) is observed in lab trial as predicted in theory. The achieved round trip time is around 5 times shorter compared with what is supported by LTE-Advanced standard."
"We present a closed-form expression for the minimal delay that is achievable in a setting that combines a buffer and an erasure code, used to mitigate the packet delay variance. The erasure code is modeled according to the recent information-theoretic results on finite block length codes. Evaluations reveal that accurate knowledge of the network parameters is essential for optimal operation. Moreover, it is shown that, when the network packet delay variance is large, the buffer delay becomes negligible. Therefore, in this case the delay budget should be spent mainly on the erasure code."
"The paper deals with fractal characteristics (Hurst exponent) and wavelet-scaleograms of the information distribution model, suggested by the authors. The authors have studied the effect of Hurst exponent change depending upon the model parameters, which have semantic meaning. The paper also considers fractal characteristics of real information streams. It is described, how the Hurst exponent dynamics depends on these information streams state in practice"
"We present the Civique system for emergency detection in urban areas by monitoring micro blogs like Tweets. The system detects emergency related events, and classifies them into appropriate categories like ""fire"", ""accident"", ""earthquake"", etc. We demonstrate our ideas by classifying Twitter posts in real time, visualizing the ongoing event on a map interface and alerting users with options to contact relevant authorities, both online and offline. We evaluate our classifiers for both the steps, i.e., emergency detection and categorization, and obtain F-scores exceeding 70% and 90%, respectively. We demonstrate Civique using a web interface and on an Android application, in realtime, and show its use for both tweet detection and visualization."
"The optimization of joint source and channel coding for a sequence of numerous progressive packets is a challenging problem. Further, the problem becomes more complicated if the space-time coding is also involved with the optimization in a multiple-input multiple-output (MIMO) system. This is because the number of ways of jointly assigning channels codes and space-time codes to progressive packets is much larger than that of solely assigning channel codes to the packets. We are unaware of any feasible and complete solution for such optimization of joint source, channel, and space-time coding of progressive packets. This paper applies a parametric approach to address that complex joint optimization problem in a MIMO system. We use the parametric methodology to derive some useful theoretical results, and then exploit those results to propose an optimization method where the joint assignment of channel codes and space-time codes to the packets can be optimized in a packet-by-packet manner. As a result, the computational complexity of the optimization is exponentially reduced, compared to the conventional exhaustive search. The numerical results show that the proposed method significantly improves the peak-signal-to-noise ratio performance of the rate-based optimal solution in a MIMO system."
"We investigate the logical foundations of hyperproperties. Hyperproperties generalize trace properties, which are sets of traces, to sets of sets of traces. The most prominent application of hyperproperties is information flow security: information flow policies characterize the secrecy and integrity of a system by comparing two or more execution traces, for example by comparing the observations made by an external observer on execution traces that result from different values of a secret variable. In this paper, we establish the first connection between temporal logics for hyperproperties and first-order logic. Kamp's seminal theorem (in the formulation due to Gabbay et al.) states that linear-time temporal logic (LTL) is expressively equivalent to first-order logic over the natural numbers with order. We introduce first-order logic over sets of traces and prove that HyperLTL, the extension of LTL to hyperproperties, is strictly subsumed by this logic. We furthermore exhibit a fragment that is expressively equivalent to HyperLTL, thereby establishing Kamp's theorem for hyperproperties."
"In this paper we propose an algorithm for path-following control of the nonholonomic mobile robot based on the idea of the guiding vector field. The desired path may be an arbitrary smooth curve in its implicit form, that is, a level set of some known smooth function. Using this function and the kinematic robot's model, we design a guiding vector field, whose integral curves converge to the trajectory. A nonlinear motion controller is then proposed which steers the robot along such an integral curve, bringing it to the desired path. We give conditions for the global convergence of our algorithm. Experiments with real wheeled robots confirm the applicability and performance of the proposed path following control algorithm."
"Linked Data technologies become increasingly important in many domains. Key factors for their breakthrough are security and trust. Classical means for access control lack granularity when parts of the Linked Data graph must be protected. The WebID, combining semantic web concepts with methods from certificate based authentication and authorization, seems promising to fulfill all requirements concerning security and trust in the semantic web.   In the PerSemID project, we challenged the WebID technology with a fully implemented proof-of-concept (PoC) addressing a workflow coming from the domain of lifelong learning and student mobility. In our use case of study enrollment, we used WebIDs for authentication and to grant access to parts of triple stores, during cross domain triple store interactions to exchange data between stakeholders."
"Social influence plays an important role in human behavior and decisions. The sources of influence can be generally divided into external, which are independent of social context, or as originating from peers, such as family and friends. An important question is how to disentangle the social contagion by peers from external influences. While a variety of experimental and observational studies provided insight into this problem, identifying the extent of social contagion based on large-scale observational data with an unknown network structure remains largely unexplored. By bridging the gap between the large-scale complex systems perspective of collective human dynamics and the detailed approach of the social sciences, we present a parsimonious model of social influence, and apply it to a central topic in political science -- elections and voting behavior. We provide an analytical expression of the county vote-share distribution in a two party system, which is in excellent agreement with 92 years of observed U.S. presidential election data. Analyzing the social influence topography over this period reveals an abrupt transition in the patterns of social contagion -- from low to high levels of social contagion. The results from our analysis reveal robust differences among regions of the United States in terms of their social influence index. In particular, we identify two regions of 'hot' and 'cold spots of social influence, each comprising states that are geographically close. These results suggest that social contagion effects are becoming more instrumental in shaping large scale collective political behavior, which is at the core of democratic societies."
"According to the distributional inclusion hypothesis, entailment between words can be measured via the feature inclusions of their distributional vectors. In recent work, we showed how this hypothesis can be extended from words to phrases and sentences in the setting of compositional distributional semantics. This paper focuses on inclusion properties of tensors; its main contribution is a theoretical and experimental analysis of how feature inclusion works in different concrete models of verb tensors. We present results for relational, Frobenius, projective, and holistic methods and compare them to the simple vector addition, multiplication, min, and max models. The degrees of entailment thus obtained are evaluated via a variety of existing word-based measures, such as Weed's and Clarke's, KL-divergence, APinc, balAPinc, and two of our previously proposed metrics at the phrase/sentence level. We perform experiments on three entailment datasets, investigating which version of tensor-based composition achieves the highest performance when combined with the sentence-level measures."
"Domain adaptation (DA) is an important and emerging field of machine learning that tackles the problem occurring when the distributions of training (source domain) and test (target domain) data are similar but different. Current theoretical results show that the efficiency of DA algorithms depends on their capacity of minimizing the divergence between source and target probability distributions. In this paper, we provide a theoretical study on the advantages that concepts borrowed from optimal transportation theory can bring to DA. In particular, we show that the Wasserstein metric can be used as a divergence measure between distributions to obtain generalization guarantees for three different learning settings: (i) classic DA with unsupervised target data (ii) DA combining source and target labeled data, (iii) multiple source DA. Based on the obtained results, we provide some insights showing when this analysis can be tighter than other existing frameworks. We also show that in the context of multiple source DA, the problem of estimating of the best joint hypothesis between source and target labeling functions can be reformulated using a Wasserstein distance-based loss function. We think that these results open the door to novel ideas and directions for DA."
"Recent years have seen an evolution of SDN control plane architectures, starting from simple monolithic controllers, over modular monolithic controllers, to distributed controllers. We observe, however, that today's distributed controllers still exhibit inflexibility with respect to the distribution of control logic. Therefore, we propose a novel architecture of a distributed SDN controller in this paper, providing maximum flexibility with respect to distribution.   Our architecture splits control logic into light-weight control modules, called controllets, based on a micro-kernel approach, reducing common controllet functionality to a bare minimum and factoring out all higher-level functionality. Light-weight controllets also allow for pushing control logic onto switches to minimize latency and communication overhead. Controllets are interconnected through a message bus supporting the publish/subscribe communication paradigm with specific extensions for content-based OpenFlow message filtering. Publish/subscribe allows for complete decoupling of controllets to further facilitate control plane distribution."
"In this paper we propose a new algorithm for detecting if a person is under the influence of alcohol. This algorithm is based on number of pauses the speaker makes by judging that if the number of pauses compared to the previous recordings of the same person, which has been recorded beforehand, is higher. If so the algorithm mark the speaker as drunk."
"As the use of crowdsourcing increases, it is important to think about performance optimization. For this purpose, it is possible to think about each worker as a HPU(Human Processing Unit), and to draw inspiration from performance optimization on traditional computers or cloud nodes with CPUs. However, as we characterize HPUs in detail for this purpose, we find that there are important differences between CPUs and HPUs, leading to the need for completely new optimization algorithms.   In this paper, we study the specific optimization problem of obtaining results fastest for a crowd sourced job with a fixed total budget. In crowdsourcing, jobs are usually broken down into sets of small tasks, which are assigned to workers one at a time. We consider three scenarios of increasing complexity: Identical Round Homogeneous tasks, Multiplex Round Homogeneous tasks, and Multiple Round Heterogeneous tasks. For each scenario, we analyze the stochastic behavior of the HPU clock-rate as a function of the remuneration offered. After that, we develop an optimum Budget Allocation strategy to minimize the latency for job completion. We validate our results through extensive simulations and experiments on Amazon Mechanical Turk."
"We study the well-known two-dimensional strip packing problem. Given is a set of rectangular axis-parallel items and a strip of width $W$ with infinite height. The objective is to find a packing of these items into the strip, which minimizes the packing height. Lately, it has been shown that the lower bound of $3/2$ of the absolute approximation ratio can be beaten when we allow a pseudo-polynomial running-time of type $(n W)^{f(1/\varepsilon)}$. If $W$ is polynomially bounded by the number of items, this is a polynomial running-time. We present a pseudo-polynomial algorithm with approximation ratio $4/3 +\varepsilon$ and running time $(n W)^{1/\varepsilon^{\mathcal{O}(2^{1/\varepsilon})}}$."
"Clark has defined the notion of $n$-avoidance basis which contains the avoidable formulas with at most $n$ variables that are closest to be unavoidable in some sense. The family $C_i$ of circular formulas is such that $C_1=AA$, $C_2=ABA.BAB$, $C_3=ABCA.BCAB.CABC$ and so on. For every $i\le n$, the $n$-avoidance basis contains $C_i$. Clark showed that the avoidability index of every circular formula and of every formula in the $3$-avoidance basis (and thus of every avoidable formula containing at most 3 variables) is at most 4. We determine exactly the avoidability index of these formulas."
"Permutation polynomials with few terms attracts researchers' interest in recent years due to their simple algebraic form and some additional extraordinary properties. In this paper, by analyzing the quadratic factors of a fifth-degree polynomial and a seventh-degree polynomial over the finite field $\mathbb{F}_{3^{2k}}$, two conjectures on permutation trinomials over $\mathbb{F}_{3^{2k}}$ proposed recently by Li, Qu, Li and Fu are settled, where $k$ is a positive integer."
"The expert advices or consultancies by the expert consultants (ECs) for any critical situation in healthcare are the most important event that occurs on a daily basis in the hospitals. As the success of any critical operation (or surgery) is heavily dependent on ECs, so they can be thought of as one of the crucial elements of the healthcare domain. In many critical situations, there may be a need to hire some ECs from outside of the hospital to provide their expertise to the needy patient(s) that are in-house to the hospital. Earlier this interesting situation of hiring ECs (mainly doctors) from outside of the hospital has been studied under both monetary environment (with infinite budget) and non-monetary environment (with zero budget). In this paper, a two pass algorithm is proposed for the fixed budget case, for the first time, in healthcare domain. In the first pass it is ensured that a substantial number of ECs must be aware of this hiring concept, so that they can participate in the bidding process and in the second pass, ECs are allocated to a patient satisfying its budget limit and it is interesting to note that the proposed algorithm satisfies truthfulness, individual rationality, and budget feasibility. Through exhaustive simulations, we evaluate the performance and validate our proposed mechanisms."
"Uniform and affordable Internet is emerging as one of the fundamental civil rights in developing countries. However in India, the connectivity is far from uniform across the regions, where the disparity is evident in the infrastructure, the cost of access and telecommunication services to provide Internet facilities among different economic classes. In spite of having a large mobile user base, the mobile Internet are still remarkably slower in some of the developing countries. Especially in India, it falls below 50% even in comparison with the performance of its developing counterparts!   This essay presents a study of connectivity and performance trends based on an exploratory analysis of mobile Internet measurement data from India. In order to assess the state of mobile networks and its readiness in adopting the different mobile standards (2G, 3G, and 4G) for commercial use, we discuss the spread, penetration, interoperability and the congestion trends. Based on our analysis, we argue that the network operators have taken negligible measures to scale the mobile Internet. Affordable Internet is definitely for everyone. But, the affordability of the Internet in terms of cost does not necessarily imply the rightful access to Internet services.   Chota recharge is possibly leading us to chota (shrunken) Internet!"
"Though the concept of sample mean in dynamic time warping (DTW) spaces is used in pattern recognition applications, its existence has neither been proved nor called into question. This article shows that a sample mean exists under general conditions that cover common variations of different DTW-spaces mentioned in the literature. The existence proofs are based on a Reduction Theorem that bounds the length of the candidate solutions we need to consider. The proposed results place the concept of sample mean in DTW-spaces on a sound mathematical foundation and serves as a first step towards a statistical theory of DTW-spaces."
"Mobile applications today often fail to be context aware when they also need to be customizable and efficient at run-time. Context-oriented programming allows programmers to develop applications that are more context aware. Its central construct, the so-called layer, however, is not customizable. We propose to use novel persistent contextual values for mobile development. Persistent contextual values automatically adapt their value to the context. Furthermore they provide access without overhead. Key-value configuration files contain the specification of contextual values and the persisted contextual values themselves. By modifying the configuration files, the contextual values can easily be customized for every context. From the specification, we generate code to simplify development. Our implementation, called Elektra, permits development in several languages including C++ and Java. In a benchmark we compare layer activations between threads and between applications. In a case study involving a web-server on a mobile embedded device the performance overhead is minimal, even with many context switches."
"The curse of outlier measurements in estimation problems is a well known issue in a variety of fields. Therefore, outlier removal procedures, which enables the identification of spurious measurements within a set, have been developed for many different scenarios and applications. In this paper, we propose a statistically motivated outlier removal algorithm for time differences of arrival (TDOAs), or equivalently range differences (RD), acquired at sensor arrays. The method exploits the TDOA-space formalism and works by only knowing the relative sensor positions. As the proposed method is completely independent from the application for which measurements are used, it can be reliably used to identify outliers within a set of TDOA/RD measurements in different fields (e.g. acoustic source localization, sensor synchronization, radar, remote sensing, etc.). The whole theoretical derivation is validated by means of synthetic simulations and real experiments."
"Minimum Physical Hop (MPH) has been proposed as a peer selection algorithm for decreasing inter-AS (Autonomous System) traffic volume in P2P live streaming. In MPH, a newly joining peer selects a peer whose physical hop count (i.e., the number of ASes traversed on the content delivery path) from it is the minimum as its providing peer. However, MPH shows high inter-AS traffic volume when the number of joining peers is large. In this paper, we propose IMPH that tries to further decrease the inter-AS traffic volume by distributing peers with one logical hop count (i.e., the number of peers or origin streaming servers (OSSes) traversed on the content delivery path from an OSS to the peer) to many ASes and encouraging the following peers to find their providing peers within the same AS. Numerical examples show that IMPH achieves at the maximum of 64% lower inter-AS traffic volume than MPH."
"Image Super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high-resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Furthermore, MAP inference is often performed via optimisation-based iterative algorithms which don't compare well with the efficiency of neural-network-based alternatives. Here we introduce new methods for amortised MAP inference whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data, achieving particularly good results in photo-realistic texture SR."
"Stochastic linear bandits are a natural and simple generalisation of finite-armed bandits with numerous practical applications. Current approaches focus on generalising existing techniques for finite-armed bandits, notably the optimism principle and Thompson sampling. While prior work has mostly been in the worst-case setting, we analyse the asymptotic instance-dependent regret and show matching upper and lower bounds on what is achievable. Surprisingly, our results show that no algorithm based on optimism or Thompson sampling will ever achieve the optimal rate, and indeed, can be arbitrarily far from optimal, even in very simple cases. This is a disturbing result because these techniques are standard tools that are widely used for sequential optimisation. For example, for generalised linear bandits and reinforcement learning."
Big data processing is a hot topic in today's computer science world. There is a significant demand for analysing big data to satisfy many requirements of many industries. Emergence of the Kappa architecture created a strong requirement for a highly capable and efficient data processing engine. Therefore data processing engines such as Apache Flink and Apache Spark emerged in open source world to fulfil that efficient and high performing data processing requirement. There are many available benchmarks to evaluate those two data processing engines. But complex deployment patterns and dependencies make those benchmarks very difficult to reproduce by our own. This project has two main goals. They are making few of community accepted benchmarks easily reproducible on cloud and validate the performance claimed by those studies.
"As Wireless Sensor Networks are penetrating into the industrial domain, many research opportunities are emerging. One such essential and challenging application is that of node localization. A feed-forward neural network based methodology is adopted in this paper. The Received Signal Strength Indicator (RSSI) values of the anchor node beacons are used. The number of anchor nodes and their configurations has an impact on the accuracy of the localization system, which is also addressed in this paper. Five different training algorithms are evaluated to find the training algorithm that gives the best result. The multi-layer Perceptron (MLP) neural network model was trained using Matlab. In order to evaluate the performance of the proposed method in real time, the model obtained was then implemented on the Arduino microcontroller. With four anchor nodes, an average 2D localization error of 0.2953 m has been achieved with a 12-12-2 neural network structure. The proposed method can also be implemented on any other embedded microcontroller system."
"A new system model reflecting the clustered structure of distributed storage is suggested to investigate bandwidth requirements for repairing failed storage nodes. Large data centers with multiple racks/disks or local networks of storage devices (e.g. sensor network) are good applications of the suggested clustered model. In realistic scenarios involving clustered storage structures, repairing storage nodes using intact nodes residing in other clusters is more bandwidth-consuming than restoring nodes based on information from intra-cluster nodes. Therefore, it is important to differentiate between intra-cluster repair bandwidth and cross-cluster repair bandwidth in modeling distributed storage. Capacity of the suggested model is obtained as a function of fundamental resources of distributed storage systems, namely, storage capacity, intra-cluster repair bandwidth and cross-cluster repair bandwidth. Based on the capacity expression, feasible sets of required resources which enable reliable storage are analyzed. It is shown that the cross-cluster traffic can be minimized to zero (i.e., intra-cluster local repair becomes possible) by allowing extra resources on storage capacity and intra-cluster repair bandwidth, according to a law specified in a closed-form. Moreover, trade-off between cross-cluster traffic and intra-cluster traffic is observed for sufficiently large storage capacity."
"It is well known that load balancing and low delivery communication cost are two critical issues in mapping requests to servers in Content Delivery Networks (CDNs). However, the trade-off between these two performance metrics has not been yet quantitatively investigated in designing efficient request mapping schemes. In this work, we formalize this trade-off through a stochastic optimization problem. While the solutions to the problem in the extreme cases of minimum communication cost and optimum load balancing can be derived in closed form, finding the general solution is hard to derive. Thus we propose three heuristic mapping schemes and compare the trade-off performance of them through extensive simulations.   Our simulation results show that at the expense of high query cost, we can achieve a good trade-off curve. Moreover, by benefiting from the power of multiple choices phenomenon, we can achieve almost the same performance with much less query cost. Finally, we can handle requests with different delay requirements at the cost of degrading network performance."
"We develop a customized proximal augmented Lagrangian method to efficiently solve a class of nonsmooth composite optimization problems. We introduce an auxiliary variable to split the objective function into the sum of a differentiable but possibly nonconvex component and a nondifferentiable convex regularization term. By utilizing the Moreau envelope of the regularization function, we transform the associated augmented Lagrangian into a continuously differentiable form that can be efficiently minimized using a variety of techniques. Our algorithm applies to a broader class of problems than proximal gradient methods and has stronger convergence guarantees and a more refined step size update than the alternating direction method of multipliers. These features make it an attractive option for solving structured optimal control problems. Furthermore, when the differentiable component of the objective function is strictly convex and the regularization function is convex, we prove asymptotic convergence of continuous-time gradient flow dynamics to the optimal solution. Finally, we apply our approach to the problem of edge addition in directed consensus networks."
"In this paper, we analyze 2CNF formulas from the perspectives of Read-Once resolution (ROR) refutation schemes. We focus on two types of ROR refutations, viz., variable-once refutation and clause-once refutation. In the former, each variable may be used at most once in the derivation of a refutation, while in the latter, each clause may be used at most once. We show that the problem of checking whether a given 2CNF formula has an ROR refutation under both schemes is NP-complete. This is surprising in light of the fact that there exist polynomial refutation schemes (tree-resolution and DAG-resolution) for 2CNF formulas. On the positive side, we show that 2CNF formulas have copy-complexity 2, which means that any unsatisfiable 2CNF formula has a refutation in which any clause needs to be used at most twice."
"A user wants to retrieve a file from a database without revealing the identity of the file retrieved at the database, which is known as the problem of private information retrieval (PIR). If it is further required that the user obtains no information about the database other than the desired file, the concept of symmetric private information retrieval (SPIR) is introduced to guarantee privacy for both parties. In this paper, the problem of SPIR is studied for a database stored among $N$ nodes in a distributed way, by using an $(N,M)$-MDS storage code. The information-theoretic capacity of SPIR, defined as the maximum number of symbols of the desired file retrieved per downloaded symbol, for the coded database is derived. It is shown that the SPIR capacity for coded database is $1-\frac{M}{N}$, when the amount of the shared common randomness of distributed nodes (unavailable at the user) is at least $\frac{M}{N-M}$ times the file size. Otherwise, the SPIR capacity for the coded database equals zero."
"Recent developments in elastic shape analysis (ESA) are motivated by the fact that it provides comprehensive frameworks for simultaneous registration, deformation, and comparison of shapes. These methods achieve computational efficiency using certain square-root representations that transform invariant elastic metrics into Euclidean metrics, allowing for applications of standard algorithms and statistical tools. For analyzing shapes of embeddings of $\mathbb{S}^2$ in $\mathbb{R}^3$, Jermyn et al. introduced square-root normal fields (SRNFs) that transformed an elastic metric, with desirable invariant properties, into the $\mathbb{L}^2$ metric. These SRNFs are essentially surface normals scaled by square-roots of infinitesimal area elements. A critical need in shape analysis is to invert solutions (deformations, averages, modes of variations, etc) computed in the SRNF space, back to the original surface space for visualizations and inferences. Due to the lack of theory for understanding SRNFs maps and their inverses, we take a numerical approach and derive an efficient multiresolution algorithm, based on solving an optimization problem in the surface space, that estimates surfaces corresponding to given SRNFs. This solution is found effective, even for complex shapes, e.g. human bodies and animals, that undergo significant deformations including bending and stretching. Specifically, we use this inversion for computing elastic shape deformations, transferring deformations, summarizing shapes, and for finding modes of variability in a given collection, while simultaneously registering the surfaces. We demonstrate the proposed algorithms using a statistical analysis of human body shapes, classification of generic surfaces and analysis of brain structures."
"Sentence similarity is considered the basis of many natural language tasks such as information retrieval, question answering and text summarization. The semantic meaning between compared text fragments is based on the words semantic features and their relationships. This article reviews a set of word and sentence similarity measures and compares them on benchmark datasets. On the studied datasets, results showed that hybrid semantic measures perform better than both knowledge and corpus based measures."
"Traditionally, object tracking and segmentation are treated as two separate problems and solved independently. However, in this paper, we argue that tracking and segmentation are actually closely related and solving one should help the other. On one hand, the object track, which is a set of bounding boxes with one bounding box in every frame, would provide strong high-level guidance for the target/background segmentation task. On the other hand, the object segmentation would separate object from other objects and background, which will be useful for determining track locations in every frame. We propose a novel framework which combines online multiple target tracking and segmentation in a video. In our approach, the tracking and segmentation problems are coupled by Lagrange dual decomposition, which leads to more accurate segmentation results and also \emph{helps resolve typical difficulties in multiple target tracking, such as occlusion handling, ID-switch and track drifting}. To track targets, an individual appearance model is learned for each target via structured learning and network flow is employed to generate tracks from densely sampled candidates. For segmentation, multi-label Conditional Random Field (CRF) is applied to a superpixel based spatio-temporal graph in a segment of video to assign background or target labels to every superpixel. The experiments on diverse sequences show that our method outperforms state-of-the-art approaches for multiple target tracking as well as segmentation."
"Let G denote a graph and let K be a subset of vertices that are a set of target vertices of G. The K-terminal reliability of G is defined as the probability that all target vertices in K are connected, considering the possible failures of non-target vertices of G. The problem of computing K-terminal reliability is known to be #P-complete for polygon-circle graphs, and can be solved in polynomial-time for t-polygon graphs, which are a subclass of polygon-circle graphs. The class of circle graphs is a subclass of polygon-circle graphs and a superclass of t-polygon graphs. Therefore, the problem of computing K-terminal reliability for circle graphs is of particular interest. This paper proves that the problem remains #P-complete even for circle graphs. Additionally, this paper proposes a linear-time algorithm for solving the problem for proper circular-arc graphs, which are a subclass of circle graphs and a superclass of proper interval graphs."
"Software produced for research, published and otherwise, suffers from a number of common problems that make it difficult or impossible to run outside the original institution, or even off the primary developer's computer. We present ten simple rules to make such software robust enough to run anywhere, and inspire confidence in your reproducibility, and thereby delight your users and collaborators."
"In this work, we propose a scheme that provides an analytical estimate for the time-dependent degree distribution of some networks. This scheme maps the problem into a random walk in degree space, and then we choose the paths that are responsible for the dominant contributions. The method is illustrated on the dynamical versions of the Erd\""os-R\'enyi and Watts-Strogatz graphs, which were introduced as static models in the original formulation. We have succeeded in obtaining an analytical form for the dynamics Watts-Strogatz model, which is asymptotically exact for some regimes."
"We propose a scalable method for forward stochastic reachability analysis for uncontrolled linear systems with affine disturbance. Our method uses Fourier transforms to efficiently compute the forward stochastic reach probability measure (density) and the forward stochastic reach set. This method is applicable to systems with bounded or unbounded disturbance sets. We also examine the properties of the forward stochastic reach set and its probability density. Motivated by the problem of a robot attempting to capture a stochastically moving, non-adversarial target, we demonstrate our method on two simple examples. Where traditional approaches provide approximations, our method provides exact analytical expressions for the densities."
"Consonance is related to the perception of pleasantness arising from the combination of sounds and has been approached quantitatively using mathematical relations, physics, information theory and psychoacoustics. Tonal consonance is present in timbre, musical tuning, harmony and melody, and it is used for conveying sensations and emotions in music. It involves the physical properties of sound waves and is used to study melody and harmony through musical intervals and chords. From the perspective of complexity, the macroscopic properties of a system with many parts frequently rely on statistical properties of its constituent elements. Here we show that melody in musical pieces can be described in terms of the physical properties of melodic intervals and the existence of an entropy extremalization principle in music with psychoacoustic macroscopic constraints given by conserved quantities with musical meaning. This result connects human perception with human creativity through the physical properties of the musical stimulus."
"Machine learning models are vulnerable to adversarial examples formed by applying small carefully chosen perturbations to inputs that cause unexpected classification errors. In this paper, we perform experiments on various adversarial example generation approaches with multiple deep convolutional neural networks including Residual Networks, the best performing models on ImageNet Large-Scale Visual Recognition Challenge 2015. We compare the adversarial example generation techniques with respect to the quality of the produced images, and measure the robustness of the tested machine learning models to adversarial examples. Finally, we conduct large-scale experiments on cross-model adversarial portability. We find that adversarial examples are mostly transferable across similar network topologies, and we demonstrate that better machine learning models are less vulnerable to adversarial examples."
"The Big Data management is a problem right now. The Big Data growth is very high. It is very difficult to manage due to various characteristics. This manuscript focuses on Big Data analytics in cloud environment using Hadoop. We have classified the Big Data according to its characteristics like Volume, Value, Variety and Velocity. We have made various nodes to process the data based on their volume, velocity, value and variety. In this work we have classify the input data and routed to various processing node. At the last after processing from each node, we can combine the output of all nodes to get the final result. We have used Hadoop to partition the data as well as process it."
"This paper studies the generalization error of invariant classifiers. In particular, we consider the common scenario where the classification task is invariant to certain transformations of the input, and that the classifier is constructed (or learned) to be invariant to these transformations. Our approach relies on factoring the input space into a product of a base space and a set of transformations. We show that whereas the generalization error of a non-invariant classifier is proportional to the complexity of the input space, the generalization error of an invariant classifier is proportional to the complexity of the base space. We also derive a set of sufficient conditions on the geometry of the base space and the set of transformations that ensure that the complexity of the base space is much smaller than the complexity of the input space. Our analysis applies to general classifiers such as convolutional neural networks. We demonstrate the implications of the developed theory for such classifiers with experiments on the MNIST and CIFAR-10 datasets."
"This paper implements and compares different techniques for face detection and recognition. One is find where the face is located in the images that is face detection and second is face recognition that is identifying the person. We study three techniques in this paper: Face detection using self organizing map (SOM), Face recognition by projection and nearest neighbor and Face recognition using SVM."
"Kernel alignment measures the degree of similarity between two kernels. In this paper, inspired from kernel alignment, we propose a new Linear Discriminant Analysis (LDA) formulation, kernel alignment LDA (kaLDA). We first define two kernels, data kernel and class indicator kernel. The problem is to find a subspace to maximize the alignment between subspace-transformed data kernel and class indicator kernel. Surprisingly, the kernel alignment induced kaLDA objective function is very similar to classical LDA and can be expressed using between-class and total scatter matrices. This can be extended to multi-label data. We use a Stiefel-manifold gradient descent algorithm to solve this problem. We perform experiments on 8 single-label and 6 multi-label data sets. Results show that kaLDA has very good performance on many single-label and multi-label problems."
"Data Mining is the process of examining the information from different point of view and compressing it for the relevant data. This data can also be utilized to build the incomes. Data Mining is also known as Data or Knowledge Discovery. The basic purpose of data mining is to search patterns which have minimal user inputs and efforts. Data Mining plays a very crucial role in the various fields. There are various data mining procedures which can be connected in different fields of innovation. By using data mining techniques, it is observed that less time is taken for the prediction of any disease with more accuracy. In this paper we would review various data mining techniques which are categorized under classification, regression and clustering and apply these algorithms over an ECG dataset. The purpose of this work is to determine the most suitable data mining technique and use it to improve the accuracy of analyzing ECG data for better decision making."
"This paper describes a new parameter-free online learning algorithm for changing environments. In comparing against algorithms with the same time complexity as ours, we obtain a strongly adaptive regret bound that is a factor of at least $\sqrt{\log(T)}$ better, where $T$ is the time horizon. Empirical results show that our algorithm outperforms state-of-the-art methods in learning with expert advice and metric learning scenarios."
"Advances in optical neuroimaging techniques now allow neural activity to be recorded with cellular resolution in awake and behaving animals. Brain motion in these recordings pose a unique challenge. The location of individual neurons must be tracked in 3D over time to accurately extract single neuron activity traces. Recordings from small invertebrates like C. elegans are especially challenging because they undergo very large brain motion and deformation during animal movement. Here we present an automated computer vision pipeline to reliably track populations of neurons with single neuron resolution in the brain of a freely moving C. elegans undergoing large motion and deformation. 3D volumetric fluorescent images of the animal's brain are straightened, aligned and registered, and the locations of neurons in the images are found via segmentation. Each neuron is then assigned an identity using a new time-independent machine-learning approach we call Neuron Registration Vector Encoding. In this approach, non-rigid point-set registration is used to match each segmented neuron in each volume with a set of reference volumes taken from throughout the recording. The way each neuron matches with the references defines a feature vector which is clustered to assign an identity to each neuron in each volume. Finally, thin-plate spline interpolation is used to correct errors in segmentation and check consistency of assigned identities. The Neuron Registration Vector Encoding approach proposed here is uniquely well suited for tracking neurons in brains undergoing large deformations. When applied to whole-brain calcium imaging recordings in freely moving C. elegans, this analysis pipeline located 150 neurons for the duration of an 8 minute recording and consistently found more neurons more quickly than manual or semi-automated approaches."
"Various alignment problems arising in cryo-electron microscopy, community detection, time synchronization, computer vision, and other fields fall into a common framework of synchronization problems over compact groups such as Z/L, U(1), or SO(3). The goal of such problems is to estimate an unknown vector of group elements given noisy relative observations. We present an efficient iterative algorithm to solve a large class of these problems, allowing for any compact group, with measurements on multiple 'frequency channels' (Fourier modes, or more generally, irreducible representations of the group). Our algorithm is a highly efficient iterative method following the blueprint of approximate message passing (AMP), which has recently arisen as a central technique for inference problems such as structured low-rank estimation and compressed sensing. We augment the standard ideas of AMP with ideas from representation theory so that the algorithm can work with distributions over compact groups. Using standard but non-rigorous methods from statistical physics we analyze the behavior of our algorithm on a Gaussian noise model, identifying phases where the problem is easy, (computationally) hard, and (statistically) impossible. In particular, such evidence predicts that our algorithm is information-theoretically optimal in many cases, and that the remaining cases show evidence of statistical-to-computational gaps."
"With the increasing demand and complexity of networks, factors such as balancing the load, improving the performance, reducing delay and finding optimal path between nodes in a computer network have become crucial. The traditional routing approaches are not able to perform up to the mark as they do not take into account factors such as reducing delay and adaptive nature. Thus there is a need for more sophisticated techniques to meet the current network problems. This paper focuses on Ant-net algorithm based on Ant colony optimization which is an adaptive routing algorithm.In this approach, models of collective intelligence are transformed into optimization techniques. The ants travel across various paths in the network, thus depositing of pheromone, collecting route information and congestion. In this paper the Ant-Net algorithm along the important data structures required has been discussed and it is implemented on a simple packet switch network. The simulations describe the adaptive nature of the algorithm"
"We report on the development of the HoTT library, a formalization of homotopy type theory in the Coq proof assistant. It formalizes most of basic homotopy type theory, including univalence, higher inductive types, and significant amounts of synthetic homotopy theory, as well as category theory and modalities. The library has been used as a basis for several independent developments. We discuss the decisions that led to the design of the library, and we comment on the interaction of homotopy type theory with recently introduced features of Coq, such as universe polymorphism and private inductive types."
"This paper is about computability. I claim the likely existence of a program DoesHalt(Program, Input) such that DoesHalt( HaltsOnItself, AntiSelf ) halts with resounding 'NO'. HaltsOnItself( Program ) is simply DoesHalt( Program, Program ). AntiSelf() is a self-referential self-contradictory program that loops when HaltsOnItself() returns 'YES' and halts when HaltsOnItself() returns 'NO'."
"Enterprise level software is implemented using multi-layer architecture. These layers are often implemented using de-coupled solutions with millions of lines of code. Programmers often have to track and debug a function call from user interface layer to the data access layer while troubleshooting an issue. They have to inspect the code based on search results or use design documents to construct the call graph. This process is time consuming and laborious. The development environment tools are insufficient or confined to analyzing only the code in the loaded solution. This paper proposes a method to construct a call graph of the call across several layers of the code residing in different code bases to help programmers better understand the design and architecture of the software. The signatures of class, methods, and properties were evaluated and then matched against the code files. A graph of matching functions was created. The recursive search stopped when there were no matches or the data layer code was detected. The method resulted in 78.26% accuracy when compared with manual search."
"Global Route-Planning Algorithms (GRPA) are required to compute paths between several points located on Earth's surface. A geodesic algorithm is employed as an auxiliary tool, increasing the precision of distance calculations. This work presents a novel simulator for GRPA, which compares and evaluates three GRPAs implemented to solve the shortest path problem for points located at different cities: A*, LPA*, and D*Lite. The performance of each algorithm is investigated with a set of experiments, which are executed to check the answers provided by the algorithms and to compare their execution time. It is shown that GRPAs implementations with consistent heuristics lead to optimal paths. The noticeable differences among those algorithms are related to the time execution after successive executions."
"We present a novel distribution-free approach, the data-driven threshold machine (DTM), for a fundamental problem at the core of many learning tasks: choose a threshold for a given pre-specified level that bounds the tail probability of the maximum of a (possibly dependent but stationary) random sequence. We do not assume data distribution, but rather relying on the asymptotic distribution of extremal values, and reduce the problem to estimate three parameters of the extreme value distributions and the extremal index. We specially take care of data dependence via estimating extremal index since in many settings, such as scan statistics, change-point detection, and extreme bandits, where dependence in the sequence of statistics can be significant. Key features of our DTM also include robustness and the computational efficiency, and it only requires one sample path to form a reliable estimate of the threshold, in contrast to the Monte Carlo sampling approach which requires drawing a large number of sample paths. We demonstrate the good performance of DTM via numerical examples in various dependent settings."
"Theoretical constructs of logical gates implemented with plant roots are morphological computing asynchronous devices. Values of Boolean variables are represented by plant roots. A presence of a plant root at a given site symbolises the logical {\sc True}, an absence the logical {\sc False}. Logical functions are calculated via interaction between roots. Two types of two-inputs-two-outputs gates are proposed: a gate $\langle x, y \rangle \rightarrow \langle xy, x+y \rangle$ where root apexes are guided by gravity and a gate $\langle x, y \rangle \rightarrow \langle \overline{x}y, x \rangle$ where root apexes are guided by humidity. We propose a design of binary half-adder based on the gates."
"A three-dimensional multi-scale computational homogenisation framework is developed for the prediction of nonlinear micro/meso-mechanical response of the fibre-reinforced polymer (FRP) composites. Two dominant damage mechanisms, i.e. matrix elasto-plastic response and fibre-matrix decohesion are considered and modelled using a non-associative pressure dependent paraboloidal yield criterion and cohesive interface elements respectively. A linear-elastic transversely isotropic material model is used to model yarns/fibres within the representative volume element (RVE). A unified approach is used to impose the RVE boundary conditions, which allows convenient switching between linear displacement, uniform traction and periodic boundary conditions. The computational model is implemented within the framework of the hierarchic finite element, which permits the use of arbitrary orders of approximation. Furthermore, the computational framework is designed to take advantage of distributed memory high-performance computing. The accuracy and performance of the computational framework are demonstrated with a variety of numerical examples, including unidirectional FRP composite, a composite comprising a multi-fibre and multi-layer RVE, with randomly generated fibres, and a single layered plain weave textile composite. Results are validated against the reference experimental/numerical results from the literature. The computational framework is also used to study the effect of matrix and fibre-matrix interfaces properties on the homogenised stress-strain responses."
"Cloud-based Radio Access Network (C-RAN) is a promising architecture for future cellular networks, in which Baseband Units (BBUs) are placed at a centralized location, with capacity-constrained fronthaul connected to multiple distributed Remote Radio Units (RRHs) that are far away from the BBUs. The centralization of signal processing enables the flexibility for coordinated multi-point transmission (CoMP) to meet high traffic demand of users. We investigate how to jointly optimize CoMP-cell selection and base station resource allocation so as to enhance the quality of service (QoS), subject to the fronthaul capacity constraint in orthogonal frequency-division multiple access (OFDMA) based C-RAN. The problem is proved to be NP-hard in this paper. To deal with the computational complexity, we derive a partial optimality condition as the foundation for designing a cell-selection algorithm. Besides, we provide a solution method of the optimum of the time-frequency resource allocation problem without loss of fairness on the QoS enhancement of all users. The simulations show good performance of the proposed algorithms for jointly optimizing the cell selection and resource allocation in a C-RAN, with respect to QoS."
"Linear Discriminant Analysis (LDA) is a widely-used supervised dimensionality reduction method in computer vision and pattern recognition. In null space based LDA (NLDA), a well-known LDA extension, between-class distance is maximized in the null space of the within-class scatter matrix. However, there are some limitations in NLDA. Firstly, for many data sets, null space of within-class scatter matrix does not exist, thus NLDA is not applicable to those datasets. Secondly, NLDA uses arithmetic mean of between-class distances and gives equal consideration to all between-class distances, which makes larger between-class distances can dominate the result and thus limits the performance of NLDA. In this paper, we propose a harmonic mean based Linear Discriminant Analysis, Multi-Class Discriminant Analysis (MCDA), for image classification, which minimizes the reciprocal of weighted harmonic mean of pairwise between-class distance. More importantly, MCDA gives higher priority to maximize small between-class distances. MCDA can be extended to multi-label dimension reduction. Results on 7 single-label data sets and 4 multi-label data sets show that MCDA has consistently better performance than 10 other single-label approaches and 4 other multi-label approaches in terms of classification accuracy, macro and micro average F1 score."
"We consider a set-up in which there are multiple servers and multiple clients in a large distributed computing system. Clients request servers to process jobs. Servers can only process one job in unit time. There is no coordinating agent to route client requests to servers, and clients choose servers independently and simultaneously, and only have access to the outcomes of their own past requests. If more than one clients choose the same server, then only one randomly chosen client's requests will be fulfilled. If some servers do not receive any request, they remain idle. In this paper, we show that a large category of strategies are not effective in terms of server utilization. We devise strategies for clients that improve server utilization of such systems over those of strategies known in the current literature."
"Refinement Reflection turns your favorite programming language into a proof assistant by reflecting the code implementing a user-defined function into the function's (output) refinement type. As a consequence, at uses of the function, the function definition is unfolded into the refinement logic in a precise, predictable and most importantly, programmer controllable way. In the logic, we encode functions and lambdas using uninterpreted symbols preserving SMT-based decidable verification. In the language, we provide a library of combinators that lets programmers compose proofs from basic refinements and function definitions. We have implemented our approach in the Liquid Haskell system, thereby converting Haskell into an interactive proof assistant, that we used to verify a variety of properties ranging from arithmetic properties of higher order, recursive functions to the Monoid, Applicative, Functor and Monad type class laws for a variety of instances."
"This paper studies the joint optimization problem of two-way relay beamforming, the receiver power splitting (PS) ratio as well as the transmit power at the sources to maximize the achievable sum-rate of a simultaneous wireless information and power transfer (SWIPT) system with a full-duplex (FD) multiple-input multiple-output (MIMO) amplify and forward (AF) relay, assuming perfect channel state information (CSI). In particular, our contribution is an iterative algorithm based on the difference of convex programming (DC) and one dimensional searching to achieve the joint solution. Simulation results are provided to demonstrate the effectiveness of the proposed algorithm."
"This paper addresses the problem of multi-class classification with an extremely large number of classes, where the class predictor is learned jointly with the data representation, as is the case in language modeling problems. The predictor admits a hierarchical structure, which allows for efficient handling of settings that deal with a very large number of labels. The predictive power of the model however can heavily depend on the structure of the tree. We address this problem with an algorithm for tree construction and training that is based on a new objective function which favors balanced and easily-separable node partitions. We describe theoretical properties of this objective function and show that it gives rise to a boosting algorithm for which we provide a bound on classification error, i.e. we show that if the objective is weakly optimized in the internal nodes of the tree, then our algorithm will amplify this weak advantage to build a tree achieving any desired level of accuracy. We apply the algorithm to the task of language modeling by re-framing conditional density estimation as a variant of the hierarchical classification problem. We empirically demonstrate on text data that the proposed approach leads to high-quality trees in terms of perplexity and computational running time compared to its non-hierarchical counterpart."
"In this paper we present and evaluate a parallel algorithm for solving a minimum spanning tree (MST) problem for supercomputers with distributed memory. The algorithm relies on the relaxation of the message processing order requirement for one specific message type compared to the original GHS (Gallager, Humblet, Spira) algorithm. Our algorithm adopts hashing and message compression optimization techniques as well. To the best of our knowledge, this is the first parallel implementation of the GHS algorithm that linearly scales to more than 32 nodes (256 cores) of Infiniband cluster."
"Melanoma is the deadliest form of skin cancer. While curable with early detection, only highly trained specialists are capable of accurately recognizing the disease. As expertise is in limited supply, automated systems capable of identifying disease could save lives, reduce unnecessary biopsies, and reduce costs. Toward this goal, we propose a system that combines recent developments in deep learning with established machine learning approaches, creating ensembles of methods that are capable of segmenting skin lesions, as well as analyzing the detected area and surrounding tissue for melanoma detection. The system is evaluated using the largest publicly available benchmark dataset of dermoscopic images, containing 900 training and 379 testing images. New state-of-the-art performance levels are demonstrated, leading to an improvement in the area under receiver operating characteristic curve of 7.5% (0.843 vs. 0.783), in average precision of 4% (0.649 vs. 0.624), and in specificity measured at the clinically relevant 95% sensitivity operating point 2.9 times higher than the previous state-of-the-art (36.8% specificity compared to 12.5%). Compared to the average of 8 expert dermatologists on a subset of 100 test images, the proposed system produces a higher accuracy (76% vs. 70.5%), and specificity (62% vs. 59%) evaluated at an equivalent sensitivity (82%)."
"Real life data often includes information from different channels. For example, in computer vision, we can describe an image using different image features, such as pixel intensity, color, HOG, GIST feature, SIFT features, etc.. These different aspects of the same objects are often called multi-view (or multi-modal) data. Low-rank regression model has been proved to be an effective learning mechanism by exploring the low-rank structure of real life data. But previous low-rank regression model only works on single view data. In this paper, we propose a multi-view low-rank regression model by imposing low-rank constraints on multi-view regression model. Most importantly, we provide a closed-form solution to the multi-view low-rank regression model. Extensive experiments on 4 multi-view datasets show that the multi-view low-rank regression model outperforms single-view regression model and reveals that multi-view low-rank structure is very helpful."
"In 2011, Aaronson gave a striking proof, based on quantum linear optics, showing that the problem of computing the permanent of a matrix is #P-hard. Aaronson's proof led naturally to hardness of approximation results for the permanent, and it was arguably simpler than Valiant's seminal proof of the same fact in 1979. Nevertheless, it did not prove that computing the permanent was #P-hard for any class of matrices which was not previously known. In this paper, we present a collection of new results about matrix permanents that are derived primarily via these linear optical techniques.   First, we show that the problem of computing the permanent of a real orthogonal matrix is #P-hard. Much like Aaronson's original proof, this will show that even a multiplicative approximation remains #P-hard to compute. The hardness result even translates to permanents over finite fields, where the problem of computing the permanent of an orthogonal matrix is ModpP-hard in the finite field F_{p^4} for all primes p not equal to 2 or 3. Interestingly, this characterization is tight: in fields of characteristic 2, the permanent coincides with the determinant; in fields of characteristic 3, one can efficiently compute the permanent of an orthogonal matrix by a nontrivial result of Kogan.   Finally, we use more elementary arguments to prove #P-hardness for the permanent of a positive semidefinite matrix, which shows that certain probabilities of boson sampling experiments with thermal states are hard to compute exactly despite the fact that they can be efficiently sampled by a classical computer."
"Automatic extraction of road curbs from uneven, unorganized, noisy and massive 3D point clouds is a challenging task. Existing methods often project 3D point clouds onto 2D planes to extract curbs. However, the projection causes loss of 3D information which degrades the performance of the detection. This paper presents a robust, accurate and efficient method to extract road curbs from 3D mobile LiDAR point clouds. Our method consists of two steps: 1) extracting the candidate points of curbs based on the proposed novel energy function and 2) refining the candidate points using the proposed least cost path model. We evaluated our method on a large-scale of residential area (16.7GB, 300 million points) and an urban area (1.07GB, 20 million points) mobile LiDAR point clouds. Results indicate that the proposed method is superior to the state-of-the-art methods in terms of robustness, accuracy and efficiency. The proposed curb extraction method achieved a completeness of 78.62% and a correctness of 83.29%. These experiments demonstrate that the proposed method is a promising solution to extract road curbs from mobile LiDAR point clouds."
"Social influence among users (e.g., collaboration on a project) creates bursty behavior in the underlying high performance computing (HPC) workloads. Using representative HPC and cluster workload logs, this paper identifies, analyzes, and quantifies the level of social influence across HPC users. We show the existence of a social graph that is characterized by a pattern of dominant users and followers. This pattern also follows a power-law distribution, which is consistent with those observed in mainstream social networks. Given its potential impact on HPC workloads prediction and scheduling, we propose a fast-converging, computationally-efficient online learning algorithm for identifying social groups. Extensive evaluation shows that our online algorithm can (1) quickly identify the social relationships by using a small portion of incoming jobs and (2) can efficiently track group evolution over time."
"As link speeds increase in datacenter networks, existing congestion control algorithms become less effective in providing fast convergence. TCP-based algorithms that probe for bandwidth take a long time to reach the fair-share and lead to long flow completion times. An ideal congestion control algorithms for datacenter must provide 1) zero data loss, 2) fast convergence, and 3) low buffer occupancy. However, these requirements present conflicting goals. For fast convergence,flows must ramp up quickly, but this risks packet losses and large queues. Thus, even the state-of-the-art algorithms, such as TIMELY and DCQCN, rely on link layer flow control (e.g.,Priority-based Flow Control) to achieve zero loss. This paper presents a new approach, called ExpressPass, an end-to-end credit-based congestion control algorithm for datacenters. ExpressPass is inspired by credit-based flow control, but extends it to work end-to-end. The switches control the amount of credit packets by rate limiting and ensure data packets flow in the reverse direction without any loss. ExpressPass leverages this to ramp up aggressively. ExpressPass converges up to 80 times faster than DCTCP at 10Gbps link, and the gap increases as link speeds become faster. Our simulation with realistic workload shows that ExpressPass significantly reduces the flow completion time especially for small and medium size flows compared to DCTCP, HULL, and DX."
"A novel non-negative matrix factorization (NMF) based subband decomposition in frequency spatial domain for acoustic source localization using a microphone array is introduced. The proposed method decomposes source and noise subband and emphasises source dominant frequency bins for more accurate source representation. By employing NMF, delay basis vectors and their subband information in frequency spatial domain for each frame is extracted. The proposed algorithm is evaluated in both simulated noise and real noise with a speech corpus database. Experimental results clearly indicate that the algorithm performs more accurately than other conventional algorithms under both reverberant and noisy acoustic environments."
"Increase in image resolution require the ability of image sensors to pack an increased number of circuit components in a given area. On the the other hand a high speed processing of signals from the sensors require the ability of pixel to carry out pixel parallel operations. In the paper, we propose a modified 3T and 4T CMOS wide dynamic range pixels, which we refer as 2T-M and 3T-M configurations, comprising of MOSFETS and memristors. The low leakage currents and low area of memristors helps to achieve the objective of reducing the area, while the possibility to create arrays of memristors and MOSFETs across different layers within the chip, ensure the possibility to scale the circuit architecture."
"Separation Logic (SL) is a well-known assertion language used in Hoare-style modular proof systems for programs with dynamically allocated data structures. In this paper we investigate the fragment of first-order SL restricted to the Bernays-Schoenfinkel-Ramsey quantifier prefix $\exists^*\forall^*$, where the quantified variables range over the set of memory locations. When this set is uninterpreted (has no associated theory) the fragment is PSPACE-complete, which matches the complexity of the quantifier-free fragment. However, SL becomes undecidable when the quantifier prefix belongs to $\exists^*\forall^*\exists^*$ instead, or when the memory locations are interpreted as integers with linear arithmetic constraints, thus setting a sharp boundary for decidability within SL. We have implemented a decision procedure for the decidable fragment of $\exists^*\forall^*$SL as a specialized solver inside a DPLL($T$) architecture, within the CVC4 SMT solver. The evaluation of our implementation was carried out using two sets of verification conditions, produced by (i) unfolding inductive predicates, and (ii) a weakest precondition-based verification condition generator. Experimental data shows that automated quantifier instantiation has little overhead, compared to manual model-based instantiation."
"In the $\ell$-Component Order Connectivity problem ($\ell \in \mathbb{N}$), we are given a graph $G$ on $n$ vertices, $m$ edges and a non-negative integer $k$ and asks whether there exists a set of vertices $S\subseteq V(G)$ such that $|S|\leq k$ and the size of the largest connected component in $G-S$ is at most $\ell$. In this paper, we give a kernel for $\ell$-Component Order Connectivity with at most $2\ell k$ vertices that takes $n^{\mathcal{O}(\ell)}$ time for every constant $\ell$. On the way to obtaining our kernel, we prove a generalization of the $q$-Expansion Lemma to weighted graphs. This generalization may be of independent interest."
"Given a set $Z$ of $n$ positive integers and a target value $t$, the Subset Sum problem asks whether any subset of $Z$ sums to $t$. A textbook pseudopolynomial time algorithm by Bellman from 1957 solves Subset Sum in time $O(nt)$. This has been improved to $O(n \max Z)$ by Pisinger [J. Algorithms'99] and recently to $\tilde O(\sqrt{n} t)$ by Koiliaris and Xu [SODA'17].   Here we present a simple and elegant randomized algorithm running in time $\tilde O(n+t)$. This improves upon a classic algorithm and is likely to be near-optimal, since it matches conditional lower bounds from Set Cover and k-Clique.   We then use our new algorithm and additional tricks to improve the best known polynomial space solution from time $\tilde O(n^3 t)$ and space $\tilde O(n^2)$ to time $\tilde O(nt)$ and space $\tilde O(n \log t)$, assuming the Extended Riemann Hypothesis. Unconditionally, we obtain time $\tilde O(n t^{1+\varepsilon})$ and space $\tilde O(n t^\varepsilon)$ for any constant $\varepsilon > 0$."
"In this short note we propose a new approach for the design and analysis of randomized gossip algorithms which can be used to solve the average consensus problem. We show how that Randomized Block Kaczmarz (RBK) method - a method for solving linear systems - works as gossip algorithm when applied to a special system encoding the underlying network. The famous pairwise gossip algorithm arises as a special case. Subsequently, we reveal a hidden duality of randomized gossip algorithms, with the dual iterative process maintaining a set of numbers attached to the edges as opposed to nodes of the network. We prove that RBK obtains a superlinear speedup in the size of the block, and demonstrate this effect through experiments."
"The article deals with the issue of modification of metric classification algorithms. In particular, it studies the algorithm k-Nearest Neighbours for its application to sequential data. A method of generalization of metric classification algorithms is proposed. As a part of it, there has been developed an algorithm for solving the problem of classification and labelling of sequential data. The advantages of the developed algorithm of classification in comparison with the existing one are also discussed in the article. There is a comparison of the effectiveness of the proposed algorithm with the algorithm of CRF in the task of chunking in the open data set CoNLL2000."
"In this paper we outline a PhD research plan. This research contributes to the field of one-class incremental learning and classification in case of non-stationary environments. The goal of this PhD is to define a new classification framework able to deal with very small learning dataset at the beginning of the process and with abilities to adjust itself according to the variability of the incoming data which create large scale datasets. As a preliminary work, incremental Covariance-guided One-Class Support Vector Machine is proposed to deal with sequentially obtained data. It is inspired from COSVM which put more emphasis on the low variance directions while keeping the basic formulation of incremental One-Class Support Vector Machine untouched. The incremental procedure is introduced by controlling the possible changes of support vectors after the addition of new data points, thanks to the Karush-Kuhn-Tucker conditions, that have to be maintained on all previously acquired data. Comparative experimental results with contemporary incremental and non-incremental one-class classifiers on numerous artificial and real data sets show that our method results in significantly better classification performance."
"Today's societies are enveloped in an ever-growing telecommunication infrastructure. This infrastructure offers important opportunities for sensing and recording a multitude of human behaviors. Human mobility patterns are a prominent example of such a behavior which has been studied based on cell phone towers, Bluetooth beacons, and WiFi networks as proxies for location. However, while mobility is an important aspect of human behavior, understanding complex social systems requires studying not only the movement of individuals, but also their interactions. Sensing social interactions on a large scale is a technical challenge and many commonly used approaches---including RFID badges or Bluetooth scanning---offer only limited scalability. Here we show that it is possible, in a scalable and robust way, to accurately infer person-to-person physical proximity from the lists of WiFi access points measured by smartphones carried by the two individuals. Based on a longitudinal dataset of approximately 800 participants with ground-truth interactions collected over a year, we show that our model performs better than the current state-of-the-art. Our results demonstrate the value of WiFi signals in social sensing as well as potential threats to privacy that they imply."
"A pervasive problem in Data Science is that the knowledge generated by possibly expensive analytics processes is subject to decay over time, as the data used to compute it drifts, the algorithms used in the processes are improved, and the external knowledge embodied by reference datasets used in the computation evolves. Deciding when such knowledge outcomes should be refreshed, following a sequence of data change events, requires problem-specific functions to quantify their value and its decay over time, as well as models for estimating the cost of their re-computation. What makes this problem challenging is the ambition to develop a decision support system for informing data analytics re-computation decisions over time, that is both generic and customisable. With the help of a case study from genomics, in this vision paper we offer an initial formalisation of this problem, highlight research challenges, and outline a possible approach based on the collection and analysis of metadata from a history of past computations."
"Instant messaging and push notifications play important roles in modern digital life. To enable robust sense-making and rich context awareness in computer mediated communications, we introduce EmotionPush, a system that automatically conveys the emotion of received text with a colored push notification on mobile devices. EmotionPush is powered by state-of-the-art emotion classifiers and is deployed for Facebook Messenger clients on Android. The study showed that the system is able to help users prioritize interactions."
"Clustering network is one of which complex network attracting plenty of scholars to discuss and study the structures and cascading process. We primarily analyzed the effect of clustering coefficient to other various of the single clustering network under localized attack. These network models including double clustering network and star-like NON with clustering and random regular (RR) NON of ER networks with clustering are made up of at least two networks among which exist interdependent relation among whose degree of dependence is measured by coupling strength. We show both analytically and numerically, how the coupling strength and clustering coefficient effect the percolation threshold, size of giant component, critical coupling point where the behavior of phase transition changes from second order to first order with the increase of coupling strength between the networks. Last, we study the two types of clustering network: one type is same with double clustering network in which each subnetwork satisfies identical degree distribution and the other is that their subnetwork satisfies different degree distribution. The former type is treated both analytically and numerically while the latter is treated only numerically. In each section, we compared two results obtained from localized attack and random attack according to Shao et al:[22]."
"Modern control is implemented with digital microcontrollers, which are designed to work in an embedded environment, within a dynamical plant that represents physical components. We present a new algorithm based on counterexample guided inductive synthesis that automates the design of digital controllers that are correct by construction. The synthesis result is sound with respect to the complete range of approximations, including time discretization, quantization effects, and finite-precision arithmetic and their related rounding errors. We have implemented our new algorithm in a tool called DSSynth, and are able to automatically generate stable controllers for a set of intricate plant models taken from the literature within minutes."
"The problem of source localization with ad hoc microphone networks in noisy and reverberant enclosures, given a training set of prerecorded measurements, is addressed in this paper. The training set is assumed to consist of a limited number of labelled measurements, attached with corresponding positions, and a larger amount of unlabelled measurements from unknown locations. However, microphone calibration is not required. We use a Bayesian inference approach for estimating a function that maps measurement-based feature vectors to the corresponding positions. The central issue is how to combine the information provided by the different microphones in a unified statistical framework. To address this challenge, we model this function using a Gaussian process with a covariance function that encapsulates both the connections between pairs of microphones and the relations among the samples in the training set. The parameters of the process are estimated by optimizing a maximum likelihood (ML) criterion. In addition, a recursive adaptation mechanism is derived where the new streaming measurements are used to update the model. Performance is demonstrated for 2-D localization of both simulated data and real-life recordings in a variety of reverberation and noise levels."
"Stakeholder involvement is one of the major success factors in integrating user experience (UX) practices into software development processes and organizations. It is also a necessity for agile software development. However, practitioners still have limited access to guidelines on successful involvement of UX stakeholders in agile settings. Moreover, agile UX literature does not well address the specific characteristics of UX and it does not clearly differentiate between UX and usability work. This paper presents two guidelines for supporting stakeholder involvement in both UX integration and the daily UX work. In particular, we focus on the special characteristics of UX: being dynamic, subjective, holistic, and context-dependent. The guidelines clarify practical implications of these characteristics for practitioners. In addition, they can help researchers in addressing these characteristics better in agile UX research."
"This paper indicates two errors in the formulation of the main optimization model in the article ""Control system for reducing energy consumption in backbone computer network"" by Niewiadomska-Szynkiewicz et al. and shows how to fix them."
"A new computationally efficient dependence measure, and an adaptive statistical test of independence, are proposed. The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at a finite set of locations (features). These features are chosen so as to maximize a lower bound on the test power, resulting in a test that is data-efficient, and that runs in linear time (with respect to the sample size n). The optimized features can be interpreted as evidence to reject the null hypothesis, indicating regions in the joint domain where the joint distribution and the product of the marginals differ most. Consistency of the independence test is established, for an appropriate choice of features. In real-world benchmarks, independence tests using the optimized features perform comparably to the state-of-the-art quadratic-time HSIC test, and outperform competing O(n) and O(n log n) tests."
"Multivariate time series naturally exist in many fields, like energy, bioinformatics, signal processing, and finance. Most of these applications need to be able to compare these structured data. In this context, dynamic time warping (DTW) is probably the most common comparison measure. However, not much research effort has been put into improving it by learning. In this paper, we propose a novel method for learning similarities based on DTW, in order to improve time series classification. Making use of the uniform stability framework, we provide the first theoretical guarantees in the form of a generalization bound for linear classification. The experimental study shows that the proposed approach is efficient, while yielding sparse classifiers."
"The seminar assignment problem is a variant of the generalized assignment problem in which items have unit size and the amount of space allowed in each bin is restricted to an arbitrary set of values. The problem has been shown to be NP-complete and to not admit a PTAS. However, the only constant factor approximation algorithm known to date is randomized and it is not guaranteed to always produce a feasible solution.   In this paper we show that a natural greedy algorithm outputs a solution with value within a factor of $(1 - e^{-1})$ of the optimal, and that unless $NP\subseteq DTIME(n^{\log\log n})$, this is the best approximation guarantee achievable by any polynomial time algorithm."
"Collecting training images for all visual categories is not only expensive but also impractical. Zero-shot learning (ZSL), especially using attributes, offers a pragmatic solution to this problem. However, at test time most attribute-based methods require a full description of attribute associations for each unseen class. Providing these associations is time consuming and often requires domain specific knowledge. In this work, we aim to carry out attribute-based zero-shot classification in an unsupervised manner. We propose an approach to learn relations that couples class embeddings with their corresponding attributes. Given only the name of an unseen class, the learned relationship model is used to automatically predict the class-attribute associations. Furthermore, our model facilitates transferring attributes across data sets without additional effort. Integrating knowledge from multiple sources results in a significant additional improvement in performance. We evaluate on two public data sets: Animals with Attributes and aPascal/aYahoo. Our approach outperforms state-of-the-art methods in both predicting class-attribute associations and unsupervised ZSL by a large margin."
"There is great interest in supporting imprecise queries over databases today. To support such queries, the system is typically required to disambiguate parts of the user-specified query against the database, using whatever resources are intrinsically available to it (the database schema, value distributions, natural language models etc). Often, systems will also have a user-interaction log available, which can supplement their model based on their own intrinsic resources. This leads to a problem of how best to combine the system's prior ranking with insight derived from the user-interaction log. Statistical inference techniques such as maximum likelihood or Bayesian updates from a subjective prior turn out not to apply in a straightforward way due to possible noise from user search behavior and to encoding biases endemic to the system's models. In this paper, we address such learning problems in interactive data retrieval, with specific focus on type classification for user-specified query terms. We develop a novel Bayesian smoothing algorithm, Bsmooth, which is simple, fast, flexible and accurate. We analytically establish some desirable properties and show, through experiments against an independent benchmark, that the addition of such a learning layer performs much better than standard methods."
"Programmers routinely trade space for time to increase performance, often in the form of caching or memoization. In managed languages like Java or JavaScript, however, this space-time tradeoff is complex. Using more space translates into higher garbage collection costs, especially at the limit of available memory. Existing runtime systems provide limited support for space-sensitive algorithms, forcing programmers into difficult and often brittle choices about provisioning.   This paper presents prioritized garbage collection, a cooperative programming language and runtime solution to this problem. Prioritized GC provides an interface similar to soft references, called priority references, which identify objects that the collector can reclaim eagerly if necessary. The key difference is an API for defining the policy that governs when priority references are cleared and in what order. Application code specifies a priority value for each reference and a target memory bound. The collector reclaims references, lowest priority first, until the total memory footprint of the cache fits within the bound. We use this API to implement a space-aware least-recently-used (LRU) cache, called a Sache, that is a drop-in replacement for existing caches, such as Google's Guava library. The garbage collector automatically grows and shrinks the Sache in response to available memory and workload with minimal provisioning information from the programmer. Using a Sache, it is almost impossible for an application to experience a memory leak, memory pressure, or an out-of-memory crash caused by software caching."
"Low Power Wide Area (LPWA) networks are making spectacular progress from design, standardisation, to commercialisation. At this time of fast-paced adoption, it is of utmost importance to analyse how well these technologies will scale as the number of devices connected to the Internet of Things (IoT) inevitably grows. In this letter, we provide a stochastic geometry framework for modelling the performance of a single gateway LoRa network, a leading LPWA technology. Our analysis formulates unique peculiarities of LoRa, including its chirp spread-spectrum modulation technique, regulatory limitations on radio duty cycle, and use of ALOHA protocol on top, all of which are not as common in today's commercial cellular networks. We show that the coverage probability drops exponentially as the number of end-devices grows due to interfering signals using the same spreading sequence. We conclude that this fundamental limiting factor is perhaps more significant towards LoRa scalability than for instance spectrum restrictions. Our derivations for co-spreading factor interference found in LoRa networks enables rigorous scalability analysis of such networks."
"Most learning approaches treat dimensionality reduction (DR) and clustering separately (i.e., sequentially), but recent research has shown that optimizing the two tasks jointly can substantially improve the performance of both. The premise behind the latter genre is that the data samples are obtained via linear transformation of latent representations that are easy to cluster; but in practice, the transformation from the latent space to the data can be more complicated. In this work, we assume that this transformation is an unknown and possibly nonlinear function. To recover the `clustering-friendly' latent representations and to better cluster the data, we propose a joint DR and K-means clustering approach in which DR is accomplished via learning a deep neural network (DNN). The motivation is to keep the advantages of jointly optimizing the two tasks, while exploiting the deep neural network's ability to approximate any nonlinear function. This way, the proposed approach can work well for a broad class of generative models. Towards this end, we carefully design the DNN structure and the associated joint optimization criterion, and propose an effective and scalable algorithm to handle the formulated optimization problem. Experiments using five different real datasets are employed to showcase the effectiveness of the proposed approach."
"Learning policies for bipedal locomotion can be difficult, as experiments are expensive and simulation does not usually transfer well to hardware. To counter this, we need al- gorithms that are sample efficient and inherently safe. Bayesian Optimization is a powerful sample-efficient tool for optimizing non-convex black-box functions. However, its performance can degrade in higher dimensions. We develop a distance metric for bipedal locomotion that enhances the sample-efficiency of Bayesian Optimization and use it to train a 16 dimensional neuromuscular model for planar walking. This distance metric reflects some basic gait features of healthy walking and helps us quickly eliminate a majority of unstable controllers. With our approach we can learn policies for walking in less than 100 trials for a range of challenging settings. In simulation, we show results on two different costs and on various terrains including rough ground and ramps, sloping upwards and downwards. We also perturb our models with unknown inertial disturbances analogous with differences between simulation and hardware. These results are promising, as they indicate that this method can potentially be used to learn control policies on hardware."
"We study the notion of extensibility in functional data types, as a new approach to the problem of decorating abstract syntax trees with additional sets of information. We observed the need for such extensibility while redesigning the data types representing Haskell abstract syntax inside GHC.   Specifically, we describe our approach to the tree-decoration problem using a novel syntactic machinery in Haskell for expressing extensible data types. We show that the syntactic machinery is complete in that it can express all the syntactically possible forms of extensions to algebraic data type declarations. Then, we describe an encoding of the syntactic machinery based on the existing features in Glasgow Haskell Compiler(GHC)."
"We propose a novel stacked generalization (stacking) method as a dynamic ensemble technique using a pool of heterogeneous classifiers for node label classification on networks. The proposed method assigns component models a set of functional coefficients, which can vary smoothly with certain topological features of a node. Compared to the traditional stacking model, the proposed method can dynamically adjust the weights of individual models as we move across the graph and provide a more versatile and significantly more accurate stacking model for label prediction on a network. We demonstrate the benefits of the proposed model using both a simulation study and real data analysis."
"When modeling geo-spatial data, it is critical to capture spatial correlations for achieving high accuracy. Spatial Auto-Regression (SAR) is a common tool used to model such data, where the spatial contiguity matrix (W) encodes the spatial correlations. However, the efficacy of SAR is limited by two factors. First, it depends on the choice of contiguity matrix, which is typically not learnt from data, but instead, is assumed to be known apriori. Second, it assumes that the observations can be explained by linear models. In this paper, we propose a Convolutional Neural Network (CNN) framework to model geo-spatial data (specifi- cally housing prices), to learn the spatial correlations automatically. We show that neighborhood information embedded in satellite imagery can be leveraged to achieve the desired spatial smoothing. An additional upside of our framework is the relaxation of linear assumption on the data. Specific challenges we tackle while implementing our framework include, (i) how much of the neighborhood is relevant while estimating housing prices? (ii) what is the right approach to capture multiple resolutions of satellite imagery? and (iii) what other data-sources can help improve the estimation of spatial correlations? We demonstrate a marked improvement of 57% on top of the SAR baseline through the use of features from deep neural networks for the cities of London, Birmingham and Liverpool."
"In 1988, Johnson, Papadimitriou and Yannakakis wrote that ""Practically all the empirical evidence would lead us to conclude that finding locally optimal solutions is much easier than solving NP-hard problems"". Since then the empirical evidence has continued to amass, but formal proofs of this phenomenon have remained elusive. A canonical (and indeed complete) example is the local max-cut problem, for which no polynomial time method is known. In a breakthrough paper, Etscheid and R\""oglin proved that the smoothed complexity of local max-cut is quasi-polynomial, i.e., if arbitrary bounded weights are randomly perturbed, a local maximum can be found in $n^{O(\log n)}$ steps. In this paper we prove smoothed polynomial complexity for local max-cut, thus confirming that finding local optima for max-cut is much easier than solving it."
"""Net neutrality"" often refers to the policy dictating that an Internet service provider (ISP) cannot charge content providers (CPs) for delivering their content to consumers. Many past quantitative models designed to determine whether net neutrality is a good idea have been rather equivocal in their conclusions. Here we propose a very simple two-sided market model, in which the types of the consumers and the CPs are {\em power-law distributed} --- a kind of distribution known to often arise precisely in connection with Internet-related phenomena. We derive mostly analytical, closed-form results for several regimes: (a) Net neutrality, (b) social optimum, (c) maximum revenue by the ISP, or (d) maximum ISP revenue under quality differentiation. One unexpected conclusion is that (a) and (b) will differ significantly, unless average CP productivity is very high."
"In this paper, a simple text categorization method using term-class relevance measures is proposed. Initially, text documents are processed to extract significant terms present in them. For every term extracted from a document, we compute its importance in preserving the content of a class through a novel term-weighting scheme known as Term_Class Relevance (TCR) measure proposed by Guru and Suhil (2015) [1]. In this way, for every term, its relevance for all the classes present in the corpus is computed and stored in the knowledgebase. During testing, the terms present in the test document are extracted and the term-class relevance of each term is obtained from the stored knowledgebase. To achieve quick search of term weights, Btree indexing data structure has been adapted. Finally, the class which receives maximum support in terms of term-class relevance is decided to be the class of the given test document. The proposed method works in logarithmic complexity in testing time and simple to implement when compared to any other text categorization techniques available in literature. The experiments conducted on various benchmarking datasets have revealed that the performance of the proposed method is satisfactory and encouraging."
"Biological and advanced cyberphysical control systems often have limited, sparse, uncertain, and distributed communication and computing in addition to sensing and actuation. Fortunately, the corresponding plants and performance requirements are also sparse and structured, and this must be exploited to make constrained controller design feasible and tractable. We introduce a new ""system level"" (SL) approach involving three complementary SL elements. System Level Parameterizations (SLPs) generalize state space and Youla parameterizations of all stabilizing controllers and the responses they achieve, and combine with System Level Constraints (SLCs) to parameterize the largest known class of constrained stabilizing controllers that admit a convex characterization, generalizing quadratic invariance (QI). SLPs also lead to a generalization of detectability and stabilizability, suggesting the existence of a rich separation structure, that when combined with SLCs, is naturally applicable to structurally constrained controllers and systems. We further provide a catalog of useful SLCs, most importantly including sparsity, delay, and locality constraints on both communication and computing internal to the controller, and external system performance. The resulting System Level Synthesis (SLS) problems that arise define the broadest known class of constrained optimal control problems that can be solved using convex programming. An example illustrates how this system level approach can systematically explore tradeoffs in controller performance, robustness, and synthesis/implementation complexity."
"Automatic face recognition performance has improved remarkably in the last decade. Much of this success can be attributed to the development of deep learning techniques like convolutional neural networks (CNNs). But the training process for CNNs requires a large amount of clean and well-labelled training data. If a CNN is intended to work with non-frontal face images, should this training data be diverse in terms of facial poses, or should face images be frontalized as a pre-processing step? We address this question in this paper. We evaluate a set of popular facial landmarking and pose frontalization algorithms to understand their effect on facial recognition performance. We also introduce a new automatic frontalization scheme that operates over a single image without the need for a subject-specific 3D model, and perform a comparative analysis between the new scheme and other methods in the literature. A CNN trained on face images frontalized using different pre-processing methods is used to extract features from the Point and Shoot Challenge (PaSC) video dataset. The verification and identification performance of the CNN serves to quantify the effectiveness of each landmarking and frontalization scheme. We find that frontalization, although an intuitive pre-processing strategy, does not significantly improve face recognition performance when compared with a simple 2D face alignment."
"The ability to open a door is essential for robots to perform home-serving and rescuing tasks. A substantial problem is to obtain the necessary parameters such as the width of the door and the length of the handle. Many researchers utilize computer vision techniques to extract the parameters automatically which lead to fine but not very stable results because of the complexity of the environment. We propose a method that utilizes an RGBD sensor and a GUI for users to 'point' at the target region with a mouse to acquire 3D information. Algorithms that can extract important parameters from the selected points are designed. To avoid large internal force induced by the misalignment of the robot orientation and the normal of the door plane, we design a module that can compute the normal of the plane by pointing at three non-collinear points and then drive the robot to the desired orientation. We carried out experiments on real robot. The result shows that the designed GUI and algorithms can help find the necessary parameters stably and get the robot prepared for further operations."
"The millimeter wave (mmWave) frequencies offer the potential of orders of magnitude increases in capacity for next-generation cellular systems. However, links in mmWave networks are susceptible to blockage and may suffer from rapid variations in quality. Connectivity to multiple cells - in the mmWave and in the traditional frequencies - is considered essential for robust connectivity. One of the challenges in supporting multi-connectivity in mmWaves is the requirement for the network to track the direction of each link in addition to its power and timing. To address this challenge, this paper proposes a novel uplink measurement system based on (i) the UE transmitting sounding signals in directions that sweep the angular space, (ii) the mmWave cells measuring the instantaneous received signal strength along with its variance to capture the dynamics and the reliability of a channel/direction and, finally, (iii) a centralized controller making scheduling decisions based on the mmWave cell reports and transmitting the decisions either via a mmWave cell or a conventional LTE cell (when the paths are not available). We argue that the proposed scheme enables fair and robust cell selection, in addition to efficient and periodical tracking of the user, in the presence of the channel variability expected at mmWaves."
"We consider the problem of selecting a minimum size subset of nodes in a network, that allows to activate all the nodes of the network. We present a fast and simple algorithm that, in real-life networks, produces solutions that outperform the ones obtained by using the best algorithms in the literature. We also investigate the theoretical performances of our algorithm and give proofs of optimality for some classes of graphs. From an experimental perspective, experiments also show that the performance of the algorithms correlates with the modularity of the analyzed network. Moreover, the more the influence among communities is hard to propagate, the less the performances of the algorithms differ. On the other hand, when the network allows some propagation of influence between different communities, the gap between the solutions returned by the proposed algorithm and by the previous algorithms in the literature increases."
"This paper describes our submission to the shared task on word/phrase level Quality Estimation (QE) in the First Conference on Statistical Machine Translation (WMT16). The objective of the shared task was to predict if the given word/phrase is a correct/incorrect (OK/BAD) translation in the given sentence. In this paper, we propose a novel approach for word level Quality Estimation using Recurrent Neural Network Language Model (RNN-LM) architecture. RNN-LMs have been found very effective in different Natural Language Processing (NLP) applications. RNN-LM is mainly used for vector space language modeling for different NLP problems. For this task, we modify the architecture of RNN-LM. The modified system predicts a label (OK/BAD) in the slot rather than predicting the word. The input to the system is a word sequence, similar to the standard RNN-LM. The approach is language independent and requires only the translated text for QE. To estimate the phrase level quality, we use the output of the word level QE system."
"Cold start problem in Collaborative Filtering can be solved by asking new users to rate a small seed set of representative items or by asking representative users to rate a new item. The question is how to build a seed set that can give enough preference information for making good recommendations. One of the most successful approaches, called Representative Based Matrix Factorization, is based on Maxvol algorithm. Unfortunately, this approach has one important limitation --- a seed set of a particular size requires a rating matrix factorization of fixed rank that should coincide with that size. This is not necessarily optimal in the general case. In the current paper, we introduce a fast algorithm for an analytical generalization of this approach that we call Rectangular Maxvol. It allows the rank of factorization to be lower than the required size of the seed set. Moreover, the paper includes the theoretical analysis of the method's error, the complexity analysis of the existing methods and the comparison to the state-of-the-art approaches."
"With the dense deployment of the remote radio heads (RRHs), the huge network power consumption has become a great challenge for green cloud radio access networks (Cloud-RANs), and multiuser downlink beamforming has been proposed as a promising solution. Moreover, the increasing number of mobile users (MUs) causes that admission control is essential for Cloud-RAN with limited fronthaul capacity and predefined power budget. In this paper, we consider the problem of joint multiuser downlink beamforming and admission control (JBAC) to enhance the admitted MUs in the network and reduce the network power consumption, while taking into account the Quality of Service requirements of the MUs, the power budget constraints and fronthaul limitation. It is shown that the JBAC problem is a mixed integer nonlinear problem, and still non-convex even though the continuous relaxation is adopted. Therefore, we first transform the JBAC problem into a Mixed-Integer Semidefinite Program. Then, we propose a bound improving Branch and Bound algorithm to yield the near-optimal solution. For practical application, a polynomial-time heuristic algorithm is proposed to derive the sub-optimal solution. Extensive simulations are conducted with different system configurations to show the effectiveness of the proposed two schemes."
"We present a novel approach of color transfer between images by exploring their high-level semantic information. First, we set up a database which consists of the collection of downloaded images from the internet, which are segmented automatically by using matting techniques. We then, extract image foregrounds from both source and multiple target images. Then by using image matting algorithms, the system extracts the semantic information such as faces, lips, teeth, eyes, eyebrows, etc., from the extracted foregrounds of the source image. And, then the color is transferred between corresponding parts with the same semantic information. Next we get the color transferred result by seamlessly compositing different parts together using alpha blending. In the final step, we present an efficient method of color consistency to optimize the color of a collection of images showing the common scene. The main advantage of our method over existing techniques is that it does not need face matching, as one could use more than one target images. It is not restricted to head shot images as we can also change the color style in the wild. Moreover, our algorithm does not require to choose the same color style, same pose and image size between source and target images. Our algorithm is not restricted to one-to-one image color transfer and can make use of more than one target images to transfer the color in different parts in the source image. Comparing with other approaches, our algorithm is much better in color blending in the input data."
"We propose a probabilistic filtering method, which fuses joint measurements with depth images, in order to correct biases in the joint measurements, as well as inaccuracies in the robot model, such as poor extrinsic camera calibration. The proposed method yields an accurate, real-time estimate of the end-effector pose in the camera frame, which avoids the need for frame transformations when using it in combination with visual object tracking methods.   We quantitatively evaluate our approach on a dataset recorded from a real robotic system and annotated with ground truth from a motion capture system. We show that our approach is robust and accurate even under challenging conditions such as fast motion, significant and long-term occlusions, time-varying biases, and the robot arm getting in and out of view. We release the dataset along with open-source code of our approach to allow for quantitative comparison with alternative approaches."
"With the advancement of huge data generation and data handling capability, Machine Learning and Probabilistic modelling enables an immense opportunity to employ predictive analytics platform in high security critical industries namely data centers, electricity grids, utilities, airport etc. where downtime minimization is one of the primary objectives. This paper proposes a novel, complete architecture of an intelligent predictive analytics platform, Fault Engine, for huge device network connected with electrical/information flow. Three unique modules, here proposed, seamlessly integrate with available technology stack of data handling and connect with middleware to produce online intelligent prediction in critical failure scenarios. The Markov Failure module predicts the severity of a failure along with survival probability of a device at any given instances. The Root Cause Analysis model indicates probable devices as potential root cause employing Bayesian probability assignment and topological sort. Finally, a community detection algorithm produces correlated clusters of device in terms of failure probability which will further narrow down the search space of finding route cause. The whole Engine has been tested with different size of network with simulated failure environments and shows its potential to be scalable in real-time implementation."
"The Gibbard-Satterthwaite Impossibility Theorem holds that dictatorship is the only unanimous and strategyproof social choice function on the full domain of preferences. Much of the work in mechanism design aims at getting around this impossibility theorem. Three grand success stories stand out. On the domains of single peaked preferences, house matching, and of quasilinear preferences, there are appealing unanimous and strategyproof social choice functions. We investigate whether these success stories are robust to strengthening strategyproofness to obvious strategyproofness, a stronger incentive constraint recently introduced by Li (2015).   For single-peaked preferences, we characterize the class of OSP-implementable and unanimous social choice rules as dictatorships with safeguards against extremism - mechanisms (which turn out to also be Pareto optimal) in which the dictator can choose the outcome, but other agents may prevent the dictator from choosing an outcome which is too extreme. Median voting is consequently not OSP-implementable. Indeed the only OSP-implementable quantile rules either choose the minimal or the maximal ideal point. For house matching, we characterize the class of OSP-implementable and Pareto optimal matching rules as sequential barter with lurkers - a significant generalization over bossy variants of bipolar serially dictatorial rules. While Li (2015) shows that second-price auctions are OSP-implementable when only one good is sold, we show that this positive result does not extend to the case of multiple goods. Even when all agents' preferences over goods are quasilinear and additive, no welfare-maximizing auction where losers pay nothing is OSP-implementable when more than one good is sold. Our analysis makes use of a gradual revelation principle, an analog of the (direct) revelation principle for OSP mechanisms that we present and prove."
"Real-time simultaneous tracking of hands manipulating and interacting with external objects has many potential applications in augmented reality, tangible computing, and wearable computing. However, due to difficult occlusions, fast motions, and uniform hand appearance, jointly tracking hand and object pose is more challenging than tracking either of the two separately. Many previous approaches resort to complex multi-camera setups to remedy the occlusion problem and often employ expensive segmentation and optimization steps which makes real-time tracking impossible. In this paper, we propose a real-time solution that uses a single commodity RGB-D camera. The core of our approach is a 3D articulated Gaussian mixture alignment strategy tailored to hand-object tracking that allows fast pose optimization. The alignment energy uses novel regularizers to address occlusions and hand-object contacts. For added robustness, we guide the optimization with discriminative part classification of the hand and segmentation of the object. We conducted extensive experiments on several existing datasets and introduce a new annotated hand-object dataset. Quantitative and qualitative results show the key advantages of our method: speed, accuracy, and robustness."
"We consider the problem of estimating the states of weakly coupled linear systems from sampled measurements. We assume that the total capacity available to the sensors to transmit their samples to a network manager in charge of the estimation is bounded above, and that each sample requires the same amount of communication. Our goal is then to find an optimal allocation of the capacity to the sensors so that the average estimation error is minimized. We show that when the total available channel capacity is large, this resource allocation problem can be recast as a strictly convex optimization problem, and hence there exists a unique optimal allocation of the capacity. We further investigate how this optimal allocation varies as the available capacity increases. In particular, we show that if the coupling among the subsystems is weak, then the sampling rate allocated to each sensor is nondecreasing in the total sampling rate, and is strictly increasing if and only if the total sampling rate exceeds a certain threshold."
"We consider the reordering buffer problem on a line consisting of n equidistant points. We show that, for any constant delta, an (offline) algorithm that has a buffer (1-delta) k performs worse by a factor of Omega(log n) than an offline algorithm with buffer k. In particular, this demonstrates that the O(log n)-competitive online algorithm MovingPartition by Gamzu and Segev (ACM Trans. on Algorithms, 6(1), 2009) is essentially optimal against any offline algorithm with a slightly larger buffer."
"We consider a mathematical model for streaming media packets (as the motivating key example) from a transmitter buffer to a receiver over a wireless link while controlling the transmitter power (hence, the packet/job processing rate). When each packet comes to the head-of-line (HOL) in the buffer, it is given a deadline $D$ which is the maximum number of times the transmitter can attempt retransmission in order to successfully transmit the packet. If this number of transmission attempts is exhausted, the packet is ejected from the buffer and the next packet comes to the HOL. Costs are incurred in each time slot for holding packets in the buffer, expending transmitter power, and ejecting packets which exceed their deadlines. We investigate how transmission power should be chosen so as to minimize the total cost of transmitting the items in the buffer. We formulate the optimal power control problem in a dynamic programming framework and then hone in on the special case of fixed interference. For this special case, we are able to provide a precise analytic characterization of how the power control should vary with the backlog and how the power control should react to approaching deadlines. In particular, we show monotonicity results for how the transmitter should adapt power levels to the backlog and approaching deadlines. We leverage these analytic results from the special case to build a power control scheme for the general case. Monte Carlo simulations are used to evaluate the performance of the resulting power control scheme as compared to the optimal scheme. The resulting power control scheme is sub-optimal but it provides a low-complexity approximation of the optimal power control. Simulations show that our proposed schemes outperform benchmark algorithms. We also discuss applications of the model to other practical operational scenarios."
"For more than the past twenty years, Csound has been one of the leaders in the world of the computer music research, implementing innovative synthesis methods and making them available beyond the academic environments from which they often arise, and into the hands of musicians and sound designers throughout the world. In its present state, Csound offers an efficient environment for sound experimentation, allowing the user to work with almost any known sound synthesis or signal processing method through its vast collection of ready-made opcodes. But despite all this potential, the shared resource of Csound instruments still lacks quality reproductions of well-known synthesizers; even with its ability to generate commercial standard user interfaces and with the possibility to compile Csound instruments in such as fashion so that they can be used with no knowledge of Csound code. To fill this gap, the authors have implemented two commercial-style synthesizers as VST plug-ins using the Csound front-end ""Cabbage"". This paper describes their architecture and some of the Csound specific challenges involved in the development of fully featured synthesizers."
"We consider the problem of using polar codes over slow fading wireless channels. For design, we focus on a parallel slow fading channel with 2 blocks, and polar codes with rate <= 1/2. Motivated by Arikan's systematic polar code construction, we propose an interleaver design for a general polar code. The interleaver comprises of using the bit reversal of the order of polarized bit channels. This interleaver is called a diversity interleaver. In addition to the diversity interleaver, a diversity polar code is proposed to further increase the diversity gain.   The proposed designs are evaluated via link simulations for AWGN and fading channels. The simulation results show a performance close to the outage probability (within 2 dB) and significant gains over using a random interleaver."
"We propose a novel probabilistic dimensionality reduction framework that can naturally integrate the generative model and the locality information of data. Based on this framework, we present a new model, which is able to learn a smooth skeleton of embedding points in a low-dimensional space from high-dimensional noisy data. The formulation of the new model can be equivalently interpreted as two coupled learning problem, i.e., structure learning and the learning of projection matrix. This interpretation motivates the learning of the embedding points that can directly form an explicit graph structure. We develop a new method to learn the embedding points that form a spanning tree, which is further extended to obtain a discriminative and compact feature representation for clustering problems. Unlike traditional clustering methods, we assume that centers of clusters should be close to each other if they are connected in a learned graph, and other cluster centers should be distant. This can greatly facilitate data visualization and scientific discovery in downstream analysis. Extensive experiments are performed that demonstrate that the proposed framework is able to obtain discriminative feature representations, and correctly recover the intrinsic structures of various real-world datasets."
"For any given $\epsilon>0$ we provide an algorithm for the Densest $k$-Subhypergraph Problem with an approximation ratio of at most $O(n^{\theta_m+2\epsilon})$ for $\theta_m=\frac{1}{2}m-\frac{1}{2}-\frac{1}{2m}$ and run time at most $O(n^{m-2+1/\epsilon})$, where the hyperedges have at most $m$ vertices. We use this result to give an algorithm for the Set Union Knapsack Problem with an approximation ratio of at most $O(n^{\alpha_m+\epsilon})$ for $\alpha_m=\frac{2}{3}[m-1-\frac{2m-2}{m^2+m-1}]$ and run time at most $O(n^{5(m-2)+9/\epsilon})$, where the subsets have at most $m$ elements. The author is not aware of any previous results on the approximation of either of these two problems."
"Geometric model fitting is a fundamental task in computer graphics and computer vision. However, most geometric model fitting methods are unable to fit an arbitrary geometric model (e.g. a surface with holes) to incomplete data, due to that the similarity metrics used in these methods are unable to measure the rigid partial similarity between arbitrary models. This paper hence proposes a novel rigid geometric similarity metric, which is able to measure both the full similarity and the partial similarity between arbitrary geometric models. The proposed metric enables us to perform partial procedural geometric model fitting (PPGMF). The task of PPGMF is to search a procedural geometric model space for the model rigidly similar to a query of non-complete point set. Models in the procedural model space are generated according to a set of parametric modeling rules. A typical query is a point cloud. PPGMF is very useful as it can be used to fit arbitrary geometric models to non-complete (incomplete, over-complete or hybrid-complete) point cloud data. For example, most laser scanning data is non-complete due to occlusion. Our PPGMF method uses Markov chain Monte Carlo technique to optimize the proposed similarity metric over the model space. To accelerate the optimization process, the method also employs a novel coarse-to-fine model dividing strategy to reject dissimilar models in advance. Our method has been demonstrated on a variety of geometric models and non-complete data. Experimental results show that the PPGMF method based on the proposed metric is able to fit non-complete data, while the method based on other metrics is unable. It is also shown that our method can be accelerated by several times via early rejection."
"In this paper, we aim to maximize the energy efficiency of cellular wireless networks. Specifically, we address the power allocation problem in multi-cell multi-carrier systems. Considering realistic base station power consumption models, we formulate a network-wide energy efficiency maximization problem. Using tools from fractional programming, we cast this problem in the framework of bi-criterion optimization where rate maximization and power minimization are weighted accordingly. Interference pricing mechanism is applied to reduce the intercell interference and to achieve a higher network performance. We decompose the main problem into subproblems via dual decomposition. These subproblems are independently solved per sector using limited information exchange between base stations. We first derive our expressions and present algorithms for the single-tier networks. Then, we extend our analysis to two-tier networks where picocell base stations are deployed to improve the network performance and reduce the link distances. Lastly, we extend our framework and include the quality-of-service constraints.We obtain closed-form expressions for the power level updates which are determined by the multi-level water-filling algorithm, or, as it is sometimes called as, the modified waterfilling algorithm. Based on our simulation results, we demonstrate that the proposed algorithms can outperform the benchmark approaches in terms of energy efficiency by a factor of 2.7."
"Automatic attribute discovery methods have gained in popularity to extract sets of visual attributes from images or videos for various tasks. Despite their good performance in some classification tasks, it is difficult to evaluate whether the attributes discovered by these methods are meaningful and which methods are the most appropriate to discover attributes for visual descriptions. In its simplest form, such an evaluation can be performed by manually verifying whether there is any consistent identifiable visual concept distinguishing between positive and negative exemplars labelled by an attribute. This manual checking is tedious, expensive and labour intensive. In addition, comparisons between different methods could also be problematic as it is not clear how one could quantitatively decide which attribute is more meaningful than the others. In this paper, we propose a novel attribute meaningfulness metric to address this challenging problem. With this metric, automatic quantitative evaluation can be performed on the attribute sets; thus, reducing the enormous effort to perform manual evaluation. The proposed metric is applied to some recent automatic attribute discovery and hashing methods on four attribute-labelled datasets. To further validate the efficacy of the proposed method, we conducted a user study. In addition, we also compared our metric with a semi-supervised attribute discover method using the mixture of probabilistic PCA. In our evaluation, we gleaned several insights that could be beneficial in developing new automatic attribute discovery methods."
"As data-driven methods are becoming pervasive in a wide variety of disciplines, there is an urgent need to develop scalable and sustainable tools to simplify the process of data science, to make it easier to keep track of the analyses being performed and datasets being generated, and to enable introspection of the workflows. In this paper, we describe our vision of a unified provenance and metadata management system to support lifecycle management of complex collaborative data science workflows. We argue that a large amount of information about the analysis processes and data artifacts can, and should be, captured in a semi-passive manner; and we show that querying and analyzing this information can not only simplify bookkeeping and debugging tasks for data analysts but can also enable a rich new set of capabilities like identifying flaws in the data science process itself. It can also significantly reduce the time spent in fixing post-deployment problems through automated analysis and monitoring. We have implemented an initial prototype of our system, called ProvDB, on top of git (a version control system) and Neo4j (a graph database), and we describe its key features and capabilities."
"The problem of solving $(n^2-1)$-puzzle and cooperative path-finding (CPF) sub-optimally by rule based algorithms is addressed in this manuscript. The task in the puzzle is to rearrange $n^2-1$ pebbles on the square grid of the size of n x n using one vacant position to a desired goal configuration. An improvement to the existent polynomial-time algorithm is proposed and experimentally analyzed. The improved algorithm is trying to move pebbles in a more efficient way than the original algorithm by grouping them into so-called snakes and moving them jointly within the snake. An experimental evaluation showed that the algorithm using snakes produces solutions that are 8% to 9% shorter than solutions generated by the original algorithm.   The snake-based relocation has been also integrated into rule-based algorithms for solving the CPF problem sub-optimally, which is a closely related task. The task in CPF is to relocate a group of abstract robots that move over an undirected graph to given goal vertices. Robots can move to unoccupied neighboring vertices and at most one robot can be placed in each vertex. The $(n^2-1)$-puzzle is a special case of CPF where the underlying graph is represented by a 4-connected grid and there is only one vacant vertex. Two major rule-based algorithms for CPF were included in our study - BIBOX and PUSH-and-SWAP (PUSH-and-ROTATE). Improvements gained by using snakes in the BIBOX algorithm were stable around 30% in $(n^2-1)$-puzzle solving and up to 50% in CPFs over bi-connected graphs with various ear decompositions and multiple vacant vertices. In the case of the PUSH-and-SWAP algorithm the improvement achieved by snakes was around 5% to 8%. However, the improvement was unstable and hardly predictable in the case of PUSH-and-SWAP."
"This paper analyses the short utterance probabilistic linear discriminant analysis (PLDA) speaker verification with utterance partitioning and short utterance variance (SUV) modelling approaches. Experimental studies have found that instead of using single long-utterance as enrolment data, if long enrolled utterance is partitioned into multiple short utterances and average of short utterance i-vectors is used as enrolled data, that improves the Gaussian PLDA (GPLDA) speaker verification. This is because short utterance i-vectors have speaker, session and utterance variations, and utterance-partitioning approach compensates the utterance variation. Subsequently, SUV-PLDA is also studied with utterance partitioning approach, and utterance partitioning-based SUV-GPLDA system shows relative improvement of 9% and 16% in EER for NIST 2008 and NIST 2010 truncated 10sec-10sec evaluation condition as utterance partitioning approach compensates the utterance variation and SUV modelling approach compensates the mismatch between full-length development data and short-length evaluation data."
"A Brain Computer Interface (BCI) is a communication system that receives neurological signals from the brain and translates them into control commands for electrical (e.g., computer mouse) and electromechanical (e.g., Wheelchair) devices. The development of such systems was intended originally to aid individuals with a condition called locked-in syndrome. Individuals with this condition have lost all their voluntary muscle control but remain cognitively intact (i.e., mentally aware of their surroundings- can feel emotions, recognize objects/people but are unable to move). This means that they are trapped in their own bodies. The use of BCI may one day improve the independence and quality of life of people with this disability."
"Attack detection is usually approached as a classification problem. However, standard classification tools often perform poorly because an adaptive attacker can shape his attacks in response to the algorithm. This has led to the recent interest in developing methods for adversarial classification, but, to the best of our knowledge, there has not been a prior study that takes into account the attacker's tradeoff between adapting to the classifier being used against him with his desire to maintain the efficacy of his attack. Including this effect is key to derive solutions that perform well in practice.   In this investigation we model the interaction as a game between a defender who chooses a classifier to distinguish between attacks and normal behavior based on a set of observed features and an attacker who chooses his attack features (class 1 data). Normal behavior (class 0 data) is random and exogenous. The attacker's objective balances the benefit from attacks and the cost of being detected while the defender's objective balances the benefit of a correct attack detection and the cost of false alarm. We provide an efficient algorithm to compute all Nash equilibria and a compact characterization of the possible forms of a Nash equilibrium that reveals intuitive messages on how to perform classification in the presence of an attacker. We also explore qualitatively and quantitatively the impact of the non-attacker and underlying parameters on the equilibrium strategies."
"Fuzzy logic is a powerful tool to model knowledge uncertainty, measurements imprecision, and vagueness. However, there is another type of vagueness that arises when data have multiple forms of expression that fuzzy logic does not address quite well. This is the case for multiple instance learning problems (MIL). In MIL, an object is represented by a collection of instances, called a bag. A bag is labeled negative if all of its instances are negative, and positive if at least one of its instances is positive. Positive bags encode ambiguity since the instances themselves are not labeled. In this paper, we introduce fuzzy inference systems and neural networks designed to handle bags of instances as input and capable of learning from ambiguously labeled data. First, we introduce the Multiple Instance Sugeno style fuzzy inference (MI-Sugeno) that extends the standard Sugeno style inference to handle reasoning with multiple instances. Second, we use MI-Sugeno to define and develop Multiple Instance Adaptive Neuro Fuzzy Inference System (MI-ANFIS). We expand the architecture of the standard ANFIS to allow reasoning with bags and derive a learning algorithm using backpropagation to identify the premise and consequent parameters of the network. The proposed inference system is tested and validated using synthetic and benchmark datasets suitable for MIL problems. We also apply the proposed MI-ANFIS to fuse the output of multiple discrimination algorithms for the purpose of landmine detection using Ground Penetrating Radar."
"A full-duplex two-way relay channel with multiple antennas is considered. For this three-node network, the beamforming design needs to suppress self-interference. While a traditional way is to apply zero-forcing for self-interference mitigation, it may harm the desired signals. In this paper, a design which reserves a fraction of self-interference is proposed by solving a quality-of-service constrained beamforming design problem. Since the problem is challenging due to the loop self-interference, a convergence-guaranteed alternating optimization algorithm is proposed to jointly design the relay-user beamformers. Numerical results show that the proposed scheme outperforms zero-forcing method, and achieves a transmit power close to the ideal case."
"Device-to-Device (D2D) communication, which enables direct communication between nearby mobile devices, is an attractive add-on component to improve spectrum efficiency and user experience by reusing licensed cellular spectrum. Nowadays, LTE-unlicensed (LTE-U) emerges to extend the cellular network to the unlicensed spectrum to alleviate the spectrum scarcity issue. In this paper, we propose to enable D2D communication in unlicensed spectrum (D2D-U) as an underlay of the uplink cellular network for further booming the network capacity. A sensing-based protocol is designed to support the unlicensed channel access for both LTE users and D2D pairs, based on which we investigate the subchannel allocation problem to maximize the sum rate of LTE users and D2D pairs while taking into account their interference to the existing Wi-Fi systems. Specifically, we formulate the subchannel allocation as a many-to-many matching problem with externalities, and develop an iterative usersubchannel swap algorithm. Analytical and simulation results show that the proposed D2D-U scheme can significantly improve the network capacity."
"We study the recovery of sparse vectors from subsampled random convolutions via $\ell_1$-minimization. We consider the setup in which both the subsampling locations as well as the generating vector are chosen at random. For a subgaussian generator with independent entries, we improve previously known estimates: if the sparsity $s$ is small enough, i.e.~$s \lesssim \sqrt{n/\log(n)}$, we show that $m \gtrsim s \log(en/s)$ measurements are sufficient to recover $s$-sparse vectors in dimension $n$ with high probability, matching the well-known condition for recovery from standard Gaussian measurements. If $s$ is larger, then essentially $m \geq s \log^2(s) \log(\log(s)) \log(n)$ measurements are sufficient, again improving over previous estimates. Moreover, we also provide robustness estimates for measurement errors that are bounded in $\ell_q$ for $q > 2$ -- in particular, allowing the case $q=\infty$ which is important for quantized compressive sensing. All these results are shown via $\ell_q$-robust versions of the null space property and for $q > 2$ they represent the first non-trivial bounds for structured random matrices. As a crucial ingredient, our approach requires to lower bound expressions of the form $\inf_{v \in V_r} \| \Gamma_v \xi\|_q$, where $\Gamma_v$ is a set of matrices indexed by unit norm $r$-sparse vectors and $\xi$ is a subgaussian random vector. This involves the combination of small ball estimates with chaining techniques."
"This paper provides a comparative analysis of impedance models for power electronic converters and systems for the purpose of stability investigations. Such models can be divided into either decoupled models or matrix models. A decoupled impedance model is highly appealing since the Single-Input-Single-Output (SISO) structure makes the analysis and result interpretation very simple. On the other hand, matrix impedance models are more accurate, and in some cases necessary. Previous works have applied various approximations to obtain decoupled models, and both the dq- and sequence domains have been used. This paper introduces the terms decoupled and semi-decoupled impedance models in order to have a clear classification of the available approximations.   The accuracy of 4 decoupled impedance models are discussed based on the concept of Mirror Frequency Coupling (MFC). By definition the decoupled models based on sequence domain impedances will be exact for systems without MFC. In the general case, they are expected to be more accurate than the decoupled dq-impedance models. The paper defines a norm $\epsilon$ to measure the degree of coupling in the impedance matrices. This norm equals the error in the eigenvalue loci between the matrix and semi-decoupled models. This can also be viewed as the error in the semi-decoupled Nyquist plot.   An example case study consisting of a grid-connected VSC with current controller and PLL is used to compare the different methods. It is found that decoupled and semi-decoupled models in the dq-domain are only applicable in grids with very low X/R-ratio. Furthermore, it is concluded that the decoupled model in the sequence domain gives close to equal results as the semi-decoupled model."
"Recently, neural networks have achieved great success on sentiment classification due to their ability to alleviate feature engineering. However, one of the remaining challenges is to model long texts in document-level sentiment classification under a recurrent architecture because of the deficiency of the memory unit. To address this problem, we present a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. CLSTM introduces a cache mechanism, which divides memory into several groups with different forgetting rates and thus enables the network to keep sentiment information better within a recurrent unit. The proposed CLSTM outperforms the state-of-the-art models on three publicly available document-level sentiment analysis datasets."
"Automatic video captioning is challenging due to the complex interactions in dynamic real scenes. A comprehensive system would ultimately localize and track the objects, actions and interactions present in a video and generate a description that relies on temporal localization in order to ground the visual concepts. However, most existing automatic video captioning systems map from raw video data to high level textual description, bypassing localization and recognition, thus discarding potentially valuable information for content localization and generalization. In this work we present an automatic video captioning model that combines spatio-temporal attention and image classification by means of deep neural network structures based on long short-term memory. The resulting system is demonstrated to produce state-of-the-art results in the standard YouTube captioning benchmark while also offering the advantage of localizing the visual concepts (subjects, verbs, objects), with no grounding supervision, over space and time."
Accurate prediction of wind ramp events is critical for ensuring the reliability and stability of the power systems with high penetration of wind energy. This paper proposes a classification based approach for estimating the future class of wind ramp event based on certain thresholds. A parallelized gradient boosted regression tree based technique has been proposed to accurately classify the normal as well as rare extreme wind power ramp events. The model has been validated using wind power data obtained from the National Renewable Energy Laboratory database. Performance comparison with several benchmark techniques indicates the superiority of the proposed technique in terms of superior classification accuracy.
"Conventional attention-based Neural Machine Translation (NMT) conducts dynamic alignment in generating the target sentence. By repeatedly reading the representation of source sentence, which keeps fixed after generated by the encoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced state-of-the-art NMT. In this paper, we propose a new attention mechanism, called INTERACTIVE ATTENTION, which models the interaction between the decoder and the representation of source sentence during translation by both reading and writing operations. INTERACTIVE ATTENTION can keep track of the interaction history and therefore improve the translation performance. Experiments on NIST Chinese-English translation task show that INTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our INTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets."
"The weekly maintenance schedule specifies when maintenance activities should be performed on the equipment, taking into account the availability of workers and maintenance bays, and other operational constraints. The current approach to generating this schedule is labour intensive and requires coordination between the maintenance schedulers and operations staff to minimise its impact on the operation of the mine. This paper presents methods for automatically generating this schedule from the list of maintenance tasks to be performed, the availability roster of the maintenance staff, and time windows in which each piece of equipment is available for maintenance. Both Mixed-Integer Linear Programming (MILP) and genetic algorithms are evaluated, with the genetic algorithm shown to significantly outperform the MILP. Two fitness functions for the genetic algorithm are also examined, with a linear fitness function outperforming an inverse fitness function by up to 5% for the same calculation time. The genetic algorithm approach is computationally fast, allowing the schedule to be rapidly recalculated in response to unexpected delays and breakdowns."
"An algorithmic summary and comparison of the methodological and numerical properties of competing parametric model reduction techniques is presented for a planar nonlinear thermal conduction problem. First, the Galerkin reduced basis (RB) formulation is presented which fails at providing significant gains with respect to the computational efficiency for the nonlinear problem. Renown methods for the reduction of the computing time of nonlinear reduced order models are the Hyper-Reduction and the (Discrete) Empirical Interpolation Method (EIM, DEIM). An algorithmic description and a methodological comparison of both methods are provided. The accuracy of the predictions of the hyper-reduced model and the (D)EIM in comparison to the Galerkin RB is investigated. All three approaches are applied to uncertainty quantification and the results are compared to computationally intense finite element simulations."
"In this work, we propose a novel framework to encode the local connectivity patterns of brain, using Fisher Vectors (FV), Vector of Locally Aggregated Descriptors (VLAD) and Bag-of-Words (BoW) methods. We first obtain local descriptors, called Mesh Arc Descriptors (MADs) from fMRI data, by forming local meshes around anatomical regions, and estimating their relationship within a neighborhood. Then, we extract a dictionary of relationships, called \textit{brain connectivity dictionary} by fitting a generative Gaussian mixture model (GMM) to a set of MADs, and selecting the codewords at the mean of each component of the mixture. Codewords represent the connectivity patterns among anatomical regions. We also encode MADs by VLAD and BoW methods using the k-Means clustering.   We classify the cognitive states of Human Connectome Project (HCP) task fMRI dataset, where we train support vector machines (SVM) by the encoded MADs. Results demonstrate that, FV encoding of MADs can be successfully employed for classification of cognitive tasks, and outperform the VLAD and BoW representations. Moreover, we identify the significant Gaussians in mixture models by computing energy of their corresponding FV parts, and analyze their effect on classification accuracy. Finally, we suggest a new method to visualize the codewords of brain connectivity dictionary."
"The large amount of work on community detection and its applications leaves unaddressed one important question: the statistical validation of the results. In this paper we present a methodology able to clearly detect if the community structure found by some algorithms is statistically significant or is a result of chance, merely due to edge positions in the network. Given a community detection method and a network of interest, our proposal examines the stability of the partition recovered against random perturbations of the original graph structure. To address this issue, we specify a perturbation strategy and a null model to build a set of procedures based on a special measure of clustering distance, namely Variation of Information, using tools set up for functional data analysis. The procedures determine whether the obtained clustering departs significantly from the null model. This strongly supports the robustness against perturbation of the algorithm used to identify the community structure. We show the results obtained with the proposed technique on simulated and real datasets."
"Person re-identification (re-id) is the task of matching multiple occurrences of the same person from different cameras, poses, lighting conditions, and a multitude of other factors which alter the visual appearance. Typically, this is achieved by learning either optimal features or matching metrics which are adapted to specific pairs of camera views dictated by the pairwise labelled training datasets. In this work, we formulate a deep learning based novel approach to automatic prototype-domain discovery for domain perceptive (adaptive) person re-id (rather than camera pair specific learning) for any camera views scalable to new unseen scenes without training data. We learn a separate re-id model for each of the discovered prototype-domains and during model deployment, use the person probe image to select automatically the model of the closest prototype domain. Our approach requires neither supervised nor unsupervised domain adaptation learning, i.e. no data available from the target domains. We evaluate extensively our model under realistic re-id conditions using automatically detected bounding boxes with low-resolution and partial occlusion. We show that our approach outperforms most of the state-of-the-art supervised and unsupervised methods on the latest CUHK-SYSU and PRW benchmarks."
"To understand the sophisticated control mechanisms of the human's endocrine system is a challenging task that is a crucial step towards precise medical treatment of many disfunctions and diseases. Although mathematical models describing the endocrine system as a whole are still elusive, recently some substantial progress has been made in analyzing theoretically its subsystems (or \emph{axes}) that regulate production of specific hormones. Many of the relevant mathematical models are similar in structure to (or squarely based on) the celebrated \emph{Goodwin's oscillator}. Such models are convenient to explain stable periodic oscillations at hormones' level by representing the corresponding endocrine regulation circuits as \emph{cyclic} feedback systems. However, many real hormonal regulation mechanisms (in particular, testosterone regulation) are in fact known to have non-cyclic structures and involve multiple feedbacks; a Goodwin-type model thus represents only a part of such a complicated mechanism. In this paper, we examine a new mathematical model of hormonal regulation, obtained from the classical Goodwin's oscillator by introducing an additional negative feedback. Local stability properties of the proposed model are studied, and we show that the local instability of its unique equilibrium implies oscillatory behavior of almost all solutions. Furthermore, under additional restrictions we prove that almost all solutions converge to periodic ones."
"Presence of a logically centralized controller in software-defined networks enables smart and fine-grained management of network traffic. Generally, traffic management includes measurement, analysis and control of traffic in order to improve resource utilization. This is done by inspecting corresponding performance requirements using metrics such as packet delay, jitter, loss rate and bandwidth utilization from global network view. There has been many works regarding traffic management of software-defined networks and how it could help to efficiently allocate resources. However, the vast majority of these solutions are bounded to indirect information retrieved within the border of ingress and egress switches. This means that the three stage loop of measurement, analysis and control is performed on switches in between this border while the traffic flowing in network originates from applications on end hosts. In this work, we present a framework for incorporating network applications into the task of traffic management using the concept of software-defined networking. We demonstrate how this could help applications to receive desired level of quality of service by implementing a prototype of an API for flow bandwidth reservation using OpenFlow and OVSDB protocols."
"In this paper, we propose a ternary knowing how operator to express that the agent knows how to achieve $\phi$ given $\psi$ while maintaining $\chi$ in-between. It generalizes the logic of goal-directed knowing how proposed by Yanjing Wang 2015 'A logic of knowing how'. We give a sound and complete axiomatization of this logic."
"The supertree problem asking for a tree displaying a set of consistent input trees has been largely considered for the reconstruction of species trees. Here, we rather explore this framework for the sake of reconstructing a gene tree from a set of input gene trees on partial data. In this perspective, the phylogenetic tree for the species containing the genes of interest can be used to choose among the many possible compatible ""supergenetrees"", the most natural criteria being to minimize a reconciliation cost. We develop a variety of algorithmic solutions for the construction and correction of gene trees using the supertree framework. A dynamic programming supertree algorithm for constructing or correcting gene trees, exponential in the number of input trees, is first developed for the less constrained version of the problem. It is then adapted to gene trees with nodes labeled as duplication or speciation, the additional constraint being to preserve the orthology and paralogy relations between genes. Then, a quadratic time algorithm is developed for efficiently correcting an initial gene tree while preserving a set of ""trusted"" subtrees, as well as the relative phylogenetic distance between them, in both cases of labeled or unlabeled input trees. By applying these algorithms to the set of Ensembl gene trees, we show that this new correction framework is particularly useful to correct weaklysupported duplication nodes. The C++ source code for the algorithms and simulations described in the paper are available at https://github.com/UdeM-LBIT/SuGeT."
"Cauchy reals can be defined as a quotient of Cauchy sequences of rationals. The limit of a Cauchy sequence of Cauchy reals is defined through lifting it to a sequence of Cauchy sequences of rationals. This lifting requires the axiom of countable choice or excluded middle, neither of which is available in homotopy type theory. To address this, the Univalent Foundations Program uses a higher inductive-inductive type to define the Cauchy reals as the free Cauchy complete metric space generated by the rationals. We generalize this construction to define the free Cauchy complete metric space generated by an arbitrary metric space. This forms a monad in the category of metric spaces with Lipschitz functions. When applied to the rationals it defines the Cauchy reals. Finally, we can use Altenkirch and Danielson (2016)'s partiality monad to define a semi-decision procedure comparing a real number and a rational number.   The entire construction has been formalized in the Coq proof assistant. It is available at https://github.com/SkySkimmer/HoTTClasses/tree/CPP2017 ."
"The purpose of this study is to propose a model that predicts the social and psychological factors that affect the individuals collaborative learning outcome in group projects. The model is established on the basis of two theories, namely, the multilevel analysis and the cooperative game theory (CGT). In CGT, a group of players form a coalition and a set of payoffs for each member in the coalition. Shapely values is one of the most important solution concepts in CGT. It represents a fair and efficient distribution of payoffs among members of a coalition. The proposed approach was applied on a sample that consisted of 78 freshman students, in their first semester, who were studying philosophical thinking course and instructed by the same professor. Tools for the data collection included self-assessments, peer assessments, quizzes and observations. The research concluded that learning outcome and contribution are best prophesied by the extent of engagement the content is purveying. Whereas personality traits, as well as, learning styles have the least impact on contribution. In addition, results show that Shapley values can be used as good vaticinators for individuals learning outcomes. These results indicate that CGT can be used as a good engine for analyzing interactions that recur in collaborative learning."
"We investigate metric learning in the context of dynamic time warping (DTW), the by far most popular dissimilarity measure used for the comparison and analysis of motion capture data. While metric learning enables a problem-adapted representation of data, the majority of meth- ods has been proposed for vectorial data only. In this contribution, we extend the popular principle offered by the large margin nearest neighbours learner (LMNN) to DTW by treating the resulting component-wise dissimilarity values as features. We demonstrate, that this principle greatly enhances the classification accuracy in several benchmarks. Further, we show that recent auxiliary concepts such as metric regularisation can be transferred from the vectorial case to component-wise DTW in a similar way. We illustrate, that metric regularisation constitutes a crucial prerequisite for the interpretation of the resulting relevance profiles."
"This letter presents an experimental study and a novel modelling approach of the wireless channel of smart utility meters placed in basements or sculleries. The experimental data consist of signal strength measurements of consumption report packets. Since such packets are only registered if they can be decoded by the receiver, the part of the signal strength distribution that falls below the receiver sensitivity threshold is not observable. We combine a Rician fading model with a bias function that captures the cut-off in the observed signal strength measurements. Two sets of experimental data are analysed. It is shown that the proposed method offers an approximation of the distribution of the signal strength measurements that is better than a na\""ive Rician fitting."
"Wearable photoplethysmography (WPPG) has recently become a common technology in heart rate (HR) monitoring. General observation is that the motion artifacts change the statistics of the acquired PPG signal. Consequently, estimation of HR from such a corrupted PPG signal is challenging. However, if an accelerometer is also used to acquire the acceleration signal simultaneously, it can provide helpful information that can be used to reduce the motion artifacts in the PPG signal. By dint of repetitive movements of the subjects hands while running, the accelerometer signal is found to be quasi-periodic. Over short-time intervals, it can be modeled by a finite harmonic sum (HSUM). Using the harmonic sum (HSUM) model, we obtain an estimate of the instantaneous fundamental frequency of the accelerometer signal. Since the PPG signal is a composite of the heart rate information (that is also quasi-periodic) and the motion artifact, we fit a joint harmonic sum (HSUM) model to the PPG signal. One of the harmonic sums corresponds to the heart-beat component in PPG and the other models the motion artifact. However, the fundamental frequency of the motion artifact has already been determined from the accelerometer signal. Subsequently, the HR is estimated from the joint HSUM model. The mean absolute error in HR estimates was 0.7359 beats per minute (BPM) with a standard deviation of 0.8328 BPM for 2015 IEEE Signal Processing (SP) cup data. The ground-truth HR was obtained from the simultaneously acquired ECG for validating the accuracy of the proposed method. The proposed method is compared with four methods that were recently developed and evaluated on the same dataset."
"Given a directed graph, it is known that the problem of finnding a set of vertex-disjoint cycles with the maximum total number of vertices (MAX SIZE EXCHANGE) can be solved in polynomial time. Given a vertex-colored graph, if a set of vertices contains a vertex of each color in the graph then the set is said to be tropical. A set of cycles is said to be tropical if for every color there is a cycle that contains a vertex of that color. We show that the problem of determining whether there exists a tropical set of vertex-disjoint cycles in a digraph (TROPICAL EXCHANGE) is NP-complete. Likewise, the problem of determining whether there exists a tropical set of vertex-disjoint cycles where the set of cycles contains the maximum total number of vertices (TROPMAX-EXCHANGE) is NP-complete. We reduce the satisfiability problem in conjunctive normal form (CNFSAT) to these two problems in order to demonstrate that they are NP-complete. Finally, we show that the problem of finding a set of vertex-disjoint cycles that has as many colors as any other set of disjoint-cycles and as many vertices as any set with the same number of colors (MAXTROP-EXCHANGE) is NP-hard."
"FP-Growth algorithm is a Frequent Pattern Min- ing (FPM) algorithm that has been extensively used to study correlations and patterns in large scale datasets. While several researchers have designed distributed memory FP-Growth algorithms, it is pivotal to consider fault tolerant FP-Growth, which can address the increasing fault rates in large scale systems. In this work, we propose a novel parallel, algorithm-level fault-tolerant FP-Growth algorithm. We leverage algorithmic properties and MPI advanced features to guarantee an O(1) space complexity, achieved by using the dataset memory space itself for checkpointing. We also propose a recovery algorithm that can use in-memory and disk-based checkpointing, though in many cases the recovery can be completed without any disk access, and incurring no memory overhead for checkpointing. We evaluate our FT algorithm on a large scale InfiniBand cluster with several large datasets using up to 2K cores. Our evaluation demonstrates excellent efficiency for checkpointing and recovery in comparison to the disk-based approach. We have also observed 20x average speed-up in comparison to Spark, establishing that a well designed algorithm can easily outperform a solution based on a general fault-tolerant programming model."
"The purpose of the project is an analysis of the modernization prospects of the WLCG monitoring framework's messaging subsystem based on Nagios monitoring software and Apache ActiveMQ technologies. The modernization process demands thorough examination of the existing subsystem to determine the vital upgrade requirements. Thus the work is focused on research of the main underlying technologies, the existing subsystem's structure and revision of its design and used software."
"Conditional gradient algorithms (also often called Frank-Wolfe algorithms) are popular due to their simplicity of only requiring a linear optimization oracle and more recently they also gained significant traction for online learning. While simple in principle, in many cases the actual implementation of the linear optimization oracle is costly. We show a general method to lazify various conditional gradient algorithms, which in actual computations leads to several orders of magnitude of speedup in wall-clock time. This is achieved by using a faster separation oracle instead of a linear optimization oracle, relying only on few linear optimization oracle calls."
"Key-based workload partitioning is a common strategy used in parallel stream processing engines, enabling effective key-value tuple distribution over worker threads in a logical operator. While randomized hashing on the keys is capable of balancing the workload for key-based partitioning when the keys generally follow a static distribution, it is likely to generate poor balancing performance when workload variance occurs on the incoming data stream. This paper presents a new key-based workload partitioning framework, with practical algorithms to support dynamic workload assignment for stateful operators. The framework combines hash-based and explicit key-based routing strategies for workload distribution, which specifies the destination worker threads for a handful of keys and assigns the other keys with the hashing function. When short-term distribution fluctuations occur to the incoming data stream, the system adaptively updates the routing table containing the chosen keys, in order to rebalance the workload with minimal migration overhead within the stateful operator. We formulate the rebalance operation as an optimization problem, with multiple objectives on minimizing state migration costs, controlling the size of the routing table and breaking workload imbalance among worker threads. Despite of the NP-hardness nature behind the optimization formulation, we carefully investigate and justify the heuristics behind key (re)routing and state migration, to facilitate fast response to workload variance with ignorable cost to the normal processing in the distributed system. Empirical studies on synthetic data and real-world stream applications validate the usefulness of our proposals and prove the huge advantage of our approaches over state-of-the-art solutions in the literature."
"In this work we consider adversarial contextual bandits with risk constraints. At each round, nature prepares a context, a cost for each arm, and additionally a risk for each arm. The learner leverages the context to pull an arm and then receives the corresponding cost and risk associated with the pulled arm. In addition to minimizing the cumulative cost, the learner also needs to satisfy long-term risk constraints -- the average of the cumulative risk from all pulled arms should not be larger than a pre-defined threshold. To address this problem, we first study the full information setting where in each round the learner receives an adversarial convex loss and a convex constraint. We develop a meta algorithm leveraging online mirror descent for the full information setting and extend it to contextual bandit with risk constraints setting using expert advice. Our algorithms can achieve near-optimal regret in terms of minimizing the total cost, while successfully maintaining a sublinear growth of cumulative risk constraint violation."
"We consider the problem of sampling $n$ numbers from the range $\{1,\ldots,N\}$ without replacement on modern architectures. The main result is a simple divide-and-conquer scheme that makes sequential algorithms more cache efficient and leads to a parallel algorithm running in expected time $\mathcal{O}\left(n/p+\log p\right)$ on $p$ processors. The amount of communication between the processors is very small and independent of the sample size. We also discuss modifications needed for load balancing, reservoir sampling, online sampling, sampling with replacement, Bernoulli sampling, and vectorization on SIMD units or GPUs."
"This paper proposes method for detection, estimation of Thevenin equivalent parameters to describe power system behavior. Thevenin equivalent estimation is a challenge due to variation in system states caused by power flow in the network. Thevenin equivalent calculation based on changes in system with multiple sources integrated with grid, isolated distributed generator system is analysed and nonlinear least square fit estimation technique for algorithm is adopted. Linear least square fit is used with a linearized model. Performance evaluation of proposed method is carried out through mathematical model, nonlinear and linear least square fit based algorithm technique and simulation through MATLAB/SIMULINK package. Accurate grid and source side impedance estimation technique is applicable for distributed generation sources interfaced with grid to improve dynamic response, stability, reliability when subjected to faults or any other disturbances in network. Algorithm can accurately estimate Thevenin equivalent of multiple sources connected in parallel simultaneously with voltage and current phasor measurements at point of common coupling. Mathematical analysis and simulation results validate the effectiveness of proposed method."
"Neural Machine Translation (NMT) is a new approach to machine translation that has made great progress in recent years. However, recent studies show that NMT generally produces fluent but inadequate translations (Tu et al. 2016; He et al. 2016). This is in contrast to conventional Statistical Machine Translation (SMT), which usually yields adequate but non-fluent translations. It is natural, therefore, to leverage the advantages of both models for better translations, and in this work we propose to incorporate SMT model into NMT framework. More specifically, at each decoding step, SMT offers additional recommendations of generated words based on the decoding information from NMT (e.g., the generated partial translation and attention history). Then we employ an auxiliary classifier to score the SMT recommendations and a gating function to combine the SMT recommendations with NMT generations, both of which are jointly trained within the NMT architecture in an end-to-end manner. Experimental results on Chinese-English translation show that the proposed approach achieves significant and consistent improvements over state-of the-art NMT and SMT systems on multiple NIST test sets."
"We consider the problem of online Min-cost Perfect Matching with Delays (MPMD) recently introduced by Emek et al, (STOC 2016). This problem is defined on an underlying $n$-point metric space. An adversary presents real-time requests online at points of the metric space, and the algorithm is required to match them, possibly after keeping them waiting for some time. The cost incurred is the sum of the distances between matched pairs of points (the connection cost), and the sum of the waiting times of the requests (the delay cost). We present an algorithm with a competitive ratio of $O(\log n)$, which improves the upper bound of $O(\log^2n+\log\Delta)$ of Emek et al, by removing the dependence on $\Delta$, the aspect ratio of the metric space (which can be unbounded as a function of $n$). The core of our algorithm is a deterministic algorithm for MPMD on metrics induced by edge-weighted trees of height $h$, whose cost is guaranteed to be at most $O(1)$ times the connection cost plus $O(h)$ times the delay cost of every feasible solution. The reduction from MPMD on arbitrary metrics to MPMD on trees is achieved using the result on embedding $n$-point metric spaces into distributions over weighted hierarchically separated trees of height $O(\log n)$, with distortion $O(\log n)$. We also prove a lower bound of $\Omega(\sqrt{\log n})$ on the competitive ratio of any randomized algorithm. This is the first lower bound which increases with $n$, and is attained on the metric of $n$ equally spaced points on a line.   The problem of Min-cost Bipartite Perfect Matching with Delays (MBPMD) is the same as MPMD except that every request is either positive or negative, and requests can be matched only if they have opposite polarity. We prove an upper bound of $O(\log n)$ and a lower bound of $\Omega(\log^{1/3}n)$ on the competitive ratio of MBPMD with a more involved analysis."
"We consider the problem of inferring a grammar describing the output of a functional program given a grammar describing its input. Solutions to this problem are helpful for detecting bugs or proving safety properties of functional programs, and several rewriting tools exist for solving this problem. However, known grammar inference techniques are not able to take evaluation strategies of the program into account. This yields very imprecise results when the evaluation strategy matters. In this work, we adapt the Tree Automata Completion algorithm to approximate accurately the set of terms reachable by rewriting under the innermost strategy. We formally prove that the proposed technique is sound and precise w.r.t. innermost rewriting. We show that those results can be extended to the leftmost and rightmost innermost case. The algorithms for the general innermost case have been implemented in the Timbuk reachability tool. Experiments show that it noticeably improves the accuracy of static analysis for functional programs using the call-by-value evaluation strategy."
"For the supervised least squares classifier, when the number of training objects is smaller than the dimensionality of the data, adding more data to the training set may first increase the error rate before decreasing it. This, possibly counterintuitive, phenomenon is known as peaking. In this work, we observe that a similar but more pronounced version of this phenomenon also occurs in the semi-supervised setting, where instead of labeled objects, unlabeled objects are added to the training set. We explain why the learning curve has a more steep incline and a more gradual decline in this setting through simulation studies and by applying an approximation of the learning curve based on the work by Raudys & Duin."
"The human action classification task is a widely researched topic and is still an open problem. Many state-of-the-arts approaches involve the usage of bag-of-video-words with spatio-temporal local features to construct characterizations for human actions. In order to improve beyond this standard approach, we investigate the usage of co-occurrences between local features. We propose the usage of co-occurrences information to characterize human actions. A trade-off factor is used to define an optimal trade-off between vocabulary size and classification rate. Next, a spatio-temporal co-occurrence technique is applied to extract co-occurrence information between labeled local features. Novel characterizations for human actions are then constructed. These include a vector quantized correlogram-elements vector, a highly discriminative PCA (Principal Components Analysis) co-occurrence vector and a Haralick texture vector. Multi-channel kernel SVM (support vector machine) is utilized for classification. For evaluation, the well known KTH as well as the challenging UCF-Sports action datasets are used. We obtained state-of-the-arts classification performance. We also demonstrated that we are able to fully utilize co-occurrence information, and improve the standard bag-of-video-words approach."
"We present a novel approach and database which combines the inexpensive generation of 3D object models via monocular or RGB-D camera images with 3D printing and a state of the art object tracking algorithm. Unlike recent efforts towards the creation of 3D object databases for robotics, our approach does not require expensive and controlled 3D scanning setups and enables anyone with a camera to scan, print and track complex objects for manipulation research. The proposed approach results in highly detailed mesh models whose 3D printed replicas are at times difficult to distinguish from the original. A key motivation for utilizing 3D printed objects is the ability to precisely control and vary object properties such as the mass distribution and size in the 3D printing process to obtain reproducible conditions for robotic manipulation research. We present CapriDB - an extensible database resulting from this approach containing initially 40 textured and 3D printable mesh models together with tracking features to facilitate the adoption of the proposed approach."
"We study a novel architecture and training procedure for locomotion tasks. A high-frequency, low-level ""spinal"" network with access to proprioceptive sensors learns sensorimotor primitives by training on simple tasks. This pre-trained module is fixed and connected to a low-frequency, high-level ""cortical"" network, with access to all sensors, which drives behavior by modulating the inputs to the spinal network. Where a monolithic end-to-end architecture fails completely, learning with a pre-trained spinal module succeeds at multiple high-level tasks, and enables the effective exploration required to learn from sparse rewards. We test our proposed architecture on three simulated bodies: a 16-dimensional swimming snake, a 20-dimensional quadruped, and a 54-dimensional humanoid. Our results are illustrated in the accompanying video at https://youtu.be/sboPYvhpraQ"
"This paper investigates a dynamic buffer management scheme for QoS control of multimedia services in beyond 3G wireless systems. The scheme is studied in the context of the state-of-the-art 3.5G system i.e. the High Speed Downlink Packet Access (HSDPA) which enhances 3G UMTS to support high-speed packet switched services. Unlike earlier systems, UMTS-evolved systems from HSDPA and beyond incorporate mechanisms such as packet scheduling and HARQ in the base station necessitating data buffering at the air interface. This introduces a potential bottleneck to end-to-end communication. Hence, buffer management at the air interface is crucial for end-to-end QoS support of multimedia services with multiplexed parallel diverse flows such as video and data in the same end-user session. The dynamic buffer management scheme for HSDPA multimedia sessions with aggregated real-time and non real-time flows is investigated via extensive HSDPA simulations. The impact of the scheme on end-to-end traffic performance is evaluated with an example multimedia session comprising a real-time streaming flow concurrent with TCP-based non real-time flow. Results demonstrate that the scheme can guarantee the end-to-end QoS of the real-time streaming flow, whilst simultaneously protecting the non real-time flow from starvation resulting in improved end-to-end throughput performance."
"The capability of a given channel to communicate information is, a priori, distinct from its capability to distribute shared randomness. In this article we define randomness distribution capacities of quantum channels assisted by forward, back, or two-way classical communication and compare these to the corresponding communication capacities. With forward assistance or no assistance, we find that they are equal. We establish the mutual information of the channel as an upper bound on the two-way assisted randomness distribution capacity. This implies that all of the capacities are equal for classical-quantum channels. On the other hand, we show that the back-assisted randomness distribution capacity of a quantum-classical channels is equal to its mutual information. This is often strictly greater than the back-assisted communication capacity. We give an explicit example of such a separation where the randomness distribution protocol is noiseless."
"We consider a set of learning agents in a collaborative peer-to-peer network, where each agent learns a personalized model according to its own learning objective. The question addressed in this paper is: how can agents improve upon their locally trained model by communicating with other agents that have similar objectives? We introduce and analyze two asynchronous gossip algorithms running in a fully decentralized manner. Our first approach, inspired from label propagation, aims to smooth pre-trained local models over the network while accounting for the confidence that each agent has in its initial model. In our second approach, agents jointly learn and propagate their model by making iterative updates based on both their local dataset and the behavior of their neighbors. Our algorithm to optimize this challenging objective in a decentralized way is based on ADMM."
"Antennas of transmitters and receivers have been manipulated to increase the capacity of transmission and reception of signals. Using many elements in antennas to shape beams and direct nulls in a particular point for optimum signal transmission and reception has over decades, had tremendous positive influence in received power and signal to noise ratio (SNR). However, since the antenna elements manipulation can be done both at base station and device terminal, it gives rise to an important method of using several antennas to put and obtain signals to and from space with increased capacity. This principle is termed Multiple-input and Multiple-output (MIMO). This paper discusses application of MIMO in the state of the art and next generation of wireless systems (5G). It also discusses four models of MIMO, SISO, SIMO, MISO and MIMO, considering three method of combing the signals from multipath propagations, Selection combining (SC), Equal gain combing (EGC) and maximum ratio combining (MRC). Spatial diversity and spatial multiplexing are also discussed as form of MIMO. Finally, Massive or Hyper MIMO which is a new method of increasing transmission capacity by very large scale for fifth generation of wireless system is discussed with its challenges and opportunities. Key terms-Diversity combining techniques, spatial multiplexing, channel state information (CSI). Massive MIMO"
"The popularity of mobile devices and location-based services (LBS) has created great concern regarding the location privacy of their users. Anonymization is a common technique that is often used to protect the location privacy of LBS users. Here, we present an information-theoretic approach to define the notion of perfect location privacy. We show how LBS's should use the anonymization method to ensure that their users can achieve perfect location privacy. First, we assume that a user's current location is independent from her past locations. Using this i.i.d model, we show that if the pseudonym of the user is changed before O(n^(2/r-1)) observations are made by the adversary for that user, then the user has perfect location privacy. Here, n is the number of the users in the network and r is the number of all possible locations that users can go to. Next, we model users' movements using Markov chains to better model real-world movement patterns. We show that perfect location privacy is achievable for a user if the user's pseudonym is changed before O(n^(2/|E|-r)) observations are collected by the adversary for the user, where |E| is the number of edges in the user's Markov chain model."
"Subspace clustering refers to the problem of segmenting data drawn from a union of subspaces. State of the art approaches for solving this problem follow a two-stage approach. In the first step, an affinity matrix is learned from the data using sparse or low-rank minimization techniques. In the second step, the segmentation is found by applying spectral clustering to this affinity. While this approach has led to state-of-the-art results in many applications, it is sub-optimal because it does not exploit the fact that the affinity and the segmentation depend on each other. In this paper, we propose a joint optimization framework --- Structured Sparse Subspace Clustering (S$^3$C) --- for learning both the affinity and the segmentation. The proposed S$^3$C framework is based on expressing each data point as a structured sparse linear combination of all other data points, where the structure is induced by a norm that depends on the unknown segmentation. Moreover, we extend the proposed S$^3$C framework into Constrained Structured Sparse Subspace Clustering (CS$^3$C) in which available partial side-information is incorporated into the stage of learning the affinity. We show that both the structured sparse representation and the segmentation can be found via a combination of an alternating direction method of multipliers with spectral clustering. Experiments on a synthetic data set, the Extended Yale B data set, the Hopkins 155 motion segmentation database, and three cancer data sets demonstrate the effectiveness of our approach."
"Internet of Things (IoT) is one of the most fast-growing field in high technologies nowadays. Therefore, lots of electronic devices include wireless connections with several communication protocols (WiFi, ZigBee, Sigfox, LoRa and so on). Nevertheless, designers of such components do not take care of security features most of the time while focusing on communication reliability (speed, throughput and low power consumption). As a consequence, several wireless IoT devices transmit data in plaintext creating lots of security breaches for both eavesdropping and data injection attacks. This work introduces KeyJack, a preliminary proof-of-concept of a solution aiming to eavesdrop wireless devices and hopefully perform injection attacks afterwards. KeyJack operates on widely-used devices: our keyboards! This solution is based on low-cost embedded electronics and gives an attacker or a white hat hacker the possibility to retrieve data from John Doe's computer. This work also shows that this approach could be used to any wireless device using 2.4GHz radio chips like the NRF24L01 from Nordic Semiconductor."
"The Gromov-Hausdorff distance provides a metric on the set of isometry classes of compact metric spaces. Unfortunately, computing this metric directly is believed to be computationally intractable. Motivated by applications in shape matching and point-cloud comparison, we study a semidefinite programming relaxation of the Gromov-Hausdorff metric. This relaxation can be computed in polynomial time, and somewhat surprisingly is itself a pseudometric. We describe the induced topology on the set of compact metric spaces. Finally, we demonstrate the numerical performance of various algorithms for computing the relaxed distance and apply these algorithms to several relevant data sets. In particular we propose a greedy algorithm for finding the best correspondence between finite metric spaces that can handle hundreds of points."
"Various variants of the well known Covariance Matrix Adaptation Evolution Strategy (CMA-ES) have been proposed recently, which improve the empirical performance of the original algorithm by structural modifications. However, in practice it is often unclear which variation is best suited to the specific optimization problem at hand. As one approach to tackle this issue, algorithmic mechanisms attached to CMA-ES variants are considered and extracted as functional \emph{modules}, allowing for combinations of them. This leads to a configuration space over ES structures, which enables the exploration of algorithm structures and paves the way toward novel algorithm generation. Specifically, eleven modules are incorporated in this framework with two or three alternative configurations for each module, resulting in $4\,608$ algorithms. A self-adaptive Genetic Algorithm (GA) is used to efficiently evolve effective ES-structures for given classes of optimization problems, outperforming any classical CMA-ES variants from literature. The proposed approach is evaluated on noiseless functions from BBOB suite. Furthermore, such an observation is again confirmed on different function groups and dimensionality, indicating the feasibility of ES configuration on real-world problem classes."
"A quantum network promises to enable long distance quantum communication, and assemble small quantum devices into a large quantum computing cluster. Each network node can thereby be seen as a small few qubit quantum computer. Qubits can be sent over direct physical links connecting nearby quantum nodes, or by means of teleportation over pre-established entanglement amongst distant network nodes. Such pre-shared entanglement effectively forms a shortcut - a virtual quantum link - which can be used exactly once.   Here, we present an abstraction of a quantum network that allows ideas from computer science to be applied to the problem of routing qubits, and manage entanglement in the network. Specifically, we consider a scenario in which each quantum network node can create EPR pairs with its immediate neighbours over a physical connection, and perform entanglement swapping operations in order to create long distance virtual quantum links. We proceed to discuss the features unique to quantum networks, which call for the development of new routing techniques. As an example, we present two simple hierarchical routing schemes for a quantum network of N nodes for a ring and sphere topology. For these topologies we present efficient routing algorithms requiring O(log N) qubits to be stored at each network node, O(polylog N) time and space to perform routing decisions, and O(log N) timesteps to replenish the virtual quantum links in a model of entanglement generation."
"Recently, the development of neural machine translation (NMT) has significantly improved the translation quality of automatic machine translation. While most sentences are more accurate and fluent than translations by statistical machine translation (SMT)-based systems, in some cases, the NMT system produces translations that have a completely different meaning. This is especially the case when rare words occur.   When using statistical machine translation, it has already been shown that significant gains can be achieved by simplifying the input in a preprocessing step. A commonly used example is the pre-reordering approach.   In this work, we used phrase-based machine translation to pre-translate the input into the target language. Then a neural machine translation system generates the final hypothesis using the pre-translation. Thereby, we use either only the output of the phrase-based machine translation (PBMT) system or a combination of the PBMT output and the source sentence.   We evaluate the technique on the English to German translation task. Using this approach we are able to outperform the PBMT system as well as the baseline neural MT system by up to 2 BLEU points. We analyzed the influence of the quality of the initial system on the final result."
"We study the problem of model-free dependence detection. This problem can be difficult even when the marginal distributions are known. We explain this difficulty by showing the impossibility to uniformly consistently distinguish degeneracy from independence with any single test. To make model-free dependence detection a tractable problem, we introduce the concept of binary expansion statistics (BEStat) and propose the binary expansion testing (BET) framework. Through simple mathematics, we convert the dependence detection problem to a multiple testing problem. Besides being model-free, the BET also enjoys many other advantages which include (1) invariance to monotone marginal transformations, (2) clear interpretability of local relationships upon rejection, and (3) close connections to computing for efficient algorithms. We illustrate the BET by studying the distribution of the brightest stars in the night sky."
"Conversational speech recognition has served as a flagship speech recognition task since the release of the DARPA Switchboard corpus in the 1990s. In this paper, we measure the human error rate on the widely used NIST 2000 test set, and find that our latest automated system has reached human parity. The error rate of professional transcriptionists is 5.9% for the Switchboard portion of the data, in which newly acquainted pairs of people discuss an assigned topic, and 11.3% for the CallHome portion where friends and family members have open-ended conversations. In both cases, our automated system establishes a new state-of-the-art, and edges past the human benchmark. This marks the first time that human parity has been reported for conversational speech. The key to our system's performance is the systematic use of convolutional and LSTM neural networks, combined with a novel spatial smoothing method and lattice-free MMI acoustic training."
"Like many numerical methods, solvers for initial value problems (IVPs) on ordinary differential equations estimate an analytically intractable quantity, using the results of tractable computations as inputs. This structure is closely connected to the notion of inference on latent variables in statistics. We describe a class of algorithms that formulate the solution to an IVP as inference on a latent path that is a draw from a Gaussian process probability measure (or equivalently, the solution of a linear stochastic differential equation). We then show that certain members of this class are identified exactly with existing generalized linear methods for ODEs, in particular a number of Runge--Kutta methods and Nordsieck methods. This probabilistic formulation of classic methods is valuable in two ways: analytically, it highlights implicit prior assumptions favoring certain approximate solutions to the IVP over others, and gives a precise meaning to the old observation that these methods act like filters. Practically, it endows the classic solvers with `docking points' for notions of uncertainty and prior information about the initial value, the value of the ODE itself, and the solution of the problem."
"The sensitivity (i.e. dynamic response) of complex networked systems has not been well understood, making difficult to predict whether new macroscopic dynamic behavior will emerge even if we know exactly how individual nodes behave and how they are coupled. Here we build a framework to quantify the sensitivity of complex networked system of coupled dynamic units. We characterize necessary and sufficient conditions for the emergence of new macroscopic dynamic behavior in the thermodynamic limit. We prove that these conditions are satisfied only for architectures with power-law degree distributions. Surprisingly, we find that highly connected nodes (i.e. hubs) only dominate the sensitivity of the network up to certain critical frequency."
"Despite the highest classification accuracy in wide varieties of application areas, artificial neural network has one disadvantage. The way this Network comes to a decision is not easily comprehensible. The lack of explanation ability reduces the acceptability of neural network in data mining and decision system. This drawback is the reason why researchers have proposed many rule extraction algorithms to solve the problem. Recently, Deep Neural Network (DNN) is achieving a profound result over the standard neural network for classification and recognition problems. It is a hot machine learning area proven both useful and innovative. This paper has thoroughly reviewed various rule extraction algorithms, considering the classification scheme: decompositional, pedagogical, and eclectics. It also presents the evaluation of these algorithms based on the neural network structure with which the algorithm is intended to work. The main contribution of this review is to show that there is a limited study of rule extraction algorithm from DNN."
"Coquand's cubical set model for homotopy type theory provides the basis for a computational interpretation of the univalence axiom and some higher inductive types, as implemented in the cubical proof assistant. This paper contributes to the understanding of this model. We make three contributions:   1. Johnstone's topological topos was created to present the geometric realization of simplicial sets as a geometric morphism between toposes. Johnstone shows that simplicial sets classify strict linear orders with disjoint endpoints and that (classically) the unit interval is such an order. Here we show that it can also be a target for cubical realization by showing that Coquand's cubical sets classify the geometric theory of flat distributive lattices. As a side result, we obtain a simplicial realization of a cubical set.   2. Using the internal `interval' in the topos of cubical sets, we construct a Moore path model of identity types.   3. We construct a premodel structure internally in the cubical type theory and hence on the fibrant objects in cubical sets."
"In animal monitoring applications, both animal detection and their movement prediction are major tasks. While a variety of animal monitoring strategies exist, most of them rely on mounting devices. However, in real world, it is difficult to find these animals and install mounting devices. In this paper, we propose an animal monitoring application by utilizing wireless sensor networks (WSNs) and unmanned aerial vehicle (UAV). The objective of the application is to detect locations of endangered species in large-scale wildlife areas and monitor movement of animals without any attached devices. In this application, sensors deployed throughout the observation area are responsible for gathering animal information. The UAV flies above the observation area and collects the information from sensors. To achieve the information efficiently, we propose a path planning approach for the UAV based on a Markov decision process (MDP) model. The UAV receives a certain amount of reward from an area if some animals are detected at that location. We solve the MDP using Q-learning such that the UAV prefers going to those areas that animals are detected before. Meanwhile, the UAV explores other areas as well to cover the entire network and detects changes in the animal positions. We first define the mathematical model underlying the animal monitoring problem in terms of the value of information (VoI) and rewards. We propose a network model including clusters of sensor nodes and a single UAV that acts as a mobile sink and visits the clusters. Then, one MDP-based path planning approach is designed to maximize the VoI while reducing message delays. The effectiveness of the proposed approach is evaluated using two real-world movement datasets of zebras and leopard. Simulation results show that our approach outperforms greedy, random heuristics and the path planning based on the traveling salesman problem."
"We present new versions of the previously published C and CUDA programs for solving the dipolar Gross-Pitaevskii equation in one, two, and three spatial dimensions, which calculate stationary and non-stationary solutions by propagation in imaginary or real time. Presented programs are improved and parallelized versions of previous programs, divided into three packages according to the type of parallelization. First package contains improved and threaded version of sequential C programs using OpenMP. Second package additionally parallelizes three-dimensional variants of the OpenMP programs using MPI, allowing them to be run on distributed-memory systems. Finally, previous three-dimensional CUDA-parallelized programs are further parallelized using MPI, similarly as the OpenMP programs. We also present speedup test results obtained using new versions of programs in comparison with the previous sequential C and parallel CUDA programs. The improvements to the sequential version yield a speedup of 1.1 to 1.9, depending on the program. OpenMP parallelization yields further speedup of 2 to 12 on a 16-core workstation, while OpenMP/MPI version demonstrates a speedup of 11.5 to 16.5 on a computer cluster with 32 nodes used. CUDA/MPI version shows a speedup of 9 to 10 on a computer cluster with 32 nodes."
"Link prediction aims to uncover the underlying relationship behind networks, which could be utilized to predict the missing edges or identify the spurious edges, and attracts much attention from various fields. The key issue of link prediction is to estimate the likelihood of two nodes in networks. Most current approaches of link prediction base on static structural analysis and ignore the temporal aspects of evolving networks. Unlike previous work, in this paper, we propose a popularity based structural perturbation method (PBSPM) that characterizes the similarity of an edge not only from existing connections of networks, but also from the popularity of its two endpoints, since popular nodes have much more probability to form links between themselves. By taking popularity of nodes into account, PBSPM could suppress nodes that have high importance, but gradually become inactive. Therefore the proposed method is inclined to predict potential edges between active nodes, rather than edges between inactive nodes. Experimental results on four real networks show that the proposed method outperforms the state-of-the-art methods both in accuracy and robustness in evolving networks."
"Several probabilistic models from high-dimensional statistics and machine learning reveal an intriguing --and yet poorly understood-- dichotomy. Either simple local algorithms succeed in estimating the object of interest, or even sophisticated semi-definite programming (SDP) relaxations fail.   In order to explore this phenomenon, we study a classical SDP relaxation of the minimum graph bisection problem, when applied to Erd\H{o}s-Renyi random graphs with bounded average degree $d>1$, and obtain several types of results. First, we use a dual witness construction (using the so-called non-backtracking matrix of the graph) to upper bound the SDP value. Second, we prove that a simple local algorithm approximately solves the SDP to within a factor $2d^2/(2d^2+d-1)$ of the upper bound. In particular, the local algorithm is at most $8/9$ suboptimal, and $1+O(1/d)$ suboptimal for large degree.   We then analyze a more sophisticated local algorithm, which aggregates information according to the harmonic measure on the limiting Galton-Watson (GW) tree. The resulting lower bound is expressed in terms of the conductance of the GW tree and matches surprisingly well the empirically determined SDP values on large-scale Erd\H{o}s-Renyi graphs.   We finally consider the planted partition model. In this case, purely local algorithms are known to fail, but they do succeed if a small amount of side information is available. Our results imply quantitative bounds on the threshold for partial recovery using SDP in this model."
"T-junctions support merging or spreading out feature lines. This paper develops a new way to create smooth piecewise polynomial free-form spline surfaces that include T-junctions. The construction is based on varying the parameterization and therefore does not require the non-local coordination of knot intervals. Conversely, a simple example shows that some T-junctions do not admit C 1 hierarchical splines and T-splines in particular. Our recommended G 1 cap of the T-junction consists of two pieces of degree bi-4 framed by bi-cubic splines. Numerous experiments show good highlight line distribution where alternatives, such as Catmull-Clark subdivision, fail and hierarchical splines do not apply."
"Over the past decade, online social media has had a tremendous impact on the way people engage in social activism. For instance, about 26M Facebook users expressed their support in upholding the cause of marriage equality by overlaying their profile pictures with rainbow-colored filters. Similarly, hundreds of thousands of users changed their profile pictures to a black dot condemning incidents of sexual violence in India. This act of demonstrating support for social causes by changing online profile pictures is being referred to as pictivism. In this paper, we analyze the psycho-demographic profiles, social networking behavior, and personal interests of users who participated in the Facebook Rainbow campaign. Our study is based on a sample of about 800K detailed profiles of Facebook users combining questionnaire-based psychological scores with Facebook profile data. Our analysis provides detailed insights into psycho-demographic profiles of the campaign participants. We found that personality traits such as openness and neuroticism are both positively associated with the likelihood of supporting the campaign, while conscientiousness exhibited a negative correlation. We also observed that females, religious disbelievers, democrats and adults in the age group of 20 to 30 years are more likely to be a part of the campaign. Our research further confirms the findings of several previous studies which suggest that a user is more likely to participate in an online campaign if a large fraction of his/her friends are already doing so. We also developed machine learning models for predicting campaign participation. Users' personal interests, approximated by Facebook user like activity, turned out to be the best indicator of campaign participation. Our results demonstrated that a predictive model which leverages the aforementioned features accurately identifies campaign participants (AUC=0.76)."
"End-to-end attention-based models have been shown to be competitive alternatives to conventional DNN-HMM models in the Speech Recognition Systems. In this paper, we extend existing end-to-end attention-based models that can be applied for Distant Speech Recognition (DSR) task. Specifically, we propose an end-to-end attention-based speech recognizer with multichannel input that performs sequence prediction directly at the character level. To gain a better performance, we also incorporate Highway long short-term memory (HLSTM) which outperforms previous models on AMI distant speech recognition task."
Achieving safe control under uncertainty is a key problem that needs to be tackled for enabling real-world autonomous robots and cyber-physical systems. This paper introduces Probabilistic Safety Programs (PSP) that embed both the uncertainty in the environment as well as invariants that determine safety parameters. The goal of these PSPs is to evaluate future actions or trajectories and determine how likely it is that the system will stay safe under uncertainty. We propose to perform these evaluations by first compiling the PSP to a graphical model then using a fast variational inference algorithm. We highlight the efficacy of the framework on the task of safe control of quadrotors and autonomous vehicles in dynamic environments.
"Crowdsourcing is the primary means to generate training data at scale, and when combined with sophisticated machine learning algorithms, crowdsourcing is an enabler for a variety of emergent automated applications impacting all spheres of our lives. This paper surveys the emerging field of formally reasoning about and optimizing open-ended crowdsourcing, a popular and crucially important, but severely understudied class of crowdsourcing---the next frontier in crowdsourced data management. The underlying challenges include distilling the right answer when none of the workers agree with each other, teasing apart the various perspectives adopted by workers when answering tasks, and effectively selecting between the many open-ended operators appropriate for a problem. We describe the approaches that we've found to be effective for open-ended crowdsourcing, drawing from our experiences in this space."
"In many security and healthcare systems a sequence of features/sensors/tests are used for detection and diagnosis. Each test outputs a prediction of the latent state, and carries with it inherent costs. Our objective is to {\it learn} strategies for selecting tests to optimize accuracy \& costs. Unfortunately it is often impossible to acquire in-situ ground truth annotations and we are left with the problem of unsupervised sensor selection (USS). We pose USS as a version of stochastic partial monitoring problem with an {\it unusual} reward structure (even noisy annotations are unavailable). Unsurprisingly no learner can achieve sublinear regret without further assumptions. To this end we propose the notion of weak-dominance. This is a condition on the joint probability distribution of test outputs and latent state and says that whenever a test is accurate on an example, a later test in the sequence is likely to be accurate as well. We empirically verify that weak dominance holds on real datasets and prove that it is a maximal condition for achieving sublinear regret. We reduce USS to a special case of multi-armed bandit problem with side information and develop polynomial time algorithms that achieve sublinear regret."
"The number of optimization techniques in the combinatorial domain is large and diversified. Nevertheless, there is still a lack of real benchmarks to validate optimization algorithms. In this work we introduce VRPBench, a tool to create instances and visualize solutions to the Vehicle Routing Problem (VRP) in a planar graph embedded in the Euclidean 2D space. We use VRPBench to model a real-world mail delivery case of the city of Artur Nogueira. Such scenarios were characterized as a multi-objective optimization of the VRP. We extracted a weighted graph from a digital map of the city to create a challenging benchmark for the VRP. Each instance models one generic day of mail delivery with hundreds to thousands of delivery points, thus allowing both the comparison and validation of optimization algorithms for routing problems."
"In this paper, we introduce two indoor Wireless Local Area Network (WLAN) positioning methods using augmented sparse recovery algorithms. These schemes render a sparse user's position vector, and in parallel, minimize the distance between the online measurement and radio map. The overall localization scheme for both methods consists of three steps: 1) coarse localization, obtained from comparing the online measurements with clustered radio map. A novel graph-based method is proposed to cluster the offline fingerprints. In the online phase, a Region Of Interest (ROI) is selected within which we search for the user's location; 2) Access Point (AP) selection; and 3) fine localization through the novel sparse recovery algorithms. Since the online measurements are subject to inordinate measurement readings, called outliers, the sparse recovery methods are modified in order to jointly estimate the outliers and user's position vector. The outlier detection procedure identifies the APs whose readings are either not available or erroneous. The proposed localization methods have been tested with Received Signal Strength (RSS) measurements in a typical office environment and the results show that they can localize the user with significantly high accuracy and resolution which is superior to the results from competing WLAN fingerprinting localization methods."
"This paper introduces novel schemes for indoor localization, outlier detection, and radio map interpolation using Wireless Local Area Networks (WLANs). The localization method consists of a novel multicomponent optimization technique that minimizes the squared $\ell_{2}$-norm of the residuals between the radio map and the online Received Signal Strength (RSS) measurements, the $\ell_{1}$-norm of the user's location vector, and weighted $\ell_{2}$-norms of layered groups of Reference Points (RPs). RPs are grouped using a new criterion based on the similarity between the so-called Access Point (AP) coverage vectors. In addition, since AP readings are prone to containing inordinate readings, called outliers, an augmented optimization problem is proposed to detect the outliers and localize the user with cleaned online measurements. Moreover, a novel scheme to record fingerprints from a smaller number of RPs and estimate the radio map at RPs without recorded fingerprints is developed using sparse recovery techniques. All localization schemes are tested on RSS fingerprints collected from a real environment. The overall scheme has comparable complexity with competing approaches, while it performs with high accuracy under a small number of APs and finer granularity of RPs."
"Wireless Local Area Network (WLAN) has become a promising choice for indoor positioning as the only existing and established infrastructure, to localize the mobile and stationary users indoors. However, since WLAN has been initially designed for wireless networking and not positioning, the localization task based on WLAN signals has several challenges. Amongst the WLAN positioning methods, WLAN fingerprinting localization has recently achieved great attention due to its promising results. WLAN fingerprinting faces several challenges and hence, in this paper, our goal is to overview these challenges and the state-of-the-art solutions. This paper consists of three main parts: 1) Conventional localization schemes; 2) State-of-the-art approaches; 3) Practical deployment challenges. Since all the proposed methods in WLAN literature have been conducted and tested in different settings, the reported results are not equally comparable. So, we compare some of the main localization schemes in a single real environment and assess their localization accuracy, positioning error statistics, and complexity. Our results depict illustrative evaluation of WLAN localization systems and guide to future improvement opportunities."
"At the tipping point, it is known that small incident can trigger dramatic societal shift. Getting early-warning signals for such changes are valuable to avoid detrimental outcomes such as riots or collapses of nations. However, it is notoriously hard to capture the processes of such transitions in the real-world. Here, we demonstrate the occurrence of a major shift in public opinion in the form of political support. Instead of simple swapping of ruling parties, we study the regime shift of a party popularity based on its attractiveness by examining the American presidential elections during 1980-2012. A single irreversible transition is detected in 1991. Once a transition happens, recovery to the original level of attractiveness does not bring popularity of the political party back. Remarkably, this transition is corroborated by tell-tale early-warning signature of critical slowing down. Our approach is applicable to shifts in public attitude within any social system."
"With the seamless coverage of wireless cellular networks in modern society, it is interesting to consider the shape of wireless cellular coverage. Is the shape a regular hexagon, an irregular polygon, or another complex geometrical shape? Based on fractal theory, the statistical characteristic of the wireless cellular coverage boundary is determined by the measured wireless cellular data collected from Shanghai, China. The measured results indicate that the wireless cellular coverage boundary presents an extremely irregular geometrical shape, which is also called a statistical fractal shape. Moreover, the statistical fractal characteristics of the wireless cellular coverage boundary have been validated by values of the Hurst parameter estimated in angular scales. The statistical fractal characteristics of the wireless cellular coverage boundary can be used to evaluate and design the handoff scheme of mobile user terminals in wireless cellular networks."
"To have a robot actively supporting a human during a collaborative task, it is crucial that robots are able to identify the current action in order to predict the next one. Common approaches make use of high-level knowledge, such as object affordances, semantics or understanding of actions in terms of pre- and post-conditions. These approaches often require hand-coded a priori knowledge, time- and resource- intensive or supervised learning techniques. We propose to reframe this problem as an appearance- based place recognition problem. In our framework, we regard sequences of visual images of human actions as a map in analogy to the visual place recognition problem. Observing the task for the second time, our approach is able to recognize pre-observed actions in a one-shot learning approach and is thereby able to recognize the current observation in the task space. We propose two new methods for creating and aligning action observations within a task map. We compare and verify our approaches with real data of humans assembling an IKEA flat pack drawer."
"This article introduces a Tensor Network Kalman filter, which can estimate state vectors that are exponentially large without ever having to explicitly construct them. The Tensor Network Kalman filter also easily accommodates the case where several different state vectors need to be estimated simultaneously. The key lies in rewriting the standard Kalman equations as tensor equations and then implementing them using Tensor Networks, which effectively transforms the exponential storage cost and computational complexity into a linear one. We showcase the power of the proposed framework through an application in recursive nonlinear system identification of high-order discrete-time multiple-input multiple-output (MIMO) Volterra systems. The identification problem is transformed into a linear state estimation problem wherein the state vector contains all Volterra kernel coefficients and is estimated using the Tensor Network Kalman filter. The accuracy and robustness of the scheme are demonstrated via numerical experiments, which show that updating the Kalman filter estimate of a state vector of length $10^9$ and its covariance matrix takes about 0.007s on a standard desktop computer in Matlab."
"Hierarchical modulation (HM) is able to provide different levels of protection for data streams and achieve a rate region that cannot be realized by traditional orthogonal schemes, such as time division (TD). Nevertheless, criterions and algorithms for general HM design are not available in existing literatures. In this paper, we jointly optimize the constellation positions and binary labels for HM to be used in additive white gaussian noise (AWGN) channel. Based on bit-interleaved coded modulation (BICM) with successive interference cancellation (SIC) capacity, our main purpose is to maximize the rate of one data stream, with power constrains and the constrain that the rate of other data streams should be larger than given thresholds. Multi-start interior-point algorithm is used to carry out the constellation optimization problems and methods to reduce optimization complexity are also proposed in this paper. Numerical results verify the performance gains of optimized HM compared with optimized quadrature amplidude modulation (QAM) based HM and other orthogonal transmission methods."
"When delegations to an assembly or council represent differently sized constituencies, they are often allocated voting weights which increase in population numbers (EU Council, US Electoral College, etc.). The Penrose square root rule (PSRR) is the main benchmark for fair representation of all bottom-tier voters in the top-tier decision making body, but rests on the restrictive assumption of independent binary decisions. We consider intervals of alternatives with single-peaked preferences instead, and presume positive correlation of local voters. This calls for a replacement of the PSRR by a linear Shapley rule: representation is fair if the Shapley value of the delegates is proportional to their constituency sizes."
"To improve the performance of Linear Discriminant Analysis (LDA) for early detection of diseases using Electronic Health Records (EHR) data, we propose \TheName{} -- a novel framework for \emph{\underline{E}HR based \underline{E}arly \underline{D}etection of \underline{D}iseases} on top of \emph{Covariance-Regularized} LDA models. Specifically, \TheName\ employs a \emph{non-sparse} inverse covariance matrix (or namely precision matrix) estimator derived from graphical lasso and incorporates the estimator into LDA classifiers to improve classification accuracy. Theoretical analysis on \TheName\ shows that it can bound the expected error rate of LDA classification, under certain assumptions. Finally, we conducted extensive experiments using a large-scale real-world EHR dataset -- CHSN. We compared our solution with other regularized LDA and downstream classifiers. The result shows \TheName\ outperforms all baselines and backups our theoretical analysis."
The problem of makespan optimal solving of cooperative path finding (CPF) is addressed in this paper. The task in CPF is to relocate a group of agents in a non-colliding way so that each agent eventually reaches its goal location from the given initial location. The abstraction adopted in this work assumes that agents are discrete items moving in an undirected graph by traversing edges. Makespan optimal solving of CPF means to generate solutions that are as short as possi-ble in terms of the total number of time steps required for the execution of the solution.   We show that reducing CPF to propositional satisfiability (SAT) represents a viable option for obtaining makespan optimal solutions. Several encodings of CPF into propositional formulae are suggested and experimentally evaluated. The evaluation indicates that SAT based CPF solving outperforms other makespan optimal methods significantly in highly constrained situations (environments that are densely occupied by agents).
"Mobile applications and on-body devices are becoming increasingly ubiquitous tools for physical activity tracking. We propose utilizing a self-tracker's habits to support continuous prediction of whether they will reach their daily step goal, thus enabling a variety of potential persuasive interventions. Our aim is to improve the prediction by leveraging historical data and other qualitative (motivation for using the systems, location, gender) and, quantitative (age) features. We have collected datasets from two activity tracking platforms (Moves and Fitbit) and aim to check if the model we derive from one is generalizable over the other. In the following paper we establish a pipeline for extracting the data and formatting it for modeling. We discuss the approach we took and our findings while selecting the features and classification models for the dataset. We further discuss the notion of generalizability of the model across different types of dataset and the probable inclusion of non standard features to further improve the model's accuracy."
"The language that we produce reflects our personality, and various personal and demographic characteristics can be detected in natural language texts. We focus on one particular personal trait, gender, and study how it is manifested in original texts and in translations. We show that gender has a powerful, clear signal in originals, but this signal is obfuscated in human and machine translation. We then propose simple domain-adaptation techniques that help retain the original gender traits in the translation outcome, without harming the quality of the translation, thereby creating more personalized machine translation systems."
"Digital forensic evidence acquisition speed is traditionally limited by two main factors: the read speed of the storage device being investigated, i.e., the read speed of the disk, memory, remote storage, mobile device, etc.), and the write speed of the system used for storing the acquired data. Digital forensic investigators can somewhat mitigate the latter issue through the use of high-speed storage options, such as networked RAID storage, in the controlled environment of the forensic laboratory. However, traditionally, little can be done to improve the acquisition speed past its physical read speed from the target device itself. The protracted time taken for data acquisition wastes digital forensic experts' time, contributes to digital forensic investigation backlogs worldwide, and delays pertinent information from potentially influencing the direction of an investigation. In a remote acquisition scenario, a third contributing factor can also become a detriment to the overall acquisition time - typically the Internet upload speed of the acquisition system. This paper explores an alternative to the traditional evidence acquisition model through the leveraging of a forensic data deduplication system. The advantages that a deduplicated approach can provide over the current digital forensic evidence acquisition process are outlined and some preliminary results of a prototype implementation are discussed."
"Machine learning (ML) is believed to be an effective and efficient tool to build reliable prediction model or extract useful structure from an avalanche of data. However, ML is also criticized by its difficulty in interpretation and complicated parameter tuning. In contrast, visualization is able to well organize and visually encode the entangled information in data and guild audiences to simpler perceptual inferences and analytic thinking. But large scale and high dimensional data will usually lead to the failure of many visualization methods. In this paper, we close a loop between ML and visualization via interaction between ML algorithm and users, so machine intelligence and human intelligence can cooperate and improve each other in a mutually rewarding way. In particular, we propose ""transparent boosting tree (TBT)"", which visualizes both the model structure and prediction statistics of each step in the learning process of gradient boosting tree to user, and involves user's feedback operations to trees into the learning process. In TBT, ML is in charge of updating weights in learning model and filtering information shown to user from the big data, while visualization is in charge of providing a visual understanding of ML model to facilitate user exploration. It combines the advantages of both ML in big data statistics and human in decision making based on domain knowledge. We develop a user friendly interface for this novel learning method, and apply it to two datasets collected from real applications. Our study shows that making ML transparent by using interactive visualization can significantly improve the exploration of ML algorithms, give rise to novel insights of ML models, and integrates both machine and human intelligence."
"The rating score prediction is widely studied in recommender system, which predicts the rating scores of users on items through making use of the user-item interaction information. Besides the rating information between users and items, lots of additional information have been employed to promote recommendations, such as social relation and geographic location. Expenditure information on each transaction between users and items is widely available on e-commerce websites, often appearing next to the rating information, while there is seldom study on the correlation between expenditures and rating scores. In this paper, we first study their correlations in real data sets and propose the expenditure aware rating prediction problem. From the data sets crawled from a well-known social media platform Dianping in China, we find some insightful correlations between expenditures and rating scores: 1) transactions or experiences with higher expenditures usually lead to higher rating scores; 2) when the real expenditures are higher than users' normal spending behavior, the users usually give higher scores; and 3) there are multiple grades of expenditure behaviors. Based on these three observations, we propose an Expenditure ware RatingPrediction method (EARP), based on low-rank matrix factorization, to effectively incorporate the expenditure information. Extensive experiments on five real data sets show that EARP not only always outperforms other state-of-the-art baselines but also discovers the latent characteristics of users and businesses."
"The automatic analysis of the surgical process, from videos recorded during surgeries, could be very useful to surgeons, both for training and for acquiring new techniques. The training process could be optimized by automatically providing some targeted recommendations or warnings, similar to the expert surgeon's guidance. In this paper, we propose to reuse videos recorded and stored during cataract surgeries to perform the analysis. The proposed system allows to automatically recognize, in real time, what the surgeon is doing: what surgical phase or, more precisely, what surgical step he or she is performing. This recognition relies on the inference of a multilevel statistical model which uses 1) the conditional relations between levels of description (steps and phases) and 2) the temporal relations among steps and among phases. The model accepts two types of inputs: 1) the presence of surgical tools, manually provided by the surgeons, or 2) motion in videos, automatically analyzed through the Content Based Video retrieval (CBVR) paradigm. Different data-driven statistical models are evaluated in this paper. For this project, a dataset of 30 cataract surgery videos was collected at Brest University hospital. The system was evaluated in terms of area under the ROC curve. Promising results were obtained using either the presence of surgical tools ($A_z$ = 0.983) or motion analysis ($A_z$ = 0.759). The generality of the method allows to adapt it to any kinds of surgeries. The proposed solution could be used in a computer assisted surgery tool to support surgeons during the surgery."
In this note we determine the set of expansions such that a partial cube is planar if and only if it arises by a sequence of such expansions from a single vertex. This corrects a result of Peterin.
"This paper considers a sequential sensor scheduling and remote estimation problem with one sensor and one estimator. The sensor makes sequential observations about the state of an underlying memoryless stochastic process and makes a decision as to whether or not to send this measurement to the estimator. The sensor and the estimator have the common objective of minimizing expected distortion in the estimation of the state of the process, over a finite time horizon. The sensor is either charged a cost for each transmission or constrained on transmission times. As opposed to the prior work where communication between the sensor and the estimator was assumed to be perfect (noiseless), in this work an additive noise channel with fixed power constraint is considered; hence, the sensor has to encode its message before transmission. Under some technical assumptions, we obtain the optimal encoding and estimation policies in conjunction with the optimal transmission schedule. The impact of the presence of a noisy channel is analyzed numerically based on dynamic programming. This analysis yields some rather surprising results such as a phase transition phenomenon in the number of used transmission opportunities, which was not encountered in the noiseless communication setting."
"Electrical machines commonly consist of moving and stationary parts. The field simulation of such devices can be very demanding if the underlying numerical scheme is solely based on a domain discretization, such as in case of the Finite Element Method (FEM). Here, a coupling scheme based on FEM together with Boundary Element Methods (BEM) is presented that neither hinges on re-meshing techniques nor deals with a special treatment of sliding interfaces. While the numerics are certainly more involved the reward is obvious: The modeling costs decrease and the application engineer is provided with an easy-to-use, versatile, and accurate simulation tool."
"As a wide class of resource management problems in wireless communications are nonconvex and even NP-hard in many cases, finding globally optimal solutions to these problems is of little practical interest. Towards more pragmatic approaches, there is a rich literature on iterative methods aiming at finding a solution satisfying necessary optimality conditions to these problems. These approaches have been derived under several similar mathematical frameworks such as inner approximation algorithm, concave-convex procedure, majorization-minimization algorithm, and successive convex approximation (SCA). However, a large portion of existing algorithms arrive at a relatively generic program at each iteration, which is less computationally efficient compared to a more standard convex formulation. The purpose of this paper is to present useful transformations and approximations to deal with nonconvexity in wireless communications design problems. More specifically, the proposed formulations can approximate nonconvex problems by a series of second-order cone programs (SOCPs) in light of SCA framework. We revisit various design problems in wireless communications to demonstrate the advantages of the proposed idea. Theoretical analysis and numerical results show the superior efficiency in terms of computational cost of our proposed solutions compared to the existing ones."
"Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model with training data distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of utmost importance. In this paper, we propose two ways to reduce the uplink communication costs. The proposed methods are evaluated on the application of training a deep neural network to perform image classification. Our best approach reduces the upload communication required to train a reasonable model by two orders of magnitude."
"Complexity theory provides a wealth of complexity classes for analyzing the complexity of decision and counting problems. Despite the practical relevance of enumeration problems, the tools provided by complexity theory for this important class of problems are very limited. In particular, complexity classes analogous to the polynomial hierarchy and an appropriate notion of problem reduction are missing. In this work, we lay the foundations for a complexity theory of hard enumeration problems by proposing a hierarchy of complexity classes and by investigating notions of reductions for enumeration problems."
"Reconstructing weighted networks from partial information is necessary in many important circumstances, e.g. for a correct estimation of systemic risk. It has been shown that, in order to achieve an accurate reconstruction, it is crucial to reliably replicate the empirical degree sequence, which is however unknown in many realistic situations. More recently, it has been found that the knowledge of the degree sequence can be replaced by the knowledge of the strength sequence, which is typically accessible, complemented by that of the total number of links, thus considerably relaxing the observational requirements. Here we further relax these requirements and devise a procedure valid when even the the total number of links is unavailable. We assume that, apart from the heterogeneity induced by the degree sequence itself, the network is homogeneous, so that its link density can be estimated by sampling subsets of nodes with representative density. We show that the best way of sampling nodes is the random selection scheme, any other procedure being biased towards unrealistically large, or small, link density. We then introduce our core technique for reconstructing in detail both the topology and the link weights of the unknown network. When tested on real economic and financial data, our method achieves a remarkable accuracy and is very robust with respect to the nodes sampled, thus representing a reliable practical tool whenever the available topological information is restricted to a small subset of nodes."
"Consider a hypergraph $H_n^d$ where the vertices are points of the $d$-dimensional combinatorial cube $n^d$ and the edges are all sets of $n$ points such that they are in one line. We study the structure of the group of automorphisms of $H_n^d$, i.e., permutations of points of $n^d$ preserving the edges. In this paper we provide a complete characterization. Moreover, we consider the Colored Cube Isomorphism problem of deciding whether for two colorings of the vertices of $H_n^d$ there exists an automorphism of $H_n^d$ preserving the colors. We show that this problem is ${\sf GI}$-complete."
"This paper presents an asynchronous incremental aggregated gradient algorithm and its implementation in a parameter server framework for solving regularized optimization problems. The algorithm can handle both general convex (possibly non-smooth) regularizers and general convex constraints. When the empirical data loss is strongly convex, we establish linear convergence rate, give explicit expressions for step-size choices that guarantee convergence to the optimum, and bound the associated convergence factors. The expressions have an explicit dependence on the degree of asynchrony and recover classical results under synchronous operation. Simulations and implementations on commercial compute clouds validate our findings."
"This paper describes Team Delft's robot, which won the Amazon Picking Challenge 2016, including both the Picking and the Stowing competitions. The goal of the challenge is to automate pick and place operations in unstructured environments, specifically the shelves in an Amazon warehouse. Team Delft's robot is based on an industrial robot arm, 3D cameras and a customized gripper. The robot's software uses ROS to integrate off-the-shelf components and modules developed specifically for the competition, implementing Deep Learning and other AI techniques for object recognition and pose estimation, grasp planning and motion planning. This paper describes the main components in the system, and discusses its performance and results at the Amazon Picking Challenge 2016 finals."
"Network alignment is the problem of matching the nodes of two graphs, maximizing the similarity of the matched nodes and the edges between them. This problem is encountered in a wide array of applications - from biological networks to social networks to ontologies - where multiple networked data sources need to be integrated. Due to the difficulty of the task, an accurate alignment can rarely be found without human assistance. Thus, it is of great practical importance to develop network alignment algorithms that can optimally leverage experts who are able to provide the correct alignment for a small number of nodes. Yet, only a handful of existing works address this active network alignment setting. The majority of the existing active methods focus on absolute queries (""are nodes $a$ and $b$ the same or not?""), whereas we argue that it is generally easier for a human expert to answer relative queries (""which node in the set $\{b_1, \ldots, b_n\}$ is the most similar to node $a$?""). This paper introduces a novel relative-query strategy, TopMatchings, which can be applied on top of any network alignment method that constructs and solves a bipartite matching problem. Our method identifies the most informative nodes to query based on the top-$k$ matchings of a bipartite graph. We compare the proposed approach to several commonly-used query strategies and perform experiments on both synthetic and real-world datasets. Our matching-based strategy yields the highest overall performance, outperforming all the baseline methods by more than 15 percentage points in some cases."
"The aim of this work is to classify the aerospace structure defects detected by eddy current non-destructive testing. The proposed method is based on the assumption that the defect is bound to the reaction of the probe coil impedance during the test. Impedance plane analysis is used to extract a feature vector from the shape of the coil impedance in the complex plane, through the use of some geometric parameters. Shape recognition is tested with three different machine-learning based classifiers: decision trees, neural networks and Naive Bayes. The performance of the proposed detection system are measured in terms of accuracy, sensitivity, specificity, precision and Matthews correlation coefficient. Several experiments are performed on dataset of eddy current signal samples for aircraft structures. The obtained results demonstrate the usefulness of our approach and the competiveness against existing descriptors."
"This study concerns with the diagnosis of aerospace structure defects by applying a HPC parallel implementation of a novel learning algorithm, named U-BRAIN. The Soft Computing approach allows advanced multi-parameter data processing in composite materials testing. The HPC parallel implementation overcomes the limits due to the great amount of data and the complexity of data processing. Our experimental results illustrate the effectiveness of the U-BRAIN parallel implementation as defect classifier in aerospace structures. The resulting system is implemented on a Linux-based cluster with multi-core architecture."
"This paper studies the impact of different types of features applied to learning to re-rank questions in community Question Answering. We tested our models on two datasets released in SemEval-2016 Task 3 on ""Community Question Answering"". Task 3 targeted real-life Web fora both in English and Arabic. Our models include bag-of-words features (BoW), syntactic tree kernels (TKs), rank features, embeddings, and machine translation evaluation features. To the best of our knowledge, structural kernels have barely been applied to the question reranking task, where they have to model paraphrase relations. In the case of the English question re-ranking task, we compare our learning to rank (L2R) algorithms against a strong baseline given by the Google-generated ranking (GR). The results show that i) the shallow structures used in our TKs are robust enough to noisy data and ii) improving GR is possible, but effective BoW features and TKs along with an accurate model of GR features in the used L2R algorithm are required. In the case of the Arabic question re-ranking task, for the first time we applied tree kernels on syntactic trees of Arabic sentences. Our approaches to both tasks obtained the second best results on SemEval-2016 subtasks B on English and D on Arabic."
"Today's Internet utilizes a multitude of different protocols. While some of these protocols were first implemented and used and later documented, other were first specified and then implemented. Regardless of how protocols came to be, their definitions can contain traps that lead to insecure implementations or deployments. A classical example is insufficiently strict authentication requirements in a protocol specification. The resulting Misconfigurations, i.e., not enabling strong authentication, are common root causes for Internet security incidents. Indeed, Internet protocols have been commonly designed without security in mind which leads to a multitude of misconfiguration traps. While this is slowly changing, to strict security considerations can have a similarly bad effect. Due to complex implementations and insufficient documentation, security features may remain unused, leaving deployments vulnerable.   In this paper we provide a systematization of the security traps found in common Internet protocols. By separating protocols in four classes we identify major factors that lead to common security traps. These insights together with observations about end-user centric usability and security by default are then used to derive recommendations for improving existing and designing new protocols---without such security sensitive traps for operators, implementors and users."
"Since the first online demonstration of Neural Machine Translation (NMT) by LISA, NMT development has recently moved from laboratory to production systems as demonstrated by several entities announcing roll-out of NMT engines to replace their existing technologies. NMT systems have a large number of training configurations and the training process of such systems is usually very long, often a few weeks, so role of experimentation is critical and important to share. In this work, we present our approach to production-ready systems simultaneously with release of online demonstrators covering a large variety of languages (12 languages, for 32 language pairs). We explore different practical choices: an efficient and evolutive open-source framework; data preparation; network architecture; additional implemented features; tuning for production; etc. We discuss about evaluation methodology, present our first findings and we finally outline further work.   Our ultimate goal is to share our expertise to build competitive production systems for ""generic"" translation. We aim at contributing to set up a collaborative framework to speed-up adoption of the technology, foster further research efforts and enable the delivery and adoption to/by industry of use-case specific engines integrated in real production workflows. Mastering of the technology would allow us to build translation engines suited for particular needs, outperforming current simplest/uniform systems."
"Our approach is among the three best to tackle the M2CAI Workflow challenge. The latter consists in recognizing the operation phase for each frames of endoscopic videos. In this technical report, we compare several classification models and temporal smoothing methods. Our submitted solution is a fine tuned Residual Network-200 on 80% of the training set with temporal smoothing using simple temporal averaging of the predictions and a Hidden Markov Model modeling the sequence."
"Recent work on weighted model counting has been very successfully applied to the problem of probabilistic inference in Bayesian networks. The probability distribution is encoded into a Boolean normal form and compiled to a target language, in order to represent local structure expressed among conditional probabilities more efficiently. We show that further improvements are possible, by exploiting the knowledge that is lost during the encoding phase and incorporating it into a compiler inspired by Satisfiability Modulo Theories. Constraints among variables are used as a background theory, which allows us to optimize the Shannon decomposition. We propose a new language, called Weighted Positive Binary Decision Diagrams, that reduces the cost of probabilistic inference by using this decomposition variant to induce an arithmetic circuit of reduced size."
"Conceived in the early 1990s, Experience Replay (ER) has been shown to be a successful mechanism to allow online learning algorithms to reuse past experiences. Traditionally, ER can be applied to all machine learning paradigms (i.e., unsupervised, supervised, and reinforcement learning). Recently, ER has contributed to improving the performance of deep reinforcement learning. Yet, its application to many practical settings is still limited by the memory requirements of ER, necessary to explicitly store previous observations. To remedy this issue, we explore a novel approach, Online Contrastive Divergence with Generative Replay (OCD_GR), which uses the generative capability of Restricted Boltzmann Machines (RBMs) instead of recorded past experiences. The RBM is trained online, and does not require the system to store any of the observed data points. We compare OCD_GR to ER on 9 real-world datasets, considering a worst-case scenario (data points arriving in sorted order) as well as a more realistic one (sequential random-order data points). Our results show that in 64.28% of the cases OCD_GR outperforms ER and in the remaining 35.72% it has an almost equal performance, while having a considerably reduced space complexity (i.e., memory usage) at a comparable time complexity."
"In this paper we propose a causal analog to the purely observational Dynamic Bayesian Networks, which we call Dynamic Causal Networks. We provide a sound and complete algorithm for identification of Dynamic Causal Net- works, namely, for computing the effect of an intervention or experiment, based on passive observations only, whenever possible. We note the existence of two types of confounder variables that affect in substantially different ways the iden- tification procedures, a distinction with no analog in either Dynamic Bayesian Networks or standard causal graphs. We further propose a procedure for the transportability of causal effects in Dynamic Causal Network settings, where the re- sult of causal experiments in a source domain may be used for the identification of causal effects in a target domain."
"An important goal of online comparison shopping services is to ""convert"" a viewer from general product category pages (for example product groups such as ""smartphones"" or ""air-conditioners"") to detailed product pages and ultimately to order pages. Comparison shopping websites provide a familiar web interface as well as a chance for consumers to purchase items at competitive prices. In return for providing access to a large market of potential consumers, the comparison shopping service usually receives financial compensation for product clicks and orders. This study looked at 2.5 million product listing visits at price.com.hk to determine whether a modification in the way prices are displayed on general category pages resulted in more ""conversions"" to product detail pages. We found a statistically significant improvement over-all as a result of the new price display resulting in 3.6% more product clicks over all categories. Additional analysis showed that the effect is heterogeneous among different categories, and in a few cases there may be some categories negatively affected by the display modification."
"Information entropy has been proved to be an effective tool to quantify the structural importance of complex networks. In the previous work (Xu et al, 2016 \cite{xu2016}), we measure the contribution of a path in link prediction with information entropy. In this paper, we further quantify the contribution of a path with both path entropy and path weight, and propose a weighted prediction index based on the contributions of paths, namely Weighted Path Entropy (WPE), to improve the prediction accuracy in weighted networks. Empirical experiments on six weighted real-world networks show that WPE achieves higher prediction accuracy than three typical weighted indices."
Numerous automatic continuous emotion detection system studies have examined mostly use of videos and images containing individual person expressing emotions. This study examines the detection of spontaneous emotions in a group and crowd settings. Edge detection was used with a grid of lines superimposition to extract the features. The feature movement in terms of movement from the reference point was used to track across sequences of images from the color channel. Additionally the video data capturing was done on spontaneous emotions invoked by watching sports events from group of participants. The method was view and occlusion independent and the results were not affected by presence of multiple people chaotically expressing various emotions. The edge thresholds of 0.2 and grid thresholds of 20 showed the best accuracy results. The overall accuracy of the group emotion classifier was 70.9%.
"The goal of our research is to develop methods advancing automatic visual recognition. In order to predict the unique or multiple labels associated to an image, we study different kind of Deep Neural Networks architectures and methods for supervised features learning. We first draw up a state-of-the-art review of the Convolutional Neural Networks aiming to understand the history behind this family of statistical models, the limit of modern architectures and the novel techniques currently used to train deep CNNs. The originality of our work lies in our approach focusing on tasks with a low amount of data. We introduce different models and techniques to achieve the best accuracy on several kind of datasets, such as a medium dataset of food recipes (100k images) for building a web API, or a small dataset of satellite images (6,000) for the DSG online challenge that we've won. We also draw up the state-of-the-art in Weakly Supervised Learning, introducing different kind of CNNs able to localize regions of interest. Our last contribution is a framework, build on top of Torch7, for training and testing deep models on any visual recognition tasks and on datasets of any scale."
Moss\'e proved that primitive morphisms are recognizable. In this paper we give a computable upper bound for the constant of recognizability of such a morphism. This bound can be expressed only using the cardinality of the alphabet and the length of the longest image under the morphism of a letter.
"The growing trends of data centers over last decades including social networking, cloud-based applications and storage technologies enabled many advances to take place in the networking area. Recent changes imply continuous demand for bandwidth to manage the large amount of packetized traffic. Cluster switches and routers make the switching fabric in a Data Center Network (DCN) environment and provide interconnectivity between elements of the same DC and inter DCs. To handle the constantly variable loads, switches need deliver outstanding throughput along with resiliency and scalability for DCN requirements. Conventional DCN switches adopt crossbars or/and blocks of memories mounted in a multistage fashion (commonly 2-Tiers or 3-Tiers). However, current multistage switches, with their space-memory variants, are either too complex to implement, have poor performance, or not cost effective. We propose a novel and highly scalable multistage switch based on Networks-on-Chip (NoC) fabrics for DCNs. In particular, we describe a three-stage Clos packet-switch with a Round Robin packets dispatching scheme where each central stage module is based on a Unidirectional NoC (UDN), instead of the conventional single-hop crossbar. The design, referred to as Clos-UDN, overcomes shortcomings of traditional multistage architectures as it (i) Obviates the need for a complex and costly input modules, by means of few, yet simple, input FIFO queues. (ii) Avoids the need for a complex and synchronized scheduling process over a high number of input-output modules and/or port pairs. (iii) Provides speedup, load balancing and path-diversity thanks to a dynamic dispatching scheme as well as the NoC based fabric nature. Simulations show that the Clos-UDN outperforms some common multistage switches under a range of input traffics, making it highly appealing for ultra-high capacity DC networks."
"This paper presents a Deep convolutional network model for Identity-Aware Transfer (DIAT) of facial attributes. Given the source input image and the reference attribute, DIAT aims to generate a facial image (i.e., target image) that not only owns the reference attribute but also keep the same or similar identity to the input image. We develop a two-stage scheme to transfer the input image to each reference attribute label. A feed-forward transform network is first trained by combining perceptual identity-aware loss and GAN-based attribute loss, and a face enhancement network is then introduced to improve the visual quality. We further define perceptual identity loss on the convolutional feature maps of the attribute discriminator, resulting in a DIAT-A model. Our DIAT and DIAT-A models can provide a unified solution for several representative facial attribute transfer tasks such as expression transfer, accessory removal, age progression, and gender transfer. The experimental results validate their effectiveness. Even for some identity-related attribute (e.g., gender), our DIAT-A can obtain visually impressive results by changing the attribute while retaining most identity features of the source image."
"Channel estimation is of critical importance in millimeter-wave (mmWave) multiple-input multiple-output (MIMO) system. Due to the use of large antenna arrays, low-complexity mmWave specific channel estimation algorithms are required. In this paper, an auxiliary beam pair design is proposed to provide high-resolution estimates of the channel's angle-of-departure (AoD) and angle-of-arrival (AoA) for mmWave MIMO system. By performing an amplitude comparison with respect to each auxiliary beam pair, a set of ratio measures that characterize the channel's AoD and AoA are obtained by the receiver. Either the best ratio measure or the estimated AoD is quantized and fed back to the transmitter via a feedback channel. The proposed technique can be incorporated into control channel design to minimize initial access delay. Though the design principles are derived assuming a high-power regime, evaluation under more realistic assumption show that by employing the proposed method, good angle estimation performance is achieved under various signal-to-noise ratio levels and channel conditions."
"The use of a functional language to implement proof strategies as proof tactics in interactive theorem provers, often provides short, concise and elegant implementations. Whilst being elegant, the use of higher order features and combinator languages often results in a very procedural view of a strategy, which may deviate significantly from the high-level ideas behind it. This can make a tactic hard to understand and hence difficult to to debug and maintain for experts and non experts alike: one often has to tear apart complex combinations of lower level tactics manually in order to analyse a failure in the overall strategy. In an industrial technology transfer project, we have been working on porting a very large and complex proof tactic into PSGraph, a graphical language for representing proof strategies. The goal of this work is to improve understandability and maintainability of tactics. Motivated by some initial successes with this, we here extend PSGraph with additional features for development and debugging. Through the re-implementation and refactoring of several existing tactics, we demonstrates the advantages of PSGraph compared with a typical sentential tactic language with respect to debugging, readability and maintenance. In order to act as guidance for others, we give a fairly detailed comparison of the user experience with the two approaches. The paper is supported by a web page providing further details about the implementation as well as interactive illustrations of the examples."
"In this work, we study the optimal configuration of the physical layer in wireless networks by means of Semi-Markov Decision Process (SMDP) modeling. In particular, assume the physical layer is characterized by a set of potential operating points, with each point corresponding to a rate and reliability pair; for example, these pairs might be obtained through a now-standard diversity-vs-multiplexing trade-off characterization. Given the current network state (e.g., buffer occupancies), a Decision Maker (DM) needs to dynamically decide which operating point to use. The SMDP problem formulation allows us to choose from these pairs an optimal selection, which is expressed by a decision rule as a function of the number of awaiting packets in the source finite queue, channel state, size of the packet to be transmitted. We derive a general solution which covers various model configurations, including packet size distributions and varying channels. For the specific case of exponential transmission time, we analytically prove the optimal policy has a threshold structure. Numerical results validate this finding, as well as depict muti-threshold policies for time varying channels such as the Gilbert-Elliot channel"
"In this paper, we define a novel census signal temporal logic (CensusSTL) that focuses on the number of agents in different subsets of a group that complete a certain task specified by the signal temporal logic (STL). CensusSTL consists of an ""inner logic"" STL formula and an ""outer logic"" STL formula. We present a new inference algorithm to infer CensusSTL formulae from the trajectory data of a group of agents. We first identify the ""inner logic"" STL formula and then infer the subgroups based on whether the agents' behaviors satisfy the ""inner logic"" formula at each time point. We use two different approaches to infer the subgroups based on similarity and complementarity, respectively. The ""outer logic"" CensusSTL formula is inferred from the census trajectories of different subgroups. We apply the algorithm in analyzing data from a soccer match by inferring the CensusSTL formula for different subgroups of a soccer team."
"Short internet video clips like vines present a significantly wild distribution compared to traditional video datasets. In this paper, we focus on the problem of unsupervised action classification in wild vines using traditional labeled datasets. To this end, we use a data augmentation based simple domain adaptation strategy. We utilise semantic word2vec space as a common subspace to embed video features from both, labeled source domain and unlablled target domain. Our method incrementally augments the labeled source with target samples and iteratively modifies the embedding function to bring the source and target distributions together. Additionally, we utilise a multi-modal representation that incorporates noisy semantic information available in form of hash-tags. We show the effectiveness of this simple adaptation technique on a test set of vines and achieve notable improvements in performance."
"This paper investigates the downlink performance of K-tier heteregeneous cellular networks (HCNs) under general settings. First, Gaussian approximation bounds for the standardized aggregate wireless interference (AWI) in dense K-tier HCNs are obtained for when base stations (BSs) in each tier are distributed over the plane according to a spatial and general Poisson point process. The Kolmogorov-Smirnov (KS) distance is used to measure deviations of the distribution of the standardized AWI from the standard normal distribution. An explicit and analytical expression bounding the KS distance between these two distributions is obtained as a function of a broad range of network parameters such as per-tier transmission power levels, per-tier BS intensity, BS locations, general fading statistics, and general bounded path-loss models. Bounds achieve a good statistical match between the standardized AWI distribution and its normal approximation even for moderately dense HCNs. Second, various spatial performance metrics of interest such as outage capacity, ergodic capacity and area spectral efficiency in the downlink of K-tier HCNs for general signal propogation models are investigated by making use of the derived distribution approximation results. Considering two specific BS association policies, it is shown that the derived performance bounds track the actual performance metrics reasonably well for a wide range of BS intensities, with the gap among them becoming negligibly small for denser HCN deployments."
"We provide conditions under which trajectory outcomes in mechanical systems subject to unilateral constraints depend piecewise-differentiably on initial conditions, even as the sequence of constraint activations and deactivations varies. This builds on prior work that provided conditions ensuring existence, uniqueness, and continuity of trajectory outcomes, and extends previous differentiability results that applied only to fixed constraint (de)activation sequences. We discuss extensions of our result and implications for assessing stability and controllability."
"The mixing time of a graph is an important metric, which is not only useful in analyzing connectivity and expansion properties of the network, but also serves as a key parameter in designing efficient algorithms. We present an efficient distributed algorithm for computing the mixing time of undirected graphs. Our algorithm estimates the mixing time $\tau_s$ (with respect to a source node $s$) of any $n$-node undirected graph in $O(\tau_s \log n)$ rounds. Our algorithm is based on random walks and require very little memory and use lightweight local computations, and work in the CONGEST model. Hence our algorithm is scalable under bandwidth constraints and can be an helpful building block in the design of topologically aware networks."
"This paper describes an efficient approach to improve the accuracy of a named entity recognition system for Vietnamese. The approach combines regular expressions over tokens and a bidirectional inference method in a sequence labelling model. The proposed method achieves an overall $F_1$ score of 89.66% on a test set of an evaluation campaign, organized in late 2016 by the Vietnamese Language and Speech Processing (VLSP) community."
"Acoustic reflector localization is an important issue in audio signal processing, with direct applications in spatial audio, scene reconstruction, and source separation. Several methods have recently been proposed to estimate the 3D positions of acoustic reflectors given room impulse responses (RIRs). In this article, we categorize these methods as ""image-source reversion"", which localizes the image source before finding the reflector position, and ""direct localization"", which localizes the reflector without intermediate steps. We present five new contributions. First, an onset detector, called the clustered dynamic programming projected phase-slope algorithm, is proposed to automatically extract the time of arrival for early reflections within the RIRs of a compact microphone array. Second, we propose an image-source reversion method that uses the RIRs from a single loudspeaker. It is constructed by combining an image source locator (the image source direction and range (ISDAR) algorithm), and a reflector locator (using the loudspeaker-image bisection (LIB) algorithm). Third, two variants of it, exploiting multiple loudspeakers, are proposed. Fourth, we present a direct localization method, the ellipsoid tangent sample consensus (ETSAC), exploiting ellipsoid properties to localize the reflector. Finally, systematic experiments on simulated and measured RIRs are presented, comparing the proposed methods with the state-of-the-art. ETSAC generates errors lower than the alternative methods compared through our datasets. Nevertheless, the ISDAR-LIB combination performs well and has a run time 200 times faster than ETSAC."
"Vocalizations and less often gestures have been the object of linguistic research over decades. However, the development of a general theory of communication with human language as a particular case requires a clear understanding of the organization of communication through other means. Infochemicals are chemical compounds that carry information and are employed by small organisms that cannot emit acoustic signals of optimal frequency to achieve successful communication. Here the distribution of infochemicals across species is investigated when they are ranked by their degree or the number of species with which it is associated (because they produce or they are sensitive to it). The quality of the fit of different functions to the dependency between degree and rank is evaluated with a penalty for the number of parameters of the function. Surprisingly, a double Zipf (a Zipf distribution with two regimes with a different exponent each) is the model yielding the best fit although it is the function with the largest number of parameters. This suggests that the world wide repertoire of infochemicals contains a chemical nucleus shared by many species and reminiscent of the core vocabularies found for human language in dictionaries or large corpora."
"We revisit the method of Kirschenhofer, Prodinger and Tichy to calculate asymptotic expressions for the moments of number of comparisons used by the randomized quick sort algorithm."
"Function word adjacency networks (WANs) are used to study the authorship of plays from the Early Modern English period. In these networks, nodes are function words and directed edges between two nodes represent the likelihood of ordered co-appearance of the two words. For every analyzed play a WAN is constructed and these are aggregated to generate author profile networks. We first study the similarity of writing styles between Early English playwrights by comparing the profile WANs. The accuracy of using WANs for authorship attribution is then demonstrated by attributing known plays among six popular playwrights. The WAN method is shown to additionally outperform other frequency-based methods on attributing Early English plays. This high classification power is then used to investigate the authorship of anonymous plays. Moreover, WANs are shown to be reliable classifiers even when attributing collaborative plays. For several plays of disputed co- authorship, a deeper analysis is performed by attributing every act and scene separately, in which we both corroborate existing breakdowns and provide evidence of new assignments. Finally, the impact of genre on attribution accuracy is examined revealing that the genre of a play partially conditions the choice of the function words used in it."
"Computing partition functions, the normalizing constants of probability distributions, is often hard. Variants of importance sampling give unbiased estimates of a normalizer Z, however, unbiased estimates of the reciprocal 1/Z are harder to obtain. Unbiased estimates of 1/Z allow Markov chain Monte Carlo sampling of ""doubly-intractable"" distributions, such as the parameter posterior for Markov Random Fields or Exponential Random Graphs. We demonstrate how to construct unbiased estimates for 1/Z given access to black-box importance sampling estimators for Z. We adapt recent work on random series truncation and Markov chain coupling, producing estimators with lower variance and a higher percentage of positive estimates than before. Our debiasing algorithms are simple to implement, and have some theoretical and empirical advantages over existing methods."
"Conventional deep neural networks (DNN) for speech acoustic modeling rely on Gaussian mixture models (GMM) and hidden Markov model (HMM) to obtain binary class labels as the targets for DNN training. Subword classes in speech recognition systems correspond to context-dependent tied states or senones. The present work addresses some limitations of GMM-HMM senone alignments for DNN training. We hypothesize that the senone probabilities obtained from a DNN trained with binary labels can provide more accurate targets to learn better acoustic models. However, DNN outputs bear inaccuracies which are exhibited as high dimensional unstructured noise, whereas the informative components are structured and low-dimensional. We exploit principle component analysis (PCA) and sparse coding to characterize the senone subspaces. Enhanced probabilities obtained from low-rank and sparse reconstructions are used as soft-targets for DNN acoustic modeling, that also enables training with untranscribed data. Experiments conducted on AMI corpus shows 4.6% relative reduction in word error rate."
"Understanding continuous human actions is a non-trivial but important problem in computer vision. Although there exists a large corpus of work in the recognition of action sequences, most approaches suffer from problems relating to vast variations in motions, action combinations, and scene contexts. In this paper, we introduce a novel method for semantic segmentation and recognition of long and complex manipulation action tasks, such as ""preparing a breakfast"" or ""making a sandwich"". We represent manipulations with our recently introduced ""Semantic Event Chain"" (SEC) concept, which captures the underlying spatiotemporal structure of an action invariant to motion, velocity, and scene context. Solely based on the spatiotemporal interactions between manipulated objects and hands in the extracted SEC, the framework automatically parses individual manipulation streams performed either sequentially or concurrently. Using event chains, our method further extracts basic primitive elements of each parsed manipulation. Without requiring any prior object knowledge, the proposed framework can also extract object-like scene entities that exhibit the same role in semantically similar manipulations. We conduct extensive experiments on various recent datasets to validate the robustness of the framework."
"We present a generic approximation of the packet error rate (PER) function of uncoded schemes in the AWGN channel using extreme value theory (EVT). The PER function can assume both the exponential and the Gaussian Q-function bit error rate (BER) forms. The EVT approach leads us to a best closed-form approximation, in terms of accuracy and computational efficiency, of the average PER in block-fading channels. The numerical analysis shows that the approximation holds tight for any value of SNR and packet length whereas the earlier studies approximate the average PER only at asymptotic SNRs and packet lengths."
"In the area of data classification, one of the prominent algorithms is the large margin nearest neighbor (LMNN) approach which is a metric learning to enhance the performance of the popular k-nearest neighbor classifier. In principles, LMNN learns a more efficient metric in the input space by using a linear mapping as the outcome of a convex optimization problem. However, one of the greatest weak points of LMNN is the strong reliance of its optimization paradigm on how the neighboring points are chosen. In this paper, it is mathematically proved for the first time that the regular way of choosing the target points can lead to non-feasible optimization conditions regardless of the number of chosen neighboring points. We present a mathematical approach to categorize the target points into feasible and infeasible exemplars, an also we provide a feasibility measure for preference of the target candidates. In our proposed Feasibility Based-LMNN algorithm, we use the above clue to construct the optimization problem based on the most promising general mapping directions in the input space. Our empirical results shows that via using the proposed FB-LMNN approach the optimization problem will converge in a better optimum point, and therefor leads to better classification on the well-known benchmark datasets."
"The boom in mobile apps has changed the traditional landscape of software development by introducing new challenges due to the limited resources of mobile devices, e.g., memory, CPU, network bandwidth and battery. The energy consumption of mobile apps is nowadays a hot topic and researchers are actively investigating the role of coding practices on energy efficiency. Recent studies suggest that design quality can conflict with energy efficiency. Therefore, it is important to take into account energy efficiency when evolving the design of a mobile app. The research community has proposed approaches to detect and remove anti-patterns (i.e., poor solutions to design and implementation problems) in software systems but, to the best of our knowledge, none of these approaches have included anti-patterns that are specific to mobile apps and--or considered the energy efficiency of apps. In this paper, we fill this gap in the literature by analyzing the impact of eight type of anti-patterns on a testbed of 59 android apps extracted from F-Droid. First, we (1) analyze the impact of anti-patterns in mobile apps with respect to energy efficiency; then (2) we study the impact of different types of anti-patterns on energy efficiency. We found that then energy consumption of apps containing anti-patterns and not (refactored apps) is statistically different. Moreover, we find that the impact of refactoring anti-patterns can be positive (7 type of anti-patterns) or negative (2 type of anti-patterns). Therefore, developers should consider the impact on energy efficiency of refactoring when applying maintenance activities."
"In this work we introduce a comprehensive algorithmic pipeline for multiple parametric model estimation. The proposed approach analyzes the information produced by a random sampling algorithm (e.g., RANSAC) from a machine learning/optimization perspective, using a \textit{parameterless} biclustering algorithm based on L1 nonnegative matrix factorization (L1-NMF). The proposed framework exploits consistent patterns that naturally arise during the RANSAC execution, while explicitly avoiding spurious inconsistencies. Contrarily to the main trends in the literature, the proposed technique does not impose non-intersecting parametric models. A new accelerated algorithm to compute L1-NMFs allows to handle medium-sized problems faster while also extending the usability of the algorithm to much larger datasets. This accelerated algorithm has applications in any other context where an L1-NMF is needed, beyond the biclustering approach to parameter estimation here addressed. We accompany the algorithmic presentation with theoretical foundations and numerous and diverse examples."
"Microbial fuel cells (MFCs) perform wastewater treatment and electricity production through the conversion of organic matter using microorganisms. For practical applications, it has been suggested that greater efficiency can be achieved by arranging multiple MFC units into physical stacks in a cascade with feedstock flowing sequentially between units. In this paper, we investigate the use of computational intelligence to physically explore and optimise (potentially) heterogeneous MFC designs in a cascade, i.e. without simulation. Conductive structures are 3-D printed and inserted into the anodic chamber of each MFC unit, augmenting a carbon fibre veil anode and affecting the hydrodynamics, including the feedstock volume and hydraulic retention time, as well as providing unique habitats for microbial colonisation. We show that it is possible to use design mining to identify new conductive inserts that increase both the cascade power output and power density."
"Modern software development teams are distributed across onsite and off-shore locations. Each team has developers with varying experience levels and English communication skills. In such a diverse development environment it is important to maintain the software quality, coding standards, timely delivery of features and bug fixes. It is also important to reduce testing effort, minimize side effects such as change in functionality, user experience or application performance. Code reviews are intended to control code quality. Unfortunately, many projects lack enforcement of processes and standards because of approaching deadlines, live production issues and lack of resource availability. This study examines a novel structured, unit testable templated code method to enforce code review standards with an intent to reduce coding effort, minimize revisions and eliminate functional and performance side effects on the system. The proposed method would also result in unit-testable code that can also be easily rolled back and increase team productivity. The baseline for traditional code review processes using metrics such as code review duration, bug regression rate, revision count was measured. These metrics were then compared with results from the proposed code review process that used structured unit testable templated code. The performance on 2 large enterprise level applications spanning over 2 years and 9 feature and maintenance release cycles was evaluated. The structured unit testable templated code method resulted in a decrease in total code review time, revision count and coding effort. It also decreased the number of live production issues caused by code churn or side effects of bug fix when compared to traditional code review process."
"The recently introduced Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) is an evolutionary algorithm capable of producing a large archive of diverse, high-performing solutions in a single run. It works by discretizing a continuous feature space into unique regions according to the desired discretization per dimension. While simple, this algorithm has a main drawback: it cannot scale to high-dimensional feature spaces since the number of regions increase exponentially with the number of dimensions. In this paper, we address this limitation by introducing a simple extension of MAP-Elites that has a constant, pre-defined number of regions irrespective of the dimensionality of the feature space. Our main insight is that methods from computational geometry could partition a high-dimensional space into well-spread geometric regions. In particular, our algorithm uses a centroidal Voronoi tessellation (CVT) to divide the feature space into a desired number of regions; it then places every generated individual in its closest region, replacing a less fit one if the region is already occupied. We demonstrate the effectiveness of the new ""CVT-MAP-Elites"" algorithm in high-dimensional feature spaces through comparisons against MAP-Elites in a hexapod locomotion task."
"We consider the configuration formation problem in modular robotic systems where a set of singleton modules that are spatially distributed in an environment are required to assume appropriate positions so that they can configure into a new, user-specified target configuration, while simultaneously maximizing the amount of information collected while navigating from their initial to final positions. Each module has a limited energy budget to expend while moving from its initial to goal location. To solve this problem, we propose a budget-limited, heuristic search-based algorithm that finds a path that maximizes the entropy of the expected information along the path. We have analytically proved that our proposed approach converges within finite time. Experimental results show that our planning approach has lower run-time than an auction-based allocation algorithm for selecting modules' spots."
"Probabilistic programming languages (PPLs) are a powerful modeling tool, able to represent any computable probability distribution. Unfortunately, probabilistic program inference is often intractable, and existing PPLs mostly rely on expensive, approximate sampling-based methods. To alleviate this problem, one could try to learn from past inferences, so that future inferences run faster. This strategy is known as amortized inference; it has recently been applied to Bayesian networks and deep generative models. This paper proposes a system for amortized inference in PPLs. In our system, amortization comes in the form of a parameterized guide program. Guide programs have similar structure to the original program, but can have richer data flow, including neural network components. These networks can be optimized so that the guide approximately samples from the posterior distribution defined by the original program. We present a flexible interface for defining guide programs and a stochastic gradient-based scheme for optimizing guide parameters, as well as some preliminary results on automatically deriving guide programs. We explore in detail the common machine learning pattern in which a 'local' model is specified by 'global' random values and used to generate independent observed data points; this gives rise to amortized local inference supporting global model learning."
"The last decade has seen great progress in both dynamic network modeling and topic modeling. This paper draws upon both areas to create a Bayesian method that allows topic discovery to inform the latent network model and the network structure to facilitate topic identification. We apply this method to the 467 top political blogs of 2012. Our results find complex community structure within this set of blogs, where community membership depends strongly upon the set of topics in which the blogger is interested."
"Parallel acquisition systems are employed successfully in a variety of different sensing applications when a single sensor cannot provide enough measurements for a high-quality reconstruction. In this paper, we consider compressed sensing (CS) for parallel acquisition systems when the individual sensors use subgaussian random sampling. Our main results are a series of uniform recovery guarantees which relate the number of measurements required to the basis in which the solution is sparse and certain characteristics of the multi-sensor system, known as sensor profile matrices. In particular, we derive sufficient conditions for optimal recovery, in the sense that the number of measurements required per sensor decreases linearly with the total number of sensors, and demonstrate explicit examples of multi-sensor systems for which this holds. We establish these results by proving the so-called Asymmetric Restricted Isometry Property (ARIP) for the sensing system and use this to derive both nonuniversal and universal recovery guarantees. Compared to existing work, our results not only lead to better stability and robustness estimates but also provide simpler and sharper constants in the measurement conditions. Finally, we show how the problem of CS with block-diagonal sensing matrices can be viewed as a particular case of our multi-sensor framework. Specializing our results to this setting leads to a new recovery guarantee depending only on the incoherence of the sparsity basis. This improves existing results in the literature and confirms the existence of deterministic sparsity bases for which CS with subgaussian block-diagonal matrices has a comparable recovery guarantee to CS with full subgaussian matrices."
"Users in social networks whose posts stay at the top of their followers'{} feeds the longest time are more likely to be noticed. Can we design an online algorithm to help them decide when to post to stay at the top? In this paper, we address this question as a novel optimal control problem for jump stochastic differential equations. For a wide variety of feed dynamics, we show that the optimal broadcasting intensity for any user is surprisingly simple -- it is given by the position of her most recent post on each of her follower's feeds. As a consequence, we are able to develop a simple and highly efficient online algorithm, RedQueen, to sample the optimal times for the user to post. Experiments on both synthetic and real data gathered from Twitter show that our algorithm is able to consistently make a user's posts more visible over time, is robust to volume changes on her followers' feeds, and significantly outperforms the state of the art."
"People are increasingly relying on the Web and social media to find solutions to their problems in a wide range of domains. In this online setting, closely related problems often lead to the same characteristic learning pattern, in which people sharing these problems visit related pieces of information, perform almost identical queries or, more generally, take a series of similar actions. In this paper, we introduce a novel modeling framework for clustering continuous-time grouped streaming data, the hierarchical Dirichlet Hawkes process (HDHP), which allows us to automatically uncover a wide variety of learning patterns from detailed traces of learning activity. Our model allows for efficient inference, scaling to millions of actions taken by thousands of users. Experiments on real data gathered from Stack Overflow reveal that our framework can recover meaningful learning patterns in terms of both content and temporal dynamics, as well as accurately track users' interests and goals over time."
"Monitoring the execution of business processes and activities composing them is an essential capability of Business Process Management (BPM) Suites. Human tasks are a particular type of business activities, and the understanding of their execution is essential in effectively managing both the processes and human resources. This paper proposes a monitoring framework with a capability to monitor and analyze the human tasks in a domain specific setting and contextually correlate the task execution patterns to the workload distribution on human users. The framework uses the notion of concept probes that match the business concepts used in definition of business processes. The proposed human task monitoring and contextual analysis (HTMCA) component considers multiple artifacts involved in the execution of a human task, rather than focusing only on classic activity/task metrics retrieved from BPM engines.This approach aspires to provide two main advantages to organizations using it. Firstly, it enhances the understanding of the workload of human users that participate in people-intensive business processes under various roles. Secondly, it gives organizations tools and insight for fine-tuning their user performance taking into account the specific context of their business various artifacts domains. The proposed framework builds on previous work that lays the basis of vendor-independent, concept-centric BPM monitoring, and provides the critical missing element of human task understanding. This has the potential to significantly benefit any BPM deployment and the validation work is in advanced stages of building a full prototype that demonstrates this value in a realistic industrial setting."
"This paper proposes a client-server decision tree learning method for outsourced private data. The privacy model is anatomization/fragmentation: the server sees data values, but the link between sensitive and identifying information is encrypted with a key known only to clients. Clients have limited processing and storage capability. Both sensitive and identifying information thus are stored on the server. The approach presented also retains most processing at the server, and client-side processing is amortized over predictions made by the clients. Experiments on various datasets show that the method produces decision trees approaching the accuracy of a non-private decision tree, while substantially reducing the client's computing resource requirements."
"State-of-the-art speech recognition systems typically employ neural network acoustic models. However, compared to Gaussian mixture models, deep neural network (DNN) based acoustic models often have many more model parameters, making it challenging for them to be deployed on resource-constrained platforms, such as mobile devices. In this paper, we study the application of the recently proposed highway deep neural network (HDNN) for training small-footprint acoustic models. HDNNs are a depth-gated feedforward neural network, which include two types of gate functions to facilitate the information flow through different layers. Our study demonstrates that HDNNs are more compact than regular DNNs for acoustic modeling, i.e., they can achieve comparable recognition accuracy with many fewer model parameters. Furthermore, HDNNs are more controllable than DNNs: the gate functions of an HDNN largely control the behavior of the whole network using a very small number of model parameters. Finally, we show that HDNNs are more adaptable than DNNs. For example, simply updating the gate functions using the adaptation data can result in considerable gains in accuracy. We demonstrate these aspects by experiments using the publicly available AMI corpus, which has around 80 hours of training data."
"Corporations are retaining ever-larger corpuses of personal data; the frequency or breaches and corresponding privacy impact have been rising accordingly. One way to mitigate this risk is through use of anonymized data, limiting the exposure of individual data to only where it is absolutely needed. This would seem particularly appropriate for data mining, where the goal is generalizable knowledge rather than data on specific individuals. In practice, corporate data miners often insist on original data, for fear that they might ""miss something"" with anonymized or differentially private approaches. This paper provides a theoretical justification for the use of anonymized data. Specifically, we show that a support vector classifier trained on anatomized data satisfying l-diversity should be expected to do as well as on the original data. Anatomy preserves all data values, but introduces uncertainty in the mapping between identifying and sensitive values, thus satisfying l-diversity. The theoretical effectiveness of the proposed approach is validated using several publicly available datasets, showing that we outperform the state of the art for support vector classification using training data protected by k-anonymity, and are comparable to learning on the original data."
"The idea of representation has been used in various fields of study from data analysis to political science. In this paper, we define representativeness and describe a method to isolate data points that can represent the entire data set. Also, we show how the minimum set of representative data points can be generated. We use data from GLOBE (a project to study the effects on Land Change based on a set of parameters that include temperature, forest cover, human population, atmospheric parameters and many other variables) to test & validate the algorithm. Principal Component Analysis (PCA) is used to reduce the dimensions of the multivariate data set, so that the representative points can be generated efficiently and its Representativeness has been compared against Random Sampling of points from the data set."
"We investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine whether the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference attack model to recognize differences in the target model's predictions on inputs that it trained on versus inputs that it did not use during training.   We empirically evaluate our inference techniques on classification models trained by commercial ""machine learning as a service"" providers such as Google and Amazon. Using realistic datasets and classification tasks, we show that these models can be significantly vulnerable to membership inference attacks."
"Network reliability is the probability that a dynamical system composed of discrete elements interacting on a network will be found in a configuration that satisfies a particular property. We introduce a new reliability property, Ising feasibility, for which the network reliability is the Ising model s partition function. As shown by Moore and Shannon, the network reliability can be separated into two factors: structural, solely determined by the network topology, and dynamical, determined by the underlying dynamics. In this case, the structural factor is known as the joint density of states. Using methods developed to approximate the structural factor for other reliability properties, we simulate the joint density of states, yielding an approximation for the partition function. Based on a detailed examination of why naive Monte Carlo sampling gives a poor approximation, we introduce a novel parallel scheme for estimating the joint density of states using a Markov chain Monte Carlo method with a spin exchange random walk. This parallel scheme makes simulating the Ising model in the presence of an external field practical on small computer clusters for networks with arbitrary topology with 10 to 6 energy levels and more than 10 to 308 microstates."
"This paper presents a novel robot vision architecture for perceiving generic 3D clothes configurations. Our architecture is hierarchically structured, starting from low-level curvatures, across mid-level geometric shapes \& topology descriptions; and finally approaching high-level semantic surface structure descriptions. We demonstrate our robot vision architecture in a customised dual-arm industrial robot with our self-designed, off-the-self stereo vision system, carrying out autonomous grasping and dual-arm flattening. It is worth noting that the proposed dual-arm flattening approach is unique among the state-of-the-art robot autonomous system, which is the major contribution of this paper. The experimental results show that the proposed dual-arm flattening using stereo vision system remarkably outperforms the single-arm flattening and widely-cited Kinect-based sensing system for dexterous manipulation tasks. In addition, the proposed grasping approach achieves satisfactory performance on grasping various kind of garments, verifying the capability of proposed visual perception architecture to be adapted to more than one clothing manipulation tasks."
"Mahaney's Theorem states that, assuming $\mathsf{P} \neq \mathsf{NP}$, no NP-hard set can have a polynomially bounded number of yes-instances at each input length. We give an exposition of a very simple unpublished proof of Manindra Agrawal whose ideas appear in Agrawal-Arvind (""Geometric sets of low information content,"" Theoret. Comp. Sci., 1996). This proof is so simple that it can easily be taught to undergraduates or a general graduate CS audience - not just theorists! - in about 10 minutes, which the author has done successfully several times. We also include applications of Mahaney's Theorem to fundamental questions that bright undergraduates would ask which could be used to fill the remaining hour of a lecture, as well as an application (due to Ikenmeyer, Mulmuley, and Walter, arXiv:1507.02955) to the representation theory of the symmetric group and the Geometric Complexity Theory Program. To this author, the fact that sparsity results on NP-complete sets have an application to classical questions in representation theory says that they are not only a gem of classical theoretical computer science, but indeed a gem of mathematics."
"Conventional imaging uses a set of lenses to form an image on the sensor plane. This pure hardware-based approach doesn't use any signal processing, nor the extra information in the time of arrival of photons to the sensor. Recently, modern compressive sensing techniques have been applied for lensless imaging. However, this computational approach tends to depend as much as possible on signal processing (for example, single pixel camera) and results in a long acquisition time. Here we propose using compressive ultrafast sensing for lensless imaging. We use extremely fast sensors (picosecond time resolution) to time tag photons as they arrive to an omnidirectional pixel. Thus, each measurement produces a time series where time is a function of the photon source location in the scene. This allows lensless imaging with significantly fewer measurements compared to regular single pixel imaging ($ 33 \times$ less measurements in our experiments). To achieve this goal, we developed a framework for using ultrafast pixels with compressive sensing, including an algorithm for ideal sensor placement, and an algorithm for optimized active illumination patterns. We show that efficient lensless imaging is possible with ultrafast imaging and compressive sensing. This paves the way for novel imaging architectures, and remote sensing in extreme situations where imaging with a lens is not possible."
"This paper addresses the design of robust dynamic output feedback control for highly uncertain systems in which the unknown disturbance might be excited by the derivative of the control input. This context appears in many industrial problems such as the speed control of the hydraulic turbines and the frequency stabilization in micro grids to cite but few examples. A key feature that has to be carefully addressed in this context is that too agressive feedback might lead to the loss of controllability and/or a significant drop in the closed-loop performance. The paper formulates the problem, underlines its relevance and gives a rigorous solution in which a dynamic output feedback is given together with a realistic set of sufficient conditions on the controller's parameters that enable to assess the behavior of the closed-loop under different circumstances. A numerical example is given to illustrate the relevance of the proposed successful design characterization."
"What is computable with limited resources? How can we verify the correctness of computations? How to measure computational power with precision? Despite the immense scientific and engineering progress in computing, we still have only partial answers to these questions. In order to make these problems more precise, we describe an abstract algebraic definition of classical computation, generalizing traditional models to semigroups. The mathematical abstraction also allows the investigation of different computing paradigms (e.g. cellular automata, reversible computing) in the same framework. Here we summarize the main questions and recent results of the research of finite computation."
"Semantic segmentation is challenging as it requires both object-level information and pixel-level accuracy. Recently, FCN-based systems gained great improvement in this area. Unlike classification networks, combining features of different layers plays an important role in these dense prediction models, as these features contains information of different levels. A number of models have been proposed to show how to use these features. However, what is the best architecture to make use of features of different layers is still a question. In this paper, we propose a module, called mixed context network, and show that our presented system outperforms most existing semantic segmentation systems by making use of this module."
"Extraction of concepts present in patient clinical records is an essential step in clinical research. The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for clinical records presented concept extraction (CE) task, with aim to identify concepts (such as treatments, tests, problems) and classify them into predefined categories. State-of-the-art CE approaches heavily rely on hand crafted features and domain specific resources which are hard to collect and tune. For this reason, this paper employs bidirectional LSTM with CRF decoding initialized with general purpose off-the-shelf word embeddings for CE. The experimental results achieved on 2010 i2b2/VA reference standard corpora using bidirectional LSTM CRF ranks closely with top ranked systems."
"We propose a Convolutional Neural Network (CNN) based algorithm - StuffNet - for object detection. In addition to the standard convolutional features trained for region proposal and object detection [31], StuffNet uses convolutional features trained for segmentation of objects and 'stuff' (amorphous categories such as ground and water). Through experiments on Pascal VOC 2010, we show the importance of features learnt from stuff segmentation for improving object detection performance. StuffNet improves performance from 18.8% mAP to 23.9% mAP for small objects. We also devise a method to train StuffNet on datasets that do not have stuff segmentation labels. Through experiments on Pascal VOC 2007 and 2012, we demonstrate the effectiveness of this method and show that StuffNet also significantly improves object detection performance on such datasets."
"Traditional learning approaches proposed for controlling quadrotors or helicopters have focused on improving performance for specific trajectories by iteratively improving upon a nominal controller, for example learning from demonstrations, iterative learning, and reinforcement learning. In these schemes, however, it is not clear how the information gathered from the training trajectories can be used to synthesize controllers for more general trajectories. Recently, the efficacy of deep learning in inferring helicopter dynamics has been shown. Motivated by the generalization capability of deep learning, this paper investigates whether a neural network based dynamics model can be employed to synthesize control for trajectories different than those used for training. To test this, we learn a quadrotor dynamics model using only translational and only rotational training trajectories, each of which can be controlled independently, and then use it to simultaneously control the yaw and position of a quadrotor, which is non-trivial because of nonlinear couplings between the two motions. We validate our approach in experiments on a quadrotor testbed."
"The realizability problem in requirements engineering is to decide existence of an implementation that meets the given formal requirements. A step forward after the realizability is proven is to construct such an implementation automatically, and thus solve the problem of program synthesis. In this paper, we propose a novel approach to pro- gram synthesis guided by the proofs of realizability represented by the set of valid forall-exists formulas. In particular, we propose to extract Skolem functions witnessing the existential quantification, and to compose the Skolem functions into an implementation that is guaranteed to comply with the user-defined requirements. We implemented the approach for requirements in the form of Assume-Guarantee contracts, using the Lustre specification language. It naturally extends the realizability check by the JKind model checker. Furthermore, we developed a compiler to translate pure Skolem-containing implementations to the C programming language. For a vast variety of models, we test their corresponding implementations against the ones provided by the LustreV6 compiler, yielding meaningful results."
"Network representations of systems from various scientific and societal domains are neither completely random nor fully regular, but instead appear to contain recurring structural building blocks. These features tend to be shared by networks belonging to the same broad class, such as the class of social networks or the class of biological networks. At a finer scale of classification within each such class, networks describing more similar systems tend to have more similar features. This occurs presumably because networks representing similar purposes or constructions would be expected to be generated by a shared set of domain specific mechanisms, and it should therefore be possible to classify these networks into categories based on their features at various structural levels. Here we describe and demonstrate a new, hybrid approach that combines manual selection of features of potential interest with existing automated classification methods. In particular, selecting well-known and well-studied features that have been used throughout social network analysis and network science and then classifying with methods such as random forests that are of special utility in the presence of feature collinearity, we find that we achieve higher accuracy, in shorter computation time, with greater interpretability of the network classification results."
"This paper presents a robust and accurate positioning system that adapts its behavior to the surrounding environment like the visual brain, mimicking its capability of filtering out clutter and focusing attention on activity and relevant information. Especially in indoor environments, which are characterized by harsh multipath propagation, it is still elusive to achieve the needed level of accuracy robustly under the constraint of reasonable infrastructural needs. In such environments it is essential to separate relevant from irrelevant information and attain an appropriate uncertainty model for measurements that are used for positioning."
"Recent advances of 3D acquisition devices have enabled large-scale acquisition of 3D scene data. Such data, if completely and well annotated, can serve as useful ingredients for a wide spectrum of computer vision and graphics works such as data-driven modeling and scene understanding, object detection and recognition. However, annotating a vast amount of 3D scene data remains challenging due to the lack of an effective tool and/or the complexity of 3D scenes (e.g. clutter, varying illumination conditions). This paper aims to build a robust annotation tool that effectively and conveniently enables the segmentation and annotation of massive 3D data. Our tool works by coupling 2D and 3D information via an interactive framework, through which users can provide high-level semantic annotation for objects. We have experimented our tool and found that a typical indoor scene could be well segmented and annotated in less than 30 minutes by using the tool, as opposed to a few hours if done manually. Along with the tool, we created a dataset of over a hundred 3D scenes associated with complete annotations using our tool. The tool and dataset are available at www.scenenn.net."
"This work is devoted to the construction of explicit feedback control laws for the robust, global, exponential stabilization of general, uncertain, discrete-time, acyclic traffic networks. We consider discrete-time, uncertain network models which satisfy very weak assumptions. The construction of the controllers and the rigorous proof of the robust, global, exponential stability for the closed-loop system are based on recently proposed vector-Lyapunov function criteria, as well as the fact that the network is acyclic. It is shown, in this study, that the latter requirement is necessary for the existence of a robust, global, exponential stabilizer of the desired uncongested equilibrium point of the network. An illustrative example demonstrates the applicability of the obtained results to realistic traffic flow networks."
"We propose a new method for assessing agents influence in network structures, which takes into consideration nodes attributes, individual and group influences of nodes, and the intensity of interactions. This approach helps us to identify both explicit and hidden central elements which cannot be detected by classical centrality measures or other indices."
"Given a large graph, the densest-subgraph problem asks to find a subgraph with maximum average degree. When considering the top-k version of this problem, a naive solution is to iteratively find the densest subgraph and remove it in each iteration. However, such a solution is impractical due to high processing cost. The problem is further complicated when dealing with dynamic graphs, since adding or removing an edge requires re-running the algorithm. In this paper, we study the top-k densest subgraph problem in the sliding-window model and propose an efficient fully-dynamic algorithm. The input of our algorithm consists of an edge stream, and the goal is to find the node-disjoint subgraphs that maximize the sum of their densities. In contrast to existing state-of-the-art solutions that require iterating over the entire graph upon any update, our algorithm profits from the observation that updates only affect a limited region of the graph. Therefore, the top-k densest subgraphs are maintained by only applying local updates. We provide a theoretical analysis of the proposed algorithm and show empirically that the algorithm often generates denser subgraphs than state-of-the-art competitors. Experiments show an improvement in efficiency of up to three to five orders of magnitude compared to state-of-the-art solutions."
"We propose a new class of determinantal point processes (DPPs) which can be manipulated for inference and parameter learning in potentially sublinear time in the number of items. This class, based on a specific low-rank factorization of the marginal kernel, is particularly suited to a subclass of continuous DPPs and DPPs defined on exponentially many items. We apply this new class to modelling text documents as sampling a DPP of sentences, and propose a conditional maximum likelihood formulation to model topic proportions, which is made possible with no approximation for our class of DPPs. We present an application to document summarization with a DPP on $2^{500}$ items."
"For most hyperspectral remote sensing applications, removing bad bands, such as water absorption bands, is a required preprocessing step. Currently, the commonly applied method is by visual inspection, which is very time-consuming and it is easy to overlook some noisy bands. In this study, we find an inherent connection between target detection algorithms and the corrupted band removal. As an example, for the matched filter (MF), which is the most widely used target detection method for hyperspectral data, we present an automatic MF-based algorithm for bad band identification. The MF detector is a filter vector, and the resulting filter output is the sum of all bands weighted by the MF coefficients. Therefore, we can identify bad bands only by using the MF filter vector itself, the absolute value of whose entry accounts for the importance of each band for the target detection. For a specific target of interest, the bands with small MF weights correspond to the noisy or bad ones. Based on this fact, we develop an automatic bad band preremoval algorithm by utilizing the average absolute value of MF weights for multiple targets within a scene. Experiments with three well known hyperspectral datasets show that our method can always identify the water absorption and other low signal-to-noise (SNR) bands that are usually chosen as bad bands manually."
"The nonlinearity of a Boolean function is a key property in deciding its suitability for cryptographic purposes, e.g. as a combining function in stream ciphers, and so the nonlinearity computation is an important problem for applications. Traditional methods to compute the nonlinearity are based on transforms, such as the Fast Walsh Transform. In 2007 Simonetti proposed a method to solve the above problem seen as a decision problem on the existence of solutions for some multivariate polynomial systems. Although novel as approach, her algorithm suffered from a direct application of Groebner bases and was thus impractical. We now propose two more practical approaches, one that determines the existence of solutions for Simonetti's systems in a faster way and another that writes similar systems but over fields with a different characteristics. For our algorithms we provide an efficient implementation in the software package MAGMA."
"Compared to former mobile networks, Long Term Evolution (LTE) offers higher transfer speeds, significantly lower latencies and a widespread availability, qualifying LTE for a wide range of different applications and services in the field of conventional Human-to-Human (H2H) as well as fast growing Vehicle-To-X (V2X) and Cyber Physical Systems (CPS) communications. As a result, a steady growth of mobile data traffic, which is reflected in an increased interaction between different traffic classes, can be observed. In order to ensure timely transmissions of time-critical data in the future, we propose the novel Payload-Size and Deadline-Aware (PayDA) scheduling approach and compare its performance regarding the compliance for deadlines with those of other common packet scheduling mechanisms. The performance analysis is done with the complex and open-source LTE simulation environment LTE-Sim. The results show that the average latency can be reduced by the factor of 20 and the mean data rates can be enhanced by a factor of about 3.5 for a high miscellaneous data traffic. In the case of a heavy homogeneous and time-critical data traffic the mean Deadline-Miss-Ratio (DMR) can be decreased by about 35%."
"By analyzing a unique dataset of more than 270,000 scientists, we discovered substantial gender differences in scientific collaborations. While men are more likely to collaborate with other men, women are more egalitarian. This is consistently observed over all fields and regardless of the number of collaborators a scientist has. The only exception is observed in the field of engineering, where this gender bias disappears with increasing number of collaborators. We also found that the distribution of the number of collaborators follows a truncated power law with a cut-off that is gender dependent and related to the gender differences in the number of published papers. Considering interdisciplinary research, our analysis shows that men and women behave similarly across fields, except in the case of natural sciences, where women with many collaborators are more likely to have collaborators from other fields."
"Based on API call sequences, semantic-aware and machine learning (ML) based malware classifiers can be built for malware detection or classification. Previous works concentrate on crafting and extracting various features from malware binaries, disassembled binaries or API calls via static or dynamic analysis and resorting to ML to build classifiers. However, they tend to involve too much feature engineering and fail to provide interpretability. We solve these two problems with the recent advances in deep learning: 1) RNN-based autoencoders (RNN-AEs) can automatically learn low-dimensional representation of a malware from its raw API call sequence. 2) Multiple decoders can be trained under different supervisions to give more information, other than the class or family label of a malware. Inspired by the works of document classification and automatic sentence summarization, each API call sequence can be regarded as a sentence. In this paper, we make the first attempt to build a multi-task malware learning model based on API call sequences. The model consists of two decoders, one for malware classification and one for $\emph{file access pattern}$ (FAP) generation given the API call sequence of a malware. We base our model on the general seq2seq framework. Experiments show that our model can give competitive classification results as well as insightful FAP information."
"In this work, a Bayesian approach to speaker normalization is proposed to compensate for the degradation in performance of a speaker independent speech recognition system. The speaker normalization method proposed herein uses the technique of vocal tract length normalization (VTLN). The VTLN parameters are estimated using a novel Bayesian approach which utilizes the Gibbs sampler, a special type of Markov Chain Monte Carlo method. Additionally the hyperparameters are estimated using maximum likelihood approach. This model is used assuming that human vocal tract can be modeled as a tube of uniform cross section. It captures the variation in length of the vocal tract of different speakers more effectively, than the linear model used in literature. The work has also investigated different methods like minimization of Mean Square Error (MSE) and Mean Absolute Error (MAE) for the estimation of VTLN parameters. Both single pass and two pass approaches are then used to build a VTLN based speech recognizer. Experimental results on recognition of vowels and Hindi phrases from a medium vocabulary indicate that the Bayesian method improves the performance by a considerable margin."
"In recent years there have been excellent results in Visual-Inertial Odometry techniques, which aim to compute the incremental motion of the sensor with high accuracy and robustness. However these approaches lack the capability to close loops, and trajectory estimation accumulates drift even if the sensor is continually revisiting the same place. In this work we present a novel tightly-coupled Visual-Inertial Simultaneous Localization and Mapping system that is able to close loops and reuse its map to achieve zero-drift localization in already mapped areas. While our approach can be applied to any camera configuration, we address here the most general problem of a monocular camera, with its well-known scale ambiguity. We also propose a novel IMU initialization method, which computes the scale, the gravity direction, the velocity, and gyroscope and accelerometer biases, in a few seconds with high accuracy. We test our system in the 11 sequences of a recent micro-aerial vehicle public dataset achieving a typical scale factor error of 1% and centimeter precision. We compare to the state-of-the-art in visual-inertial odometry in sequences with revisiting, proving the better accuracy of our method due to map reuse and no drift accumulation."
"We investigate the relationship between finite terms in {\lambda}-letrec, the {\lambda}-calculus with letrec, and the infinite {\lambda}-terms they express. We say that a lambda-letrec term expresses a lambda-term if the latter can be obtained as an infinite unfolding of the former. Unfolding is the process of substituting occurrences of function variables by the right-hand side of their definition.   We consider the following questions: (i) How can we characterise those infinite {\lambda}-terms that are {\lambda}-letrec-expressible? (ii) Given two {\lambda}-letrec terms, how can we determine whether they have the same unfolding? (iii) Given a {\lambda}-letrec term, can we find a more compact version of the term with the same unfolding? To tackle these questions we introduce and study the following formalisms: (i) a rewriting system for unfolding {\lambda}-letrec terms into {\lambda}-terms (ii) a rewriting system for `observing' {\lambda}-terms by dissecting their term structure (iii) higher-order and first-order graph formalisms together with translations between them as well as translations from and to {\lambda}-letrec.   We identify a first-order term graph formalism on which bisimulation preserves and reflects the unfolding semantics of {\lambda}-letrec and which is closed under functional bisimulation. From this we derive efficient methods to determine whether two terms are equivalent under infinite unfolding and to compute the maximally shared form of a given {\lambda}-letrec term."
"We consider load balancing in a network of caching servers delivering contents to end users. Randomized load balancing via the so-called power of two choices is a well-known approach in parallel and distributed systems that reduces network imbalance. In this paper, we propose a randomized load balancing scheme which simultaneously considers cache size limitation and proximity in the server redirection process.   Since the memory limitation and the proximity constraint cause correlation in the server selection process, we may not benefit from the power of two choices in general. However, we prove that in certain regimes, in terms of memory limitation and proximity constraint, our scheme results in the maximum load of order $\Theta(\log\log n)$ (here $n$ is the number of servers and requests), and at the same time, leads to a low communication cost. This is an exponential improvement in the maximum load compared to the scheme which assigns each request to the nearest available replica. Finally, we investigate our scheme performance by extensive simulations."
"In order to understand the underlying mechanisms that lead to certain network properties (i.e. scalability, energy efficiency) we apply a complex systems science approach to analyze clustering in Wireless Sensor Networks (WSN). We represent different implementations of clustering in WSNs with a functional topology graph. Different characteristics of the functional topology provide insight into the relationships between system parts that result in certain properties of the whole system. Moreover, we employ a complexity metric - functional complexity (C_F) - to explain how local interactions give rise to the global behavior of the network. Our analysis shows that higher values of C_F indicate higher scalability and lower energy efficiency."
"The Internet of Things (IoT) is a global ecosystem of information and communication technologies aimed at connecting any type of object (thing), at any time and in any place, to each other and to the Internet. One of the major problems associated with the IoT is maintaining security; the heterogeneous nature of such deployments poses a challenge to many aspects of security, including security testing and analysis. In addition, there is no existing mechanism that performs security testing for IoT devices in different contexts. In this paper, we propose an innovative security testbed framework targeted at IoT devices. The security testbed supports both standard and context-based security testing, with a set of security tests conducted under the different environmental conditions in which IoT devices operate. The requirements and architectural design of the proposed testbed are discussed, and the testbed operation is demonstrated in several testing scenarios."
"Fuzzy controllers are known to serve as efficient and interpretable system controllers for continuous state and action spaces. To date these controllers have been constructed by hand, or automatically trained either on expert generated problem specific cost functions or by incorporating detailed knowledge about the optimal control strategy. Both requirements for automatic training processes are not given in the majority of real world reinforcement learning (RL) problems. We introduce a new particle swarm reinforcement learning (PSRL) approach which is capable of constructing fuzzy RL policies solely by training parameters on world models produced from randomly generated samples of the real system. This approach relates self-organizing fuzzy controllers to model-based RL for the first time. PSRL can be used straightforward on any RL problem, which is demonstrated on three standard RL benchmarks, mountain car, cart pole balancing and cart pole swing up. Our experiments yielded high performing and well interpretable fuzzy policies."
"Aligning video sequences is a fundamental yet still unsolved component for a wide range of applications in computer graphics and vision. Especially when targeting video clips containing an extensively varying appearance. Using recent advances in deep learning, we present a scalable and robust method for computing optimal non-linear temporal video alignments. The presented algorithm learns to retrieve and match similar video frames from input sequences without any human interaction or additional annotations in an unsupervised fashion. An iterative scheme is presented which leverages on the nature of the videos themselves in order to remove the need for labels. We incorporate a variation of Dijkstra's shortest-path algorithm for extracting meaningful training examples as well as a robust video alignment. While previous methods assume similar settings as weather conditions, season and illumination, our approach is able to robustly align videos regardless of such noise. This provides new ways of compositing non-seasonal video clips from data recorded months apart."
"Approximate message passing (AMP) is an efficient iterative signal recovery algorithm for compressed sensing (CS). For sensing matrices with independent and identically distributed (i.i.d.) Gaussian entries, the behavior of AMP can be asymptotically described by a scaler recursion called state evolution. Orthogonal AMP (OAMP) is a variant of AMP that imposes a divergence-free constraint on the denoiser. In this paper, we extend OAMP to incorporate generic denoisers, hence the name D-OAMP. Our numerical results show that state evolution predicts the performance of D-OAMP well for generic denoisers when i.i.d. Gaussian or partial orthogonal sensing matrices are involved. We compare the performances of denosing-AMP (D-AMP) and D-OAMP for recovering natural images from CS measurements. Simulation results show that D-OAMP outperforms D-AMP in both convergence speed and recovery accuracy for partial orthogonal sensing matrices."
"The wavelet tree has become a very useful data structure to efficiently represent and query large volumes of data in many different domains, from bioinformatics to geographic information systems. One problem with wavelet trees is their construction time. In this paper, we introduce two algorithms that reduce the time complexity of a wavelet tree's construction by taking advantage of nowadays ubiquitous multicore machines.   Our first algorithm constructs all the levels of the wavelet in parallel in $O(n)$ time and $O(n\lg\sigma + \sigma\lg n)$ bits of working space, where $n$ is the size of the input sequence and $\sigma$ is the size of the alphabet. Our second algorithm constructs the wavelet tree in a domain-decomposition fashion, using our first algorithm in each segment, reaching $O(\lg n)$ time and $O(n\lg\sigma + p\sigma\lg n/\lg\sigma)$ bits of extra space, where $p$ is the number of available cores. Both algorithms are practical and report good speedup for large real datasets."
"We perform an in-depth analysis on the inequality in 863 Wikimedia projects. We take the complete editing history of 267,304,095 Wikimedia items until 2016, which not only covers every language edition of Wikipedia, but also embraces the complete versions of Wiktionary, Wikisource, Wikivoyage, etc. Our findings of common growth pattern described by the interrelation between four characteristic growth yardsticks suggest a universal law of the communal data formation. In this encyclopedic data set, we observe the interplay between the number of edits and the degree of inequality. In particular, the rapid increasing of the Gini coefficient suggests that this entrenched inequality stems from the nature of such open-editing communal data sets, namely abiogenesis of supereditors' cartel. We show that these groups are created at the early stage of these open-editing media and still alive. Furthermore, our model taking both short-term and long-term memories into account successfully elucidates the underlying mechanism to establish the oligarchy in Wikipedia. Eventually, our results forewarn a rather pessimistic prospect of such communal databases in the future: the inequality will endure extendedly."
"Finding dense subgraphs in a graph is a fundamental graph mining task, with applications in several fields. Algorithms for identifying dense subgraphs are used in biology, in finance, in spam detection, etc. Standard formulations of this problem such as the problem of finding the maximum clique of a graph are hard to solve. However, some tractable formulations of the problem have also been proposed, focusing mainly on optimizing some density function, such as the degree density and the triangle density. However, maximization of degree density usually leads to large subgraphs with small density. In this paper, we introduce the k-clique-graph densest subgraph problem, k >= 3, a novel formulation for the discovery of dense subgraphs. Given an input graph, its k-clique-graph is a new graph created from the input graph where each vertex of the new graph corresponds to a k-clique of the input graph and two vertices are connected with an edge if they share a common (k-1)-clique. We define a simple density function, the k-clique-graph density, which gives compact and at the same time dense subgraphs, and we project its resulting subgraphs back to the input graph. In this paper we focus on the triangle-graph densest subgraph problem obtained for k=3. To optimize the proposed function, we provide an exact algorithm. Furthermore, we present an efficient greedy approximation algorithm that scales well to larger graphs. We evaluate the proposed algorithms on real datasets and compare them with other algorithms in terms of the size and the density of the extracted subgraphs. The results verify the ability of the proposed algorithms in finding high-quality subgraphs in terms of size and density. Finally, we apply the proposed method to the important problem of keyword extraction from textual documents."
"Most of the metaheuristics can efficiently solve unconstrained problems; however, their performance may degenerate if the constraints are involved. This paper proposes two constraint handling approaches for an emerging metaheuristic of Cohort Intelligence (CI). More specifically CI with static penalty function approach (SCI) and CI with dynamic penalty function approach (DCI) are proposed. The approaches have been tested by solving several constrained test problems. The performance of the SCI and DCI have been compared with algorithms like GA, PSO, ABC, d-Ds. In addition, as well as three real world problems from mechanical engineering domain with improved solutions. The results were satisfactory and validated the applicability of CI methodology for solving real world problems."
"Although anti-virus software has significantly evolved over the last decade, classic signature matching based on byte patterns is still a prevalent concept for identifying security threats. Anti-virus signatures are a simple and fast detection mechanism that can complement more sophisticated analysis strategies. However, if signatures are not designed with care, they can turn from a defensive mechanism into an instrument of attack. In this paper, we present a novel method for automatically deriving signatures from anti-virus software and demonstrate how the extracted signatures can be used to attack sensible data with the aid of the virus scanner itself. We study the practicability of our approach using four commercial products and exemplarily discuss a novel attack vector made possible by insufficiently designed signatures. Our research indicates that there is an urgent need to improve pattern-based signatures if used in anti-virus software and to pursue alternative detection approaches in such products."
"Tree rotations (left and right) are basic local deformations allowing to transform between two unlabeled binary trees of the same size. Hence, there is a natural problem of practically finding such transformation path with low number of rotations, the optimal minimal number is called the rotation distance. Such distance could be used for instance to quantify similarity between two trees for various machine learning problems, for example to compare hierarchical clusterings or arbitrarily chosen spanning trees of two graphs, like in SMILES notation popular for describing chemical molecules.   There will be presented inexpensive practical greedy algorithm for finding a short rotation path, optimality of which has still to be determined. It uses introduced partial order for binary trees of the same size: $t_1 \leq t_2$ iff $t_2$ can be obtained from $t_1$ by a sequence of only right rotations. Intuitively, the shortest rotation path should go through the least upper bound or the greatest lower bound for this partial order. The algorithm finds a path through candidates for both points in representation of binary tree as stack graph: describing evolution of content of stack while processing a formula described by a given binary tree. The article is accompanied with Mathematica implementation of all used procedures (Appendix)."
"This work considers weak deterministic B\""uchi automata reading encodings of non-negative reals in a fixed base. A Real Number Automaton is an automaton which recognizes all encoding of elements of a set of reals. It is explained how to decide in linear time whether a set of reals recognized by a given minimal weak deterministic RNA is ${FO}[\mathbb R;+,<,1]$-definable. Furthermore, it is explained how to compute in quasi-quadratic (respectively, quasi-linear) time an existential (respectively, existential-universal) ${FO}[\mathbb R;+,<,1]$-formula which defines the set of reals recognized by the automaton. It is also shown that techniques given by Muchnik and by Honkala for automata over vector of natural numbers also works on vector of real numbers. It implies that some problems such as deciding whether a set of tuples of reals $R\subseteq\mathbb R^{d}$ is a subsemigroup of $(\mathbb R^{d},+)$ or is ${FO}[\mathbb R;+,<,1]$-definable is decidable."
"This paper introduces novel air actuated spherical robot called ""RollRoller"". The RollRoller robot consists of two essential parts: tubes covered with a shell as a frame and mechanical controlling parts to correspond movements. The RollRoller is proposed to be high potential alternative for exploration and rescue missions robots because robot utilizing its locomotion via all possible deriving methods (gravity, torque and angular momentum forces). In beginning , characteristic and role of each of component and features were explained. Next, to determine the uniqueness of this robot, the known and other extra possible motions are shown by proposing their own algorithmic movements. To illustrate main motion of this robot was inherent to mathematical models, the forward direction dynamical behavior on flat surface was derived. Additionally, Matlab Simulink was used to plot the results to validate the behavior for both fractional and non-fractional terrains. Lastly, after designing the model of robot in Solidworks Program, Adams/View visualization software ( the robot simulated form ) was utilized to proof the Matlab Simulink results and to show the more detailed and complete form of locomotion including the forward direction and circular locomotion in proposed robot."
"Scientific discovery is increasingly dependent on a scientist's ability to acquire, curate, integrate, analyze, and share large and diverse collections of data. While the details vary from domain to domain, these data often consist of diverse digital assets (e.g. image files, sequence data, or simulation outputs) that are organized with complex relationships and context which may evolve over the course of an investigation. In addition, discovery is often collaborative, such that sharing of the data and its organizational context is highly desirable. Common systems for managing file or asset metadata hide their inherent relational structures, while traditional relational database systems do not extend to the distributed collaborative environment often seen in scientific investigations. To address these issues, we introduce ERMrest, a collaborative data management service which allows general entity-relationship modeling of metadata manipulated by RESTful access methods. We present the design criteria, architecture, and service implementation, as well as describe an ecosystem of tools and services that we have created to integrate metadata into an end-to-end scientific data life cycle. ERMrest has been deployed to hundreds of users across multiple scientific research communities and projects. We present two representative use cases: an international consortium and an early-phase, multidisciplinary research project."
"Mobile devices have been playing vital roles in modern day education delivery as students can access or download learning materials on their smartphones and tablets, they can also install educational apps and study anytime, anywhere. The need to provide adequate security for portable devices being used for learning cannot be underestimated. In this paper, we present a mobile security enhancement app, designed and developed for Android smart mobile devices in order to promote security awareness among students. The app can also identify major and the most significant security weaknesses, scan or check for vulnerabilities in m-learning devices and report any security threat."
"This paper analyzes k nearest neighbor classification with training data anonymized using anatomy. Anatomy preserves all data values, but introduces uncertainty in the mapping between identifying and sensitive values. We first study the theoretical effect of the anatomized training data on the k nearest neighbor error rate bounds, nearest neighbor convergence rate, and Bayesian error. We then validate the derived bounds empirically. We show that 1) Learning from anatomized data approaches the limits of learning through the unprotected data (although requiring larger training data), and 2) nearest neighbor using anatomized data outperforms nearest neighbor on generalization-based anonymization."
"The integration of surface normals for the purpose of computing the shape of a surface in 3D space is a classic problem in computer vision. However, even nowadays it is still a challenging task to devise a method that combines the flexibility to work on non-trivial computational domains with high accuracy, robustness and computational efficiency. By uniting a classic approach for surface normal integration with modern computational techniques we construct a solver that fulfils these requirements. Building upon the Poisson integration model we propose to use an iterative Krylov subspace solver as a core step in tackling the task. While such a method can be very efficient, it may only show its full potential when combined with a suitable numerical preconditioning and a problem-specific initialisation. We perform a thorough numerical study in order to identify an appropriate preconditioner for our purpose. To address the issue of a suitable initialisation we propose to compute this initial state via a recently developed fast marching integrator. Detailed numerical experiments illuminate the benefits of this novel combination. In addition, we show on real-world photometric stereo datasets that the developed numerical framework is flexible enough to tackle modern computer vision applications."
"Powerful Forward Error Correction (FEC) schemes are used in optical communications to achieve bit-error rates below $10^{-15}$. These FECs follow one of two approaches: concatenation of simpler hard-decision codes or usage of inherently powerful soft-decision codes. The first approach yields lower Net Coding Gains (NCG), but can usually work at higher code rates and have lower complexity decoders. In this work, we propose a novel FEC scheme based on a product code and a post-processing technique. It can achieve an NCG of 9.96 dB at a BER of $10^{-18}$ without encountering an error floor, an error-correction performance that sits between that of current hard-decision and soft-decision FECs. A decoder architecture is designed, tested on FPGA and synthesized in 65 nm CMOS technology: its 164 bits/cycle worst-case information throughput can reach 100 Gb/s at the achieved frequency of 609 MHz. Its complexity is shown to be lower than that of hard-decision decoders in literature, and an order of magnitude lower than the estimated complexity of soft-decision decoders."
"Broadcasts and timelines are the primary mechanism of information exchange in online social platforms today. Services like Facebook, Twitter and Instagram have enabled ordinary people to reach large audiences spanning cultures and countries, while their massive popularity has created increasingly competitive marketplaces of attention. Timing broadcasts to capture the attention of such geographically diverse audiences has sparked interest from many startups and social marketing gurus. However, formal study is lacking on both the timing and frequency problems. We study for the first time the broadcast scheduling problem of specifying the timing and frequency of publishing content to maximise the attention received.   We validate and quantify three interacting behavioural phenomena to parametrise social platform users: information overload, bursty circadian rhythms and monotony aversion, which is defined here for the first time. We formalise a timeline information exchange process based on these phenomena, and formulate an objective function that quantifies the expected collective attention. We finally present experiments on real data from Twitter, where we discover a counter-intuitive scheduling strategy that outperforms popular heuristics while producing fewer posts."
"In this paper, we introduce a threshold free approach, motivated from Chinese Restaurant Process, for the purpose of cognate clustering. We show that our approach yields similar results to a linguistically motivated cognate clustering system known as LexStat. Our Chinese Restaurant Process system is fast and does not require any threshold and can be applied to any language family of the world."
"We give a combinatorial condition for the existence of efficient, LP-based FPT algorithms for a broad class of graph-theoretical optimisation problems. Our condition is based on the notion of biased graphs known from matroid theory. Specifically, we show that given a biased graph $\Psi=(G,\mathcal{B})$, where $\mathcal{B}$ is a class of balanced cycles in $G$, the problem of finding a set $X$ of at most $k$ vertices in $G$ which intersects every unbalanced cycle in $G$ admits an FPT algorithm using an LP-branching approach, similar to those previously seen for VCSP problems (Wahlstr\""om, SODA 2014). Our algorithm has two parts. First we define a \emph{local problem}, where we are additionally given a root vertex $v_0 \in V$ and asked only to delete vertices $X$ (excluding $v_0$) so that the connected component of $v_0$ in $G-X$ contains no unbalanced cycle. We show that this local problem admits a persistent, half-integral LP-relaxation with a polynomial-time solvable separation oracle, and can therefore be solved in FPT time via LP-branching, assuming only oracle membership queries for the class of balanced cycles in $G$. We then show that solutions to this local problem can be used to tile the graph, producing an optimal solution to the original, global problem as well. This framework captures many of the problems previously solved via the VCSP approach to LP-branching, as well as new generalisations, such as Group Feedback Vertex Set for infinite groups (e.g., for graphs whose edges are labelled by matrices). A major advantage compared to previous work is that it is immediate to check the applicability of the result for a given problem, whereas testing applicability of the VCSP approach for a specific VCSP requires determining the existence of an embedding language with certain algebraically defined properties, which is not known to be decidable in general."
"Epidemic propagation on complex networks has been widely investigated, mostly with invariant parameters. However, the process of epidemic propagation is not always constant. Epidemics can be affected by various perturbations, and may bounce back to its original state, which is considered resilient. Here, we study the resilience of epidemics on networks, by introducing a different infection rate ${\lambda_{2}}$ during SIS (susceptible-infected-susceptible) epidemic propagation to model perturbations (control state), whereas the infection rate is ${\lambda_{1}}$ in the rest of time. Through simulations and theoretical analysis, we find that even for ${\lambda_{2}<\lambda_{c}}$, epidemics eventually could bounce back if control duration is below a threshold. This critical control time for epidemic resilience, i.e., ${cd_{max}}$ can be predicted by the diameter (${d}$) of the underlying network, with the quantitative relation ${cd_{max}\sim d^{\alpha}}$. Our findings can help to design a better mitigation strategy for epidemics."
"We explore the following question: Is a decision-making program fair, for some useful definition of fairness? First, we describe how several algorithmic fairness questions can be phrased as program verification problems. Second, we discuss an automated verification technique for proving or disproving fairness of decision-making programs with respect to a probabilistic model of the population."
"Meta-learning consists in learning learning algorithms. We use a Long Short Term Memory (LSTM) based network to learn to compute on-line updates of the parameters of another neural network. These parameters are stored in the cell state of the LSTM. Our framework allows to compare learned algorithms to hand-made algorithms within the traditional train and test methodology. In an experiment, we learn a learning algorithm for a one-hidden layer Multi-Layer Perceptron (MLP) on non-linearly separable datasets. The learned algorithm is able to update parameters of both layers and generalise well on similar datasets."
"The paper proclaims a summary of Li-Fi technology. It is an efficient data communication mechanism involving visible light as a medium of transmission. This monograph introduces the concept of Li-Fi and its working model. Furthermore, it discusses its benefits, shortcomings and examines ways to mitigate the drawbacks. Additionally, it analyzes its present applications and explores its future scope in the competitive wireless network market."
"Multicarrier phase-based ranging is fast emerging as a cost-optimized solution for a wide variety of proximity-based applications due to its low power requirement, low hardware complexity and compatibility with existing standards such as ZigBee and 6LoWPAN. Given potentially critical nature of the applications in which phase-based ranging can be deployed (e.g., access control, asset tracking), it is important to evaluate its security guarantees. Therefore, in this work, we investigate the security of multicarrier phase-based ranging systems and specifically focus on distance decreasing relay attacks that have proven detrimental to the security of proximity-based access control systems (e.g., vehicular passive keyless entry and start systems). We show that phase-based ranging, as well as its implementations, are vulnerable to a variety of distance reduction attacks. We describe different attack realizations and verify their feasibility by simulations and experiments on a commercial ranging system. Specifically, we successfully reduced the estimated range to less than 3 m even though the devices were more than 50 m apart. We discuss possible countermeasures against such attacks and illustrate their limitations, therefore demonstrating that phase-based ranging cannot be fully secured against distance decreasing attacks."
"Secret sharing schemes with optimal and universal communication overheads have been obtained independently by Bitar et al. and Huang et al. However, their constructions require a finite field of size q > n, where n is the number of shares, and do not provide strong security. In this work, we give a general framework to construct communication efficient secret sharing schemes based on sequences of nested linear codes, which allows to use in particular algebraic geometry codes and allows to obtain strongly secure and communication efficient schemes. Using this framework, we obtain: 1) schemes with universal and close to optimal communication overheads for arbitrarily large lengths n and a fixed finite field, 2) the first construction of schemes with universal and optimal communication overheads and optimal strong security (for restricted lengths), which has the security advantages of perfect schemes and the storage efficiency of ramp schemes, and 3) schemes with universal and close to optimal communication overheads and close to optimal strong security defined for arbitrarily large lengths n and a fixed finite field."
"A significant barrier to the portability of queries across di- verse physical implementations of large data stores, espe- cially NoSQL data stores, is that the queries reference the physical storage attributes, such as the table and column names. In this paper, we describe a technique for embed- ding ontological expressions called Address Expressions, or A-Expressions, in NoSQL queries to improve their portability across diverse physical implementations. We discuss an implementation of such queries over a MongoDB data store of the Enron email corpus with examples, and conduct a preliminary performance assessment."
"Nowadays, internet has changed the world into a global village. Social Media has reduced the gaps among the individuals. Previously communication was a time consuming and expensive task between the people. Social Media has earned fame because it is a cheaper and faster communication provider. Besides, social media has allowed us to reduce the gaps of physical distance, it also generates and preserves huge amount of data. The data are very valuable and it presents association degree between people and their opinions. The comprehensive analysis of the methods which are used on user behavior prediction is presented in this paper. This comparison will provide a detailed information, pros and cons in the domain of sentiment and opinion mining."
"We present a method to approximate functionals $\text{Tr} \, f(A)$ of very high-dimensional hermitian matrices $A$ represented as Matrix Product Operators (MPOs). Our method is based on a reformulation of a block Lanczos algorithm in tensor network format. We state main properties of the method and show how to adapt the basic Lanczos algorithm to the tensor network formalism to allow for high-dimensional computations. Additionally, we give an analysis of the complexity of our method and provide numerical evidence that it yields good approximations of the entropy of density matrices represented by MPOs while being robust against truncations."
"Energy conservative devices are the need of the modern technology which leads to the development of reversible logic. The synthesis of reversible logic has become an intensely studied area as it overcomes the problem of power dissipation associated with irreversibility. Storage device such as Read-Only-Memory (ROM) can be realized in a reversible way with low power dissipation. The reversibility of ROM has not been yet realized in literature and hence, this paper presents a novel reversible ROM with its Complementary Metal Oxide Semiconductor (CMOS) realization. On the way to present the architecture of reversible ROM, we propose a new reversible gate named as Nowrin Papiya (NP) gate. All the proposed circuits and gates are realized with CMOS based pass transistor logic. Finally, an algorithm as well as several theorems on the numbers of gates, transistors and garbage outputs have been presented to show the optimality of the reversible ROM. Simulations using Microwind DSCH software has been shown to verify the correctness of the proposed design. The comparative results prove that the proposed designs are efficient and optimized in terms of numbers of gates, transistors, garbage outputs, quantum cost and delay."
"Clustering is often used for reverse engineering network protocols from captured network traces. The performance of clustering techniques is often contingent upon the selection of various parameters, which can have a severe impact on clustering quality. In this paper we experimentally investigate the effect of four different parameters with respect to network traces. We also determining the optimal parameter configuration with respect to traces from four different network protocols. Our results indicate that the choice of distance measure and the length of the message has the most substantial impact on cluster accuracy. Depending on the type of protocol, the $n$-gram length can also have a substantial impact."
"We study the dynamics of the out-of-equilibrium nonlinear q-voter model with two types of susceptible voters and zealots, introduced in [EPL 113, 48001 (2016)]. In this model, each individual supports one of two parties and is either a susceptible voter of type $q_1$ or $q_2$, or is an inflexible zealot. At each time step, a $q_i$-susceptible voter ($i = 1,2$) consults a group of $q_i$ neighbors and adopts their opinion if all group members agree, while zealots are inflexible and never change their opinion. This model violates detailed balance whenever $q_1 \neq q_2$ and is characterized by two distinct regimes of low and high density of zealotry. Here, by combining analytical and numerical methods, we investigate the non-equilibrium stationary state of the system in terms of its probability distribution, non-vanishing currents and unequal-time two-point correlation functions. We also study the switching times properties of the model by exploiting approximate mappings onto the model of [Phys. Rev. E 92, 012803 (2015)] that satisfies the detailed balance, and also outline some properties of the model near criticality."
"In this paper, we review the state of the art of privacy-preserving schemes for ad hoc social networks, including, mobile social networks (MSNs) and vehicular social networks (VSNs). Specifically, we select and in-detail examine thirty-three privacy preserving schemes developed for or applied in the context of ad hoc social networks. These schemes are published between 2008 and 2016. Based on this existing privacy preservation schemes, we survey privacy preservation models, including location privacy, identity privacy, anonymity, traceability, interest privacy, backward privacy, and content oriented privacy. The recent important attacks of leaking privacy, countermeasures, and game theoretic approaches in VSNs and MSNs are summarized in form of tables. In addition, an overview of recommendations for further research is also provided. With this survey, readers can have a more thorough understanding of research trends in privacy-preserving schemes for ad hoc social networks"
"This paper considers recovering $L$-dimensional vectors $\boldsymbol{w}$, and $\boldsymbol{x}_n,~ n =1 , \ldots, N$ from their circular convolutions $\boldsymbol{y}_n = \boldsymbol{w}*\boldsymbol{x}_n$. The vector $\boldsymbol{w}$ is assumed to be $S$-sparse in a known basis that is spread out in the Fourier domain, and each input $\boldsymbol{x}_n$ is a member of a known $K$-dimensional random subspace.   We prove that whenever $K + S\log^2S \lesssim L /\log^4(LN)$, the problem can be solved effectively by using only the nuclear-norm minimization as the convex relaxation, as long as the inputs are sufficiently diverse and obey $N \gtrsim \log^2(LN)$. By ""diverse inputs"", we mean that the $\boldsymbol{x}_n$ belong to different, generic subspaces. To our knowledge, this is the first theoretical result on blind deconvolution where the subspace to which the impulse response belongs is not fixed, but needs to be determined.   We discuss the result in the context of multipath channel estimation in wireless communications. Both the fading coefficients, and the delays in the channel impulse response $\boldsymbol{w}$ are unknown. The encoder codes the $K$-dimensional message vectors randomly and then transmits them over a fixed channel one after the other. The decoder then discovers all of the messages and the channel response when the number of samples taken for each received message are roughly greater than $(K+S\log^2S)\log^4(LN)$, and the number of messages is roughly at least $\log^2(LN)$."
"Crowdsourcing has been successfully employed in the past as an effective and cheap way to execute classification tasks and has therefore attracted the attention of the research community. However, we still lack a theoretical understanding of how to collect the labels from the crowd in an optimal way. In this paper we focus on the problem of worker allocation and compare two active learning policies proposed in the empirical literature with a uniform allocation of the available budget. To this end we make a thorough mathematical analysis of the problem and derive a new bound on the performance of the system. Furthermore we run extensive simulations in a more realistic scenario and show that our theoretical results hold in practice."
"COOJA is a network simulator developed for wireless sensor networks. It can be used for high-level algorithm development as well as low-level device driver implementations for accurate simulation of wireless sensor networks before deployment. However, in a simulation Cooja assumes that the nodes are only equipped with omnidirectional antennas. There is currently no support for directional antennas. Due to the growing interest in the use of directional or smart antennas in wireless sensor networks, a model that can support directional antennas is essential for the realistic simulations of protocols relying on directional communication. This paper presents work on extending COOJA with a directional antenna model."
"During the last years, several algorithmic meta-theorems have appeared (Bodlaender et al. [FOCS 2009], Fomin et al. [SODA 2010], Kim et al. [ICALP 2013]) guaranteeing the existence of linear kernels on sparse graphs for problems satisfying some generic conditions. The drawback of such general results is that it is usually not clear how to derive from them constructive kernels with reasonably low explicit constants. To fill this gap, we recently presented [STACS 2014] a framework to obtain explicit linear kernels for some families of problems whose solutions can be certified by a subset of vertices. In this article we enhance our framework to deal with packing problems, that is, problems whose solutions can be certified by collections of subgraphs of the input graph satisfying certain properties. ${\mathcal F}$-Packing is a typical example: for a family ${\mathcal F}$ of connected graphs that we assume to contain at least one planar graph, the task is to decide whether a graph $G$ contains $k$ vertex-disjoint subgraphs such that each of them contains a graph in ${\mathcal F}$ as a minor. We provide explicit linear kernels on sparse graphs for the following two orthogonal generalizations of ${\mathcal F}$-Packing: for an integer $\ell \geq 1$, one aims at finding either minor-models that are pairwise at distance at least $\ell$ in $G$ ($\ell$-${\mathcal F}$-Packing), or such that each vertex in $G$ belongs to at most $\ell$ minors-models (${\mathcal F}$-Packing with $\ell$-Membership). Finally, we also provide linear kernels for the versions of these problems where one wants to pack subgraphs instead of minors."
"Detection and learning based appearance feature play the central role in data association based multiple object tracking (MOT), but most recent MOT works usually ignore them and only focus on the hand-crafted feature and association algorithms. In this paper, we explore the high-performance detection and deep learning based appearance feature, and show that they lead to significantly better MOT results in both online and offline setting. We make our detection and appearance feature publicly available. In the following part, we first summarize the detection and appearance feature, and then introduce our tracker named Person of Interest (POI), which has both online and offline version."
"Wireless information-centric networks consider storage as one of the network primitives, and propose to cache data within the network in order to improve latency and reduce bandwidth consumption. We study the throughput capacity and latency in an information-centric network when the data cached in each node has a limited lifetime. The results show that with some fixed request and cache expiration rates, the order of the data access time does not change with network growth, and the maximum throughput order is not changing with the network growth in grid networks, and is inversely proportional to the number of nodes in one cell in random networks. Comparing these values with the corresponding throughput and latency with no cache capability (throughput inversely proportional to the network size, and latency of order $\sqrt{n}$ and the inverse of the transmission range in grid and random networks, respectively), we can actually quantify the asymptotic advantage of caching. Moreover, we compare these scaling laws for different content discovery mechanisms and illustrate that not much gain is lost when a simple path search is used."
"In the last decade, Tor proved to be a very successful and widely popular system to protect users' anonymity. However, Tor remains a practical system with a variety of limitations, some of which were indeed exploited in the recent past. In particular, Tor's security relies on the fact that a substantial number of its nodes do not misbehave. In this work we introduce, the concept of honey onions, a framework to detect misbehaving Tor relays with HSDir capability. This allows to obtain lower bounds on misbehavior among relays. We propose algorithms to both estimate the number of snooping HSDirs and identify the most likely snoopers. Our experimental results indicate that during the period of the study (72 days) at least 110 such nodes were snooping information about hidden services they host. We reveal that more than half of them were hosted on cloud infrastructure and delayed the use of the learned information to prevent easy traceback."
"We systematically explored a spectrum of normalization algorithms related to Batch Normalization (BN) and propose a generalized formulation that simultaneously solves two major limitations of BN: (1) online learning and (2) recurrent learning. Our proposal is simpler and more biologically-plausible. Unlike previous approaches, our technique can be applied out of the box to all learning scenarios (e.g., online learning, batch learning, fully-connected, convolutional, feedforward, recurrent and mixed --- recurrent and convolutional) and compare favorably with existing approaches. We also propose Lp Normalization for normalizing by different orders of statistical moments. In particular, L1 normalization is well-performing, simple to implement, fast to compute, more biologically-plausible and thus ideal for GPU or hardware implementations."
"We study which standard operators of probabilistic process calculi allow for compositional reasoning with respect to bisimulation metric semantics. We argue that uniform continuity (generalizing the earlier proposed property of non-expansiveness) captures the essential nature of compositional reasoning and allows now also to reason compositionally about recursive processes. We characterize the distance between probabilistic processes composed by standard process algebra operators. Combining these results, we demonstrate how compositional reasoning about systems specified by continuous process algebra operators allows for metric assume-guarantee like performance validation."
"We study the classic NP-Hard problem of finding the maximum $k$-set coverage in the data stream model: given a set system of $m$ sets that are subsets of a universe $\{1,\cdots,n \}$, find the $k$ sets that cover the most number of distinct elements. The problem can be approximated up to a factor $1-1/e$ in polynomial time. In the streaming-set model, the sets and their elements are revealed online. The main goal of our work is to design algorithms, with approximation guarantees as close as possible to $1-1/e$, that use sublinear space $o(mn)$.   We present two $(1-1/e-\epsilon)$-approximation algorithms: The first uses $O(\epsilon^{-1}\log (k/\epsilon))$ passes and $\tilde{O}(\epsilon^{-2} k)$ space whereas the second uses only a single pass but $\tilde{O}(\epsilon^{-3} m)$ space. We show that any approximation factor better than $(1-1/e)$ in constant passes requires $\Omega(m)$ space for constant $k$ even if the algorithm is allowed unbounded processing time.   We also study the maximum $k$-vertex coverage problem in the dynamic graph stream model. In this model, the stream consists of edge insertions and deletions of a graph on $N$ vertices. The goal is to find $k$ vertices that cover the most number of distinct edges. We show that any constant approximation in constant passes requires $\Omega(N)$ space for constant $k$ whereas $\tilde{O}(\epsilon^{-2}N)$ space is sufficient for a $(1-\epsilon)$ approximation and arbitrary $k$ in a single pass. For regular graphs, we show that $\tilde{O}(\epsilon^{-3}k)$ space is sufficient for a $(1-\epsilon)$ approximation in a single pass. We generalize this to a $\kappa(1-\epsilon)$ approximation when the ratio between the minimum and maximum degree is bounded below by $\kappa$."
"We introduce a novel, fully automated solution method for sensor planning problem for 3D models. By modeling the human approach to the problem first, we put the problem into a reinforcement learning (RL) framework and successfully solve it using the well-known RL algorithms with function approximation. We compare our method with the greedy algorithm in various test cases and show that we can out-perform the baseline greedy algorithm in all cases."
"Texts present coherent stories that have a particular theme or overall setting, for example science fiction or western. In this paper, we present a text generation method called {\it rewriting} that edits existing human-authored narratives to change their theme without changing the underlying story. We apply the approach to math word problems, where it might help students stay more engaged by quickly transforming all of their homework assignments to the theme of their favorite movie without changing the math concepts that are being taught. Our rewriting method uses a two-stage decoding process, which proposes new words from the target theme and scores the resulting stories according to a number of factors defining aspects of syntactic, semantic, and thematic coherence. Experiments demonstrate that the final stories typically represent the new theme well while still testing the original math concepts, outperforming a number of baselines. We also release a new dataset of human-authored rewrites of math word problems in several themes."
"We design a custom spectrum sensing network, called RadioHound, capable of tuning from 25 MHz to 6 GHz, which covers nearly all widely-deployed wireless activity. We describe the system hardware and network infrastructure in detail with a view towards driving the cost, size, and power usage of the sensors as low as possible. The system estimates the spatial variation of radio-frequency power from an unknown random number of sources. System performance is measured by computing the mean square error against a simulated radio-frequency environment. We find that the system performance depends heavily on the deployment density of the sensors. Consequently, we derive an expression for the sensor density as a function of environmental characteristics and confidence in measurement quality."
"Infant speech perception and learning is modeled using Echo State Network classification and Reinforcement Learning. Ambient speech for the modeled infant learner is created using the speech synthesizer Vocaltractlab. An auditory system is trained to recognize vowel sounds from a series of speakers of different anatomies in Vocaltractlab. Having formed perceptual targets, the infant uses Reinforcement Learning to imitate his ambient speech. A possible way of bridging the problem of speaker normalisation is proposed, using direct imitation but also including a caregiver who listens to the infants sounds and imitates those that sound vowel-like."
"This paper introduces a simulation study of fluid actuated multi-driven closed system as spherical mobile robot called ""RollRoller"". Robot's mechanism design consists of two essential parts: tubes to lead a core and mechanical controlling parts to correspond movements. Our robot gets its motivation force by displacing the spherical movable mass known as core in curvy manners inside certain pipes. This simulation investigates by explaining the mechanical and structural features of the robot for creating hydraulic-base actuation via force and momentum analysis. Next, we categorize difficult and integrated 2D motions to omit unstable equilibrium points through derived nonlinear dynamics. We propose an algorithmic position control in forward direction that creates hybrid model as solution for motion planning problem in spherical robot. By deriving nonlinear dynamics of the spherical robot and implementing designed motion planning, we show how RollRoller can be efficient in high speed movements in comparison to the other pendulum-driven models. Then, we validate the results of this position control obtained by nonlinear dynamics via Adams/view simulation which uses the imported solid model of RollRoller. Lastly, We have a look to the circular maneuver of this robot by the same simulator."
"We describe a simple but effective method for cross-lingual syntactic transfer of dependency parsers, in the scenario where a large amount of translation data is not available.The method makes use of three steps: 1) a method for deriving cross-lingual word clusters, that can then be used in a multilingual parser; 2) a method for transferring lexical information from a target language to source language treebanks; 3) a method for integrating these steps with the density-driven annotation projection method of Rasooli and Collins(2015). Experiments show improvements over the state-of-the-art in several languages used in previous work (Rasooli and Collins, 2015;Zhang and Barzilay, 2015; Ammar et al.,2016), in a setting where the only source of translation data is the Bible, a considerably smaller corpus than the Europarl corpus used in previous work. Results using the Europarl corpus as a source of translation data show additional improvements over the results of Rasooli and Collins (2015). We conclude with results on 38 datasets (26 languages) from the Universal Dependencies corpora: 13 datasets(10 languages) have unlabeled attachment ac-curacies of 80% or higher; the average unlabeled accuracy on the 38 datasets is 74.8%."
This document describes the DRAT format for clausal proofs and the DRAT-trim proof checker.
"The Prisoner's Dilemma Process on a graph $G$ is an iterative process where each vertex, with a fixed strategy (\emph{cooperate} or \emph{defect}), plays the game with each of its neighbours. At the end of a round each vertex may change its strategy to that of its neighbour with the highest pay-off. Here we study the spread of cooperative and selfish behaviours on a toroidal grid, where each vertex is initially a cooperator with probability $p$. When vertices are permitted to change their strategies via a randomized asynchronous update scheme, we find that for some values of $p$ the limiting ratio of cooperators may be modelled as a polynomial in $p$. Theoretical bounds for this ratio are confirmed via simulation."
"Online extremists in social networks pose a new form of threat to the general public. These extremists range from cyberbullies who harass innocent users to terrorist organizations such as the Islamic State of Iraq and Syria (ISIS) that use social networks to recruit and incite violence. Currently social networks suspend the accounts of such extremists in response to user complaints. The challenge is that these extremist users simply create new accounts and continue their activities. In this work we present a new set of operational capabilities to deal with the threat posed by online extremists in social networks.   Using data from several hundred thousand extremist accounts on Twitter, we develop a behavioral model for these users, in particular what their accounts look like and who they connect with. This model is used to identify new extremist accounts by predicting if they will be suspended for extremist activity. We also use this model to track existing extremist users as they create new accounts by identifying if two accounts belong to the same user. Finally, we present a model for searching the social network to efficiently find suspended users' new accounts based on a variant of the classic Polya's urn setup. We find a simple characterization of the optimal search policy for this model under fairly general conditions. Our urn model and main theoretical results generalize easily to search problems in other fields."
"Anomalies are those deviating from the norm. Unsupervised anomaly detection often translates to identifying low density regions. Major problems arise when data is high-dimensional and mixed of discrete and continuous attributes. We propose MIXMAD, which stands for MIXed data Multilevel Anomaly Detection, an ensemble method that estimates the sparse regions across multiple levels of abstraction of mixed data. The hypothesis is for domains where multiple data abstractions exist, a data point may be anomalous with respect to the raw representation or more abstract representations. To this end, our method sequentially constructs an ensemble of Deep Belief Nets (DBNs) with varying depths. Each DBN is an energy-based detector at a predefined abstraction level. At the bottom level of each DBN, there is a Mixed-variate Restricted Boltzmann Machine that models the density of mixed data. Predictions across the ensemble are finally combined via rank aggregation. The proposed MIXMAD is evaluated on high-dimensional realworld datasets of different characteristics. The results demonstrate that for anomaly detection, (a) multilevel abstraction of high-dimensional and mixed data is a sensible strategy, and (b) empirically, MIXMAD is superior to popular unsupervised detection methods for both homogeneous and mixed data."
"The topological (or graph) structures of real-world networks are known to be predictive of multiple dynamic properties of the networks. Conventionally, a graph structure is represented using an adjacency matrix or a set of hand-crafted structural features. These representations either fail to highlight local and global properties of the graph or suffer from a severe loss of structural information. There lacks an effective graph representation, which hinges the realization of the predictive power of network structures.   In this study, we propose to learn the represention of a graph, or the topological structure of a network, through a deep learning model. This end-to-end prediction model, named DeepGraph, takes the input of the raw adjacency matrix of a real-world network and outputs a prediction of the growth of the network. The adjacency matrix is first represented using a graph descriptor based on the heat kernel signature, which is then passed through a multi-column, multi-resolution convolutional neural network. Extensive experiments on five large collections of real-world networks demonstrate that the proposed prediction model significantly improves the effectiveness of existing methods, including linear or nonlinear regressors that use hand-crafted features, graph kernels, and competing deep learning methods."
"Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These ""fast weights"" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns."
"We survey foundational features underlying modern graph query languages. We first discuss two popular graph data models: edge-labelled graphs, where nodes are connected to other nodes by directed, labelled edges; and property graphs, where nodes and edges can have attributes. Next we discuss the two most basic graph querying functionalities: graph patterns and navigational expressions.   We start with graph patterns, in which a graph-structured query is matched against the data. Thereafter we discuss navigational expressions, in which patterns can be matched recursively against the graph to navigate paths of arbitrary length; we give an overview of what kinds of expressions have been proposed, and how such expressions can be combined with graph patterns. We also discuss a variety of semantics under which queries using the previous features can be evaluated, what effects the introduction of additional features and the selection of semantics has on complexity, as well as offering examples of said features in three modern languages that can be used to query graphs: SPARQL, Cypher and Gremlin. We conclude with discussion of the importance of formalisation for graph query languages, as well as possible future directions in which such languages can be extended."
"In this paper, we propose a stand-alone mobile visual search system based on binary features and the bag-of-visual words framework. The contribution of this study is three-fold: (1) We propose an adaptive substring extraction method that adaptively extracts informative bits from the original binary vector and stores them in the inverted index. These substrings are used to refine visual word-based matching. (2) A modified local NBNN scoring method is proposed in the context of image retrieval, which considers the density of binary features in scoring each feature matching. (3) In order to suppress false positives, we introduce a convexity check step that imposes a convexity constraint on the configuration of a transformed reference image. The proposed system improves retrieval accuracy by 11% compared with a conventional method without increasing the database size. Furthermore, our system with the convexity check does not lead to false positive results."
"Reservoir Computing is a bio-inspired computing paradigm for processing time dependent signals. The performance of its analogue implementation are comparable to other state of the art algorithms for tasks such as speech recognition or chaotic time series prediction, but these are often constrained by the offline training methods commonly employed. Here we investigated the online learning approach by training an opto-electronic reservoir computer using a simple gradient descent algorithm, programmed on an FPGA chip. Our system was applied to wireless communications, a quickly growing domain with an increasing demand for fast analogue devices to equalise the nonlinear distorted channels. We report error rates up to two orders of magnitude lower than previous implementations on this task. We show that our system is particularly well-suited for realistic channel equalisation by testing it on a drifting and a switching channels and obtaining good performances"
"Delay-coupled electro-optical systems have received much attention for their dynamical properties and their potential use in signal processing. In particular it has recently been demonstrated, using the artificial intelligence algorithm known as reservoir computing, that photonic implementations of such systems solve complex tasks such as speech recognition. Here we show how the backpropagation algorithm can be physically implemented on the same electro-optical delay-coupled architecture used for computation with only minor changes to the original design. We find that, compared when the backpropagation algorithm is not used, the error rate of the resulting computing device, evaluated on three benchmark tasks, decreases considerably. This demonstrates that electro-optical analog computers can embody a large part of their own training process, allowing them to be applied to new, more difficult tasks."
"In this paper, we study the benefits of full-duplex (FD) receiver jamming in enhancing the physical-layer security of a two-tier decentralized wireless network with each tier deployed with a large number of pairs of a single-antenna transmitter and a multi-antenna receiver. In the underlying tier, the transmitter sends unclassified information, and the receiver works in the halfduplex (HD) mode receiving the desired signal. In the overlaid tier, the transmitter deliveries confidential information in the presence of randomly located eavesdroppers, and the receiver works in the FD mode radiating jamming signals to confuse eavesdroppers and receiving the desired signal simultaneously. We provide a comprehensive performance analysis and network design under a stochastic geometry framework. Specifically, we consider the scenarios where each FD receiver uses single- and multi-antenna jamming, and analyze the connection probability and the secrecy outage probability of a typical FD receiver by providing accurate expressions and more tractable approximations for the two metrics. We further determine the optimal deployment of the FD-mode tier in order to maximize networkwide secrecy throughput subject to constraints including the given dual probabilities and the network-wide throughput of the HD-mode tier. Numerical results are demonstrated to verify our theoretical findings, and show that network-wide secrecy throughput is significantly improved by properly deploying the FD-mode tier."
"With the advent of word embeddings, lexicons are no longer fully utilized for sentiment analysis although they still provide important features in the traditional setting. This paper introduces a novel approach to sentiment analysis that integrates lexicon embeddings and an attention mechanism into Convolutional Neural Networks. Our approach performs separate convolutions for word and lexicon embeddings and provides a global view of the document using attention. Our models are experimented on both the SemEval'16 Task 4 dataset and the Stanford Sentiment Treebank, and show comparative or better results against the existing state-of-the-art systems. Our analysis shows that lexicon embeddings allow to build high-performing models with much smaller word embeddings, and the attention mechanism effectively dims out noisy words for sentiment analysis."
"We perform an asymptotic study on the performance of filter bank multicarrier (FBMC) in the context of massive multi-input multi-output (MIMO). We show that the signal-to-interference-plus-noise ratio (SINR) cannot grow unboundedly by increasing the number of base station (BS) antennas, and is upper bounded by a certain deterministic value. This is a result of the correlation between the multi-antenna combining tap values and the channel impulse responses between the terminals and the BS antennas. To solve this problem, we introduce a simple FBMC prototype filter design method that removes this correlation, enabling us to achieve arbitrarily large SINR values by increasing the number of BS antennas."
Present day machine learning is computationally intensive and processes large amounts of data. It is implemented in a distributed fashion in order to address these scalability issues. The work is parallelized across a number of computing nodes. It is usually hard to estimate in advance how many nodes to use for a particular workload. We propose a simple framework for estimating the scalability of distributed machine learning algorithms. We measure the scalability by means of the speedup an algorithm achieves with more nodes. We propose time complexity models for gradient descent and graphical model inference. We validate our models with experiments on deep learning training and belief propagation. This framework was used to study the scalability of machine learning algorithms in Apache Spark.
"Trajectory tracking control for quadrotors is important for applications ranging from surveying and inspection, to film making. However, designing and tuning classical controllers, such as proportional-integral-derivative (PID) controllers, to achieve high tracking precision can be time-consuming and difficult, due to hidden dynamics and other non-idealities. The Deep Neural Network (DNN), with its superior capability of approximating abstract, nonlinear functions, proposes a novel approach for enhancing trajectory tracking control. This paper presents a DNN-based algorithm that improves the tracking performance of a classical feedback controller. Given a desired trajectory, the DNNs provide a tailored input to the controller based on their gained experience. The input aims to achieve a unity map between the desired and the output trajectory. The motivation for this work is an interactive ""fly-as-you-draw"" application, in which a user draws a trajectory on a mobile device, and a quadrotor instantly flies that trajectory with the DNN-enhanced control system. Experimental results demonstrate that the proposed approach improves the tracking precision for user-drawn trajectories after the DNNs are trained on selected periodic trajectories, suggesting the method's potential in real-world applications. Tracking errors are reduced by around 40-50 % for both training and testing trajectories from users, highlighting the DNNs' capability of generalizing knowledge."
"In this paper, we consider the recovery of block sparse signals, whose nonzero entries appear in blocks (or clusters) rather than spread arbitrarily throughout the signal, from incomplete linear measurement. A high order sufficient condition based on block RIP is obtained to guarantee the stable recovery of all block sparse signals in the presence of noise, and robust recovery when signals are not exactly block sparse via mixed $l_{2}/l_{1}$ minimization. Moreover, a concrete example is established to ensure the condition is sharp. The significance of the results presented in this paper lies in the fact that recovery may be possible under more general conditions by exploiting the block structure of the sparsity pattern instead of the conventional sparsity pattern."
"In this paper, we introduce the concept of influential communities in a co-author network. We term a community as the most influential if the community has the highest influence among all other communities in the entire network. Influence of a community depends on the impact of the contents (e.g., citations of papers) generated by the members of that community. We propose an algorithm to identify the top K influential communities of an online social network. As a working prototype, we develop a visualization system that allows a user to find the top K influential communities from a co-author network. A user can search top K influential communities of particular research fields and our system provides him/her with a visualization of these communities. A user can explore the details of a community, such as authors, citations, and collaborations with other communities."
"A breakdown of a benchmark score is how much each aspect of the system performance affects the score. Existing methods require internal analysis on the benchmarking program and then involve the following problems: (1) require a certain amount of labor for code analysis, profiling, simulation, and so on and (2) require the benchmarking program itself. In this paper, we present a method for breaking down a benchmark score without internal analysis of the benchmarking program. The method utilizes regression analysis of benchmark scores on a number of systems. Experimental results with 3 benchmarks on 15 Android smartphones showed that our method could break down those benchmark scores even though there is room for improvement in accuracy."
"Multi-server systems have received increasing attention with important implementations such as Google MapReduce, Hadoop, and Spark. Common to these systems are a fork operation, where jobs are first divided into tasks that are processed in parallel, and a later join operation, where completed tasks wait until the results of all tasks of a job can be combined and the job leaves the system. The synchronization constraint of the join operation makes the analysis of fork-join systems challenging and few explicit results are known. In this work, we model fork-join systems using a max-plus server model that enables us to derive statistical bounds on waiting and sojourn times for general arrival and service time processes. We contribute end-to-end delay bounds for multi-stage fork-join networks that grow in $\mathcal{O}(h \ln k)$ for $h$ fork-join stages, each with $k$ parallel servers. We perform a detailed comparison of different multi-server configurations and highlight their pros and cons. We also include an analysis of single-queue fork-join systems that are non-idling and achieve a fundamental performance gain, and compare these results to both simulation and a live Spark system."
We develop a partial order reduction method for labeled transition systems over metric spaces. We introduce the notion of epsilon-independent actions such that executing these actions in any order results in states that are close to each other. Then we define epsilon-equivalent action sequences that swap epsilon-independent action pairs. We present an algorithm to over-approximate the reach set of executions that take epsilon-equivalent action sequences. We are also able to show that the over-approximation can be computed up to arbitrary precision.
"In this paper, we propose a new data based model for influence maximization in online social networks. We use the theory of belief functions to overcome the data imperfection problem. Besides, the proposed model searches to detect influencer users that adopt a positive opinion about the product, the idea, etc, to be propagated. Moreover, we present some experiments to show the performance of our model."
"In traditional e-voting protocols, privacy is often provided by a trusted authority that learns the votes and computes the tally. Some protocols replace the trusted authority by a set of authorities, and privacy is guaranteed if less than a threshold number of authorities are corrupt. For verifiability, stronger security guarantees are demanded. Typically, corrupt authorities that try to fake the result of the tally must always be detected.   To provide verifiability, many e-voting protocols use Non-Interactive Zero-Knowledge proofs (NIZKs). Thanks to their non-interactive nature, NIZKs allow anybody, including third parties that do not participate in the protocol, to verify the correctness of the tally. Therefore, NIZKs can be used to obtain universal verifiability. Additionally, NIZKs also improve usability because they allow voters to cast a vote using a non-interactive protocol.   The disadvantage of NIZKs is that their security is based on setup assumptions such as the common reference string (CRS) or the random oracle (RO) model. The former requires a trusted party for the generation of a common reference string. The latter, though a popular methodology for designing secure protocols, has been shown to be unsound.   In this paper, we address the design of an e-voting protocol that provides verifiability without any trust assumptions, where verifiability here is meant without eligibility verification. We show that Non-Interactive Witness-Indistinguishable proofs (NIWI) can be used for this purpose. The e-voting scheme is private under the Decision Linear assumption, while verifiability holds unconditionally. To our knowledge, this is the first private e-voting scheme with perfect universal verifiability, i.e. one in which the probability of a fake tally not being detected is 0, and with {\em non-interactive} protocols that does not rely on trust assumptions."
"Image Forensics has already achieved great results for the source camera identification task on images. Standard approaches for data coming from Social Network Platforms cannot be applied due to different processes involved (e.g., scaling, compression, etc.). Over 1 billion images are shared each day on the Internet and obtaining information about their history from the moment they were acquired could be exploited for investigation purposes. In this paper, a classification engine for the reconstruction of the history of an image, is presented. Specifically, exploiting K-NN and decision trees classifiers and a-priori knowledge acquired through image analysis, we propose an automatic approach that can understand which Social Network Platform has processed an image and the software application used to perform the image upload. The engine makes use of proper alterations introduced by each platform as features. Results, in terms of global accuracy on a dataset of 2720 images, confirm the effectiveness of the proposed strategy."
"Linear codes are widely employed in communication systems, consumer electronics, and storage devices. All linear codes over finite fields can be generated by a generator matrix. Due to this, the generator matrix approach is called a fundamental construction of linear codes. This is the only known construction method that can produce all linear codes over finite fields. Recently, a defining-set construction of linear codes over finite fields has attracted a lot of attention, and have been employed to produce a huge number of classes of linear codes over finite fields. It was claimed that this approach can also generate all linear codes over finite fields. But so far, no proof of this claim is given in the literature. The objective of this paper is to prove this claim, and confirm that the defining-set approach is indeed a fundamental approach to constructing all linear codes over finite fields. As a byproduct, a trace representation of all linear codes over finite fields is presented."
"Recently, a $q$-polynomial approach to the construction and analysis of cyclic codes over $\gf(q)$ was given by Ding and Ling. The objective of this paper is to give another $q$-polynomial approach to all cyclic codes over $\gf(q)$."
"We investigate some subtle issues that arise when programming distributed computations over infinite data structures. To do this, we formalise a calculus that combines a call-by-name functional core with session-based communication primitives and that allows session operations to be performed ""on demand"". We develop a typing discipline that guarantees both normalisation of expressions and progress of processes and that uncovers an unexpected interplay between evaluation and communication."
"Two new constructions of linear code pairs $C_2 \subset C_1$ are given for which the codimension and the relative minimum distances $M_1(C_1,C_2)$, $M_1(C_2^\perp,C_1^\perp)$ are good. By this we mean that for any two out of the three parameters the third parameter of the constructed code pair is large. Such pairs of nested codes are indispensable for the determination of good linear ramp secret sharing schemes [35]. They can also be used to ensure reliable communication over asymmetric quantum channels [47]. The new constructions result from carefully applying the Feng-Rao bounds [18,27] to a family of codes defined from multivariate polynomials and Cartesian product point sets."
"Natural images contain often curvilinear structures, which might be disconnected, or partly occluded. Recovering the missing connection of disconnected structures is an open issue and needs appropriate geometric reasoning. We propose to find line co-occurrence statistics from the centerlines of blood vessels in retinal images and show its remarkable similarity to a well-known probabilistic model for the connectivity pattern in the primary visual cortex. Furthermore, the probabilistic model is trained from the data via statistics and used for automated grouping of interrupted vessels in a spectral clustering based approach. Several challenging image patches are investigated around junction points, where successful results indicate the perfect match of the trained model to the profiles of blood vessels in retinal images. Also, comparisons among several statistical models obtained from different datasets reveals their high similarity i.e., they are independent of the dataset. On top of that, the best approximation of the statistical model with the symmetrized extension of the probabilistic model on the projective line bundle is found with a least square error smaller than 2%. Apparently, the direction process on the projective line bundle is a good continuation model for vessels in retinal images."
"Assisted text input techniques can save time and effort and improve text quality. In this paper, we investigate how grounded and conditional extensions to standard neural language models can bring improvements in the tasks of word prediction and completion. These extensions incorporate a structured knowledge base and numerical values from the text into the context used to predict the next word. Our automated evaluation on a clinical dataset shows extended models significantly outperform standard models. Our best system uses both conditioning and grounding, because of their orthogonal benefits. For word prediction with a list of 5 suggestions, it improves recall from 25.03% to 71.28% and for word completion it improves keystroke savings from 34.35% to 44.81%, where theoretical bound for this dataset is 58.78%. We also perform a qualitative investigation of how models with lower perplexity occasionally fare better at the tasks. We found that at test time numbers have more influence on the document level than on individual word probabilities."
"Precisely modeling complex systems like cyber-physical systems is often challenging, which may render model-based system verification techniques like model checking infeasible. To overcome this challenge, we propose a method called LAR to `verify' such complex systems through a combination of learning, abstraction and refinement. Instead of starting with system modeling, our method takes a set of concrete system traces as input. The output is either a counterexample with a bounded probability of being a spurious counterexample, or a probabilistic model based on which the given property is `verified'. The model could be viewed as a proof obligation, i.e., the property is verified if the model is correct. It can also be used for subsequent system analysis activities like runtime monitoring. Our method has been implemented as a self-contained software toolkit. The evaluation on multiple benchmark systems as well as a real-world water purification system show promising results."
"In larger organizations, multiple teams of data scientists have to integrate data from heterogeneous data sources as preparation for data analysis tasks. Writing effective analytical queries requires data scientists to have in-depth knowledge of the existence, semantics, and usage context of data sources. Once gathered, such knowledge is informally shared within a specific team of data scientists, but usually is neither formalized nor shared with other teams. Potential synergies remain unused. We therefore introduce a novel approach which extends data management systems with additional knowledge-sharing capabilities to facilitate user collaboration without altering established data analysis workflows. Relevant collective knowledge from the query log is extracted to support data source discovery and incremental data integration. Extracted knowledge is formalized and provided at query time."
"For an arbitrary category, we consider the least class of functors con- taining the projections and closed under finite products, finite coproducts, parameterized initial algebras and parameterized final coalgebras, i.e. the class of functors that are definable by $\mu$-terms. We call the category $\mu$-bicomplete if every $\mu$-term defines a functor. We provide concrete ex- amples of such categories and explicitly characterize this class of functors for the category of sets and functions. This goal is achieved through par- ity games: we associate to each game an algebraic expression and turn the game into a term of a categorical theory. We show that $\mu$-terms and parity games are equivalent, meaning that they define the same property of being $\mu$-bicomplete. Finally, the interpretation of a parity game in the category of sets is shown to be the set of deterministic winning strategies for a chosen player."
"This paper examines use of dynamic probabilistic networks (DPN) for human action recognition. The actions of lifting objects and walking in the room, sitting in the room and neutral standing pose were used for testing the classification. The research used the dynamic interrelation between various different regions of interest (ROI) on the human body (face, body, arms, legs) and the time series based events related to the these ROIs. This dynamic links are then used to recognize the human behavioral aspects in the scene. First a model is developed to identify the human activities in an indoor scene and this model is dependent on the key features and interlinks between the various dynamic events using DPNs. The sub ROI are classified with DPN to associate the combined interlink with a specific human activity. The recognition accuracy performance between indoor (controlled lighting conditions) is compared with the outdoor lighting conditions. The accuracy in outdoor scenes was lower than the controlled environment."
"We show that every (finite or not) typing derivation of system M, using non-idempotent intersection, which is the infinitary version of de Carvalho's system M_0 , can be represented in a rigid, non-idempotent intersection type system S. Namely, whereas non-idempotent intersection is represented by multisets in system M, system S resort to families of types indexed by integers, called tracks. Intersion is said to be sequential. The rigidity is here related to the fact that those indexes matter as well as the order in which the types are quoted in a familly of types."
"The long-term memory of most connectionist systems lies entirely in the weights of the system. Since the number of weights is typically fixed, this bounds the total amount of knowledge that can be learned and stored. Though this is not normally a problem for a neural network designed for a specific task, such a bound is undesirable for a system that continually learns over an open range of domains. To address this, we describe a lifelong learning system that leverages a fast, though non-differentiable, content-addressable memory which can be exploited to encode both a long history of sequential episodic knowledge and semantic knowledge over many episodes for an unbounded number of domains. This opens the door for investigation into transfer learning, and leveraging prior knowledge that has been learned over a lifetime of experiences to new domains."
"We provide a type-theoretical characterization of weakly-normalizing terms in an infinitary lambda-calculus. We adapt for this purpose the standard quantitative (with non-idempotent intersections) type assignment system of the lambda-calculus to our infinite calculus. Our work provides a new answer to Klop's HHN-problem, namely, finding out if there is a type system characterizing the hereditary head-normalizing (HHN) lambda-terms. Tatsuta showed that HHN could not be characterized by a finite type system. We prove that an infinitary type system endowed with a validity condition called approximability can achieve it."
"This paper proposes a practical approach to addressing limitations posed by use of single active electrodes in applications for sleep stage classification. Electroencephalography (EEG)-based characterizations of sleep stage progression contribute the diagnosis and monitoring of the many pathologies of sleep. Several prior reports have explored ways of automating the analysis of sleep EEG and of reducing the complexity of the data needed for reliable discrimination of sleep stages in order to make it possible to perform sleep studies at lower cost in the home (rather than only in specialized clinical facilities). However, these reports have involved recordings from electrodes placed on the cranial vertex or occiput, which can be uncomfortable or difficult for subjects to position. Those that have utilized single EEG channels which contain less sleep information, have showed poor classification performance. We have taken advantage of Rectifier Neural Network for feature detection and Long Short-Term Memory (LSTM) network for sequential data learning to optimize classification performance with single electrode recordings. After exploring alternative electrode placements, we found a comfortable configuration of a single-channel EEG on the forehead and have shown that it can be integrated with additional electrodes for simultaneous recording of the electroocuolgram (EOG). Evaluation of data from 62 people (with 494 hours sleep) demonstrated better performance of our analytical algorithm for automated sleep classification than existing approaches using vertex or occipital electrode placements. Use of this recording configuration with neural network deconvolution promises to make clinically indicated home sleep studies practical."
"We propose an architecture based on Quantum cellular Automata which allows the use of only one type of quantum gates per computational step in order to perform nearest neighbor interactions. The model is built in partial steps, each one of them analyzed using nearest neighbor interactions, starting with single qubit operations and continuing with two qubit ones. The effectiveness of the model is tested and valuated by developing a quantum circuit implementing the Quantum Fourier Transform. The important outcome of this validation was that the operations are performed in a local and controlled manner thus reducing the error rate of each computational step."
"Methods for detecting and summarizing emergent keywords have been extensively studied since social media and microblogging activities have started to play an important role in data analysis and decision making. We present a system for monitoring emergent keywords and summarizing a document stream based on the dynamic semantic graphs of streaming documents. We introduce the notion of dynamic eigenvector centrality for ranking emergent keywords, and present an algorithm for summarizing emergent events that is based on the minimum weight set cover. We demonstrate our system with an analysis of streaming Twitter data related to public security events."
"The ability of a human being to extrapolate previously gained knowledge to other domains inspired a new family of methods in machine learning called transfer learning. Transfer learning is often based on the assumption that objects in both target and source domains share some common feature and/or data space. In this paper, we propose a simple and intuitive approach that minimizes iteratively the distance between source and target task distributions by optimizing the kernel target alignment (KTA). We show that this procedure is suitable for transfer learning by relating it to Hilbert-Schmidt Independence Criterion (HSIC) and Quadratic Mutual Information (QMI) maximization. We run our method on benchmark computer vision data sets and show that it can outperform some state-of-art methods."
"This paper presents a unified framework for smooth convex regularization of discrete optimal transport problems. In this context, the regularized optimal transport turns out to be equivalent to a matrix nearness problem with respect to Bregman divergences. Our framework thus naturally generalizes a previously proposed regularization based on the Boltzmann-Shannon entropy related to the Kullback-Leibler divergence, and solved with the Sinkhorn-Knopp algorithm. We call the regularized optimal transport distance the rot mover's distance in reference to the classical earth mover's distance. We develop two generic schemes that we respectively call the alternate scaling algorithm and the non-negative alternate scaling algorithm, to compute efficiently the regularized optimal plans depending on whether the domain of the regularizer lies within the non-negative orthant or not. These schemes are based on Dykstra's algorithm with alternate Bregman projections, and further exploit the Newton-Raphson method for separable divergences. We enhance the separable case with a sparse extension to deal with high data dimensions. We also instantiate our proposed framework and discuss the inherent specificities for well-known regularizers and statistical divergences in the machine learning and information geometry communities. Finally, we demonstrate our methods with an experiment on synthetic input data that illustrates the effect of different regularizers and penalties on the output solutions."
"This paper presents a novel fixation prediction and saliency modeling framework based on inter-image similarities and ensemble of Extreme Learning Machines (ELM). The proposed framework is inspired by two observations, 1) the contextual information of a scene along with low-level visual cues modulates attention, 2) the influence of scene memorability on eye movement patterns caused by the resemblance of a scene to a former visual experience. Motivated by such observations, we develop a framework that estimates the saliency of a given image using an ensemble of extreme learners, each trained on an image similar to the input image. That is, after retrieving a set of similar images for a given image, a saliency predictor is learnt from each of the images in the retrieved image set using an ELM, resulting in an ensemble. The saliency of the given image is then measured in terms of the mean of predicted saliency value by the ensemble's members."
This paper analyses urban accessibility using a fully dynamic approach.
"Body-worn video (BWV) cameras are increasingly utilized by police departments to provide a record of police-public interactions. However, large-scale BWV deployment produces terabytes of data per week, necessitating the development of effective computational methods to identify salient changes in video. In work carried out at the 2016 RIPS program at IPAM, UCLA, we present a novel two-stage framework for video change-point detection. First, we employ state-of-the-art machine learning methods including convolutional neural networks and support vector machines for scene classification. We then develop and compare change-point detection algorithms utilizing mean squared-error minimization, forecasting methods, hidden Markov models, and maximum likelihood estimation to identify noteworthy changes. We test our framework on detection of vehicle exits and entrances in a BWV data set provided by the Los Angeles Police Department and achieve over 90% recall and nearly 70% precision -- demonstrating robustness to rapid scene changes, extreme luminance differences, and frequent camera occlusions."
"Hypothesis testing is an important cognitive process that supports human reasoning. In this paper, we introduce a computational hypothesis testing approach based on memory augmented neural networks. Our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. We apply the proposed approach to language comprehension task by using Neural Semantic Encoders (NSE). Our NSE models achieve the state-of-the-art results showing an absolute improvement of 1.2% to 2.6% accuracy over previous results obtained by single and ensemble systems on standard machine comprehension benchmarks such as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets."
"A geometric graph is a graph whose vertex set is a set of points in the plane and whose edge set contains straight-line segments. A matching in a graph is a subset of edges of the graph with no shared vertices. A matching is called perfect if it matches all the vertices of the underling graph. A geometric matching is a matching in a geometric graph. In this thesis, we study matching problems in various geometric graphs. Among the family of geometric graphs we look at complete graphs, complete bipartite graphs, complete multipartite graphs, Delaunay graphs, Gabriel graphs, and $\Theta$-graphs. The classical matching problem is to find a matching of maximum size in a given graph. We study this problem as well as some of its variants on geometric graphs. The bottleneck matching problem is to find a maximum matching that minimizes the length of the longest edge. The plane matching problem is to find a maximum matching so that the edges in the matching are pairwise non-crossing. A geometric matching is strong with respect to a given shape $S$ if we can assign to each edge in the matching a scaled version of $S$ such that the shapes representing the edges are pairwise disjoint. The strong matching problem is to find a maximum strong matching with respect to a given shape. The matching packing problem is to pack as many edge-disjoint perfect matchings as possible into a geometric graph. We study these problems and establish lower and upper bounds on the size of different kinds of matchings in various geometric graphs. We also present algorithms for computing such matchings. Some of the presented bounds are tight, while the others need to be sharpened."
"It is shown that signal energy is the only available degree-of-freedom (DOF) for fiber-optic transmission as the input power tends to infinity. With $n$ signal DOFs at the input, $n-1$ DOFs are asymptotically lost to signal-noise interactions. The main observation is that, nonlinearity introduces a multiplicative noise in the channel, similar to fading in wireless channels. The channel is viewed in the spherical coordinate system, where signal vector $\underline{X}\in\mathbb{C}^n$ is represented in terms of its norm $|\underline{X}|$ and direction $\underline{\hat{X}}$. The multiplicative noise causes signal direction $\underline{\hat{X}}$ to vary randomly on the surface of the unit $(2n-1)$-sphere in $\mathbb{C}^{n}$, in such a way that the effective area of the support of $\underline{\hat{X}}$ does not vanish as $|\underline{X}|\rightarrow\infty$. On the other hand, the surface area of the sphere is finite, so that $\underline{\hat{X}}$ carries finite information. This observation is used to show several results. Firstly, let $\mathcal C(\mathcal P)$ be the capacity of a discrete-time periodic model of the optical fiber with distributed noise and frequency-dependent loss, as a function of the average input power $\mathcal P$. It is shown that asymptotically as $\mathcal P\rightarrow\infty$, $\mathcal C=\frac{1}{n}\log\bigl(\log\mathcal P\bigr)+c$, where $n$ is the dimension of the input signal space and $c$ is a bounded number. In particular, $\lim_{\mathcal P\rightarrow\infty}\mathcal C(\mathcal P)=\infty$ in finite-dimensional periodic models. Secondly, it is shown that capacity saturates to a constant in infinite-dimensional models where $n=\infty$."
"In this paper, we consider linear state-space models with compressible innovations and convergent transition matrices in order to model spatiotemporally sparse transient events. We perform parameter and state estimation using a dynamic compressed sensing framework and develop an efficient solution consisting of two nested Expectation-Maximization (EM) algorithms. Under suitable sparsity assumptions on the innovations, we prove recovery guarantees and derive confidence bounds for the state estimates. We provide simulation studies as well as application to spike deconvolution from calcium imaging data which verify our theoretical results and show significant improvement over existing algorithms."
"Multiple forms of digital transformation are imminent. Digital Twins represent one concept. It is gaining momentum because it may offer real-time transparency. Rapid diffusion of digital duplicates faces hurdles due to lack of semantic interoperability between architectures, standards and ontologies. The technologies necessary for automated discovery are in short supply. Progression of the field depends on convergence of information technology, operational technology and protocol-agnostic telecommunications. Making sense of the data, ability to curate data and perform data analytics at the edge (or mist rather than in the fog or cloud) is key to value. Delivering engines to the edge are crucial for analytics at the edge when latency is critical. The confluence of these and other factors may chart the future path for Digital Twins. The number of unknown unknowns and the known unknowns in this process makes it imperative to create global infrastructures and organize groups to pursue the development of fundamental building blocks and new ideas through research."
"This paper explores a simple question: How would we provide a high-quality search experience on Mars, where the fundamental physical limit is speed-of-light propagation delays on the order of tens of minutes? On Earth, users are accustomed to nearly instantaneous response times from search engines. Is it possible to overcome orders-of-magnitude longer latency to provide a tolerable user experience on Mars? In this paper, we formulate the searching from Mars problem as a tradeoff between ""effort"" (waiting for responses from Earth) and ""data transfer"" (pre-fetching or caching data on Mars). The contribution of our work is articulating this design space and presenting two case studies that explore the effectiveness of baseline techniques, using publicly available data from the TREC Total Recall and Sessions Tracks. We intend for this research problem to be aspirational and inspirational - even if one is not convinced by the premise of Mars colonization, there are Earth-based scenarios such as searching from a rural village in India that share similar constraints, thus making the problem worthy of exploration and attention from researchers."
In this work we extend to the interval-valued setting the notion of an overlap functions and we discuss a method which makes use of interval-valued overlap functions for constructing OWA operators with interval-valued weights. . Some properties of interval-valued overlap functions and the derived interval-valued OWA operators are analysed. We specially focus on the homogeneity and migrativity properties.
"We present ORB-SLAM2 a complete SLAM system for monocular, stereo and RGB-D cameras, including map reuse, loop closing and relocalization capabilities. The system works in real-time in standard CPUs in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our backend based on Bundle Adjustment with monocular and stereo observations allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches to map points that allow for zero-drift localization. The evaluation in 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution. We publish the source code, not only for the benefit of the SLAM community, but with the aim of being an out-of-the-box SLAM solution for researchers in other fields."
A modification of the neo-fuzzy neuron is proposed (an extended neo-fuzzy neuron (ENFN)) that is characterized by improved approximating properties. An adaptive learning algorithm is proposed that has both tracking and smoothing properties. An ENFN distinctive feature is its computational simplicity compared to other artificial neural networks and neuro-fuzzy systems.
Neo-fuzzy elements are used as nodes for an evolving cascade system. The proposed system can tune both its parameters and architecture in an online mode. It can be used for solving a wide range of Data Mining tasks (namely time series forecasting). The evolving cascade system with neo-fuzzy nodes can process rather large data sets with high speed and effectiveness.
"A new architecture and learning algorithms for the multidimensional hybrid cascade neural network with neuron pool optimization in each cascade are proposed in this paper. The proposed system differs from the well-known cascade systems in its capability to process multidimensional time series in an online mode, which makes it possible to process non-stationary stochastic and chaotic signals with the required accuracy. Compared to conventional analogs, the proposed system provides computational simplicity and possesses both tracking and filtering capabilities."
An evolving weighted neuro-neo-fuzzy-ANARX model and its learning procedures are introduced in the article. This system is basically used for time series forecasting. This system may be considered as a pool of elements that process data in a parallel manner. The proposed evolving system may provide online processing data streams.
An architecture of a new neuro-fuzzy system is proposed. The basic idea of this approach is to tune both synaptic weights and membership functions with the help of the supervised learning and self-learning paradigms. The approach to solving the problem has to do with evolving online neuro-fuzzy systems that can process data under uncertainty conditions. The results prove the effectiveness of the developed architecture and the learning procedure.
A new approach to data stream clustering with the help of an ensemble of adaptive neuro-fuzzy systems is proposed. The proposed ensemble is formed with adaptive neuro-fuzzy self-organizing Kohonen maps in a parallel processing mode. A final result is chosen by the best neuro-fuzzy self-organizing Kohonen map.
The paper focuses on the problem of learning saccades enabling visual object search. The developed system combines reinforcement learning with a neural network for learning to predict the possible outcomes of its actions. We validated the solution in three types of environment consisting of (pseudo)-randomly generated matrices of digits. The experimental verification is followed by the discussion regarding elements required by systems mimicking the fovea movement and possible further research directions.
"This paper describes a dataset containing small images of text from everyday scenes. The purpose of the dataset is to support the development of new automated systems that can detect and analyze text. Although much research has been devoted to text detection and recognition in scanned documents, relatively little attention has been given to text detection in other types of images, such as photographs that are posted on social-media sites. This new dataset, known as COCO-Text-Patch, contains approximately 354,000 small images that are each labeled as ""text"" or ""non-text"". This dataset particularly addresses the problem of text verification, which is an essential stage in the end-to-end text detection and recognition pipeline. In order to evaluate the utility of this dataset, it has been used to train two deep convolution neural networks to distinguish text from non-text. One network is inspired by the GoogLeNet architecture, and the second one is based on CaffeNet. Accuracy levels of 90.2% and 90.9% were obtained using the two networks, respectively. All of the images, source code, and deep-learning trained models described in this paper will be publicly available"
"In this paper we propose right-angled Artin groups as platform for a secret sharing scheme based on the efficiency (linear time) of the word problem. We define two new problems: subgroup isomorphism problem for Artin subgroups and group homomorphism problem in right-angled Artin groups. We show that the group homomorphism and graph homomorphism problems are equivalent, and the later is known to be NP-complete. We propose two authentication schemes based on subgroup isomorphism problem and group homomorphism problem in right-angled Artin groups. In the case of subgroup isomorphism problem, we bring some results due to Bridson who shows there are right-angled Artin groups in which this problem in unsolvable. Note that our schemes are similar to what Grigoriev and Shpilrain proposed for graphs."
This paper uses traditional maps in animations to show traffic congestion effects in London
"Online communication channels, especially social web platforms, are rapidly replacing traditional ones. Online platforms allow users to overcome physical barriers, enabling worldwide participation. However, the power of online communication bears an important negative consequence --- we are exposed to too much information to process. Too many participants, for example, can turn online public spaces into noisy, overcrowded fora where no meaningful conversation can be held. Here we analyze a large dataset of public chat logs from Twitch, a popular video streaming platform, in order to examine how information overload affects online group communication. We measure structural and textual features of conversations such as user output, interaction, and information content per message across a wide range of information loads. Our analysis reveals the existence of a transition from a conversational state to a cacophony --- a state of overload with lower user participation, more copy-pasted messages, and less information per message. These results hold both on average and at the individual level for the majority of users. This study provides a quantitative basis for further studies of the social effects of information overload, and may guide the design of more resilient online communication systems."
"The authorship attribution is a problem of considerable practical and technical interest. Several methods have been designed to infer the authorship of disputed documents in multiple contexts. While traditional statistical methods based solely on word counts and related measurements have provided a simple, yet effective solution in particular cases; they are prone to manipulation. Recently, texts have been successfully modeled as networks, where words are represented by nodes linked according to textual similarity measurements. Such models are useful to identify informative topological patterns for the authorship recognition task. However, there is no consensus on which measurements should be used. Thus, we proposed a novel method to characterize text networks, by considering both topological and dynamical aspects of networks. Using concepts and methods from cellular automata theory, we devised a strategy to grasp informative spatio-temporal patterns from this model. Our experiments revealed an outperformance over traditional analysis relying only on topological measurements. Remarkably, we have found a dependence of pre-processing steps (such as the lemmatization) on the obtained results, a feature that has mostly been disregarded in related works. The optimized results obtained here pave the way for a better characterization of textual networks."
"The Web has become a large-scale real-time information system forcing us to revise both how to effectively assess relevance of information for a user and how to efficiently implement information retrieval and dissemination functionality. To increase information relevance, Real-time Web applications such as Twitter and Facebook, extend content and social-graph relevance scores with ""real-time"" user generated events (e.g. re-tweets, replies, likes). To accommodate high arrival rates of information items and user events we explore a publish/subscribe paradigm in which we index queries and update on the fly their results each time a new item and relevant events arrive. In this setting, we need to process continuous top-k text queries combining both static and dynamic scores. To the best of our knowledge, this is the first work addressing how non-predictable, dynamic scores can be handled in a continuous top-k query setting."
We analyze the computational complexity of the conjugacy search problem in a certain family of metabelian groups. We prove that in general the time complexity of the conjugacy search problem for these groups is at most exponential. For a subfamily of groups we prove that the conjugacy search problem is polynomial. We also show that for some of these groups the conjugacy search problem reduces to the discrete logarithm problem. We provide some experimental evidence which illustrates our results probabilistically.
"We explore the use of segments learnt using Byte Pair Encoding (referred to as BPE units) as basic units for statistical machine translation between related languages and compare it with orthographic syllables, which are currently the best performing basic units for this translation task. BPE identifies the most frequent character sequences as basic units, while orthographic syllables are linguistically motivated pseudo-syllables. We show that BPE units outperform orthographic syllables as units of translation, showing up to 11% increase in BLEU scores. In addition, BPE can be applied to any writing system, while orthographic syllables can be used only for languages whose writing systems use vowel representations. We show that BPE units outperform word and morpheme level units for translation involving languages like Urdu, Japanese whose writing systems do not use vowels (either completely or partially). Across many language pairs, spanning multiple language families and types of writing systems, we show that translation with BPE segments outperforms orthographic syllables, especially for morphologically rich languages."
"Multilayer networks are a useful way to capture and model multiple, binary relationships among a fixed group of objects. While community detection has proven to be a useful exploratory technique for the analysis of single-layer networks, the development of community detection methods for multilayer networks is still in its infancy. We propose and investigate a procedure, called Multilayer Extraction, that identifies densely connected vertex-layer sets in multilayer networks. Multilayer Extraction makes use of a significance based score that quantifies the connectivity of an observed vertex-layer set by comparison with a multilayer fixed degree random graph model. Unlike existing detection methods, Multilayer Extraction handles networks with heterogeneous layers where community structure may be different from layer to layer. The procedure is able to capture overlapping communities, and it identifies background vertex-layer pairs that do not belong to any community. We establish large-graph consistency of the vertex-layer set optimizer of our proposed multilayer score under the multilayer stochastic block model. We investigate the performance of Multilayer Extraction empirically on three applications, as well as a test bed of simulations. Our theoretical and numerical evaluations suggest that Multilayer Extraction is an effective exploratory tool for analyzing complex multilayer networks. Publicly available R software for Multilayer Extraction is available at https://github.com/jdwilson4/MultilayerExtraction."
"In multicast network design games, a set of agents choose paths from their source locations to a common sink with the goal of minimizing their individual costs, where the cost of an edge is divided equally among the agents using it. Since the work of Anshelevich et al. (FOCS 2004) that introduced network design games, the main open problem in this field has been the price of stability (PoS) of multicast games. For the special case of broadcast games (every vertex is a terminal, i.e., has an agent), a series of works has culminated in a constant upper bound on the PoS (Bilo` et al., FOCS 2013). However, no significantly sub-logarithmic bound is known for multicast games. In this paper, we make progress toward resolving this question by showing a constant upper bound on the PoS of multicast games for quasi-bipartite graphs. These are graphs where all edges are between two terminals (as in broadcast games) or between a terminal and a nonterminal, but there is no edge between nonterminals. This represents a natural class of intermediate generality between broadcast and multicast games. In addition to the result itself, our techniques overcome some of the fundamental difficulties of analyzing the PoS of general multicast games, and are a promising step toward resolving this major open problem."
"Scaling algorithms for entropic transport-type problems have become a very popular numerical method, encompassing Wasserstein barycenters, multi-marginal problems, gradient flows and unbalanced transport. However, a standard implementation of the scaling algorithm has several numerical limitations: the scaling factors diverge and convergence becomes impractically slow as the entropy regularization approaches zero. Moreover, handling the dense kernel matrix becomes unfeasible for large problems. To address this, we propose several modifications: A log-domain stabilized formulation, the well-known epsilon-scaling heuristic, an adaptive truncation of the kernel and a coarse-to-fine scheme. This allows to solve larger problems with smaller regularization and negligible truncation error. A new convergence analysis of the Sinkhorn algorithm is developed, working towards a better understanding of epsilon-scaling. Numerical examples illustrate efficiency and versatility of the modified algorithm."
"Understanding how users navigate in a network is of high interest in many applications. We consider a setting where only aggregate node-level traffic is observed and tackle the task of learning edge transition probabilities. We cast it as a preference learning problem, and we study a model where choices follow Luce's axiom. In this case, the $O(n)$ marginal counts of node visits are a sufficient statistic for the $O(n^2)$ transition probabilities. We show how to make the inference problem well-posed regardless of the network's structure, and we present ChoiceRank, an iterative algorithm that scales to networks that contains billions of nodes and edges. We apply the model to a month-long clickstream of the English Wikipedia and one year of rides on New York City's bicycle-sharing system. In both cases, we successfully recover the transition probabilities using only the network structure and marginal (node-level) traffic data."
"A novel learning Model Predictive Control technique is applied to the autonomous racing problem. The goal of the controller is to minimize the time to complete a lap. The proposed control strategy uses the data from previous laps to improve its performance while satisfying safety requirements. Moreover, a system identification technique is proposed to estimate the vehicle dynamics. Simulation results with the high fidelity simulator software CarSim show the effectiveness of the proposed control scheme."
"A connected dominating set in a graph is a dominating set of vertices that induces a connected subgraph. We introduce and study the connected-domishold graphs, defined as graphs that admit non-negative real weights associated to their vertices such that a set of vertices is a connected dominating set if and only if the sum of the corresponding weights exceeds a certain threshold.   More specifically, we show that connected-domishold graphs form a non-hereditary class of graphs properly containing two well known classes of chordal graphs: the block graphs and the trivially perfect graphs. We characterize connected-domishold graphs in terms of thresholdness of their minimal separator hypergraphs and show, conversely, that connected-domishold split graphs can be used to characterize threshold hypergraphs. Graphs every connected induced subgraph of which is connected-domishold are characterized in terms of forbidden induced subgraphs and in terms of properties of the minimal separator hypergraph. As a side result, our approach leads to new polynomially solvable cases of the minimum-weight connected domination problem."
"We propose an attention-enabled encoder-decoder model for the problem of grapheme-to-phoneme conversion. Most previous work has tackled the problem via joint sequence models that require explicit alignments for training. In contrast, the attention-enabled encoder-decoder model allows for jointly learning to align and convert characters to phonemes. We explore different types of attention models, including global and local attention, and our best models achieve state-of-the-art results on three standard data sets (CMUDict, Pronlex, and NetTalk)."
"This year, the Nara Institute of Science and Technology (NAIST)/Carnegie Mellon University (CMU) submission to the Japanese-English translation track of the 2016 Workshop on Asian Translation was based on attentional neural machine translation (NMT) models. In addition to the standard NMT model, we make a number of improvements, most notably the use of discrete translation lexicons to improve probability estimates, and the use of minimum risk training to optimize the MT system for BLEU score. As a result, our system achieved the highest translation evaluation scores for the task."
"Untraceable communication is about hiding the identity of the sender or the recipient of a message. Currently most systems used in practice (e.g., TOR) rely on the principle that a message is routed via several relays to obfuscate its path through the network. However, as this increases the end-to-end latency it is not ideal for applications like Voice-over-IP (VoIP) communication, where participants will notice annoying delays if the data does not arrive fast enough. We propose an approach based on the paradigm of Dining Cryptographer networks (DC-nets) that can be used to realize untraceable communication within small groups. The main features of our approach are low latency and resilience to packet-loss and fault packets sent by malicious players. We consider the special case of VoIP communication and propose techniques for a P2P implementation. We expose existing problems and sketch possible future large-scale systems composed of multiple groups."
"Most existing Neural Machine Translation models use groups of characters or whole words as their unit of input and output. We propose a model with a hierarchical char2word encoder, that takes individual characters both as input and output. We first argue that this hierarchical representation of the character encoder reduces computational complexity, and show that it improves translation performance. Secondly, by qualitatively studying attention plots from the decoder we find that the model learns to compress common words into a single embedding whereas rare words, such as names and places, are represented character by character."
"The notion of Reactive Turing machines (RTM) was proposed as an orthogonal extension of Turing machines with interaction. RTMs are used to define the notion of executable transition system in the same way as Turing machines are used to define the notion of computable function on natural numbers. RTMs inherited finiteness of all sets involved from Turing machines, and as a consequence, in a single step, an RTM can only communicate elements from a finite set of data. Some process calculi such as, e.g., the $\pi$-calculus, essentially use a form of infinite data in their definition and hence it immediately follows that transition systems specified in these calculi are not executable. On closer inspection, however, the $\pi$-calculus does not appear to use the infinite data in a non-computable manner.   In this paper, we investigate several ways to relax the finiteness requirement. We start by considering a variant of RTMs in which all sets are allowed to be infinite, and then refine by adding extra restrictions. The most restricted variant of RTMs in which the sets of action and data symbols are still allowed to be infinite is the notion of RTM with atoms. We propose a notion of transition systems with atoms, and show that every effective legal transition system with atoms is executable by RTM with atoms. In such a way, we show that processes specified in the $\pi$-calculus are executable by RTM with atoms."
"In this paper, we are going to focus on speed up of the Word Sense Disambiguation procedure by filtering the relevant senses of an ambiguous word through Part-of-Speech Tagging. First, this proposed approach performs the Part-of-Speech Tagging operation before the disambiguation procedure using Bigram approximation. As a result, the exact Part-of-Speech of the ambiguous word at a particular text instance is derived. In the next stage, only those dictionary definitions (glosses) are retrieved from an online dictionary, which are associated with that particular Part-of-Speech to disambiguate the exact sense of the ambiguous word. In the training phase, we have used Brown Corpus for Part-of-Speech Tagging and WordNet as an online dictionary. The proposed approach reduces the execution time upto half (approximately) of the normal execution time for a text, containing around 200 sentences. Not only that, we have found several instances, where the correct sense of an ambiguous word is found for using the Part-of-Speech Tagging before the Disambiguation procedure."
"Existing machine translation decoding algorithms generate translations in a strictly monotonic fashion and never revisit previous decisions. As a result, earlier mistakes cannot be corrected at a later stage. In this paper, we present a translation scheme that starts from an initial guess and then makes iterative improvements that may revisit previous decisions. We parameterize our model as a convolutional neural network that predicts discrete substitutions to an existing translation based on an attention mechanism over both the source sentence as well as the current translation output. By making less than one modification per sentence, we improve the output of a phrase-based translation system by up to 0.4 BLEU on WMT15 German-English translation."
"In this paper, we study the stochastic combinatorial multi-armed bandit (CMAB) framework that allows a general nonlinear reward function, whose expected value may not depend only on the means of the input random variables but possibly on the entire distributions of these variables. Our framework enables a much larger class of reward functions such as the $\max()$ function and nonlinear utility functions. Existing techniques relying on accurate estimations of the means of random variables, such as the upper confidence bound (UCB) technique, do not work directly on these functions. We propose a new algorithm called stochastically dominant confidence bound (SDCB), which estimates the distributions of underlying random variables and their stochastically dominant confidence bounds. We prove that SDCB can achieve $O(\log T)$ distribution-dependent regret and $\tilde{O}(\sqrt{T})$ distribution-independent regret, where $T$ is the time horizon. We apply our results to the $K$-MAX problem and expected utility maximization problems. In particular, for $K$-MAX, we provide the first polynomial-time approximation scheme (PTAS) for its offline problem, and give the first $\tilde{O}(\sqrt T)$ bound on the $(1-\epsilon)$-approximation regret of its online problem, for any $\epsilon>0$."
"Although multiple-input and multiple-output (MIMO) wireless systems achieve significant data rates (e.g., multiplexing) and reliability (e.g., diversity) and are main enablers for high-rate 5G wireless systems, MIMO receivers require high implementation complexity. A solution is the use of integer-forcing (IF) linear receivers which first create an effective integer channel matrix. In this paper, we propose an efficient algorithm to find the optimal integer coefficient matrix which maximizes the achievable rate of the IF linear receiver. The algorithm initializes with a suboptimal matrix, which is updated by a novel and efficient algorithm during the process of an improved version of sphere decoding until the optimal matrix is obtained. We theoretically show that the proposed algorithm indeed finds the optimal coefficient matrix. Furthermore, theoretically complexity analysis indicates that the complexity of the new algorithm in big-O notation is an order of magnitude smaller, with respect to the the dimension of the model matrix, than that of the so far most efficient algorithm. Simulation results are also presented to confirm the efficiency of our novel algorithm."
"Answering open-ended questions is an essential capability for any intelligent agent. One of the most interesting recent open-ended question answering challenges is Visual Question Answering (VQA) which attempts to evaluate a system's visual understanding through its answers to natural language questions about images. There exist many approaches to VQA, the majority of which do not exhibit deeper semantic understanding of the candidate answers they produce. We study the importance of generating plausible answers to a given question by introducing the novel task of `Answer Proposal': for a given open-ended question, a system should generate a ranked list of candidate answers informed by the semantics of the question. We experiment with various models including a neural generative model as well as a semantic graph matching one. We provide both intrinsic and extrinsic evaluations for the task of Answer Proposal, showing that our best model learns to propose plausible answers with a high recall and performs competitively with some other solutions to VQA."
"This paper studies the inverse power flow problem which is to infer the nodal admittance matrix and the network topology from voltage and current magnitudes and phase angles measured at a number of buses. We show that the admittance matrix can be uniquely identified from a sequence of measurements of different steady states when the system is fully observable, and a reduced admittance matrix (from Kron reduction) can be determined when the system is not fully observable (i.e., it contains some hidden nodes). Furthermore, we propose efficient algorithms based on graph theory and optimization to uncover the original admittance matrix of radial and mesh systems with hidden nodes. As a case study, we explore the application of the proposed algorithms to detect and locate faults and other critical events. Simulations performed on a standard test system confirm that these algorithms provide accurate estimates of the admittance matrix even when the measurements are perturbed by additive white Gaussian noise."
"The vast majority of recommender systems model preferences as static or slowly changing due to observable user experience. However, spontaneous changes in user preferences are ubiquitous in many domains like media consumption and key factors that drive changes in preferences are not directly observable. These latent sources of preference change pose new challenges. When systems do not track and adapt to users' tastes, users lose confidence and trust, increasing the risk of user churn. We meet these challenges by developing a model of novelty preferences that learns and tracks latent user tastes. We combine three innovations: a new measure of item similarity based on patterns of consumption co-occurrence; model for {\em spontaneous} changes in preferences; and a learning agent that tracks each user's dynamic preferences and learns individualized policies for variety. The resulting framework adaptively provides users with novelty tailored to their preferences for change per se."
"Inspired by connections to two dimensional quantum theory, we define several models of computation based on permuting distinguishable particles (which we call balls), and characterize their computational complexity. In the quantum setting, we find that the computational power of this model depends on the initial input states. More precisely, with a standard basis input state, we show how to approximate the amplitudes of this model within additive error using the model DQC1 (the class of problems solvable with one clean qubit), providing evidence that the model in this case is weaker than universal quantum computing. However, for specific choices of input states, the model is shown to be universal for BQP in an encoded sense. We use representation theory of the symmetric group to partially classify the computational complexity of this model for arbitrary input states. Interestingly, we find some input states which yield a model intermediate between DQC1 and BQP. Furthermore, we consider a restricted version of this model based on an integrable scattering problem in 1+1 dimensions. We show it is universal under postselection, if we allow intermediate destructive measurements and specific input states. Therefore, the existence of any classical procedure to sample from the output distribution of this model within multiplicative error implies collapse of polynomial hierarchy to its third level. Finally, we define a classical version of this model in which one can probabilistically permute balls. We find this yields a complexity class which is intermediate between L and BPP. Moreover, we find a nondeterministic version of this model is NP-complete."
"In this paper we present a new algorithm for computing a low rank approximation of the product $A^TB$ by taking only a single pass of the two matrices $A$ and $B$. The straightforward way to do this is to (a) first sketch $A$ and $B$ individually, and then (b) find the top components using PCA on the sketch. Our algorithm in contrast retains additional summary information about $A,B$ (e.g. row and column norms etc.) and uses this additional information to obtain an improved approximation from the sketches. Our main analytical result establishes a comparable spectral norm guarantee to existing two-pass methods; in addition we also provide results from an Apache Spark implementation that shows better computational and statistical performance on real-world and synthetic evaluation datasets."
"We present a mathematical analysis of linear precoders for downlink massive MIMO multiuser systems that employ one-bit digital-to-analog converters at the basestation in order to reduce complexity and mitigate power usage. The analysis is based on the Bussgang theorem, and applies generally to any linear precoding scheme. We examine in detail the special case of the quantized zero-forcing (ZF) precoder, and derive a simple asymptotic expression for the resulting symbol error rate at each terminal. Our analysis illustrates that the performance of the quantized ZF precoder depends primarily on the ratio of the number of antennas to the number of users, and our simulations show that it can outperform the much more complicated maximum likelihood encoder for low-to-moderate signal to noise ratios, where massive MIMO systems are presumed to operate. We also use the Bussgang theorem to derive a new linear precoder optimized for the case of one-bit quantization, and illustrate its improved performance."
"Stochastic gradient MCMC (SG-MCMC) has played an important role in large-scale Bayesian learning, with well-developed theoretical convergence properties. In such applications of SG-MCMC, it is becoming increasingly popular to employ distributed systems, where stochastic gradients are computed based on some outdated parameters, yielding what are termed stale gradients. While stale gradients could be directly used in SG-MCMC, their impact on convergence properties has not been well studied. In this paper we develop theory to show that while the bias and MSE of an SG-MCMC algorithm depend on the staleness of stochastic gradients, its estimation variance (relative to the expected estimate, based on a prescribed number of samples) is independent of it. In a simple Bayesian distributed system with SG-MCMC, where stale gradients are computed asynchronously by a set of workers, our theory indicates a linear speedup on the decrease of estimation variance w.r.t. the number of workers. Experiments on synthetic data and deep neural networks validate our theory, demonstrating the effectiveness and scalability of SG-MCMC with stale gradients."
"Fine-scale short-term cloud motion prediction is needed for several applications, including solar energy generation and satellite communications. In tropical regions such as Singapore, clouds are mostly formed by convection; they are very localized, and evolve quickly. We capture hemispherical images of the sky at regular intervals of time using ground-based cameras. They provide a high resolution and localized cloud images. We use two successive frames to compute optical flow and predict the future location of clouds. We achieve good prediction accuracy for a lead time of up to 5 minutes."
"Ground-based sky cameras (popularly known as Whole Sky Imagers) are increasingly used now-a-days for continuous monitoring of the atmosphere. These imagers have higher temporal and spatial resolutions compared to conventional satellite images. In this paper, we use ground-based sky cameras to detect the onset of rainfall. These images contain additional information about cloud coverage and movement and are therefore useful for accurate rainfall nowcast. We validate our results using rain gauge measurement recordings and achieve an accuracy of 89% for correct detection of rainfall onset."
We contribute a scalable implementation of Ryoo et al's Pooled Time Series algorithm from CVPR 2015. The updated algorithm has been evaluated on a large and diverse dataset of approximately 6800 videos collected from a crawl of the deep web related to human trafficking on DARPA's MEMEX effort. We describe the properties of Pooled Time Series and the motivation for using it to relate videos collected from the deep web. We highlight issues that we found while running Pooled Time Series on larger datasets and discuss solutions for those issues. Our solution centers are re-imagining Pooled Time Series as a Hadoop-based algorithm in which we compute portions of the eventual solution in parallel on large commodity clusters. We demonstrate that our new Hadoop-based algorithm works well on the 6800 video dataset and shares all of the properties described in the CVPR 2015 paper. We suggest avenues of future work in the project.
"It is critical and meaningful to make image classification since it can help human in image retrieval and recognition, object detection, etc. In this paper, three-sides efforts are made to accomplish the task. First, visual features with bag-of-words representation, not single vector, are extracted to characterize the image. To improve the performance, the idea of multi-view learning is implemented and three kinds of features are provided, each one corresponds to a single view. The information from three views is complementary to each other, which can be unified together. Then a new distance function is designed for bags by computing the weighted sum of the distances between instances. The technique of metric learning is explored to construct a data-dependent distance metric to measure the relationships between instances, meanwhile between bags and images, more accurately. Last, a novel approach, called MVML, is proposed, which optimizes the joint probability that every image is similar with its nearest image. MVML learns multiple distance metrics, each one models a single view, to unifies the information from multiple views. The method can be solved by alternate optimization iteratively. Gradient ascent and positive semi-definite projection are utilized in the iterations. Distance comparisons verified that the new bag distance function is prior to previous functions. In model evaluation, numerical experiments show that MVML with multiple views performs better than single view condition, which demonstrates that our model can assemble the complementary information efficiently and measure the distance between images more precisely. Experiments on influence of parameters and instance number validate the consistency of the method."
"The Federal Communications Commission's (FCC's) ongoing Incentive Auction will, if successful, transfer billions of dollars of radio spectrum from television broadcasters to mobile-network operators. Hundreds of broadcasters may go off the air. Most of those remaining on the air, including hundreds of Canadian broadcasters not bidding, will have to move to new channels to continue broadcasting. The auction can only end if all these broadcasters will fit into the spectrum remaining for television. Whether a given set of broadcasters fits is the broadcaster-repacking problem. The FCC must calculate its solutions thousands of times per round of bidding. Speed is essential.   By reducing the broadcaster-repacking problem to the maximum independent set problem, we show that the former is $\mathcal{NP}$-complete. This reduction also allows us to expand on sparsity-exploiting heuristics in the literature, which have made the FCC's repacking-problem instances tractable. We conclude by relating the heuristics to satisfiability and integer programming reductions. These provide a basis for implementing algorithms in off-the-shelf software to solve the broadcaster-repacking problem."
"We consider two-person zero-sum stochastic mean payoff games with perfect information, or BWR-games, given by a digraph $G = (V, E)$, with local rewards $r: E \to \ZZ$, and three types of positions: black $V_B$, white $V_W$, and random $V_R$ forming a partition of $V$. It is a long-standing open question whether a polynomial time algorithm for BWR-games exists, even when $|V_R|=0$. In fact, a pseudo-polynomial algorithm for BWR-games would already imply their polynomial solvability. In this short note, we show that BWR-games can be solved via convex programming in pseudo-polynomial time if the number of random positions is a constant."
"Respondent-driven sampling (RDS) is a popular method for sampling hard-to-survey populations that leverages social network connections through peer recruitment. While RDS is most frequently applied to estimate the prevalence of infections and risk behaviors of interest to public health, like HIV/AIDS or condom use, it is rarely used to draw inferences about the structural properties of social networks among such populations because it does not typically collect the necessary data. Drawing on recent advances in computer science, we introduce a set of data collection instruments and RDS estimators for network clustering, an important topological property that has been linked to a network's potential for diffusion of information, disease, and health behaviors. We use simulations to explore how these estimators, originally developed for random walk samples of computer networks, perform when applied to RDS samples with characteristics encountered in realistic field settings that depart from random walks. In particular, we explore the effects of multiple seeds, without vs. with replacement, branching chains, imperfect response rates, preferential recruitment, and misreporting of ties. We find that clustering coefficient estimators retain desirable properties in RDS samples. This paper takes an important step towards calculating network characteristics using non-traditional sampling methods, and it expands RDS's potential to tell researchers more about hidden populations and the social factors driving disease prevalence."
"The Sinc approximation is a function approximation formula that attains exponential convergence for rapidly decaying functions defined on the whole real axis. Even for other functions, the Sinc approximation still works accurately when combined with a proper variable transformation. The convergence rate has been analyzed for typical cases including finite, semi-infinite, and infinite intervals. Recently, for verified numerical computation, more explicit, ""computable"" error bound has been given in the case of a finite interval. In this paper, such explicit error bounds are given in the remaining cases."
"Nowadays, many applications rely on images of high quality to ensure good performance in conducting their tasks. However, noise goes against this objective as it is an unavoidable issue in most applications. Therefore, it is essential to develop techniques to attenuate the impact of noise, while maintaining the integrity of relevant information in images. We propose in this work to extend the application of the Non-Local Means filter (NLM) to the vector case and apply it for denoising multispectral images. The objective is to benefit from the additional information brought by multispectral imaging systems. The NLM filter exploits the redundancy of information in an image to remove noise. A restored pixel is a weighted average of all pixels in the image. In our contribution, we propose an optimization framework where we dynamically fine tune the NLM filter parameters and attenuate its computational complexity by considering only pixels which are most similar to each other in computing a restored pixel. Filter parameters are optimized using Stein's Unbiased Risk Estimator (SURE) rather than using ad hoc means. Experiments have been conducted on multispectral images corrupted with additive white Gaussian noise and PSNR and similarity comparison with other approaches are provided to illustrate the efficiency of our approach in terms of both denoising performance and computation complexity."
"The large spread of sensors and smart devices in urban and rural infrastructures are motivating research in the area of Internet of Thing (IoT), to develop new services and improve citizens' quality of life. Sensors and smart devices generate large amount of measurement data from sensing the environment, which is used to enable services, such as control power consumption or traffic density. To deal with such a large amount of information, and provide accurate measurements, service providers adopt information fusion, which, given the decentralized nature of urban or rural deployments, can be performed by means of consensus algorithms. These algorithms allow distributed agents to (iteratively) compute linear functions on the exchanged data, and take decisions based on the outcome, without the need for the support of a central entity. However, the use of consensus algorithms raises several security concerns, especially when private or security critical information are involved in the computation.   This paper proposes ODIN, a novel algorithm that allows information fusion over encrypted data. ODIN is a privacy-preserving extension of the popular consensus gossip algorithm, that prevents distributed agents have direct access to the data while they iteratively reach consensus; furthermore, agents cannot access even the final consensus value, but can only retrieve partial information, e.g., a binary decision. ODIN uses efficient additive obfuscation and proxy re-encryption during the update steps, and Garbled Circuits to take final decisions on the obfuscated consensus.   We discuss the security of our proposal, and show its practicability and efficiency on real-world resource constrained devices, developing a prototype implementation for Raspberry Pi devices."
"Recent work on discriminative segmental models has shown that they can achieve competitive speech recognition performance, using features based on deep neural frame classifiers. However, segmental models can be more challenging to train than standard frame-based approaches. While some segmental models have been successfully trained end to end, there is a lack of understanding of their training under different settings and with different losses.   We investigate a model class based on recent successful approaches, consisting of a linear model that combines segmental features based on an LSTM frame classifier. Similarly to hybrid HMM-neural network models, segmental models of this class can be trained in two stages (frame classifier training followed by linear segmental model weight training), end to end (joint training of both frame classifier and linear weights), or with end-to-end fine-tuning after two-stage training.   We study segmental models trained end to end with hinge loss, log loss, latent hinge loss, and marginal log loss. We consider several losses for the case where training alignments are available as well as where they are not.   We find that in general, marginal log loss provides the most consistent strong performance without requiring ground-truth alignments. We also find that training with dropout is very important in obtaining good performance with end-to-end training. Finally, the best results are typically obtained by a combination of two-stage training and fine-tuning."
"Two-stage optimization with recourse model is an important and widely used model, which has been studied extensively these years. In this article, we will look at a new variant of it, called the two-stage optimization with recourse and revocation model. This new model differs from the traditional one in that one is allowed to revoke some of his earlier decisions and withdraw part of the earlier costs, which is not unlikely in many real applications, and is therefore considered to be more realistic under many situations. We will adopt several approaches to study this model. In fact, we will develop an LP rounding scheme for some cover problems and show that they can be solved using this scheme and an adaptation of the rounding approach for the deterministic counterpart, provided the polynomial scenario assumption. Stochastic uncapacitated facility location problem will also be studied to show that the approximation algorithm that worked for the two-stage with recourse model worked for this model as well. In addition, we will use other methods to study the model."
"The explosive growth of Web 2.0, which was characterized by the creation of online social networks, has reignited the study of factors that could help us understand the growth and dynamism of these networks. Various generative network models have been proposed, including the Barabasi-Albert and Watts-Strogatz models. In this study, we revisit the problem from a perspective that seeks to compare results obtained from these generative models with those from real networks. To this end, we consider the dating network Skout Inc. An analysis is performed on the topological characteristics of the network that could explain the creation of new network links. Afterwards, the results are contrasted with those obtained from the Barabasi-Albert and Watts-Strogatz generative models. We conclude that a key factor that could explain the creation of links originates in its cluster structure, where link recommendations are more precise in Watts-Strogatz segmented networks than in Barabasi-Albert hierarchical networks. This result reinforces the need to establish more and better network segmentation algorithms that are capable of clustering large networks precisely and efficiently."
"In this paper, the sphere bound (SB) is revisited within a general bounding framework based on nested Gallager regions. The equivalence is revealed between the SB proposed by Herzberg and Poltyrev and the SB proposed by Kasami et al., whereas the latter was rarely cited in the literatures. Interestingly and importantly, the derivation of the SB based on nested Gallager regions suggests us a new simulation approach to performance evaluation of binary linear codes over additive white Gaussian noise (AWGN) channels. In order for the performance evaluation, the proposed approach decouples the geometrical structure of the code from the noise statistics. The former specifies the conditional error probabilities, which are independent of signal-to-noise ratios (SNRs) and can be simulated and estimated efficiently, while the latter determines the probabilities of those conditions, which involve SNRs and can be calculated numerically. Numerical results show that the proposed simulation approach matches well with the traditional simulation approach in the high error rate region but is able to evaluate efficiently the performance in the extremely low error rate region."
"The main difficulty when modelling gene networks is the identification of the parameters that govern their dynamics. It is particularly difficult for models in which time is continuous: parameters have real values which cannot be enumerated. The widespread idea is to infer new constraints that reduce the range of possible values. Here we present a new work based on a particular class of Hybrid automata (inspired by Thomas discrete models) where discrete parameters are replaced by signed celerities. We propose a new approach involving Hoare logic and weakest precondition calculus (a la Dijkstra) that generates constraints on the parameter values. Indeed, once proper specifications are extracted from biological traces with duration information (found in the literature or biological experiments), they play a role similar to imperative programs in the classical Hoare logic. We illustrate our hybrid Hoare logic on a small model controlling the lacI repressor of the lactose operon."
"We consider the design of an optimal mechanism for a seller selling two items to a single buyer so that the expected revenue to the seller is maximized. The buyer's valuation of the two items is assumed to be the uniform distribution over an arbitrary rectangle $[c_1,c_1+b_1]\times[c_2,c_2+b_2]$ in the positive quadrant. The solution to the case when $(c_1,c_2)=(0,0)$ was already known. We provide an explicit solution for arbitrary nonnegative values of $(c_1,c_2,b_1,b_2)$. We prove that the optimal mechanism is to sell the two items according to one of eight simple menus. We also prove that the solution is deterministic when either $c_1$ or $c_2$ is beyond a threshold. Finally, we conjecture that our methodology can be extended to a wider class of distributions. We also provide some preliminary results to support the conjecture."
"We present the forensic analysis of the artifacts generated on Android smartphones by ChatSecure, a secure Instant Messaging application that provides strong encryption for transmitted and locally-stored data to ensure the privacy of its users.   We show that ChatSecure stores local copies of both exchanged messages and files into two distinct, AES-256 encrypted databases, and we devise a technique able to decrypt them when the secret passphrase, chosen by the user as the initial step of the encryption process, is known.   Furthermore, we show how this passphrase can be identified and extracted from the volatile memory of the device, where it persists for the entire execution of ChatSecure after having been entered by the user, thus allowing one to carry out decryption even if the passphrase is not revealed by the user.   Finally, we discuss how to analyze and correlate the data stored in the databases used by ChatSecure to identify the IM accounts used by the user and his/her buddies to communicate, as well as to reconstruct the chronology and contents of the messages and files that have been exchanged among them.   For our study we devise and use an experimental methodology, based on the use of emulated devices, that provides a very high degree of reproducibility of the results, and we validate the results it yields against those obtained from real smartphones."
"This paper is about allocation of an infinitely divisible good to several rational and strategic agents. The allocation is done by a social planner who has limited information because the agents' valuation functions are taken to be private information known only to the respective agents. We allow only a scalar signal, called a bid, from each agent to the social planner. Yang and Hajek [Jour. on Selected Areas in Comm., 2007] as well as Johari and Tsitsiklis [Jour. of Oper. Res., 2009] proposed a scalar strategy Vickrey-Clarke-Groves (SSVCG) mechanism with efficient Nash equilibria. We consider a setting where the social planner desires minimal budget surplus. Example situations include fair sharing of Internet resources and auctioning of certain public goods where revenue maximization is not a consideration. Under the SSVCG framework, we propose a mechanism that is efficient and comes close to budget balance by returning much of the payments back to the agents in the form of rebates. We identify a design criterion for {\em almost budget balance}, impose feasibility and voluntary participation constraints, simplify the constraints, and arrive at a convex optimization problem to identify the parameters of the rebate functions. The convex optimization problem has a linear objective function and a continuum of linear constraints. We propose a solution method that involves a finite number of constraints, and identify the number of samples sufficient for a good approximation."
"We propose a new model-based method to accurately reconstruct human performances captured outdoors in a multi-camera setup. Starting from a template of the actor model, we introduce a new unified implicit representation for both, articulated skeleton tracking and nonrigid surface shape refinement. Our method fits the template to unsegmented video frames in two stages - first, the coarse skeletal pose is estimated, and subsequently non-rigid surface shape and body pose are jointly refined. Particularly for surface shape refinement we propose a new combination of 3D Gaussians designed to align the projected model with likely silhouette contours without explicit segmentation or edge detection. We obtain reconstructions of much higher quality in outdoor settings than existing methods, and show that we are on par with state-of-the-art methods on indoor scenes for which they were designed"
"In this work, we argue that current state-of-the-art methods of aircraft localization such as multilateration are insufficient, in particular for modern crowdsourced air traffic networks with random, unplanned deployment geometry. We propose an alternative, a grid-based localization approach using the k-Nearest Neighbor algorithm, to deal with the identified shortcomings. Our proposal does not require any changes to the existing air traffic protocols and transmitters, and is easily implemented using only low-cost, commercial-off-the-shelf hardware.   Using an algebraic multilateration algorithm for comparison, we evaluate our approach using real-world flight data collected with our collaborative sensor network OpenSky. We quantify its effectiveness in terms of aircraft location accuracy, surveillance coverage, and the verification of false position data. Our results show that the grid-based approach can increase the effective air traffic surveillance coverage compared to multilateration by a factor of up to 2.5. As it does not suffer from dilution of precision, it is much more robust in noisy environments and performs better in pre-existing, unplanned receiver deployments.   We further find that the mean aircraft location accuracy can be increased by up to 41% in comparison with multilateration while also being able to pinpoint the origin of potential spoofing attacks conducted from the ground."
"In this paper, we study the sensitivity of CNN outputs with respect to image transformations and noise in the area of fine-grained recognition. In particular, we answer the following questions (1) how sensitive are CNNs with respect to image transformations encountered during wild image capture?; (2) how can we predict CNN sensitivity?; and (3) can we increase the robustness of CNNs with respect to image degradations? To answer the first question, we provide an extensive empirical sensitivity analysis of commonly used CNN architectures (AlexNet, VGG19, GoogleNet) across various types of image degradations. This allows for predicting CNN performance for new domains comprised by images of lower quality or captured from a different viewpoint. We also show how the sensitivity of CNN outputs can be predicted for single images. Furthermore, we demonstrate that input layer dropout or pre-filtering during test time only reduces CNN sensitivity for high levels of degradation.   Experiments for fine-grained recognition tasks reveal that VGG19 is more robust to severe image degradations than AlexNet and GoogleNet. However, small intensity noise can lead to dramatic changes in CNN performance even for VGG19."
"We consider coloring problems in the distributed message-passing setting. The previously-known deterministic algorithms for edge-coloring employed at least (2Delta - 1) colors, even though any graph admits an edge-coloring with Delta + 1 colors [V64]. Moreover, the previously-known deterministic algorithms that employed at most O(Delta) colors required superlogarithmic time [B15,BE10,BE11,FHK15]. In the current paper we devise deterministic edge-coloring algorithms that employ only Delta + o(Delta) colors, for a very wide family of graphs. Specifically, as long as the arboricity is a = O(Delta^{1 - \epsilon}), for a constant epsilon > 0, our algorithm computes such a coloring within {polylogarithmic} deterministic time. We also devise significantly improved deterministic edge-coloring algorithms for {general graphs} for a very wide range of parameters. Specifically, for any value $\chi$ in the range [4Delta, 2^{o(log Delta)} \cdot Delta], our \chi-edge-coloring algorithm has smaller running time than the best previously-known \chi-edge-coloring algorithms. Our algorithms are actually much more general, since edge-coloring is equivalent to {vertex-coloring of line graphs.} Our method is applicable to vertex-coloring of the family of graphs with {bounded diversity} that contains line graphs, line graphs of hypergraphs, and many other graphs.   Our results are obtained using a novel technique that connects vertices or edges in a certain way that reduces clique size. The resulting structures, which we call {connectors}, can be colored more efficiently than the original graph. Moreover, the color classes constitute simpler subgraphs that can be colored even more efficiently using appropriate connectors. Hence, we recurse until we obtain sufficiently simple structures that are colored directly. We introduce several types of connectors that are useful for various scenarios."
We present new methods for batch anomaly detection in multivariate time series. Our methods are based on maximizing the Kullback-Leibler divergence between the data distribution within and outside an interval of the time series. An empirical analysis shows the benefits of our algorithms compared to methods that treat each time step independently from each other without optimizing with respect to all possible intervals.
"With its rapid growth and increasing adoption, big data is producing a growing impact in society. Its usage is opening both opportunities such as new business models and economic gains and risks such as privacy violations and discrimination. Europe is in need of a comprehensive strategy to optimise the use of data for a societal benefit and increase the innovation and competitiveness of its productive activities. In this paper, we contribute to the definition of this strategy with a research roadmap that considers the positive and negative externalities associated with big data, maps research and innovation topics in the areas of data management, processing, analytics, protection, visualisation, as well as non-technical topics, to the externalities they can tackle, and provides a time frame to address these topics."
"Verification problems of programs written in various paradigms (such as imperative, logic, concurrent, functional, and object-oriented ones) can be reduced to problems of solving Horn clause constraints on predicate variables that represent unknown inductive invariants. This paper presents a novel Horn constraint solving method based on inductive theorem proving: the method reduces Horn constraint solving to validity checking of first-order formulas with inductively defined predicates, which are then checked by induction on the derivation of the predicates. To automate inductive proofs, we introduce a novel proof system tailored to Horn constraint solving and use an SMT solver to discharge proof obligations arising in the proof search. The main advantage of the proposed method is that it can verify relational specifications across programs in various paradigms where multiple function calls need to be analyzed simultaneously. The class of specifications includes practically important ones such as functional equivalence, associativity, commutativity, distributivity, monotonicity, idempotency, and non-interference. Furthermore, our novel combination of Horn clause constraints with inductive theorem proving enables us to naturally and automatically axiomatize recursive functions that are possibly non-terminating, non-deterministic, higher-order, exception-raising, and over non-inductively defined data types. We have implemented a relational verification tool for the OCaml functional language based on the proposed method and obtained promising results in preliminary experiments."
"This paper analyzes the performance of linearly precoded time division duplex based multi-user massive MIMO downlink system under joint impacts of channel non-reciprocity (NRC) and imperfect channel state information (CSI). We consider a practical NRC model which accounts for transceiver frequency-response mismatches at both user equipment (UE) and base station (BS) sides as well as mutual coupling mismatches at BS. The analysis covers two most prominent forms of linear precoding schemes, namely, zero-forcing (ZF) and maximum-ratio transmission (MRT), and assumes that the statistical channel properties are used in the user side to decode the received signal. Closed-form analytical expressions are derived for the effective signal to interference and noise ratios (SINRs) and the corresponding capacity lower bounds, stemming from the developed signal and system models. The derived analytical expressions show that, in moderate to high SNR region, the additional interference caused by practical NRC levels degrades the performance of both precoders significantly. Moreover, the ZF is shown to be more sensitive to NRC with a much more severe performance loss compared to MRT. Numerical evaluations with practical NRC levels indicate that this performance loss in the received SINR can be as high as 80% for ZF, whereas it is typically less than 20% for MRT. The derived analytical expressions provide useful tools, e.g., in calculating the NRC calibration requirements in BSs and UEs for given specific performance targets in terms of the system capacity lower bound or effective SINR."
"In this paper we describe a deep network architecture that maps visual input to control actions for a robotic planar reaching task with 100% reliability in real-world trials. Our network is trained in simulation and fine-tuned with a limited number of real-world images. The policy search is guided by a kinematics-based controller (K-GPS), which works more effectively and efficiently than $\varepsilon$-Greedy. A critical insight in our system is the need to introduce a bottleneck in the network between the perception and control networks, and to initially train these networks independently."
"In this paper, we propose a general framework to learn a robust large-margin binary classifier when corrupt measurements, called anomalies, caused by sensor failure might be present in the training set. The goal is to minimize the generalization error of the classifier on non-corrupted measurements while controlling the false alarm rate associated with anomalous samples. By incorporating a non-parametric regularizer based on an empirical entropy estimator, we propose a Geometric-Entropy-Minimization regularized Maximum Entropy Discrimination (GEM-MED) method to learn to classify and detect anomalies in a joint manner. We demonstrate using simulated data and a real multimodal data set. Our GEM-MED method can yield improved performance over previous robust classification methods in terms of both classification accuracy and anomaly detection rate."
"Nowadays users get informed and shape their opinion through social media. However, the disintermediated access to contents does not guarantee quality of information. Selective exposure and confirmation bias, indeed, have been shown to play a pivotal role in content consumption and information spreading. Users tend to select information adhering (and reinforcing) their worldview and to ignore dissenting information. This pattern elicits the formation of polarized groups -- i.e., echo chambers -- where the interaction with like-minded people might even reinforce polarization. In this work we address news consumption around Brexit in UK on Facebook. In particular, we perform a massive analysis on more than 1 Million users interacting with Brexit related posts from the main news providers between January and July 2016. We show that consumption patterns elicit the emergence of two distinct communities of news outlets. Furthermore, to better characterize inner group dynamics, we introduce a new technique which combines automatic topic extraction and sentiment analysis. We compare how the same topics are presented on posts and the related emotional response on comments finding significant differences in both echo chambers and that polarization influences the perception of topics. Our results provide important insights about the determinants of polarization and evolution of core narratives on online debating."
"Cooperative Intelligent Transportation Systems (cITS) are a promising technology to enhance driving safety and efficiency. Vehicles communicate wirelessly with other vehicles and infrastructure, thereby creating a highly dynamic and heterogeneously managed ad-hoc network. It is these network properties that make it a challenging task to protect integrity of the data and guarantee its correctness. A major component is the problem that traditional security mechanisms like PKI-based asymmetric cryptography only exclude outsider attackers that do not possess key material. However, because attackers can be insiders within the network (i.e., possess valid key material), this approach cannot detect all possible attacks. In this survey, we present misbehavior detection mechanisms that can detect such insider attacks based on attacker behavior and information analysis. In contrast to well-known intrusion detection for classical IT systems, these misbehavior detection mechanisms analyze information semantics to detect attacks, which aligns better with highly application-tailored communication protocols foreseen for cITS. In our survey, we provide an extensive introduction to the cITS ecosystem and discuss shortcomings of PKI-based security. We derive and discuss a classification for misbehavior detection mechanisms, provide an in-depth overview of seminal papers on the topic, and highlight open issues and possible future research trends."
"In this paper, Kernel PCA is reinterpreted as the solution to a convex optimization problem. Actually, there is a constrained convex problem for each principal component, so that the constraints guarantee that the principal component is indeed a solution, and not a mere saddle point. Although these insights do not imply any algorithmic improvement, they can be used to further understand the method, formulate possible extensions and properly address them. As an example, a new convex optimization problem for semi-supervised classification is proposed, which seems particularly well-suited whenever the number of known labels is small. Our formulation resembles a Least Squares SVM problem with a regularization parameter multiplied by a negative sign, combined with a variational principle for Kernel PCA. Our primal optimization principle for semi-supervised learning is solved in terms of the Lagrange multipliers. Numerical experiments in several classification tasks illustrate the performance of the proposed model in problems with only a few labeled data."
"Task engagement is defined as loadings on energetic arousal (affect), task motivation, and concentration (cognition). It is usually challenging and expensive to label cognitive state data, and traditional computational models trained with limited label information for engagement assessment do not perform well because of overfitting. In this paper, we proposed two deep models (i.e., a deep classifier and a deep autoencoder) for engagement assessment with scarce label information. We recruited 15 pilots to conduct a 4-h flight simulation from Seattle to Chicago and recorded their electroencephalograph (EEG) signals during the simulation. Experts carefully examined the EEG signals and labeled 20 min of the EEG data for each pilot. The EEG signals were preprocessed and power spectral features were extracted. The deep models were pretrained by the unlabeled data and were fine-tuned by a different proportion of the labeled data (top 1%, 3%, 5%, 10%, 15%, and 20%) to learn new representations for engagement assessment. The models were then tested on the remaining labeled data. We compared performances of the new data representations with the original EEG features for engagement assessment. Experimental results show that the representations learned by the deep models yielded better accuracies for the six scenarios (77.09%, 80.45%, 83.32%, 85.74%, 85.78%, and 86.52%), based on different proportions of the labeled data for training, as compared with the corresponding accuracies (62.73%, 67.19%, 73.38%, 79.18%, 81.47%, and 84.92%) achieved by the original EEG features. Deep models are effective for engagement assessment especially when less label information was used for training."
"This work presents a multiscale framework to solve a class of stochastic optimal control problems in the context of robot motion planning \& control in a complex environment. In order to handle complications due to large decision space and complex environmental geometry, two key concepts are adopted: (a) diffusion wavelet representation of Markov chain for hierarchical abstraction of the state space; and (b) desirability function-based representation of a Markov decision process (MDP) for efficient calculation of optimal policy. In the proposed framework, a global plan that compressively takes into account long time/length-scale state transition is first obtained by approximately solving an MDP whose desirability function is represented by coarse scale bases in the hierarchical abstraction; then, a detailed local plan is computed by solving an MDP that considers wavelet bases associated with a focused region of the state space, guided by the global plan. The resulting multiscale plan is utilized to finally compute a continuous-time optimal control policy within a receding horizon implementation. Two numerical examples are presented to demonstrate the applicability and validity of the proposed approach."
"Growth in leisure travel has become increasingly significant economically, socially, and environmentally. However, flexible but uncoordinated travel behaviors exacerbate traffic congestion. Mobile phone records not only reveal human mobility patterns, but also enable us to manage travel demand for system efficiency. In this paper, we propose a location recommendation system that infers personal preferences while accounting for constraints imposed by road capacity in order to manage travel demand. We first infer unobserved preferences using a machine learning technique from phone records. We then formulate an optimization method to improve system efficiency. Coupling mobile phone data with traffic counts and road network infrastructures collected in Andorra, this study shows that uncoordinated travel behaviors lead to longer average travel delay, implying the opportunities in managing travel demand by collective decisions. The interplay between congestion relief and overall satisfied location preferences observed in extensive simulations indicate that moderate sacrifices of individual utility lead to significant travel time savings. Specifically, the results show that under full compliance rate, travel delay fell by 52% at a cost of 31% less satisfaction. Under 60% compliance rate, 41% travel delay is saved with a 17% reduction in satisfaction. This paper highlights the effectiveness of the synergy among collective behaviors in increasing system efficiency."
This paper analyze how congestion impact in accessibility in eight european cities
"In pliable index coding, we consider a server with $m$ messages and $n$ clients where each client has as side information a subset of the messages. We seek to minimize the number of broadcast transmissions, so that each client can recover any one unknown message she does not already have. Previous work has shown that the pliable index coding problem is NP-hard and requires at most $\mathcal{O}(\log^2(n))$ broadcast transmissions, which indicates exponential savings over the conventional index coding that requires in the worst case $\mathcal{O}(n)$ transmissions. In this work, building on a decoding criterion that we propose, we first design a deterministic polynomial-time algorithm that can realize the exponential benefits, by achieving, in the worst case, a performance upper bounded by $\mathcal{O}(\log^2(n))$ broadcast transmissions. We extend our algorithm to the $t$-requests case, where each client requires $t$ unknown messages that she does not have, and show that our algorithm requires at most $\mathcal{O}(t\log(n)+\log^2(n))$ broadcast transmissions. We construct lower bound instances that require at least $\Omega(\log(n))$ transmissions for linear pliable index coding and at least $\Omega(t+\log(n))$ transmissions for the $t$-requests case, indicating that our upper bounds are almost tight. Finally, we provide a probabilistic analysis and show that the required number of transmissions is almost surely $\Theta(\log(n))$, as compared to $\Theta(n/\log(n))$ for index coding. Our numerical experiments show that our algorithm outperforms existing algorithms for pliable index coding by up to $50\%$ less transmissions."
"Dense cellular networks (DenseNets) are fast becoming a reality with the rapid deployment of base stations (BSs) aimed at meeting the explosive data traffic demand. In legacy systems however this comes with the penalties of higher network interference and energy consumption. In order to support network densification in a sustainable manner, the system behavior should be made 'load-proportional' thus allowing certain portions of the network to activate on-demand. In this work, we develop an analytical framework using tools from stochastic geometry theory for the performance analysis of DenseNets where load-awareness is explicitly embedded in the design. The model leverages on a flexible cellular network architecture where there is a complete separation of the data and signaling communication functionalities. Using the proposed model, we identify the most energy- efficient deployment solution for meeting certain minimum service criteria and analyze the corresponding power savings through dynamic sleep modes. Based on state-of-the-art system parameters, a homogeneous pico deployment for the data plane with a separate layer of signaling macro-cells is revealed to be the most energy-efficient solution in future dense urban environments."
"We present a novel Metropolis-Hastings method for large datasets that uses small expected-size minibatches of data. Previous work on reducing the cost of Metropolis-Hastings tests yield variable data consumed per sample, with only constant factor reductions versus using the full dataset for each sample. Here we present a method that can be tuned to provide arbitrarily small batch sizes, by adjusting either proposal step size or temperature. Our test uses the noise-tolerant Barker acceptance test with a novel additive correction variable. The resulting test has similar cost to a normal SGD update. Our experiments demonstrate several order-of-magnitude speedups over previous work."
"Solving compressed sensing problems relies on the properties of sparse signals. It is commonly assumed that the sparsity s needs to be less than one half of the spark of the sensing matrix A, and then the unique sparsest solution exists, and recoverable by $\ell_1$-minimization or related procedures. We discover, however, a measure theoretical uniqueness exists for nearly spark-level sparsity from compressed measurements Ax = b. Specifically, suppose A is of full spark with m rows, and suppose $\frac{m}{2}$ < s < m. Then the solution to Ax = b is unique for x with $\|x\|_0 \leq s$ up to a set of measure 0 in every s-sparse plane. This phenomenon is observed and confirmed by an $\ell_1$-tail minimization procedure, which recovers sparse signals uniquely with s > $\frac{m}{2}$ in thousands and thousands of random tests. We further show instead that the mere $\ell_1$-minimization would actually fail if s > $\frac{m}{2}$ even from the same measure theoretical point of view."
"This paper gives a short survey of recent trends in the emerging field of big data. It explains the definitions and useful methods. In addition, application fields of smart buildings and smart grids are discussed."
"In recent years, traditional cybersecurity safeguards have proven ineffective against insider threats. Famous cases of sensitive information leaks caused by insiders, including the WikiLeaks release of diplomatic cables and the Edward Snowden incident, have greatly harmed the U.S. government's relationship with other governments and with its own citizens. Data Leak Prevention (DLP) is a solution for detecting and preventing information leaks from within an organization's network. However, state-of-art DLP detection models are only able to detect very limited types of sensitive information, and research in the field has been hindered due to the lack of available sensitive texts. Many researchers have focused on document-based detection with artificially labeled ""confidential documents"" for which security labels are assigned to the entire document, when in reality only a portion of the document is sensitive. This type of whole-document based security labeling increases the chances of preventing authorized users from accessing non-sensitive information within sensitive documents. In this paper, we introduce Automated Classification Enabled by Security Similarity (ACESS), a new and innovative detection model that penetrates the complexity of big text security classification/detection. To analyze the ACESS system, we constructed a novel dataset, containing formerly classified paragraphs from diplomatic cables made public by the WikiLeaks organization. To our knowledge this paper is the first to analyze a dataset that contains actual formerly sensitive information annotated at paragraph granularity."
"Clinical guidance systems have been widely adopted to help medical staffs to avoid preventable medical errors such as delay in diagnosis, treatment or untended deviations from best practice guidelines. However, because patient condition changes rapidly and medical staffs are usually overloaded in acute care setting, how to ensure the correctness of the system reaction to those rapid changes and managing interaction with physicians remains challenging. In this paper, we propose a domain-specific model-driven design approach to address these challenges for designing clinical guidance systems. Firstly, we translate the relevant medical knowledge described in generic literature and different best practice guidelines into a set of executable and indirectly verifiable finite state machine models. We introduce an organ-centric paradigm to construct clinical models, and also develop a physician model to track physician-system interactions and deviations. Secondly, for verification of the compositional system model, we translate the model into timed automata, based on which, we formalize a set of clinical and system safety requirements as computation tree logic(CTL) formulas and use the UPPAAL model checking tool to formally verify those requirements. In this way, the correctness of the model can be mathematically proved. Finally, we can automatically generate the executable code from the verified model using the corresponding code generation tools for finite state machine. For evaluation, we apply the approach to the design of clinical guidance systems for cardiac arrest. The generated code can be deployed and interact with existing guidance systems."
"General image classification approaches differentiate classes using strong distinguishing features but some classes cannot be easily separated because they contain very similar visual features. To deal with this problem, we can use keywords relevant to a particular class. To implement this concept we have newly constructed a malicious crowd dataset which contains crowd images with two events, benign and malicious, which look similar yet involve opposite semantic events. We also created a set of five malicious event-relevant keywords such as police and fire. In the evaluation, integrating malicious event classification with recognition output of these keywords enhances the overall performance on the malicious crowd dataset."
"In this paper, we introduce a novel fusion method that can enhance object detection performance by fusing decisions from two different types of computer vision tasks: object detection and image classification. In the proposed work, the class label of an image obtained from image classification is viewed as prior knowledge about existence or non-existence of certain objects. The prior knowledge is then fused with the decisions of object detection to improve detection accuracy by mitigating false positives of an object detector that are strongly contradicted with the prior knowledge. A recently introduced novel fusion approach called dynamic belief fusion (DBF) is used to fuse the detector output with the classification prior. Experimental results show that the detection performance of all the detection algorithms used in the proposed work is improved on benchmark datasets via the proposed fusion framework."
"Automatic construction of large knowledge graphs (KG) by mining web-scale text datasets has received considerable attention over the last few years, resulting in the construction of several KGs, such as NELL, Google Knowledge Vault, etc. These KGs consist of thousands of predicate-relations (e.g., isPerson, isMayorOf ) and millions of their instances (e.g., (Bill de Blasio, isMayorOf, New York City)). Estimating accuracy of such automatically constructed KGs is a challenging problem due to their size and diversity. Even though crowdsourcing is an obvious choice for such evaluation, the standard single-task crowdsourcing, where each predicate in the KG is evaluated independently, is very expensive and especially problematic if the budget available is limited. We show that such approaches are sub-optimal as they ignore dependencies among various predicates and their instances. To overcome this challenge, we propose Relational Crowdsourcing (RelCrowd), where the tasks are created while taking dependencies among predicates and instances into account. We apply this framework in the context of evaluation of large-scale KGs and demonstrate its effectiveness through extensive experiments on real-world datasets."
"We ask whether neural networks can learn to use secret keys to protect information from other neural networks. Specifically, we focus on ensuring confidentiality properties in a multiagent system, and we specify those properties in terms of an adversary. Thus, a system may consist of neural networks named Alice and Bob, and we aim to limit what a third neural network named Eve learns from eavesdropping on the communication between Alice and Bob. We do not prescribe specific cryptographic algorithms to these neural networks; instead, we train end-to-end, adversarially. We demonstrate that the neural networks can learn how to perform forms of encryption and decryption, and also how to apply these operations selectively in order to meet confidentiality goals."
"We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragmatic (PRA), an architecture that exploits it improving performance and energy efficiency. The source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product [1]. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms using a) on-the-fly conversion of the multiplicator representation into an explicit list of powers of two, and b) hybrid bit-parallel multplicand/bit-serial multiplicator processing units. PRA exploits two sources of ineffectual computations: 1) the aforementioned zero product terms which are the result of the lack of explicitness in the multiplicator representation, and 2) the excess in the representation precision used for both multiplicants and multiplicators, e.g., [2]. Measurements demonstrate that for the convolutional layers, a straightforward variant of PRA improves performance by 2.6x over the DaDiaNao (DaDN) accelerator [3] and by 1.4x over STR [4]. Similarly, PRA improves energy efficiency by 28% and 10% on average compared to DaDN and STR. An improved cross lane synchronication scheme boosts performance improvements to 3.1x over DaDN. Finally, Pragmatic benefits persist even with an 8-bit quantized representation [5]."
"Tourists and Wild-life photographers are often hindered in capturing their cherished images or videos by a fence that limits accessibility to the scene of interest. The situation has been exacerbated by growing concerns of security at public places and a need exists to provide a tool that can be used for post-processing such fenced videos to produce a de-fenced image. There are several challenges in this problem, we identify them as Robust detection of fence/occlusions and Estimating pixel motion of background scenes and Filling in the fence/occlusions by utilizing information in multiple frames of the input video. In this work, we aim to build an automatic post-processing tool that can efficiently rid the input video of occlusion artifacts like fences. Our work is distinguished by two major contributions. The first is the introduction of learning based technique to detect the fences patterns with complicated backgrounds. The second is the formulation of objective function and further minimization through loopy belief propagation to fill-in the fence pixels. We observe that grids of Histogram of oriented gradients descriptor using Support vector machines based classifier significantly outperforms detection accuracy of texels in a lattice. We present results of experiments using several real-world videos to demonstrate the effectiveness of the proposed fence detection and de-fencing algorithm."
"We present a simple algorithm for explicitly computing all k shortest paths bounded by length L from a fixed source to a target in O(m + kL) and O(mlogm + kL) time for unweighted and weighted directed graphs with m edges respectively. For many graphs, this outperforms existing algorithms by exploiting the fact that real world networks have short average path length. Consequently, we would like to adapt our almost shortest paths algorithm to find an efficient solution to the almost short- est simple paths, where we exclude paths that visit any node more than once. To this end, we consider realizations from the Chung-Lu random graph model as the Chung-Lu random graph model is not only amenable to analysis, but also emulates many of the properties frequently observed in real world networks including the small world phenomenon and degree heterogeneity. We provide theoretical and numeric evidence regarding the efficiency of utilizing our almost shortest paths algorithm to find al- most shortest simple paths for Chung-Lu random graphs for a wide range of parameters. Finally, we consider a special application of our almost shortest paths algorithm to study internet routing (withdrawals) in the Autonomous System graph."
This report presents attack analysis results of the first adversarial engagement event stream for the first engagement of the DARPA TC program conducted in October 2016. The analysis was performed by Stony Brook University and University of Illinois at Chicago. The findings in this report are obtained without prior knowledge of the attacks conducted.
"Since Shannon proved that Gaussian distribution is the optimum for a linear channel with additive white Gaussian noise and he calculated the corresponding channel capacity, it remains the most applied distribution in optical communications while the capacity result is celebrated as the seminal linear Shannon limit. Yet, when it is applied in nonlinear channels (e.g. fiber-optics) it has been shown to be non-optimum, yielding the same result as for uncoded transmission in the high nonlinear regime. This has led to the notion of nonlinear Shannon limit, which predicts vanishing capacity at high nonlinearity. However, recent findings indicate that non-Gaussian distribution may lead to improved capacity estimations, urging for an exciting search for novel methods in nonlinear optical communications. Here for the first time, we show that it is possible to transmit information above the existing limits by using a novel probabilistic shaping of the input signal, which we call ripple distribution"
"Studying metabolic networks is vital for many areas such as novel drugs and bio-fuels. For biologists, a key challenge is that many reactions are impractical or expensive to be found through experiments. Our task is to recover the missing reactions. By exploiting the problem structure, we model reaction recovery as a hyperlink prediction problem, where each reaction is regarded as a hyperlink connecting its participating vertices (metabolites). Different from the traditional link prediction problem where two nodes form a link, a hyperlink can involve an arbitrary number of nodes. Since the cardinality of a hyperlink is variable, existing classifiers based on a fixed number of input features become infeasible. Traditional methods, such as common neighbors and Katz index, are not applicable either, since they are restricted to pairwise similarities. In this paper, we propose a novel hyperlink prediction algorithm, called Matrix Boosting (MATBoost). MATBoost conducts inference jointly in the incidence space and adjacency space by performing an iterative completion-matching optimization. We carry out extensive experiments to show that MATBoost achieves state-of-the-art performance. For a metabolic network with 1805 metabolites and 2583 reactions, our algorithm can successfully recover nearly 200 reactions out of 400 missing reactions."
"Smartphones and other mobile devices are today pervasive across the globe. As an interesting side effect of the surge in mobile communications, mobile network operators can now easily collect a wealth of high-resolution data on the habits of large user populations. The information extracted from mobile network traffic data is very relevant in the context of population mapping: it provides a tool for the automatic and live estimation of population densities, overcoming the limitations of traditional data sources such as censuses and surveys. In this paper, we propose a new approach to infer population densities at urban scales, based on aggregated mobile network traffic metadata. Our approach allows estimating both static and dynamic populations, achieves a significant improvement in terms of accuracy with respect to state-of-the-art solutions in the literature, and is validated on different city scenarios."
"We present a regularity lemma for Boolean functions $f:\{-1,1\}^n \to \{-1,1\}$ based on noisy influence, a measure of how locally correlated $f$ is with each input bit. We provide an application of the regularity lemma to weaken the conditions on the Majority is Stablest Theorem. We also prove a ""homogenized"" version stating that there is a set of input bits so that most restrictions of $f$ on those bits have small noisy influences. These results were sketched out by [OSTW10], but never published. With their permission, we present the full details here."
"Saturated hydraulic conductivity Ksat is a fundamental characteristic in modeling flow and contaminant transport in soils and sediments. Therefore, many models have been developed to estimate Ksat from easily measureable parameters, such as textural properties, bulk density, etc. However, Ksat is not only affected by textural and structural characteristics, but also by scale e.g., internal diameter and height. Using the UNSODA database and the contrast pattern aided regression (CPXR) method, we recently developed scale-dependent pedotransfer functions to estimate Ksat from textural data, bulk density, and sample dimensions. The main objectives of this study were evaluating the proposed pedotransfer functions using a larger database, and comparing them with seven other models. For this purpose, we selected more than nineteen thousands soil samples from all around the United States. Results showed that the scale-dependent pedotransfer functions estimated Ksat more accurately than seven other models frequently used in the literature."
"Decision makers, such as doctors and judges, make crucial decisions such as recommending treatments to patients, and granting bails to defendants on a daily basis. Such decisions typically involve weighting the potential benefits of taking an action against the costs involved. In this work, we aim to automate this task of learning \emph{cost-effective, interpretable and actionable treatment regimes}. We formulate this as a problem of learning a decision list -- a sequence of if-then-else rules -- which maps characteristics of subjects (eg., diagnostic test results of patients) to treatments. We propose a novel objective to construct a decision list which maximizes outcomes for the population, and minimizes overall costs. We model the problem of learning such a list as a Markov Decision Process (MDP) and employ a variant of the Upper Confidence Bound for Trees (UCT) strategy which leverages customized checks for pruning the search space effectively. Experimental results on real world observational data capturing judicial bail decisions and treatment recommendations for asthma patients demonstrate the effectiveness of our approach."
"Random linear network coding (RLNC) has been shown to efficiently improve the network performance in terms of reducing transmission delays and increasing the throughput in broadcast and multicast communications. However, it can result in increased storage and computational complexity at the receivers end. In our previous work we considered the broadcast transmission of large file to N receivers. We showed that the storage and complexity requirements at the receivers end can be greatly reduced when segmenting the file into smaller blocks and applying RLNC to these blocks. To that purpose, we proposed a packet scheduling policy, namely the Least Received. In this work we will prove the optimality of our previously proposed policy, in terms of file transfer completion time, when N = 2. We will model our system as a Markov Decision Process and prove the optimality of the policy using Dynamic Programming. Our intuition is that the Least Received policy may be optimal regardless of the number of receivers. Towards that end, we will provide experimental results that verify that ntuition."
"The increasing ability to collect data from urban environments, coupled with a push towards openness by governments, has resulted in the availability of numerous spatio-temporal data sets covering diverse aspects of a city. Discovering relationships between these data sets can produce new insights by enabling domain experts to not only test but also generate hypotheses. However, discovering these relationships is difficult. First, a relationship between two data sets may occur only at certain locations and/or time periods. Second, the sheer number and size of the data sets, coupled with the diverse spatial and temporal scales at which the data is available, presents computational challenges on all fronts, from indexing and querying to analyzing them. Finally, it is non-trivial to differentiate between meaningful and spurious relationships. To address these challenges, we propose Data Polygamy, a scalable topology-based framework that allows users to query for statistically significant relationships between spatio-temporal data sets. We have performed an experimental evaluation using over 300 spatial-temporal urban data sets which shows that our approach is scalable and effective at identifying interesting relationships."
"In this paper, we propose and compare two spectral angle based approaches for spatial-spectral classification. Our methods use the spectral angle to generate unary energies in a grid-structured Markov random field defined over the pixel labels of a hyperspectral image. The first approach is to use the exponential spectral angle mapper (ESAM) kernel/covariance function, a spectral angle based function, with the support vector machine and the Gaussian process classifier. The second approach is to directly use the minimum spectral angle between the test pixel and the training pixels as the unary energy. We compare the proposed methods with the state-of-the-art Markov random field methods that use support vector machines and Gaussian processes with squared exponential kernel/covariance function. In our experiments with two datasets, it is seen that using minimum spectral angle as unary energy produces better or comparable results to the existing methods at a smaller running time."
"Statistical models have been successful in accurately estimating the biochemical contents of vegetation from the reflectance spectra. However, their performance deteriorates when there is a scarcity of sizable amount of ground truth data for modeling the complex non-linear relationship occurring between the spectrum and the biochemical quantity. We propose a novel Gaussian process based multitask learning method for improving the prediction of a biochemical through the transfer of knowledge from the learned models for predicting related biochemicals. This method is most advantageous when there are few ground truth data for the biochemical of interest, but plenty of ground truth data for related biochemicals. The proposed multitask Gaussian process hypothesizes that the inter-relationship between the biochemical quantities is better modeled by using a combination of two or more covariance functions and inter-task correlation matrices. In the experiments, our method outperformed the current methods on two real-world datasets."
"Non-orthogonal multiple access (NOMA) serves multiple users by superposing their distinct message signals. The desired message signal is decoded at the receiver by applying successive interference cancellation (SIC). Using the theory of Poisson cluster process (PCP), we provide a framework to analyze multi-cell uplink NOMA systems. Specifically, we characterize the rate coverage probability of a NOMA user who is at rank $m$ (in terms of the distance from its serving BS) among all users in a cell and the mean rate coverage probability of all users in a cell. Since the signal-to-interference-plus-noise ratio (SINR) of $m$-th user relies on efficient SIC, we consider three scenarios, i.e., perfect SIC (in which the signals of $m-1$ interferers who are stronger than $m$-th user are decoded successfully), imperfect SIC (in which the signals of of $m-1$ interferers who are stronger than $m$-th user may or may not be decoded successfully), and imperfect worst case SIC (in which the decoding of the signal of $m$-th user is always unsuccessful whenever the decoding of its relative $m-1$ stronger users is unsuccessful). The derived expressions are customized to capture the performance of a user at rank $m$ in an equivalent orthogonal multiple access (OMA) system. Finally, numerical results are presented to validate the derived expressions."
"We present a new system of automation for separation logic in the interactive theorem prover Isabelle. The system is based on the recently developed auto2 prover, and follows a natural, saturation-based approach to reasoning about imperative programs. In addition to standard examples on linked lists and binary search trees, we apply the automation to red-black trees and indexed priority queues, showing that it provides a high degree of automation even on the more complicated data structures."
"In classification problems when multiples algorithms are applied to different benchmarks a difficult issue arises, i.e., how can we rank the algorithms? In machine learning it is common run the algorithms several times and then a statistic is calculated in terms of means and standard deviations. In order to compare the performance of the algorithms, it is very common to employ statistical tests. However, these tests may also present limitations, since they consider only the means and not the standard deviations of the obtained results. In this paper, we present the so called A-TOPSIS, based on TOPSIS (Technique for Order Preference by Similarity to Ideal Solution), to solve the problem of ranking and comparing classification algorithms in terms of means and standard deviations. We use two case studies to illustrate the A-TOPSIS for ranking classification algorithms and the results show the suitability of A-TOPSIS to rank the algorithms. The presented approach is general and can be applied to compare the performance of stochastic algorithms in machine learning. Finally, to encourage researchers to use the A-TOPSIS for ranking algorithms we also presented in this work an easy-to-use A-TOPSIS web framework."
"Understanding political phenomena requires measuring the political preferences of society. We introduce a model based on mixtures of spatial voting models that infers the underlying distribution of political preferences of voters with only voting records of the population and political positions of candidates in an election. Beyond offering a cost-effective alternative to surveys, this method projects the political preferences of voters and candidates into a shared latent preference space. This projection allows us to directly compare the preferences of the two groups, which is desirable for political science but difficult with traditional survey methods. After validating the aggregated-level inferences of this model against results of related work and on simple prediction tasks, we apply the model to better understand the phenomenon of political polarization in the Texas, New York, and Ohio electorates. Taken at face value, inferences drawn from our model indicate that the electorates in these states may be less bimodal than the distribution of candidates, but that the electorates are comparatively more extreme in their variance. We conclude with a discussion of limitations of our method and potential future directions for research."
"Kernel normalization methods have been employed to improve robustness of optimization methods to reparametrization of convolution kernels, covariate shift, and to accelerate training of Convolutional Neural Networks (CNNs). However, our understanding of theoretical properties of these methods has lagged behind their success in applications. We develop a geometric framework to elucidate underlying mechanisms of a diverse range of kernel normalization methods. Our framework enables us to expound and identify geometry of space of normalized kernels. We analyze and delineate how state-of-the-art kernel normalization methods affect the geometry of search spaces of the stochastic gradient descent (SGD) algorithms in CNNs. Following our theoretical results, we propose a SGD algorithm with assurance of almost sure convergence of the methods to a solution at single minimum of classification loss of CNNs. Experimental results show that the proposed method achieves state-of-the-art performance for major image classification benchmarks with CNNs."
"In the recent years, the rapid spread of mobile device has create the vast amount of mobile data. However, some shallow-structure models such as support vector machine (SVM) have difficulty dealing with high dimensional data with the development of mobile network. In this paper, we analyze mobile data to predict human trajectories in order to understand human mobility pattern via a deep-structure model called ""DeepSpace"". To the best of out knowledge, it is the first time that the deep learning approach is applied to predicting human trajectories. Furthermore, we develop the vanilla convolutional neural network (CNN) to be an online learning system, which can deal with the continuous mobile data stream. In general, ""DeepSpace"" consists of two different prediction models corresponding to different scales in space (the coarse prediction model and fine prediction models). This two models constitute a hierarchical structure, which enable the whole architecture to be run in parallel. Finally, we test our model based on the data usage detail records (UDRs) from the mobile cellular network in a city of southeastern China, instead of the call detail records (CDRs) which are widely used by others as usual. The experiment results show that ""DeepSpace"" is promising in human trajectories prediction."
"Let G=(V,E) be a connected graph. A set U subseteq V is convex if G[U] is connected and all vertices of V\U have at most one neighbor in U. Let sigma(W) denote the unique smallest convex set that contains W subseteq V. Two players play the following game. Consider a convex set U and call it the `playground.' Initially, U = emptyset. When U=V, the player to move loses the game. Otherwise, that player chooses a vertex x in V\U which is at distance at most two from U. The effect of the move is that the playground U changes into sigma(U cup {x}) and the opponent is presented with this new playground.   A graph is chordal bipartite if it is bipartite and has no induced cycle of length more than four. In this paper we show that, when G is chordal bipartite, there is a polynomial-time algorithm that computes the Grundy number of the P_3-game played on G. This implies that there is an efficient algorithm to decide whether the first player has a winning strategy."
"The ability to accurately observe human motion and identify human activities is essential for developing automatic rehabilitation and sports training systems. In this paper, large-scale exercise motion data obtained from a forearm-worn wearable sensor are classified with a convolutional neural network (CNN). Time series data consisting of accelerometer and orientation measurements are formatted as ""images"", allowing the CNN to automatically extract discriminative features. The resulting CNN classifies 50 gym exercises with 92.14% accuracy. A comparative study on the effects of image formatting and different CNN architectures is also presented."
"Semantic data fuels many different applications, but is still lacking proper integration into programming languages. Untyped access is error-prone while mapping approaches cannot fully capture the conceptualization of semantic data. In this paper, we present $\lambda_{DL}$,a $\lambda$-calculus with a modified type system to provide type-safe integration of semantic data. This is achieved by the integration of description logics into the $\lambda$-calculus for typing and data access. It is centered around several key design principles. Among these are (1) the usage of semantic conceptualizations as types, (2) subtype inference for these types, and (3) type-checked query access to the data by both ensuring the satisfiability of queries as well as typing query results precisely in $\lambda_{DL}$. The paper motivates the use of a modified type system for semantic data and it provides the theoretic foundation for the integration of description logics as well as the core formal specifications of $\lambda_{DL}$ including a proof of type safety."
"In this paper, we propose novel non-linear precoders for the downlink of a multi-user MIMO system with the existence of multiple eavesdroppers. The proposed non-linear precoders are designed to improve the physical-layer secrecy rate. Specifically, we combine the non-linear successive optimization Tomlinson-Harashima precoding (SO-THP) with generalized matrix inversion (GMI) technique to maximize the physical-layer secrecy rate. For the purpose of comparison, we examine different traditional precoders with the proposed algorithm in terms of secrecy rate as well as BER performance. We also investigate simplified generalized matrix inversion (S-GMI) and lattice-reduction (LR) techniques in order to efficiently compute the parameters of the precoders. We further conduct computational complexity and secrecy rate analysis of the proposed and existing algorithms. In addition, in the scenario without knowledge of channel state information (CSI) to the eavesdroppers, a strategy of injecting artificial noise (AN) prior to the transmission is employed to enhance the physical-layer secrecy rate. Simulation results show that the proposed non-linear precoders outperform existing precoders in terms of BER and secrecy rate performance."
"A fairly general class of Bayesian ""large-error"" lower bounds of the Weiss-Weinstein family, essentially free from regularity conditions on the probability density functions support, and for which a limiting form yields a generalized Bayesian Cramer-Rao bound (BCRB), is introduced. In a large number of cases, the generalized BCRB appears to be the Bobrovsky-Mayer-Wolf-Zakai bound (BMZB). Interestingly enough, a regularized form of the Bobrovsky-Zakai bound (BZB), applicable when the support of the prior is a constrained parameter set, is obtained. Modified Weiss-Weinstein bound and BZB which limiting form is the BMZB are proposed, in expectation of an increased tightness in the threshold region. Some of the proposed results are exemplified with a reference problem in signal processing: the Gaussian observation model with parameterized mean and uniform prior."
"Floating point error is an inevitable drawback of embedded systems implementation. Computing rigorous upper bounds of roundoff errors is absolutely necessary to the validation of critical software. This problem is even more challenging when addressing non-linear programs. In this paper, we propose and compare two new methods based on Bernstein expansions and sparse Krivine-Stengle representations, adapted from the field of the global optimization to compute upper bounds of roundoff errors for programs implementing polynomial functions. We release two related software package FPBern and FPKiSten, and compare them with state of the art tools.   We show that these two methods achieve competitive performance compared with the other tools, and produce more accurate upper bounds for 25% of all tested benchmarks."
"We introduce heap automata, a formalism for automatic reasoning about robustness properties of the symbolic heap fragment of separation logic with user-defined inductive predicates. Robustness properties, such as satisfiability, reachability, and acyclicity, are important for a wide range of reasoning tasks in automated program analysis and verification based on separation logic. Previously, such properties have appeared in many places in the separation logic literature, but have not been studied in a systematic manner. In this paper, we develop an algorithmic framework based on heap automata that allows us to derive asymptotically optimal decision procedures for a wide range of robustness properties in a uniform way. We implemented a protoype of our framework and obtained promising results for all of the aforementioned robustness properties. Further, we demonstrate the applicability of heap automata beyond robustness properties. We apply our algorithmic framework to the model checking and the entailment problem for symbolic-heap separation logic."
"Many countries are suffering from severe air pollution. Understanding how different air pollutants accumulate and propagate is critical to making relevant public policies. In this paper, we use urban big data (air quality data and meteorological data) to identify the \emph{spatiotemporal (ST) causal pathways} for air pollutants. This problem is challenging because: (1) there are numerous noisy and low-pollution periods in the raw air quality data, which may lead to unreliable causality analysis, (2) for large-scale data in the ST space, the computational complexity of constructing a causal structure is very high, and (3) the \emph{ST causal pathways} are complex due to the interactions of multiple pollutants and the influence of environmental factors. Therefore, we present \emph{p-Causality}, a novel pattern-aided causality analysis approach that combines the strengths of \emph{pattern mining} and \emph{Bayesian learning} to efficiently and faithfully identify the \emph{ST causal pathways}. First, \emph{Pattern mining} helps suppress the noise by capturing frequent evolving patterns (FEPs) of each monitoring sensor, and greatly reduce the complexity by selecting the pattern-matched sensors as ""causers"". Then, \emph{Bayesian learning} carefully encodes the local and ST causal relations with a Gaussian Bayesian network (GBN)-based graphical model, which also integrates environmental influences to minimize biases in the final results. We evaluate our approach with three real-world data sets containing 982 air quality sensors, in three regions of China from 01-Jun-2013 to 19-Dec-2015. Results show that our approach outperforms the traditional causal structure learning methods in time efficiency, inference accuracy and interpretability."
"Ranking scientific authors is an important but challenging task, mostly due to the dynamic nature of the evolving scientific publications. The basic indicators of an author's productivity and impact are still the number of publications and the citation count (leading to the popular metrics such as h-index, g-index etc.). H-index and its popular variants are mostly effective in ranking highly-cited authors, thus fail to resolve ties while ranking medium-cited and low-cited authors who are majority in number. Therefore, these metrics are inefficient to predict the ability of promising young researchers at the beginning of their career. In this paper, we propose $C^3$-index that combines the effect of citations and collaborations of an author in a systematic way using a weighted multi-layered network to rank authors. We conduct our experiments on a massive publication dataset of Computer Science and show that - (i) $C^3$-index is consistent over time, which is one of the fundamental characteristics of a ranking metric, (ii) $C^3$-index is as efficient as h-index and its variants to rank highly-cited authors, (iii) $C^3$-index can act as a conflict resolution metric to break ties in the ranking of medium-cited and low-cited authors, (iv) $C^3$-index can also be used to predict future achievers at the early stage of their career."
"An automated counterexample reproducibility tool based on MATLAB is presented, called DSValidator, with the goal of reproducing counterexamples that refute specific properties related to digital systems. We exploit counterexamples generated by the Digital-System Verifier (DSVerifier), which is a model checking tool based on satisfiability modulo theories for digital systems. DSValidator reproduces the execution of a digital system represented by a transfer function to validate counterexamples provided by the DSVerifier tool and also to manipulate those counterexamples within other MATLAB toolboxes. Our case study shows that DSValidator can validate a set of intricate counterexamples for digital controllers used in a quadrotor attitude system within seconds."
"In this paper we give automata-based representation of LTL-FO$^+$ properties. LTL-FO$^+$ is an extension of LTL that includes first-order quantification over bounded variable, thus greatly increasing the expressivity of the language. An automata representation of this formalism allows greater ease in writing and understanding properties, as well as in performing manipulations, such as negation or emptiness checking. The automata representation of an LTL-FO$^+$ formula has finite size regardless of the domain of quantified variables, and the number of states that is linear in the size of the property."
"Deep learning is quickly becoming the leading methodology for medical image analysis. Given a large medical archive, where each image is associated with a diagnosis, efficient pathology detectors or classifiers can be trained with virtually no expert knowledge about the target pathologies. However, deep learning algorithms, including the popular ConvNets, are black boxes: little is known about the local patterns analyzed by ConvNets to make a decision at the image level. A solution is proposed in this paper to create heatmaps showing which pixels in images play a role in the image-level predictions. In other words, a ConvNet trained for image-level classification can be used to detect lesions as well. A generalization of the backpropagation method is proposed in order to train ConvNets that produce high-quality heatmaps. The proposed solution is applied to diabetic retinopathy (DR) screening in a dataset of almost 90,000 fundus photographs from the 2015 Kaggle Diabetic Retinopathy competition. For the task of detecting referable DR, the proposed system outperforms state-of-the-art methods based on deep learning: $A_z$ = 0.954, as opposed to $A_z$ = 0.946 for Colas et al. (2016) in particular. Performance at the pixel level was evaluated in the DiaretDB1 dataset, where four types of lesions are manually segmented: microaneurysms, hemorrhages, exudates and cotton-wool spots. For all lesion types, the proposed detector, trained with image-level supervision, outperforms recent algorithms, even though they were trained with pixel-level supervision. This detector is part of the RetinOpTIC system for mobile eye pathology screening. Because it does not rely on expert knowledge or manual segmentation for detecting relevant patterns, the proposed solution can potentially discover new biomarkers in images, which makes it a promising image mining tool."
"In this work, we investigate the application of Reinforcement Learning to two well known decision dilemmas, namely Newcomb's Problem and Prisoner's Dilemma. These problems are exemplary for dilemmas that autonomous agents are faced with when interacting with humans. Furthermore, we argue that a Newcomb-like formulation is more adequate in the human-machine interaction case and demonstrate empirically that the unmodified Reinforcement Learning algorithms end up with the well known maximum expected utility solution."
"In recent years, a vast amount of research has been conducted on learning people's interests from their actions. Yet their collective actions also allow us to learn something about the world, in particular, infer attributes of places people visit or interact with. Imagine classifying whether a hotel has a gym or a swimming pool, or whether a restaurant has a romantic atmosphere without ever asking its patrons. Algorithms we present can do just that.   Many web applications rely on knowing attributes of places, for instance, whether a particular restaurant has WiFi or offers outdoor seating. Such data can be used to support a range of user experiences, from explicit query-driven search to personalized place recommendations. However, obtaining these attributes is generally difficult, with existing approaches relying on crowdsourcing or parsing online reviews, both of which are noisy, biased, and have limited coverage. Here we present a novel approach to classifying place attributes, which learns from patrons' visit patterns based on anonymous observational data.   Our method, STEPS, learns from aggregated sequences of place visits. For example, if many people visit the restaurant on a Saturday evening, coming from a luxury hotel or theater, and stay for a long time, then this restaurant is more likely to have a romantic atmosphere. On the other hand, if most people visit the restaurant on weekdays, coming from work or a grocery store, then the restaurant is less likely to be romantic. We show that such transition features are highly predictive of place attributes. In an extensive empirical evaluation, STEPS nearly doubled the coverage of a state of the art approach thanks to learning from observational location data, which allowed our method to reason about many more places."
"Past work in computational sarcasm deals primarily with sarcasm detection. In this paper, we introduce a novel, related problem: sarcasm target identification (\textit{i.e.}, extracting the target of ridicule in a sarcastic sentence). We present an introductory approach for sarcasm target identification. Our approach employs two types of extractors: one based on rules, and another consisting of a statistical classifier. To compare our approach, we use two baselines: a na\""ive baseline and another baseline based on work in sentiment target identification. We perform our experiments on book snippets and tweets, and show that our hybrid approach performs better than the two baselines and also, in comparison with using the two extractors individually. Our introductory approach establishes the viability of sarcasm target identification, and will serve as a baseline for future work."
"In this paper we study the problem of recovering a structured but unknown parameter ${\bf{\theta}}^*$ from $n$ nonlinear observations of the form $y_i=f(\langle {\bf{x}}_i,{\bf{\theta}}^*\rangle)$ for $i=1,2,\ldots,n$. We develop a framework for characterizing time-data tradeoffs for a variety of parameter estimation algorithms when the nonlinear function $f$ is unknown. This framework includes many popular heuristics such as projected/proximal gradient descent and stochastic schemes. For example, we show that a projected gradient descent scheme converges at a linear rate to a reliable solution with a near minimal number of samples. We provide a sharp characterization of the convergence rate of such algorithms as a function of sample size, amount of a-prior knowledge available about the parameter and a measure of the nonlinearity of the function $f$. These results provide a precise understanding of the various tradeoffs involved between statistical and computational resources as well as a-prior side information available for such nonlinear parameter estimation problems."
"Solving quaternion kinematical differential equations is one of the most significant problems in the automation, navigation, aerospace and aeronautics literatures. Most existing approaches for this problem neither preserve the norm of quaternions nor avoid errors accumulated in the sense of long term time. We present symplectic geometric algorithms to deal with the quaternion kinematical differential equation by modeling its time-invariant and time-varying versions with Hamiltonian systems by adopting a three-step strategy. Firstly, a generalized Euler's formula for the autonomous quaternion kinematical differential equation are proved and used to construct symplectic single-step transition operators via the centered implicit Euler scheme for autonomous Hamiltonian system. Secondly, the symplecitiy, orthogonality and invertibility of the symplectic transition operators are proved rigorously. Finally, the main results obtained are generalized to design symplectic geometric algorithm for the time-varying quaternion kinematical differential equation which is a non-autonomous and nonlinear Hamiltonian system essentially. Our novel algorithms have simple algorithmic structures and low time complexity of computation, which are easy to be implemented with real-time techniques. The correctness and efficiencies of the proposed algorithms are verified and validated via numerical simulations."
"We present a framework and analysis of consistent binary classification for complex and non-decomposable performance metrics such as the F-measure and the Jaccard measure. The proposed framework is general, as it applies to both batch and online learning, and to both linear and non-linear models. Our work follows recent results showing that the Bayes optimal classifier for many complex metrics is given by a thresholding of the conditional probability of the positive class. This manuscript extends this thresholding characterization -- showing that the utility is strictly locally quasi-concave with respect to the threshold for a wide range of models and performance metrics. This, in turn, motivates simple normalized gradient ascent updates for threshold estimation. We present a finite-sample regret analysis for the resulting procedure. In particular, the risk for the batch case converges to the Bayes risk at the same rate as that of the underlying conditional probability estimation, and the risk of proposed online algorithm converges at a rate that depends on the conditional probability estimation risk. For instance, in the special case where the conditional probability model is logistic regression, our procedure achieves $O(\frac{1}{\sqrt{n}})$ sample complexity, both for batch and online training. Empirical evaluation shows that the proposed algorithms out-perform alternatives in practice, with comparable or better prediction performance and reduced run time for various metrics and datasets."
"In this paper, we prove correctness of parallelizing a string matcher using Haskell as a theorem prover. We use refinement types to specify correctness properties, Haskell terms to express proofs and Liquid Haskell to check correctness of proofs. First, we specify and prove that a class of monoid morphisms can be parallelized via parallel monoid concatenation. Then, we encode string matching as a morphism to get a provably correct parallel transformation. Our 1839LoC prototype proof shows that Liquid Haskell can be used as a fully expressive theorem prover on realistic Haskell implementations."
"We describe the 1st place winning approach for the CIKM Cup 2016 Challenge. In this paper, we provide an approach to reasonably identify same users across multiple devices based on browsing logs. Our approach regards a candidate ranking problem as pairwise classification and utilizes an unsupervised neural feature ensemble approach to learn latent features of users. Combined with traditional hand crafted features, each user pair feature is fed into a supervised classifier in order to perform pairwise classification. Lastly, we propose supervised and unsupervised inference techniques."
"In this paper, we address the multi-view subspace clustering problem. Our method utilize the circulant algebra for tensor, which is constructed by stacking the subspace representation matrices of different views and then shifting, to explore the high order correlations underlying multi-view data. By introducing a recently proposed tensor factorization, namely tensor-Singular Value Decomposition (t-SVD) \cite{kilmer13}, we can impose a new type of low-rank tensor constraint on the shifted tensor to capture the complementary information from multiple views. Different from traditional unfolding based tensor norm, this low-rank tensor constraint has optimality properties similar to that of matrix rank derived from SVD, so that complementary information among views can be explored more efficiently and thoroughly. The established model, called t-SVD based Multi-view Subspace Clustering (t-SVD-MSC), falls into the applicable scope of augmented Lagrangian method, and its minimization problem can be efficiently solved with theoretical convergence guarantee and relatively low computational complexity. Extensive experimental testing on eight challenging image dataset shows that the proposed method has achieved highly competent objective performance compared to several state-of-the-art multi-view clustering methods."
"In this paper we study the quadratic regulator problem for a process governed by a Volterra integral equation in ${\mathbb R}^n$. Our main goal is the proof that it is possible to associate a Riccati differential equation to this quadratic control problem, which leads to the feedback form of the optimal control. This is in contrast with previous papers on the subject, which confine themselves to study the Fredholm integral equation which is solved by the optimal control."
"New technologies, such as MOOCs, provide innovative methods to tackle new challenges in teaching and learning, such as globalization and changing contemporary culture and to remove the limits of conventional classrooms. However, they also bring challenges in course delivery and assessment, due to factors such as less direct student-instructor interaction. These challenges are especially severe in engineering education, which relies heavily on experiential learning, such as computer simulations and laboratory exercises, to assist students in understanding concepts. As a result, effective design of experiential learning components is extremely critical for engineering MOOCs. In this paper, we will share our experience gained through developing and offering a MOOC on communication systems, with special focus on the development and assessment of MATLAB exercises for active concept learning. Our approach introduced students to concepts using learning components commonly provided by many MOOC platforms (e.g., online lectures and quizzes), and augmented the student experience with MATLAB based computer simulations and exercises to enable more concrete and detailed understanding of the material. We describe here a systematic approach to MATLAB problem design and assessment, based on our experience with the MATLAB server provided by MathWorks and integrated with the edX MOOC platform. We discuss the effectiveness of the instructional methods as evaluated through students' learning performance. We analyze the impact of the course design tools from both the instructor and the student perspective."
"It is shown that proportional-integral (PI) control in the synchronously rotating (d, q)-reference frame and proportional-resonant (PR) control in the stationary ({\alpha}, \b{eta})-reference frame, both with anti-windup, are equivalent if and only if their implementation is done correctly in state space and the controller parameters and the initial values are identical. It is shown that an equivalence in the frequency domain does only hold if simplifying assumptions are satisfied. As consequence of the equivalence, both closed- loop control performances are identical with respect to closed-loop dynamics and steady-state accuracy. The control performance will only differ if their implementation is not done correctly or the time delay induced by the voltage source inverter becomes significant. To the best knowledge of the author, equivalence of PR and PI controllers with anti-windup has not been shown before (in particular not in state space)."
In this paper we introduce a trace-based tunnel that is resistant to traffic analysis in the sense that it provides deniability to users that a specific web page was fetched given that a packet trace is observed on the tunnel. We present a scheduler design for managing the transmission of traces to satisfy user traffic demand while maintaining reasonably low delay and throughput overhead due to dummy packets. Experimental results are also presented demonstrating the effectiveness of this scheduler under a range of realistic network conditions and actual web page fetches.
"A drawing in the plane ($\mathbb{R}^2$) of a graph $G=(V,E)$ equipped with a function $\gamma: V \rightarrow \mathbb{N}$ is \emph{$x$-bounded} if (i) $x(u) <x(v)$ whenever $\gamma(u)<\gamma(v)$ and (ii) $\gamma(u)\leq\gamma(w)\leq \gamma(v)$, where $uv\in E$ and $\gamma(u)\leq \gamma(v)$, whenever $x(w)\in x(uv)$, where $x(.)$ denotes the projection to the $x$-axis. We prove a characterization of isotopy classes of graph embeddings in the plane containing an $x$-bounded embedding. Then we present an efficient algorithm, that relies on our result, for testing the existence of an $x$-bounded embedding if the given graph is a tree or generalized $\Theta$-graph. This partially answers a question raised recently by Angelini et al. and Chang et al., and proves that c-planarity testing of flat clustered graphs with three clusters is tractable if each connected component of the underlying abstract graph is a tree."
"We present a computer-checked generic implementation for solving finite-horizon sequential decision problems. This is a wide class of problems, including inter-temporal optimizations, knapsack, optimal bracketing, scheduling, etc. The implementation can handle time-step dependent control and state spaces, and monadic representations of uncertainty (such as stochastic, non-deterministic, fuzzy, or combinations thereof). This level of genericity is achievable in a programming language with dependent types (we have used both Idris and Agda). Dependent types are also the means that allow us to obtain a formalization and computer-checked proof of the central component of our implementation: Bellman's principle of optimality and the associated backwards induction algorithm. The formalization clarifies certain aspects of backwards induction and, by making explicit notions such as viability and reachability, can serve as a starting point for a theory of controllability of monadic dynamical systems, commonly encountered in, e.g., climate impact research."
"A general framework of optimal eavesdropping on BB84 protocol was provided by Fuchs et al. [Phys. Rev. A, 1997]. An upper bound on mutual information was derived, which could be achieved by a specific type of interaction and the corresponding measurement. However, uniqueness of optimal interaction was posed as an unsolved problem there and it has remained open for almost two decades now. In this paper, we solve this open problem and establish the uniqueness of optimal interaction up to rotation. The specific choice of optimal interaction by Fuchs et al. is shown to be a special case of the form derived in our work."
"Open-domain human-computer conversation has attracted much attention in the field of NLP. Contrary to rule- or template-based domain-specific dialog systems, open-domain conversation usually requires data-driven approaches, which can be roughly divided into two categories: retrieval-based and generation-based systems. Retrieval systems search a user-issued utterance (called a query) in a large database, and return a reply that best matches the query. Generative approaches, typically based on recurrent neural networks (RNNs), can synthesize new replies, but they suffer from the problem of generating short, meaningless utterances. In this paper, we propose a novel ensemble of retrieval-based and generation-based dialog systems in the open domain. In our approach, the retrieved candidate, in addition to the original query, is fed to an RNN-based reply generator, so that the neural model is aware of more information. The generated reply is then fed back as a new candidate for post-reranking. Experimental results show that such ensemble outperforms each single part of it by a large margin."
"We present a novel approach for real-time joint reconstruction of 3D scene motion and geometry from binocular stereo videos. Our approach is based on a novel variational halfway-domain scene flow formulation, which allows us to obtain highly accurate spatiotemporal reconstructions of shape and motion. We solve the underlying optimization problem at real-time frame rates using a novel data-parallel robust non-linear optimization strategy. Fast convergence and large displacement flows are achieved by employing a novel hierarchy that stores delta flows between hierarchy levels. High performance is obtained by the introduction of a coarser warp grid that decouples the number of unknowns from the input resolution of the images. We demonstrate our approach in a live setup that is based on two commodity webcams, as well as on publicly available video data. Our extensive experiments and evaluations show that our approach produces high-quality dense reconstructions of 3D geometry and scene flow at real-time frame rates, and compares favorably to the state of the art."
"Multi-valued partial CFL functions are computed by one-way nondeterministic pushdown automata equipped with write-only output tapes. We give an answer to a fundamental question, raised by Konstantinidis, Santean, and Yu [Act. Inform. 43 (2007) 395-417], of whether all multi-valued partial CFL functions can be refined by single-valued partial CFL functions. We negatively solve this question by presenting a special multi-valued partial CFL function as an example function and by proving that no refinement of this particular function becomes a single-valued partial CFL function. This contrasts an early result of Kobayashi [Inform. Control 15 (1969) 95-109] that multi-valued partial NFA functions are always refined by single-valued NFA functions, where NFA functions are computed by nondeterministic finite automata with output tapes. Our example function turns out to be unambiguously 2-valued, and thus we obtain a stronger separation result, in which no refinement of unambiguously 2-valued partial CFL functions can be single-valued. For the proof, we first introduce a new concept of colored automata having no output tapes but having ""colors,"" which can simulate pushdown automata with constant-space output tapes. We then conduct an extensive combinatorial analysis on the behaviors of transition records of stack contents (called stack histories) of colored automata."
"The class of 2K2-free graphs and its various subclasses have been studied in a variety of contexts. In this paper, we are concerned with the colouring of (P3UP2)-free graphs, a super class of 2K2-free graphs. We derive a O(w^3) upper bound for the chromatic number of (P3UP2)-free graphs, and sharper bounds for (P3UP2), diamond)-free graphs, where w denotes the clique number. By applying similar proof techniques we obtain chromatic bounds for (2K2, diamond)-free graphs. The last two classes are perfect if w >=5 and >= 4 respectively."
"Due to the recent cases of algorithmic bias in data-driven decision-making, machine learning methods are being put under the microscope in order to understand the root cause of these biases and how to correct them. Here, we consider a basic algorithmic task that is central in machine learning: subsampling from a large data set. Subsamples are used both as an end-goal in data summarization (where fairness could either be a legal, political or moral requirement) and to train algorithms (where biases in the samples are often a source of bias in the resulting model). Consequently, there is a growing effort to modify either the subsampling methods or the algorithms themselves in order to ensure fairness. However, in doing so, a question that seems to be overlooked is whether it is possible to produce fair subsamples that are also adequately representative of the feature space of the data set - an important and classic requirement in machine learning. Can diversity and fairness be simultaneously ensured? We start by noting that, in some applications, guaranteeing one does not necessarily guarantee the other, and a new approach is required. Subsequently, we present an algorithmic framework which allows us to produce both fair and diverse samples. Our experimental results on an image summarization task show marked improvements in fairness without compromising feature diversity by much, giving us the best of both the worlds."
"We introduce a deep learning architecture for structure-based virtual screening that generates fixed-sized fingerprints of proteins and small molecules by applying learnable atom convolution and softmax operations to each compound separately. These fingerprints are further transformed non-linearly, their inner-product is calculated and used to predict the binding potential. Moreover, we show that widely used benchmark datasets may be insufficient for testing structure-based virtual screening methods that utilize machine learning. Therefore, we introduce a new benchmark dataset, which we constructed based on DUD-E and PDBBind databases."
"We present a class of hardware-based cryptographic one-way functions that, in practice, would be hard to invert even if P=NP and linear-time satisfiability algorithms exist. Such functions use a hardware-based component with omega(n^2) size circuits, and omega(n^2) run time."
"Parsers are a fundamental building block in the data processing pipeline of software systems. Unfortunately, they are also an important entry point for attacks. A number of different techniques have been proposed to address this problem; what is common to most of them is that their authors make claims about formal language theory to motivate their respective approaches. Surprisingly, for a number of idioms that are common in network protocols, we found that the claims made are vague or even contradictory, which we attribute to the fact that they lack any kind of formalization or proof. In this paper, we address this problem by performing the first rigorous, language-theoretic analysis of several common protocol idioms, laying a formal foundation for further research on the parsing problem. We then demonstrate the practical value of our findings by automatically generating a modular, robust, and efficient input validator for HTTP from a set of context-free grammars."
"We study output-sensitive algorithms and complexity for multiobjective combinatorial optimization problems. In this computational complexity framework, an algorithm for a general enumeration problem is regarded efficient if it is output-sensitive, i.e., its running time is bounded by a polynomial in the input and the output size. We provide both practical examples of MOCO problems for which such an efficient algorithm exists as well as problems for which no efficient algorithm exists under mild complexity theoretic assumptions."
"3D hand pose tracking/estimation will be very important in the next generation of human-computer interaction. Most of the currently available algorithms rely on low-cost active depth sensors. However, these sensors can be easily interfered by other active sources and require relatively high power consumption. As a result, they are currently not suitable for outdoor environments and mobile devices. This paper aims at tracking/estimating hand poses using passive stereo which avoids these limitations. A benchmark with 18,000 stereo image pairs and 18,000 depth images captured from different scenarios and the ground-truth 3D positions of palm and finger joints (obtained from the manual label) is thus proposed. This paper demonstrates that the performance of the state-of-the art tracking/estimation algorithms can be maintained with most stereo matching algorithms on the proposed benchmark, as long as the hand segmentation is correct. As a result, a novel stereo-based hand segmentation algorithm specially designed for hand tracking/estimation is proposed. The quantitative evaluation demonstrates that the proposed algorithm is suitable for the state-of-the-art hand pose tracking/estimation algorithms and the tracking quality is comparable to the use of active depth sensors under different challenging scenarios."
"We introduce XtraPuLP, a new distributed-memory graph partitioner designed to process trillion-edge graphs. XtraPuLP is based on the scalable label propagation community detection technique, which has been demonstrated as a viable means to produce high quality partitions with minimal computation time. On a collection of large sparse graphs, we show that XtraPuLP partitioning quality is comparable to state-of-the-art partitioning methods. We also demonstrate that XtraPuLP can produce partitions of real-world graphs with billion+ vertices in minutes. Further, we show that using XtraPuLP partitions for distributed-memory graph analytics leads to significant end-to-end execution time reduction."
"We examine a model of network formation in single-layer and multiplex networks in which individuals have positive incentives for social ties, closed triangles, and spillover edges. In particular, we investigate the influence of shocks to the network in which the cost of social ties changes after an initial equilibrium. We highlight the emergence of structural entrenchment, a new concept we define as the persistence of structural features, such as closed triangles and spillover edges, which are formed under historically different conditions from those currently incentivizing the creation of new social ties or the destruction of existing ties. This work has broad implications for understanding path dependence in the structure and dynamics of single-layer and multiplex networks."
"A graph is distance-hereditary if for any pair of vertices, their distance in every connected induced subgraph containing both vertices is the same as their distance in the original graph. Distance-hereditary graphs are exactly the graphs with rank-width at most 1. The Distance-Hereditary Vertex Deletion problem asks, given a graph $G$ on $n$ vertices and an integer $k$, whether there is a set $S$ of at most $k$ vertices in $G$ such that $G-S$ is distance-hereditary. It was shown by Eiben, Ganian, and Kwon (MFCS' 16) that Distance-Hereditary Vertex Deletion can be solved in time $2^{\mathcal{O}(k)}n^{\mathcal{O}(1)}$, and they asked whether the problem admits a polynomial kernelization. We show that this problem admits a polynomial kernel, answering this question positively. For this, we use a similar idea for obtaining an approximate solution for Chordal Vertex Deletion due to Jansen and Pilipczuk (SODA' 17) to obtain an approximate solution with $\mathcal{O}(k^4)$ vertices when the problem is a YES-instance, and use Mader's $\mathcal{S}$-path theorem to hit all obstructions containing exactly one vertex of the approximate solution. Then we exploit the structure of split decompositions of distance-hereditary graphs to reduce the total size. Using Mader's $\mathcal{S}$-path theorem in the context of kernelization might be of independent interest."
"In most computer vision and image analysis problems, it is necessary to define a similarity measure between two or more different objects or images. Template matching is a classic and fundamental method used to score similarities between objects using certain mathematical algorithms. In this paper, we reviewed the basic concept of matching, as well as advances in template matching and applications such as invariant features or novel applications in medical image analysis. Additionally, deformable models and templates originating from classic template matching were discussed. These models have broad applications in image registration, and they are a fundamental aspect of novel machine vision or deep learning algorithms, such as convolutional neural networks (CNN), which perform shift and scale invariant functions followed by classification. In general, although template matching methods have restrictions which limit their application, they are recommended for use with other object recognition methods as pre- or post-processing steps. Combining a template matching technique such as normalized cross-correlation or dice coefficient with a robust decision-making algorithm yields a significant improvement in the accuracy rate for object detection and recognition."
"Indoor localization for autonomous micro aerial vehicles (MAVs) requires specific localization techniques, since the Global Positioning System (GPS) is usually not available. We present an efficient onboard computer vision approach that estimates 2D positions of an MAV in real-time. This global localization system does not suffer from error accumulation over time and uses a $k$-Nearest Neighbors ($k$-NN) algorithm to predict positions based on textons---small characteristic image patches that capture the texture of an environment. A particle filter aggregates the estimates and resolves positional ambiguities. To predict the performance of the approach in a given setting, we developed an evaluation technique that compares environments and identifies critical areas within them. We conducted flight tests to demonstrate the applicability of our approach. The algorithm has a localization accuracy of approximately 0.6 m on a 5 m$\times$5 m area at a runtime of 32 ms on board of an MAV. Based on random sampling, its computational effort is scalable to different platforms, trading off speed and accuracy."
"Polyhedral compilers perform optimizations such as tiling and parallelization; when doing both, they usually generate code that executes ""barrier-synchronized wavefronts"" of tiles. We present a system to express and generate code for hybrid schedules, where some constraints are automatically satisfied through the structure of the code, and the remainder are dynamically enforced at run-time with data flow mechanisms. We prove bounds on the added overheads that are better, by at least one polynomial degree, than those of previous techniques.   We propose a generic mechanism to implement the needed synchronization, and show it can be easily realized for a variety of targets: OpenMP, Pthreads, GPU (CUDA or OpenCL) code, languages like X10, Habanero, Cilk, as well as data flow platforms like DAGuE, and OpenStream and MPI. We also provide a simple concrete implementation that works without the need of any sophisticated run-time mechanism.   Our experiments show our simple implementation to be competitive or better than the wavefront-synchronized code generated by other systems. We also show how the proposed mechanism can achieve 24% to 70% reduction in energy."
"In visual tracking, part-based trackers are attractive since they are robust against occlusion and deformation. However, a part represented by a rectangular patch does not account for the shape of the target, while a superpixel does thanks to its boundary evidence. Nevertheless, tracking superpixels is difficult due to their lack of discriminative power. Therefore, to enable superpixels to be tracked discriminatively as object parts, we propose to enhance them with keypoints. By combining properties of these two features, we build a novel element designated as a Superpixel-Keypoints structure (SPiKeS). Being discriminative, these new object parts can be located efficiently by a simple nearest neighbor matching process. Then, in a tracking process, each match votes for the target's center to give its location. In addition, the interesting properties of our new feature allows the development of an efficient model update for more robust tracking. According to experimental results, our SPiKeS-based tracker proves to be robust in many challenging scenarios by performing favorably against the state-of-the-art."
"Future cellular systems based on the use of above-6 GHz frequencies, the so-called millimeter wave (mmWave) bandwidths, will heavily rely on the use of antenna arrays both at the transmitter and at the receiver, possibly with a large number of elements. For complexity reasons, fully digital precoding and postcoding structures may turn out to be unfeasible, and thus suboptimal structures, making use of simplified hardware and a limited number of RF chains, have been investigated. This paper considers and makes a comparative assessment, both from a spectral efficiency and energy efficiency point of view, of several suboptimal precoding and postcoding beamforming structures for the downlink of a cellular multiuser MIMO (MU-MIMO) system. Based on the most recently available data for the energy consumption of phase shifters and switches, we show that there are cases where fully-digital beamformers may achieve a larger energy efficiency than lower-complexity solutions, as well as that structures based on the exclusive use of switches achieve quite unsatisfactory performance in realistic scenarios."
"The one-clean qubit model (or the DQC1 model) is a restricted model of quantum computing where only a single qubit of the initial state is pure and others are maximally mixed. Although the model is not universal, it can efficiently solve several problems whose classical efficient solutions are not known. Furthermore, it was recently shown that if the one-clean qubit model is classically efficiently simulated, the polynomial hierarchy collapses to the second level. A disadvantage of the one-clean qubit model is, however, that the clean qubit is too clean: for example, in realistic NMR experiments, polarizations are not enough high to have the perfectly pure qubit. In this paper, we consider a more realistic one-clean qubit model, where the clean qubit is not clean, but depolarized. We first show that, for any polarization, a multiplicative-error calculation of the output probability distribution of the model is possible in a classical polynomial time if we take an appropriately large multiplicative error. The result is in a strong contrast to that of the ideal one-clean qubit model where the classical efficient multiplicative-error calculation (or even the sampling) with the same amount of error causes the collapse of the polynomial hierarchy. We next show that, for any polarization lower-bounded by an inverse polynomial, a classical efficient sampling (in terms of a sufficiently small multiplicative error or an exponentially-small additive error) of the output probability distribution of the model is impossible unless BQP is contained in the second level of the polynomial hierarchy, which suggests the hardness of the classical efficient simulation of the one non-clean qubit model."
"Despite the immense technology advancement in the surgeries the criteria of assessing the surgical skills still remains based on subjective standards. With the advent of robotic-assisted surgery, new opportunities for objective and autonomous skill assessment is introduced. Previous works in this area are mostly based on structured-based method such as Hidden Markov Model (HMM) which need enormous pre-processing. In this study, in contrast with them, we develop a new shaped-based framework for automatically skill assessment and personalized surgical training with minimum parameter tuning. Our work has addressed main aspects of skill evaluation; develop gesture recognition model directly on temporal kinematic signal of robotic-assisted surgery, and build automated personalized RMIS gesture training framework which . We showed that our method, with an average accuracy of 82% for suturing, 70% for needle passing and 85% for knot tying, performs better or equal than the state-of-the-art methods, while simultaneously needs minimum pre-processing, parameter tuning and provides surgeons with online feedback for their performance during training."
"We propose a novel random multiple access (RMA) scheme with quality of service (QoS) guarantees for machine-to-machine (M2M) communications. We consider a slotted uncoordinated data transmission period during which machine type communication (MTC) devices transmit over the same radio channel. Based on the latency requirements, MTC devices are divided into groups of different sizes, and the transmission frame is divided into subframes of different lengths. In each subframe, each group is assigned an access probability based on which an MTC device decides to transmit replicas of its packet or remain silent. The base station (BS) employs successive interference cancellation (SIC) to recover all the superposed packets. We derive the closed form expressions for the average probability of device resolution for each group, and we use these expressions to design the access probabilities. The accuracy of the expressions is validated through Monte Carlo simulations. We show that the designed access probabilities can guarantee the QoS requirements with high reliability and high energy efficiency. Finally, we show that RMA can outperform standard coordinated access schemes as well as some of the recently proposed M2M access schemes for cellular networks."
"With the growing deployment of IPv6, the question arises whether and to what extent this new protocol is co-deployed with IPv4 on existing hardware or whether new hardware or proxy solutions are deployed. Understanding the resulting cross-dependencies between IPv4 and IPv6 hosts will add a significant level of insight into Internet structure and resilience research. In this work we present an active measurement technique to determine whether an IPv4-IPv6 address pair resides on the same physical host. This technique is based on measuring clock skew through TCP timestamps, and introduces new capabilities to classify nonlinear clock skews. We achieve 97.7% accuracy on a ground truth data set of 458 hosts. We prove this technique's value by applying it to 371k sibling candidates, of which we classify 80k as siblings. We release the classified siblings among other data and code from this work for public use."
"Shannon introduced the classic model of a cryptosystem in 1949, where Eve has access to an identical copy of the cyphertext that Alice sends to Bob. Shannon defined perfect secrecy to be the case when the mutual information between the plaintext and the cyphertext is zero. Perfect secrecy is motivated by error-free transmission and requires that Bob and Alice share a secret key. Wyner in 1975 and later I.~Csisz\'ar and J.~K\""orner in 1978 modified the Shannon model assuming that the channels are noisy and proved that secrecy can be achieved without sharing a secret key. This model is called wiretap channel model and secrecy capacity is known when Eve's channel is noisier than Bob's channel.   In this paper we review the concept of wiretap coding from the satellite channel viewpoint. We also review subsequently introduced stronger secrecy levels which can be numerically quantified and are keyless unconditionally secure under certain assumptions. We introduce the general construction of wiretap coding and analyse its applicability for a typical satellite channel. From our analysis we discuss the potential of keyless information theoretic physical layer security for satellite channels based on wiretap coding. We also identify system design implications for enabling simultaneous operation with additional information theoretic security protocols."
"In this paper, we propose two schemes for asynchronous multi-relay two-way relay (MR-TWR) systems in which neither the users nor the relays know the channel state information (CSI). In an MR-TWR system, two users exchange their messages with the help of $N_R$ relays. Most of the existing works on MR-TWR systems based on differential modulation assume perfect symbol-level synchronization between all communicating nodes. However, this assumption is not valid in many practical systems, which makes the design of differentially modulated schemes more challenging. Therefore, we design differential modulation schemes that can tolerate timing misalignment under frequency-selective fading. We investigate the performance of the proposed schemes in terms of either probability of bit error or pairwise error probability. Through numerical examples, we show that the proposed schemes outperform existing competing solutions in the literature, especially for high signal-to-noise ratio (SNR) values."
"We propose a new model based on the deconvolutional networks and SAX discretization to learn the representation for multivariate time series. Deconvolutional networks fully exploit the advantage the powerful expressiveness of deep neural networks in the manner of unsupervised learning. We design a network structure specifically to capture the cross-channel correlation with deconvolution, forcing the pooling operation to perform the dimension reduction along each position in the individual channel. Discretization based on Symbolic Aggregate Approximation is applied on the feature vectors to further extract the bag of features. We show how this representation and bag of features helps on classification. A full comparison with the sequence distance based approach is provided to demonstrate the effectiveness of our approach on the standard datasets. We further build the Markov matrix from the discretized representation from the deconvolution to visualize the time series as complex networks, which show more class-specific statistical properties and clear structures with respect to different labels."
"Neural Machine Translation (NMT) has become the new state-of-the-art in several language pairs. However, it remains a challenging problem how to integrate NMT with a bilingual dictionary which mainly contains words rarely or never seen in the bilingual training data. In this paper, we propose two methods to bridge NMT and the bilingual dictionaries. The core idea behind is to design novel models that transform the bilingual dictionaries into adequate sentence pairs, so that NMT can distil latent bilingual mappings from the ample and repetitive phenomena. One method leverages a mixed word/character model and the other attempts at synthesizing parallel sentences guaranteeing massive occurrence of the translation lexicon. Extensive experiments demonstrate that the proposed methods can remarkably improve the translation quality, and most of the rare words in the test sentences can obtain correct translations if they are covered by the dictionary."
"Intrusion detection is only a starting step in securing IT infrastructure. Prediction of intrusions is the next step to provide an active defense against incoming attacks. Current intrusion prediction methods focus mainly on prediction of either intrusion type or intrusion category and do not use or provide contextual information such as source and target IP address. In addition most of them are dependant on domain knowledge and specific scenario knowledge. The proposed algorithm employs a bag-of-words model together with a hidden Markov model which not depend on specific domain knowledge. Since this algorithm depends on a training process it is adaptable to different conditions. A key advantage of the proposed algorithm is the inclusion of contextual data such as source IP address, destination IP range, alert type and alert category in its prediction, which is crucial for an eventual response. Experiments conducted using a public data set generated over 2500 alert predictions and achieved accuracy of 81% and 77% for single step and five step predictions respectively for prediction of the next alert cluster. It also achieved an accuracy of prediction of 95% and 92% for single step and five step predictions respectively for prediction of the next alert category. The proposed methods achieved a prediction accuracy improvement of 5% for alert category over existing variable length Markov chain intrusion prediction methods, while providing more information for a possible defense."
"Renewable energy has attracted significant attention over the last decade, conceivably due to its environmental benefits and the recent drops in the development and deployment cost of the technology. The increase in renewable generation, however, has resulted in new challenges in supply-load balancing, owing to its intermittent, non-predictable and volatile generation features. Several methods have been introduced to cope with negative impacts of the renewable generation deployment. In this paper, a novel method, i.e., the application of microgrids in capturing the variabilities of distributed renewable generation in distribution networks is proposed and investigated. Utilizing available flexibility of microgrids represents a local and viable solution which leads to lower investments from electric utilities for increasing their flexibility and providing more reserved power. It is investigated that how the system flexibility requirements can be integrated into the microgrid optimal scheduling model to enable microgrids in supporting the grid operators by offering flexibility services. Using the proposed flexibility constraints, intra-hour and inter-hour variabilities at the distribution feeder will be efficiently captured. Numerical simulations on a test distribution feeder, with one microgrid and several renewable-equipped consumers, show the effectiveness of the proposed model."
"The problem of finding decentralized transmission policies in a wireless communication network with energy harvesting constraints is formulated and solved using the decentralized Markov decision process framework. The proposed policy defines the transmission probabilities of all devices so as to correctly balance the collision probabilities with the energy constraints. After an initial coordination phase, in which the network parameters are initialized for all devices, every node proceeds in a fully decentralized fashion. We numerically show that, because of the harvesting, a fully orthogonal scheme (e.g., TDMA-like) is sub-optimal in this scenario, and that the optimal trade-off lies between an orthogonal and a completely symmetric system."
"The levelized cost of energy (LCOE) presents the energy-normalized cost of a generation asset by considering all associated costs (investment and operation) and total generated energy over its life cycle. As LCOE is a levelized value, it provides a quick and easy measure to compare different energy resource technologies with different characteristics. The LCOE calculation for large-scale power plants and distributed generations (DGs) is extensively studied and can be found in the literature. The discussions on the LCOE calculation for energy storage systems, however, is limited. Although still relatively expensive compared to generation technologies, energy storage is gaining significant attention and has been deployed extensively during the past few years, conceivably due to its many benefits such as load shifting, energy arbitrage, and renewable coordination. Therefore, LCOE calculation of energy storage systems plays an important role in economic evaluation of power systems. This paper proposes a method for calculating the LCOE of energy storage, and further provides the sensitivity analysis with respect to changes in capacity, electricity market prices, and efficiency."
"Ar{\i}kan's polar coding, is by now a well studied technique that allows achieving the symmetric capacity of binary input memoryless channels with low complexity encoding and decoding, provided that the polar decoding architecture is used and the decoding metric is matched to the true channel. In this paper, we analyze communication rates that are achievable when the polar coding/decoding architecture is used with the decoder using an incorrect model of the channel. We define the `polar mismatched capacity' as an analogue of the classical mismatched capacity, give an expression for it, and derive bounds on it."
"This paper takes a rate-distortion approach to understanding the information-theoretic laws governing cache-aided communications systems. Specifically, we characterise the optimal tradeoffs between the delivery rate, cache capacity and reconstruction distortions for a single-user problem and some special cases of a two-user problem. Our analysis considers discrete memoryless sources, expected- and excess-distortion constraints, and separable and f-separable distortion functions. We also establish a strong converse for separable-distortion functions, and we show that lossy versions of common information (G\'{a}cs-K\""{o}rner and Wyner) play an important role in caching. Finally, we illustrate and explicitly evaluate these laws for multivariate Gaussian sources and binary symmetric sources."
"With the development of numerous 3D sensing technologies, object registration on cross-source point cloud has aroused researchers' interests. When the point clouds are captured from different kinds of sensors, there are large and different kinds of variations. In this study, we address an even more challenging case in which the differently-source point clouds are acquired from a real street view. One is produced directly by the LiDAR system and the other is generated by using VSFM software on image sequence captured from RGB cameras. When it confronts to large scale point clouds, previous methods mostly focus on point-to-point level registration, and the methods have many limitations.The reason is that the least mean error strategy shows poor ability in registering large variable cross-source point clouds. In this paper, different from previous ICP-based methods, and from a statistic view, we propose a effective coarse-to-fine algorithm to detect and register a small scale SFM point cloud in a large scale Lidar point cloud. Seen from the experimental results, the model can successfully run on LiDAR and SFM point clouds, hence it can make a contribution to many applications, such as robotics and smart city development."
"To design an efficient interference management and multiple access scheme for visible light communication (VLC) network, this letter leverages the non-orthogonal multiple access (NOMA), which has received significant attention in the $5^{th}$ generation wireless communication. With the residual interference from the successive interference cancellation in NOMA taken into account, we optimize the power allocation for NOMA VLC network to improve the achievable user rate under user quality of service (QoS) constraint. The performance of the proposed approaches is evaluated by the numerical results."
"Similarity search on time series is a frequent operation in large-scale data-driven applications. Sophisticated similarity measures are standard for time series matching, as they are usually misaligned. Dynamic Time Warping or DTW is the most widely used similarity measure for time series because it combines alignment and matching at the same time. However, the alignment makes DTW slow. To speed up the expensive similarity search with DTW, branch and bound based pruning strategies are adopted. However, branch and bound based pruning are only useful for very short queries (low dimensional time series), and the bounds are quite weak for longer queries. Due to the loose bounds branch and bound pruning strategy boils down to a brute-force search.   To circumvent this issue, we design SSH (Sketch, Shingle, & Hashing), an efficient and approximate hashing scheme which is much faster than the state-of-the-art branch and bound searching technique: the UCR suite. SSH uses a novel combination of sketching, shingling and hashing techniques to produce (probabilistic) indexes which align (near perfectly) with DTW similarity measure. The generated indexes are then used to create hash buckets for sub-linear search. Our results show that SSH is very effective for longer time sequence and prunes around 95% candidates, leading to the massive speedup in search with DTW. Empirical results on two large-scale benchmark time series data show that our proposed method can be around 20 times faster than the state-of-the-art package (UCR suite) without any significant loss in accuracy."
"The basis for most vision based applications like robotics, self-driving cars and potentially augmented and virtual reality is a robust, continuous estimation of the position and orientation of a camera system w.r.t the observed environment (scene). In recent years many vision based systems that perform simultaneous localization and mapping (SLAM) have been presented and released as open source. In this paper, we extend and improve upon a state-of-the-art SLAM to make it applicable to arbitrary, rigidly coupled multi-camera systems (MCS) using the MultiCol model. In addition, we include a performance evaluation on accurate ground truth and compare the robustness of the proposed method to a single camera version of the SLAM system. An open source implementation of the proposed multi-fisheye camera SLAM system can be found on-line https://github.com/urbste/MultiCol-SLAM."
"In data centers, up to dozens of tasks are colocated on a single physical machine. Machines are used more efficiently, but tasks' performance deteriorates, as colocated tasks compete for shared resources. As tasks are heterogeneous (CPU-, memory-, network- or disk-intensive), the resulting performance dependencies are complex.   We explore a new combinatorial optimization model that uses two parameters of a task - its size and its type - to characterize how a task influences the performance of the other tasks allocated to the same machine. We study the egalitarian optimization goal: maximizing the worst-off performance. This problem generalizes the classic makespan minimization on multiple processors (P||Cmax).   We prove that polynomially-solvable variants of multiprocessor scheduling become NP-hard and hard to approximate when the number of types is not constant. We propose a PTAS and a series of fast approximation algorithms when the number of types is constant. By simulation on instances derived from a trace of one of Google clusters, we show that our algorithms that take into account types lead to lower costs compared with P||Cmax baseline.   The notion of type enables us to model degeneration of performance caused by colocation using standard combinatorial optimization methods. Types add a layer of additional complexity. However, our results - approximation algorithms and good average-case performance - show that types can be handled efficiently."
"In the last years, the success of kernel-based regularisation techniques in solving impulse response modelling tasks has revived the interest on linear system identification. In this work, an alternative perspective on the same problem is introduced. Instead of relying on a Bayesian framework to include assumptions about the system in the definition of the covariance matrix of the parameters, here the prior knowledge is injected at the cost function level. The key idea is to define the regularisation matrix as a filtering operation on the parameters, which allows for a more intuitive formulation of the problem from an engineering point of view. Moreover, this results in a unified framework to model low-pass, band-pass and high-pass systems, and systems with one or more resonances. The proposed filter-based approach outperforms the existing regularisation method based on the TC and DC kernels, as illustrated by means of Monte Carlo simulations on several linear modelling examples."
"By recording multiple cells simultaneously, electrophysiologists have found evidence for repeating spatiotemporal spike patterns. In sensory systems in particular, repeating a sensory sequence typically elicits a reproducible spike pattern, which carries information about the sensory sequence. How this information is readout by downstream neurons is unclear. In this theoretical paper, we investigate to what extent a single cell could detect a given spike pattern and what are the optimal parameters to do so, in particular the membrane time constant $\tau$. Using a leaky integrate-and-fire (LIF) neuron with instantaneous synapses and homogeneous Poisson inputs, we computed this optimum analytically. Our results indicate that a relatively small $\tau$ (at most a few tens of ms) is usually optimal, even when the pattern is longer. This is somewhat surprising as the resulting detector ignores most of the pattern, due to its fast memory decay. Next, we wondered if spike-timing-dependent plasticity (STDP) could enable a neuron to reach the theoretical optimum. We simulated a LIF neuron equipped with additive STDP, and repeatedly exposed to a given input spike pattern. As in previous studies, the LIF progressively became selective to the repeating pattern with no supervision, even when the pattern was embedded in Poisson activity. Here we show that, using certain STDP parameters, the resulting pattern detector can be optimal. Taken together, these results may explain how humans can learn repeating visual or auditory sequences. Long sequences could be recognized thanks to coincidence detectors working at a much shorter timescale. This is consistent with the fact that recognition is still possible if a sound sequence is compressed of played backward, or scrambled using 10ms bins. Coincidence detection is a simple yet powerful mechanism, which could be the main function of neurons in the brain."
"Breaking news leads to situations of fast-paced reporting in social media, producing all kinds of updates related to news stories, albeit with the caveat that some of those early updates tend to be rumours, i.e., information with an unverified status at the time of posting. Flagging information that is unverified can be helpful to avoid the spread of information that may turn out to be false. Detection of rumours can also feed a rumour tracking system that ultimately determines their veracity. In this paper we introduce a novel approach to rumour detection that learns from the sequential dynamics of reporting during breaking news in social media to detect rumours in new stories. Using Twitter datasets collected during five breaking news stories, we experiment with Conditional Random Fields as a sequential classifier that leverages context learnt during an event for rumour detection, which we compare with the state-of-the-art rumour detection system as well as other baselines. In contrast to existing work, our classifier does not need to observe tweets querying a piece of information to deem it a rumour, but instead we detect rumours from the tweet alone by exploiting context learnt during the event. Our classifier achieves competitive performance, beating the state-of-the-art classifier that relies on querying tweets with improved precision and recall, as well as outperforming our best baseline with nearly 40% improvement in terms of F1 score. The scale and diversity of our experiments reinforces the generalisability of our classifier."
"This special issue is dedicated to get a better picture of the relationships between computational linguistics and cognitive science. It specifically raises two questions: ""what is the potential contribution of computational language modeling to cognitive science?"" and conversely: ""what is the influence of cognitive science in contemporary computational linguistics?"""
"This paper sketches the challenges to address to realise a support able to achieve an Ephemeral Cloud Federation, an innovative cloud computing paradigm that enables the exploitation of a dynamic, personalised and context-aware set of resources.   The aim of the Ephemeral Federation is to answer to the need of combining private data-centres with both federation of cloud providers and the resource on the edge of the network.   The goal of the Ephemeral Federation is to deliver a context-aware and personalised federations of computational, data and network resources, able to manage their heterogeneity in a highly distributed deployment, which can dynamically bring data and computation close to the final user."
"This article is an empirical contribution to the field of educational technology but also - and above all - a methodological contribution to the analysis of the activities enacted in this field. It takes account of a pilot study conducted within the framework of doctoral research and consisted in describing, analysing and modelling the activity of a trainee teacher in a situation of autonomous use of a video-based digital learning environment (DLE). We were particularly careful to describe the method in great detail. Two types of data were collected and processed within the framework of ""course-of-action"": (i)activity observation data (dynamic screen capture) and (ii) data from resituating interviews supported by digital traces of that activity. The findings (i) validate the method's relevance in relation to the object and issues of the research, (ii)show different levels of organization in the activity deployed in the situation of use, (iii) highlight four registers of concerns orienting use of the DLE. We conclude from a perspective of educational technology, by discussing how, according to certain conditions and different time scales, the findings inform a process of continuous DLE design."
"We present a new algorithm, truncated variance reduction (TruVaR), that treats Bayesian optimization (BO) and level-set estimation (LSE) with Gaussian processes in a unified fashion. The algorithm greedily shrinks a sum of truncated variances within a set of potential maximizers (BO) or unclassified points (LSE), which is updated based on confidence bounds. TruVaR is effective in several important settings that are typically non-trivial to incorporate into myopic algorithms, including pointwise costs and heteroscedastic noise. We provide a general theoretical guarantee for TruVaR covering these aspects, and use it to recover and strengthen existing results on BO and LSE. Moreover, we provide a new result for a setting where one can select from a number of noise levels having associated costs. We demonstrate the effectiveness of the algorithm on both synthetic and real-world data sets."
"In this paper, we propose a novel algorithm to learn a B\""uchi automaton from a teacher who knows an $\omega$-regular language. The algorithm is based on learning a formalism named \emph{family of DFAs} (FDFAs) recently proposed by Angluin and Fisman~\cite{Angluin2014}. The main catch is that we use a \emph{classification tree} structure instead of the standard \emph{observation table} structure. The worst case storage space required by our algorithm is quadratically better than the table-based algorithm proposed in~\cite{Angluin2014}. We implement the first publicly available library {\roll} (Regular Omega Language Learning ), which consists of all $\omega$-regular learning algorithms available in the literature and the new algorithms proposed in this paper. Experimental results show that our tree-based algorithms have the best performance among others regarding the number of solved learning tasks."
"Active contour models based on partial differential equations have proved successful in image segmentation, yet the study of their geometric formulation on arbitrary geometric graphs is still at an early stage. In this paper, we introduce geometric approximations of gradient and curvature, which are used in the geodesic active contour model. We prove convergence in probability of our gradient approximation to the true gradient value and derive an asymptotic upper bound for the error of this approximation for the class of random geometric graphs. Two different approaches for the approximation of curvature are presented and both are also proved to converge in probability in the case of random geometric graphs. We propose neighborhood-based filtering on graphs to improve the accuracy of the aforementioned approximations and define two variants of Gaussian smoothing on graphs which include normalization in order to adapt to graph non-uniformities. The performance of our active contour framework on graphs is demonstrated in the segmentation of regular images and geographical data defined on arbitrary graphs."
"In this paper, we study an NP-hard problem of a single machine scheduling minimizing the makespan, where the mixed-critical tasks with an uncertain processing time are scheduled. We show the derivation of F-shaped tasks from the probability distribution function of the processing time, then we study the structure of problems with two and three criticality levels for which we propose efficient exact algorithms and we present computational experiments for instances with up to 200 tasks. Moreover, we show that the considered problem is approximable within a constant multiplicative factor."
"Sending compressed video data in error-prone environments (like the Internet and wireless networks) might cause data degradation. Error concealment techniques try to conceal the received data in the decoder side. In this paper, an adaptive boundary matching algorithm is presented for recovering the damaged motion vectors (MVs). This algorithm uses an outer boundary matching or directional temporal boundary matching method to compare every boundary of candidate macroblocks (MBs), adaptively. It gives a specific weight according to the accuracy of each boundary of the damaged MB. Moreover, if each of the adjacent MBs is already concealed, different weights are given to the boundaries. Finally, the MV with minimum adaptive boundary distortion is selected as the MV of the damaged MB. Experimental results show that the proposed algorithm can improve both objective and subjective quality of reconstructed frames without any considerable computational complexity. The average PSNR in some frames of test sequences increases about 5.20, 5.78, 5.88, 4.37, 4.41, and 3.50 dB compared to average MV, classic boundary matching, directional boundary matching, directional temporal boundary matching, outer boundary matching, and dynamical temporal error concealment algorithm, respectively."
"An important concept in wireless systems has been quality of experience (QoE)-aware video transmission. Such communications are considered not only connection-based communications but also content-aware communications, since the video quality is closely related to the content itself. It becomes necessary therefore for video communications to utilize a cross-layer design (also known as joint source and channel coding). To provide efficient methods of allocating network resources, the wireless network uses its cross-layer knowledge to perform unequal error protection (UEP) solutions. In this article, we summarize the latest video transmission technologies that are based on scalable video coding (SVC) over multiple-input multiple-output (MIMO) systems with cross-layer designs. To provide insight into video transmission in wireless networks, we investigate UEP solutions in the delivering of video over massive MIMO systems. Our results show that in terms of quality of experience (QoE), SVC layer prioritization, which was considered important in the prior work, is not always beneficial in massive MIMO systems; consideration must be given to the content characteristics."
"Pairwise comparisons between alternatives are a well-known method of decision-making. Since the preferences do often not exhibit consistency, a number of inconsistency indices have been defined in order to measure the deviation from this ideal case. Recently, some axioms have been proposed to identify indices that evaluate inconsistency correctly. Inconsistency rankings are weak orders on the set of pairwise comparison matrices with respect to their inconsistency level and can be defined by any inconsistency index. This study characterizes a specific inconsistency ranking by providing six independent axioms such that they determine a unique weak order on the set of all pairwise comparison matrices."
"Verification of C/C++ programs has seen considerable progress in several areas, but not for programs that use these languages' mathematical libraries. The reason is that all libraries in widespread use come without no guarantees about the computed results. This would seem to prevent any attempt at formal verification of programs that use them: without a specification for the functions, no conclusion can be statically drawn about the behavior of the program. We propose an alternative to surrender: even if we do not have a proper specification for the functions, we might have a partial specification; at the very least we have their implementation, for which a partial specification can sometimes be extracted. When even a partial specification is unavailable, we can still detect flaws in the program and facilitate its partial verification with testing. We propose a pragmatic, practical approach that leverages the fact that most math.h/cmath functions are almost piecewise monotonic: they may have ""glitches"", often of very small size and in small quantities. We develop interval refinement techniques for such functions that enable verification via abstract interpretation, symbolic model checking and constraint-based test data generation."
"In this paper, we investigate the use of Convolutional Neural Networks for counting the number of records in historical handwritten documents. With this work we demonstrate that training the networks only with synthetic images allows us to perform a near perfect evaluation of the number of records printed on historical documents. The experiments have been performed on a benchmark dataset composed by marriage records and outperform previous results on this dataset."
"The problem of parallel thread mapping is studied for the case of discrete orthogonal $m$-simplices. The possibility of a $O(1)$ time recursive block-space map $\lambda: \mathbb{Z}^m \mapsto \mathbb{Z}^m$ is analyzed from the point of view of parallel space efficiency and potential performance improvement. The $2$-simplex and $3$-simplex are analyzed as special cases, where constant time maps are found, providing a potential improvement of up to $2\times$ and $6\times$ more efficient than a bounding-box approach, respectively. For the general case it is shown that finding an efficient recursive parallel space for an $m$-simplex depends of the choice of two parameters, for which some insights are provided which can lead to a volume that matches the $m$-simplex for $n>n_0$, making parallel space approximately $m!$ times more efficient than a bounding-box."
"Realistic propagation modeling requires a detailed understanding and characterization of the radio channel properties. This paper is based on channel sounder measurements with 1 GHz bandwidth at a carrier frequency of 5.7 GHz and particular tracking methods. We present statistical models for the number of, birth rate, lifetime, excess delay and relative Doppler frequency of individual multipath components (MPCs). Our findings are concluded from 72 measurement runs in eight relevant vehicular communication scenarios and reveal wide insights into the dynamic propagation process in vehicular communication scenarios."
"A detailed understanding of the dynamic processes of vehicular radio channels is crucial for its realistic modeling. In this paper, we present multipath components (MPCs) tracking results from a channel sounder measurement with 1 GHz bandwidth at a carrier frequency of 5.7 GHz. We describe in detail the applied algorithms and perform a tracking performance evaluation based on artificial channels and on measurement data from a tunnel scenario. The tracking performance of the proposed algorithm is comparable to the tracking performance of the state-of-the-art Gaussian mixture probability hypothesis density filter, yet with a significantly lower complexity. The fluctuation of the measured channel gain is followed very well by the proposed tracking algorithm, with a power loss of only 2.5 dB. We present statistical distributions for the number of MPCs and the birth/death rate. The applied algorithms and tracking results can be used to enhance the development of geometry-based channel models."
"This work addresses the correct translation of an Event-B model to C code via an intermediate formal language, HLL. The proof of correctness follows two main steps. First, the final refinement of the Event-B model, including invariants, is translated to HLL. At that point, additional properties (e.g., deadlock-freeness, liveness properties, etc.) are added to the HLL model. The proof of the invariants and additional properties at the HLL level guarantees the correctness of the translation. Second, the C code is automatically generated from the HLL model for most of the system functions and manually for the remaining ones; in this case, the HLL model provides formal contracts to the software developer. An equivalence proof between the C code and the HLL model guarantees the correctness of the code."
"This paper discusses Centre for Development of Advanced Computing Mumbai's (CDACM) submission to the NLP Tools Contest on Statistical Machine Translation in Indian Languages (ILSMT) 2014 (collocated with ICON 2014). The objective of the contest was to explore the effectiveness of Statistical Machine Translation (SMT) for Indian language to Indian language and English-Hindi machine translation. In this paper, we have proposed that suffix separation and word splitting for SMT from agglutinative languages to Hindi significantly improves over the baseline (BL). We have also shown that the factored model with reordering outperforms the phrase-based SMT for English-Hindi (\enhi). We report our work on all five pairs of languages, namely Bengali-Hindi (\bnhi), Marathi-Hindi (\mrhi), Tamil-Hindi (\tahi), Telugu-Hindi (\tehi), and \enhi for Health, Tourism, and General domains."
"5G networks are expected to be more dynamic and chaotic in their structure than current networks. With the advent of Network Function Virtualization (NFV), Network Functions (NF) will no longer be tightly coupled with the hardware they are running on, which poses new challenges in network management. Noisy neighbor is a term commonly used to describe situations in NFV infrastructure where an application experiences degradation in performance due to the fact that some of the resources it needs are occupied by other applications in the same cloud node. These situations cannot be easily identified using straightforward approaches, which calls for the use of sophisticated methods for NFV infrastructure management. In this paper we demonstrate how Machine Learning (ML) techniques can be used to identify such events. Through experiments using data collected at real NFV infrastructure, we show that standard models for automated classification can detect the noisy neighbor phenomenon with an accuracy of more than 90% in a simple scenario."
"Reordering is a preprocessing stage for Statistical Machine Translation (SMT) system where the words of the source sentence are reordered as per the syntax of the target language. We are proposing a rich set of rules for better reordering. The idea is to facilitate the training process by better alignments and parallel phrase extraction for a phrase-based SMT system. Reordering also helps the decoding process and hence improving the machine translation quality. We have observed significant improvements in the translation quality by using our approach over the baseline SMT. We have used BLEU, NIST, multi-reference word error rate, multi-reference position independent error rate for judging the improvements. We have exploited open source SMT toolkit MOSES to develop the system."
"China's scientific output has risen precipitously over the past decade; it is now the world's second-largest producer of scientific papers, behind only the United States. The quality of China's research is also on the rise (Van Noorden, 2016). The online visibility and impact of China's research are also important issues worth exploring. In this study, we investigate the altmetric performance of publications in the field of Biotechnology and Applied Microbiology and published by authors from Chinese affiliations. We find that papers published by those authors from Chinese affiliations have much lower visibility on the social web than articles from other countries, when there is no significant difference for the citations. Fewer of China's publications get tweeted, and those tweeted publications attract less social attention. A geographical analysis of tweeters shows that scholarly articles get most of their social attention from the authors' home countries, a finding that is also confirmed by correlation and regression analysis. This situation, which is unfavorable for researchers from Chinese affiliations, is caused, in part, by the inaccessibility of mainstream social networking platforms in mainland China."
"Meaning has been called the ""holy grail"" of a variety of scientific disciplines, ranging from linguistics to philosophy, psychology and the neurosciences. The field of Artifical Intelligence (AI) is very much a part of that list: the development of sophisticated natural language semantics is a sine qua non for achieving a level of intelligence comparable to humans. Embodiment theories in cognitive science hold that human semantic representation depends on sensori-motor experience; the abundant evidence that human meaning representation is grounded in the perception of physical reality leads to the conclusion that meaning must depend on a fusion of multiple (perceptual) modalities. Despite this, AI research in general, and its subdisciplines such as computational linguistics and computer vision in particular, have focused primarily on tasks that involve a single modality. Here, we propose virtual embodiment as an alternative, long-term strategy for AI research that is multi-modal in nature and that allows for the kind of scalability required to develop the field coherently and incrementally, in an ethically responsible fashion."
"The uncapacitated facility location has always been an important problem due to its connection to operational research and infrastructure planning. Byrka obtained an algorithm that is parametrized by $\gamma$ and proved that it is optimal when $\gamma>1.6774$. He also proved that the algorithm achieved an approximation ratio of 1.50. A later work by Shi Li achieved an approximation factor of 1.488. In this research, we studied these algorithms and several related works. Although we didn't improve upon the algorithm of Shi Li, our work did provide some insight into the problem. We also reframed the problem as a vector game, which provided a framework to design balanced algorithms for this problem."
"The aim of this paper is to develop a general framework for training neural networks (NNs) in a distributed environment, where training data is partitioned over a set of agents that communicate with each other through a sparse, possibly time-varying, connectivity pattern. In such distributed scenario, the training problem can be formulated as the (regularized) optimization of a non-convex social cost function, given by the sum of local (non-convex) costs, where each agent contributes with a single error term defined with respect to its local dataset. To devise a flexible and efficient solution, we customize a recently proposed framework for non-convex optimization over networks, which hinges on a (primal) convexification-decomposition technique to handle non-convexity, and a dynamic consensus procedure to diffuse information among the agents. Several typical choices for the training criterion (e.g., squared loss, cross entropy, etc.) and regularization (e.g., $\ell_2$ norm, sparsity inducing penalties, etc.) are included in the framework and explored along the paper. Convergence to a stationary solution of the social non-convex problem is guaranteed under mild assumptions. Additionally, we show a principled way allowing each agent to exploit a multi-core architecture (e.g., a local cloud) in order to parallelize its local optimization step, resulting in strategies that are both distributed (across the agents) and parallel (inside each agent) in nature. A comprehensive set of experimental results validate the proposed approach."
"Hashkat (http://hashkat.org) is a free, open source, agent based simulation software package designed to simulate large-scale online social networks (e.g. Twitter, Facebook, LinkedIn, etc). It allows for dynamic agent generation, edge creation, and information propagation. The purpose of hashkat is to study the growth of online social networks and how information flows within them. Like real life online social networks, hashkat incorporates user relationships, information diffusion, and trending topics. Hashkat was implemented in C++, and was designed with extensibility in mind. The software includes Shell and Python scripts for easy installation and usability. In this report, we describe all of the algorithms and features integrated into hashkat before moving on to example use cases. In general, hashkat can be used to understand the underlying topology of social networks, validate sampling methods of such networks, develop business strategy for advertising on online social networks, and test new features of an online social network before going into production."
"Network latency can have a significant impact on the performance of transactional storage systems, particularly in wide area or geo-distributed deployments. To reduce latency, systems typically rely on a cache to service read-requests closer to the client. However, caches are not effective for write-heavy workloads, which have to be processed by the storage system in order to maintain serializability.   This paper presents a new technique, called optimistic abort, which reduces network latency for high-contention, write-heavy workloads by identifying transactions that will abort as early as possible, and aborting them before they reach the store. We have implemented optimistic abort in a system called Gotthard, which leverages recent advances in network data plane programmability to execute transaction processing logic directly in network devices. Gotthard examines network traffic to observe and log transaction requests. If Gotthard suspects that a transaction is likely to be aborted at the store, it aborts the transaction early by re-writing the packet header, and routing the packets back to the client. Gotthard significantly reduces the overall latency and improves the throughput for high-contention workloads."
"In this paper, a novel label fusion method is proposed for brain magnetic resonance image segmentation. This label fusion method is formulated on a graph, which embraces both label priors from atlases and anatomical priors from target image. To represent a pixel in a comprehensive way, three kinds of feature vectors are generated, including intensity, gradient and structural signature. To select candidate atlas nodes for fusion, rather than exact searching, randomized k-d tree with spatial constraint is introduced as an efficient approximation for high-dimensional feature matching. Feature Sensitive Label Prior (FSLP), which takes both the consistency and variety of different features into consideration, is proposed to gather atlas priors. As FSLP is a non-convex problem, one heuristic approach is further designed to solve it efficiently. Moreover, based on the anatomical knowledge, parts of the target pixels are also employed as graph seeds to assist the label fusion process and an iterative strategy is utilized to gradually update the label map. The comprehensive experiments carried out on two publicly available databases give results to demonstrate that the proposed method can obtain better segmentation quality."
"Publication and citation patterns can vary significantly between related disciplines or more narrow specialties, even when sharing journals. Journal-based structures are therefore not accurate enough to approximate certain specialties, neither subject categories in global citation indices, nor cell sub-structures (Rons, 2012). This paper presents first test results of a new methodology that approximates the specialty of a highly specialized seed record by combining criteria for four publication metadata-fields, thereby broadly covering conceptual components defining disciplines and scholarly communication. To offer added value compared to journal-based structures, the methodology needs to generate sufficiently distinct results for seed directories in related specialties (sharing subject categories, cells, or even sources) with significantly different characteristics. This is tested successfully for the sub-domains of theoretical and experimental particle physics. In particular analyses of specialties with characteristics deviating from those of a broader discipline embedded in can benefit from an approach discerning down to specialty level. Such specialties are potentially present in all disciplines, for instance as cases of peripheral, emerging, frontier, or strategically prioritized research areas."
"Recently it has been established that asymptotic incoherence can be used to facilitate subsampling, in order to optimize reconstruction quality, in a variety of continuous compressed sensing problems, and the coherence structure of certain one-dimensional Fourier sampling problems was determined. This paper extends the analysis of asymptotic incoherence to cover multidimensional reconstruction problems. It is shown that Fourier sampling and separable wavelet sparsity in any dimension can yield the same optimal asymptotic incoherence as in one dimensional case. Moreover in two dimensions the coherence structure is compatible with many standard two dimensional sampling schemes that are currently in use. However, in higher dimensional problems with poor wavelet smoothness we demonstrate that there are considerable restrictions on how one can subsample from the Fourier basis with optimal incoherence. This can be remedied by using a sufficiently smooth generating wavelet. It is also shown that using tensor bases will always provide suboptimal decay marred by problems associated with dimensionality. The impact of asymptotic incoherence on the ability to subsample is demonstrated with some simple two dimensional numerical experiments."
"Dynamic complexity is concerned with updating the output of a problem when the input is slightly changed. We study the dynamic complexity of Dyck reachability problems in directed and undirected graphs, where updates may add or delete edges. We show a strong dichotomy between such problems, based on the size of the Dyck alphabet. Some of them are P-complete (under a strong notion of reduction) while the others lie either in DynFO or in NL."
This paper describes a multi-functional deep in-memory processor for inference applications. Deep in-memory processing is achieved by embedding pitch-matched low-SNR analog processing into a standard 6T 16KB SRAM array in 65 nm CMOS. Four applications are demonstrated. The prototype achieves up to 5.6X (9.7X estimated for multi-bank scenario) energy savings with negligible (<1%) accuracy degradation in all four applications as compared to the conventional architecture.
"We develop a Bayesian model for decision-making under time pressure with endogenous information acquisition. In our model, the decision maker decides when to observe (costly) information by sampling an underlying continuous-time stochastic process (time series) that conveys information about the potential occurrence or non-occurrence of an adverse event which will terminate the decision-making process. In her attempt to predict the occurrence of the adverse event, the decision-maker follows a policy that determines when to acquire information from the time series (continuation), and when to stop acquiring information and make a final prediction (stopping). We show that the optimal policy has a rendezvous structure, i.e. a structure in which whenever a new information sample is gathered from the time series, the optimal ""date"" for acquiring the next sample becomes computable. The optimal interval between two information samples balances a trade-off between the decision maker's surprise, i.e. the drift in her posterior belief after observing new information, and suspense, i.e. the probability that the adverse event occurs in the time interval between two information samples. Moreover, we characterize the continuation and stopping regions in the decision-maker's state-space, and show that they depend not only on the decision-maker's beliefs, but also on the context, i.e. the current realization of the time series."
In this paper we propose cryptosystems based on subgroup distortion in hyperbolic groups. We also include concrete examples of hyperbolic groups as possible platforms.
"In this paper, a methodology is investigated for signal recovery in the presence of non-Gaussian noise. In contrast with regularized minimization approaches often adopted in the literature, in our algorithm the regularization parameter is reliably estimated from the observations. As the posterior density of the unknown parameters is analytically intractable, the estimation problem is derived in a variational Bayesian framework where the goal is to provide a good approximation to the posterior distribution in order to compute posterior mean estimates. Moreover, a majorization technique is employed to circumvent the difficulties raised by the intricate forms of the non-Gaussian likelihood and of the prior density. We demonstrate the potential of the proposed approach through comparisons with state-of-the-art techniques that are specifically tailored to signal recovery in the presence of mixed Poisson-Gaussian noise. Results show that the proposed approach is efficient and achieves performance comparable with other methods where the regularization parameter is manually tuned from an available ground truth."
"This work proposes a low complexity nonlinearity model and develops adaptive algorithms over it. The model is based on the decomposable---or rank-one, in tensor language---Volterra kernels. It may also be described as a product of FIR filters, which explains its low-complexity. The rank-one model is also interesting because it comes from a well-posed problem in approximation theory. The paper uses such model in an estimation theory context to develop an exact gradient-type algorithm, from which adaptive algorithms such as the least mean squares (LMS) filter and its data-reuse version---the TRUE-LMS---are derived. Stability and convergence issues are addressed. The algorithms are then tested in simulations, which show its good performance when compared to other nonlinear processing algorithms in the literature."
"Recidivism prediction instruments provide decision makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. While such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This paper discusses a fairness criterion originating in the field of educational and psychological testing that has recently been applied to assess the fairness of recidivism prediction instruments. We demonstrate how adherence to the criterion may lead to considerable disparate impact when recidivism prevalence differs across groups."
"In the past decade, complex network structures have penetrated nearly every aspect of our lives. The detection of anomalous vertices in these networks can uncover important insights, such as exposing intruders in a computer network. In this study, we present a novel unsupervised two-layered meta classifier that can be employed to detect irregular vertices in complex networks using solely features extracted from the network topology. Our method is based on the hypothesis that a vertex having many links with low probabilities of existing has a higher likelihood of being anomalous. We evaluated our method on ten networks, using three fully simulated, five semi-simulated, and two real world datasets. In all the scenarios, our method was able to identify anomalous and irregular vertices with low false positive rates and high AUCs. Moreover, we demonstrated that our method can be applied to security-related use cases and is able to detect malicious profiles in online social networks."
"In this paper we study the Target Set Selection problem, a fundamental problem in computational social choice, from a parameterized complexity perspective. Here for a given graph and a threshold for each vertex the task is to find a set of active vertices that activates whole graph. A vertex becomes active if the number of activated vertices in its neighborhood is at least its threshold.   We give two parameterized algorithms for a special case where each vertex has threshold set to half of its neighbors (the so called Majority Target Set Selection problem) for parameterizations by neighborhood diversity and twin cover number of the input graph. From the opposite side we give hardness proof for the Majority Target Set Selection problem when parameterized by (restriction of) the modular-width - a natural generalization of both previous structural parameters. Finally, we give hardness proof for the Target Set Selection problem parameterized by the neighborhood diversity when there is no restriction on the thresholds."
"We consider the recovery of a (real- or complex-valued) signal from magnitude-only measurements, known as phase retrieval. We formulate phase retrieval as a convex optimization problem, which we call PhaseMax. Unlike other convex methods that use semidefinite relaxation and lift the phase retrieval problem to a higher dimension, PhaseMax operates in the original signal dimension. We show that the dual problem to PhaseMax is Basis Pursuit, which implies that phase retrieval can be performed using algorithms initially designed for sparse signal recovery. We develop sharp lower bounds on the success probability of PhaseMax for a broad range of random measurement ensembles, and we analyze the impact of measurement noise on the solution accuracy. We use numerical results to demonstrate the accuracy of our recovery guarantees, and we showcase the efficacy and limits of PhaseMax in practice."
"While automata theory often concerns itself with regular predicates, relations corresponding to acceptance by a finite state automaton, in this article we study the regular functions, such relations which are also functions in the set-theoretic sense. Here we present a small (but necessarily infinite) collection of (multi-ary) functions which generate the regular functions under composition. To this end, this paper presents an interpretation of the powerset determinization construction in terms of compositions of input-to-run maps. Furthermore, known results using the Krohn-Rhodes theorem to further decompose our generating set are spelled out in detail, alongside some coding tricks for dealing with variable length words. This will include two clear proofs of the Krohn-Rhodes Theorem in modern notation."
"Differential Privacy (DP) has received increased attention as a rigorous privacy framework. Existing studies employ traditional DP mechanisms (e.g., the Laplace mechanism) as primitives, which assume that the data are independent, or that adversaries do not have knowledge of the data correlations. However, continuously generated data in the real world tend to be temporally correlated, and such correlations can be acquired by adversaries. In this paper, we investigate the potential privacy loss of a traditional DP mechanism under temporal correlations in the context of continuous data release. First, we model the temporal correlations using Markov model and analyze the privacy leakage of a DP mechanism when adversaries have knowledge of such temporal correlations. Our analysis reveals that the privacy leakage of a DP mechanism may accumulate and increase over time. We call it temporal privacy leakage. Second, to measure such privacy leakage, we design an efficient algorithm for calculating it in polynomial time. Although the temporal privacy leakage may increase over time, we also show that its supremum may exist in some cases. Third, to bound the privacy loss, we propose mechanisms that convert any existing DP mechanism into one against temporal privacy leakage. Experiments with synthetic data confirm that our approach is efficient and effective."
"The hippocampus is a seminal structure in the most common surgically-treated form of epilepsy. Accurate segmentation of the hippocampus aids in establishing asymmetry regarding size and signal characteristics in order to disclose the likely site of epileptogenicity. With sufficient refinement, it may ultimately aid in the avoidance of invasive monitoring with its expense and risk for the patient. To this end, a reliable and consistent method for segmentation of the hippocampus from magnetic resonance imaging (MRI) is needed. In this work, we present a systematic and statistical analysis approach for evaluation of automated segmentation methods in order to establish one that reliably approximates the results achieved by manual tracing of the hippocampus."
"Minimizing the peak power consumption and matching demand to supply, under fixed threshold polices, are two key requirements for the success of the future electricity market. In this work, we consider dynamic pricing methods to minimize the peak load and match demand to supply in the smart grid. As these optimization problems are computationally hard to solve in general, we propose generic heuristics for approximating their solutions. Further, we provide theoretical analysis of uniform pricing in peak-demand minimization. Moreover, we propose optimal-pricing algorithms for scenarios in which the time-period in which tasks must be executed is relatively small. Finally, we conduct several experiments to evaluate the various algorithms on real data."
"This paper presents a novel automated system that segments six sub-retinal layers from optical coherence tomography (OCT) image stacks of healthy patients and patients with diabetic macular edema (DME). First, each image in the OCT stack is denoised using a Wiener deconvolution algorithm that estimates the additive speckle noise variance using a novel Fourier-domain based structural error. This denoising method enhances the image SNR by an average of 12dB. Next, the denoised images are subjected to an iterative multi-resolution high-pass filtering algorithm that detects seven sub-retinal surfaces in six iterative steps. The thicknesses of each sub-retinal layer for all scans from a particular OCT stack are then compared to the manually marked groundtruth. The proposed system uses adaptive thresholds for denoising and segmenting each image and hence it is robust to disruptions in the retinal micro-structure due to DME. The proposed denoising and segmentation system has an average error of 1.2-5.8 $\mu m$ and 3.5-26$\mu m$ for segmenting sub-retinal surfaces in normal and abnormal images with DME, respectively. For estimating the sub-retinal layer thicknesses, the proposed system has an average error of 0.2-2.5 $\mu m$ and 1.8-18 $\mu m$ in normal and abnormal images, respectively. Additionally, the average inner sub-retinal layer thickness in abnormal images is estimated as 275$\mu m (r=0.92)$ with an average error of 9.3 $\mu m$, while the average thickness of the outer layers in abnormal images is estimated as 57.4$\mu m (r=0.74)$ with an average error of 3.5 $\mu m$. The proposed system can be useful for tracking the disease progression for DME over a period of time."
"We investigate a general framework of multiplicative multitask feature learning which decomposes each task's model parameters into a multiplication of two components. One of the components is used across all tasks and the other component is task-specific. Several previous methods have been proposed as special cases of our framework. We study the theoretical properties of this framework when different regularization conditions are applied to the two decomposed components. We prove that this framework is mathematically equivalent to the widely used multitask feature learning methods that are based on a joint regularization of all model parameters, but with a more general form of regularizers. Further, an analytical formula is derived for the across-task component as related to the task-specific component for all these regularizers, leading to a better understanding of the shrinkage effect. Study of this framework motivates new multitask learning algorithms. We propose two new learning formulations by varying the parameters in the proposed framework. Empirical studies have revealed the relative advantages of the two new formulations by comparing with the state of the art, which provides instructive insights into the feature learning problem with multiple tasks."
"Massive multiuser (MU) multiple-input multiple-output (MIMO) is foreseen to be one of the key technologies in fifth-generation wireless communication systems. In this paper, we investigate the problem of downlink precoding for a narrowband massive MU-MIMO system with low-resolution digital-to-analog converters (DACs) at the base station (BS). We analyze the performance of linear precoders, such as maximal-ratio transmission and zero-forcing, subject to coarse quantization. Using Bussgang's theorem, we derive a closed-form approximation of the achievable rate of the coarsely quantized system. Our results reveal that the infinite-resolution performance can be approached with DACs using only 3 to 4 bits of resolution, depending on the number of BS antennas and the number of user equipments (UEs). For the case of 1-bit DACs, we also propose novel nonlinear precoding algorithms that significantly outperform linear precoders at the cost of an increased computational complexity. Specifically, we show that nonlinear precoding incurs only a 3 dB penalty compared to the infinite-resolution case for an uncoded bit error rate of 10^-3 in a system with 128 BS antennas that uses 1-bit DACs and serves 16 single-antenna UEs; in contrast, the penalty is about 8 dB for linear precoders."
"Vector representations of words have heralded a transformational approach to classical problems in NLP; the most popular example is word2vec. However, a single vector does not suffice to model the polysemous nature of many (frequent) words, i.e., words with multiple meanings. In this paper, we propose a three-fold approach for unsupervised polysemy modeling: (a) context representations, (b) sense induction and disambiguation and (c) lexeme (as a word and sense pair) representations. A key feature of our work is the finding that a sentence containing a target word is well represented by a low rank subspace, instead of a point in a vector space. We then show that the subspaces associated with a particular sense of the target word tend to intersect over a line (one-dimensional subspace), which we use to disambiguate senses using a clustering algorithm that harnesses the Grassmannian geometry of the representations. The disambiguation algorithm, which we call $K$-Grassmeans, leads to a procedure to label the different senses of the target word in the corpus -- yielding lexeme vector representations, all in an unsupervised manner starting from a large (Wikipedia) corpus in English. Apart from several prototypical target (word,sense) examples and a host of empirical studies to intuit and justify the various geometric representations, we validate our algorithms on standard sense induction and disambiguation datasets and present new state-of-the-art results."
"There are several confounding factors that can reduce the accuracy of gait recognition systems. These factors can reduce the distinctiveness, or alter the features used to characterise gait, they include variations in clothing, lighting, pose and environment, such as the walking surface. Full invariance to all confounding factors is challenging in the absence of high-quality labelled training data. We introduce a simulation-based methodology and a subject-specific dataset which can be used for generating synthetic video frames and sequences for data augmentation. With this methodology, we generated a multi-modal dataset. In addition, we supply simulation files that provide the ability to simultaneously sample from several confounding variables. The basis of the data is real motion capture data of subjects walking and running on a treadmill at different speeds. Results from gait recognition experiments suggest that information about the identity of subjects is retained within synthetically generated examples. The dataset and methodology allow studies into fully-invariant identity recognition spanning a far greater number of observation conditions than would otherwise be possible."
"We investigate the secure connectivity of wireless sensor networks under a heterogeneous random key predistribution scheme and a heterogeneous channel model. In particular, we study a random graph formed by the intersection of an inhomogeneous random key graph with an inhomogeneous Erd\H{o}s-R\'enyi graph. The former graph is naturally induced by the heterogeneous random key predistribution scheme while the latter graph constitutes a heterogeneous on/off channel model; wherein, the wireless channel between a class-$i$ node and a class-$j$ node is on with probability $\alpha_{ij}$ independently. We present conditions (in the form of zero-one laws) on how to scale the parameters of the intersection model so that it has no isolated node with high probability as the number of nodes gets large. We also present numerical results to support these zero-one laws in the finite-node regime."
"The arrival of head-mounted displays (HMDs) to the consumer market requires a novel content format that is, first, adapted to the specificities of immersive displays and, second, that takes into account the current reality of multi-display media consumption. We review the requirements for such content format and report on existing initiatives, some of our own, towards implementing such content format."
"We study the problem of designing optical receivers to discriminate between multiple coherent states using coherent processing receivers---i.e., one that uses arbitrary coherent feedback control and quantum-noise-limited direct detection---which was shown by Dolinar to achieve the minimum error probability in discriminating any two coherent states. We first derive and re-interpret Dolinar's binary-hypothesis minimum-probability-of-error receiver as the one that optimizes the information efficiency at each time instant, based on recursive Bayesian updates within the receiver. Using this viewpoint, we propose a natural generalization of Dolinar's receiver design to discriminate $M$ coherent states each of which could now be a codeword, i.e., a sequence of $n$ coherent states each drawn from a modulation alphabet. We analyze the channel capacity of the pure-loss optical channel with a general coherent processing receiver in the low-photon number regime and compare it with the capacity achievable with direct detection and the Holevo limit (achieving the latter would require a quantum joint-detection receiver). We show compelling evidence that despite the optimal performance of Dolinar's receiver for the binary coherent-state hypothesis test (either in error probability or mutual information), the asymptotic communication rate achievable by such a coherent processing receiver is only as good as direct detection. This suggests that in the infinitely-long codeword limit, all potential benefits of coherent processing at the receiver can be obtained by designing a good code and direct detection, with no feedback within the receiver."
"We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods."
"We consider the network reliability problem in wireless sensor networks secured by the heterogeneous random key predistribution scheme. This scheme generalizes Eschenauer-Gligor scheme by considering the cases when the network comprises sensor nodes with varying level of resources; e.g., regular nodes vs. cluster heads. The scheme induces the inhomogeneous random key graph, denoted $\mathbb{G}(n;\pmb{\mu},\pmb{K},P)$. We analyze the reliability of $\mathbb{G}(n;\pmb{\mu},\pmb{K},P)$ against random link failures. Namely, we consider $\mathbb{G}(n;\pmb{\mu},\pmb{K}, P,\alpha)$ formed by deleting each edge of $\mathbb{G}(n;\pmb{\mu},\pmb{K},P)$ independently with probability $1-\alpha$, and study the probability that the resulting graph i) has no isolated node; and ii) is connected. We present scaling conditions on $\pmb{K}$, $P$, and $\alpha$ such that both events take place with probability zero or one, respectively, as the number of nodes gets large. We present numerical results to support these in the finite-node regime."
"The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style."
"Despite popularity of Fingerprinting Localization Algorithms (FPS), general theoretical frameworks for their performance studies have rarely been discussed in the literature. In this work, after setting up an abstract model for the FPS, it is shown that fingerprinting-based localization problem can be cast as a Hypothesis Testing (HT) problem and therefore various results in the HT literature can be used to provide insights for the general FPS. This includes scaling limits of localization reliability in terms of measurement numbers and the precise characterization of a geometric error. The main quantity that encapsulates this information is shown to be the Kullback-Leibler (KL) divergence between probability distributions of a selected feature for fingerprinting at different locations. The KL divergence can be used as a central performance metric, indicating how well a localization algorithm can distinguish two points. The framework is instantiated for Received Signal Strength (RSS)-based algorithms, where the effect of various parameters on the performance of fingerprinting algorithms is discussed, including path loss and fading characteristics, number of measurements, number of anchors and their locations and placement of training points. Simulations and experimental results characterize numerically the findings of the theoretical framework and demonstrate its consistency with realistic localization scenarios."
"We investigate the ability of a quantum measurement device to discriminate two states or, generically, two hypothesis. In full generality, the measurement can be performed a number $n$ of times, and arbitrary pre-processing of the states and post-processing of the obtained data is allowed. Even if the two hypothesis correspond to orthogonal states, perfect discrimination is not always possible. There is thus an intrinsic error associated to the measurement device, which we aim to quantify, that limits its discrimination power. We minimize various error probabilities (averaged or constrained) over all pairs of $n$-partite input states. These probabilities, or their exponential rates of decrease in the case of large $n$, give measures of the discrimination power of the device. For the asymptotic rate of the averaged error probability, we obtain a Chernoff-type bound, dual to the standard Chernoff bound for which the state pair is fixed and the optimization is over all measurements. The key point in the derivation is that i.i.d. states become optimal in asymptotic settings. Minimum asymptotic rates are also obtained for constrained error probabilities, dual to Stein's Lemma and Hoeffding's bound. We further show that adaptive protocols where the state preparer gets feedback from the measurer do not improve the asymptotic rates. These rates thus quantify the ultimate discrimination power of a measurement device."
"ALPINE is to our knowledge the first anytime algorithm to mine frequent itemsets and closed frequent itemsets. It guarantees that all itemsets with support exceeding the current checkpoint's support have been found before it proceeds further. Thus, it is very attractive for extremely long mining tasks with very high dimensional data (for example in genetics) because it can offer intermediate meaningful and complete results. This ANYTIME feature is the most important contribution of ALPINE, which is also fast but not necessarily the fastest algorithm around. Another critical advantage of ALPINE is that it does not require the apriori decided minimum support value."
"Subspace clustering is the problem of partitioning unlabeled data points into a number of clusters so that data points within one cluster lie approximately on a low-dimensional linear subspace. In many practical scenarios, the dimensionality of data points to be clustered are compressed due to constraints of measurement, computation or privacy. In this paper, we study the theoretical properties of a popular subspace clustering algorithm named sparse subspace clustering (SSC) and establish formal success conditions of SSC on dimensionality-reduced data. Our analysis applies to the most general fully deterministic model where both underlying subspaces and data points within each subspace are deterministically positioned, and also a wide range of dimensionality reduction techniques (e.g., Gaussian random projection, uniform subsampling, sketching) that fall into a subspace embedding framework (Meng & Mahoney, 2013; Avron et al., 2014). Finally, we apply our analysis to a differentially private SSC algorithm and established both privacy and utility guarantees of the proposed method."
"This document briefly describes the systems submitted by the Center for Robust Speech Systems (CRSS) from The University of Texas at Dallas (UTD) to the 2016 National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE). We developed several UBM and DNN i-Vector based speaker recognition systems with different data sets and feature representations. Given that the emphasis of the NIST SRE 2016 is on language mismatch between training and enrollment/test data, so-called domain mismatch, in our system development we focused on: (1) using unlabeled in-domain data for centralizing data to alleviate the domain mismatch problem, (2) finding the best data set for training LDA/PLDA, (3) using newly proposed dimension reduction technique incorporating unlabeled in-domain data before PLDA training, (4) unsupervised speaker clustering of unlabeled data and using them alone or with previous SREs for PLDA training, (5) score calibration using only unlabeled data and combination of unlabeled and development (Dev) data as separate experiments."
"In this paper, we develop new test statistics for private hypothesis testing. These statistics are designed specifically so that their asymptotic distributions, after accounting for noise added for privacy concerns, match the asymptotics of the classical (non-private) chi-square tests for testing if the multinomial data parameters lie in lower dimensional manifolds (examples include goodness of fit and independence testing). Empirically, these new test statistics outperform prior work, which focused on noisy versions of existing statistics."
"When a new treatment is considered for use, whether a pharmaceutical drug or a search engine ranking algorithm, a typical question that arises is, will its performance exceed that of the current treatment? The conventional way to answer this counterfactual question is to estimate the effect of the new treatment in comparison to that of the conventional treatment by running a controlled, randomized experiment. While this approach theoretically ensures an unbiased estimator, it suffers from several drawbacks, including the difficulty in finding representative experimental populations as well as the cost of running such trials. Moreover, such trials neglect the huge quantities of available control-condition data which are often completely ignored.   In this paper we propose a discriminative framework for estimating the performance of a new treatment given a large dataset of the control condition and data from a small (and possibly unrepresentative) randomized trial comparing new and old treatments. Our objective, which requires minimal assumptions on the treatments, models the relation between the outcomes of the different conditions. This allows us to not only estimate mean effects but also to generate individual predictions for examples outside the randomized sample.   We demonstrate the utility of our approach through experiments in three areas: Search engine operation, treatments to diabetes patients, and market value estimation for houses. Our results demonstrate that our approach can reduce the number and size of the currently performed randomized controlled experiments, thus saving significant time, money and effort on the part of practitioners."
"We present methods for offline generation of sparse roadmap spanners that result in graphs 79% smaller than existing approaches while returning solutions of equivalent path quality. Our method uses a hybrid approach to sampling that combines traditional graph discretization with random sampling. We present techniques that optimize the graph for the L1-norm metric function commonly used in joint-based robotic planning, purposefully choosing a $t$-stretch factor based on the geometry of the space, and removing redundant edges that do not contribute to the graph quality. A high-quality pre-processed sparse roadmap is then available for re-use across many different planning scenarios using standard repair and re-plan methods. Pre-computing the roadmap offline results in more deterministic solutions, reduces the memory requirements by affording complex rejection criteria, and increases the speed of planning in high-dimensional spaces allowing more complex problems to be solved such as multi-modal task planning. Our method is validated through simulated benchmarks against the SPARS2 algorithm. The source code is freely available online as an open source extension to OMPL."
"Methods for unsupervised anomaly detection suffer from the fact that the data is unlabeled, making it difficult to assess the optimality of detection algorithms. Ensemble learning has shown exceptional results in classification and clustering problems, but has not seen as much research in the context of outlier detection. Existing methods focus on combining output scores of individual detectors, but this leads to outputs that are not easily interpretable. In this paper, we introduce a theoretical foundation for combining individual detectors with Bayesian classifier combination. Not only are posterior distributions easily interpreted as the probability distribution of anomalies, but bias, variance, and individual error rates of detectors are all easily obtained. Performance on real-world datasets shows high accuracy across varied types of time series data."
"We classify the rare events of structured, memoryful stochastic processes and use this to analyze sequential and parallel generators for these events. Given a stochastic process, we introduce a method to construct a new process whose typical realizations are a given process' rare events. This leads to an expression for the minimum memory required to generate rare events. We then show that the recently discovered classical-quantum ambiguity of simplicity also occurs when comparing the structure of process fluctuations."
"We introduce co-occurring directions sketching, a deterministic algorithm for approximate matrix product (AMM), in the streaming model. We show that co-occuring directions achieves a better error bound for AMM than other randomized and deterministic approaches for AMM. Co-occurring directions gives a $1 + \epsilon$ -approximation of the optimal low rank approximation of a matrix product. Empirically our algorithm outperforms competing methods for AMM, for a small sketch size. We validate empirically our theoretical findings and algorithms"
"Air conditioning systems are responsible for the major percentage of energy consumption in buildings. Shared spaces constitute considerable office space area, in which most office employees perform their meetings and daily tasks, and therefore the ACs in these areas have significant impact on the energy usage of the entire office building. The cost of this energy consumption, however, is not paid by the shared space users, and the AC's temperature set-point is not determined based on the users' preferences. This latter factor is compounded by the fact that different people may have different choices of temperature set-points and sensitivities to change of temperature. Therefore, it is a challenging task to design an office policy to decide on a particular set-point based on such a diverse preference set. As a result, users are not aware of the energy consumption in shared spaces, which may potentially increase the energy wastage and related cost of office buildings. In this context, this paper proposes an energy policy for an office shared space by exploiting an established temperature control mechanism. In particular, we choose meeting rooms in an office building as the test case and design a policy according to which each user of the room can give a preference on the temperature set-point and is paid for felt discomfort if the set-point is not fixed according to the given preference. On the other hand, users who enjoy the thermal comfort compensate the other users of the room. Thus, the policy enables the users to be cognizant and responsible for the payment on the energy consumption of the office space they are sharing, and at the same time ensures that the users are satisfied either via thermal comfort or through incentives. The policy is also shown to be beneficial for building management. Through experiment based case studies, we show the effectiveness of the proposed policy."
"In this paper, we develop the theory of analytic virtual machines, that implement analytic programming spaces and operators acting upon them.   A programming space is a subspace of the function space of maps on the virtual memory. We can construct a differential operator on programming spaces as we extend the virtual memory to a tensor product of a virtual space with tensor algebra of its dual. Extended virtual memory serves by itself as an algebra of programs, giving the expansion of the original program as an infinite tensor series at program's input values.   We present a theory of operators on programming spaces, that enables analysis of programs and computations on the operator level, which favors general implementation. Theory enables approximation and transformations of programs to a more appropriate function basis'. We also present several examples of how the theory can be used in computer science.   We generalize neural networks by constructing general tensor networks, that naturally exist in virtual memory. Transformations of programs to these trainable networks are derived, providing a meaningful way of network initialization. Theory opens new doors in program analysis, while fully retaining algorithmic control flow. We develop a general procedure which takes a program that tests an object for a property and constructs a program that imposes that property upon any object. We use it to generalize state of the art methods for analyzing neural networks to general programs and tensor networks. Expanding upon them, we study dynamics of computation through principles they induce into the system."
"This paper considers a multiple-input-multiple-output (MIMO) system with low-resolution analog-to-digital converters (ADCs). In this system, we present a novel blind detection framework that performs data symbol detection without explicitly knowing channel state information at a receiver. The underlying idea of the proposed framework is to exploit supervised learning. Specifically, during channel training, the proposed approach sends a sequence of data symbols as pilots so that the receiver learns a nonlinear function that is determined by both a channel matrix and a quantization function of the ADCs. During data transmission, the receiver uses the estimated nonlinear function from labeled training data to detect which data symbols were transmitted. We propose three blind detection methods, which are connected to a $K$-nearest neighbors classification and a nearest-centroid classification. We also provide an analytical expression for the symbol-vector-error probability of the MIMO systems with one-bit ADCs when employing the proposed framework. One major observation is that the symbol-vector-error probability decreases exponentially with the inverse of the number of transmit antennas, the operating signal-to-noise ratio, and the minimum distance that can increase with the number of receive antennas. Simulations demonstrate the performance improvement of the proposed framework compared to existing detection techniques."
"This volume contains the post-proceedings of the 14th International Workshop on Quantitative Aspects of Programming Languages and Systems (QAPL), held as a satellite workshop of ETAPS 2016 in Eindhoven, The Netherlands, on 2-3 April 2016."
"Cloud solutions are increasingly used for a plethora of purposes, including solving memory-intensive and computation-intensive problems. Ensuring the reliability, availability, scalability, and security of cloud solutions, as networked distributed systems with properties such as dynamic reallocation of resources, is a challenging problem that requires rigorous modeling, analysis, and verification tools. Such tools can be devised using the techniques provided by the formal methods community. On the other hand, many formal analysis and verification tools are memory-intensive and computation-intensive solutions, which can benefit from the cloud technology.   The goal of the iFMCloud workshop is to identify and better understand challenges of using formal and semi-formal methods for modeling and verification of Cloud-based systems and computer and communication networks, as well as challenges and opportunities in providing formal analysis and verification as services on the Cloud. We aim to reach these goals by bringing together researchers and practitioners from these, and other related fields."
"We give an example of a three person chess-like game that has no Nash equilibrium in pure stationary strategies. The game has seven positions, four outcomes (a unique cycle and three terminal positions), and its normal form is of size 2 x 2 x 4 only. Thus, our example strengthens significantly the one obtained in 2014 by Gurvich and Oudalov, which has four players, five terminals, and 2 x 4 x 6 x 8 normal form. Due to its small size, we can strengthen our new example further, by showing that it has no Nash equilibrium not only in pure but also in independently mixed strategies, for both Markov and a priori realizations."
"Topic modeling is an increasingly important component of Big Data analytics, enabling the sense-making of highly dynamic and diverse streams of text data. Traditional methods such as Dynamic Topic Modeling (DTM), while mathematically elegant, do not lend themselves well to direct parallelization because of dependencies from one time step to another. Data decomposition approaches that partition data across time segments and then combine results in a global view of the dynamic change of topics enable execution of topic models on much larger datasets than is possibly without data decomposition. However, these methods are difficult to analyze mathematically and are relatively untested for quality of topics and performance on parallel systems. In this paper, we introduce and empirically analyze Clustered Latent Dirichlet Allocation (CLDA), a method for extracting dynamic latent topics from a collection of documents. CLDA uses a data decomposition strategy to partition data. CLDA takes advantage of parallelism, enabling fast execution for even very large datasets and a large number of topics. A large corpus is split into local segments to extract textual information from different time steps. Latent Dirichlet Allocation (LDA) is applied to infer topics at local segments. The results are merged, and clustering is used to combine topics from different segments into global topics. Results show that the perplexity is comparable and that topics generated by this algorithm are similar to those generated by DTM. In addition, CLDA is two orders of magnitude faster than existing approaches and allows for more freedom of experiment design. In this paper CLDA is applied successfully to seventeen years of NIPS conference papers, seventeen years of computer science journal abstracts, and to forty years of the PubMed corpus."
"The class of queries for detecting path is an important as those can extract implicit binary relations over the nodes of input graphs. Most of the path querying languages used by the RDF community, like property paths in W3C SPARQL 1.1 and nested regular expressions in nSPARQL are based on the regular expressions. Federated queries allow for combining graph patterns and relational database that enables the evaluations over several heterogeneous data resources within a single query. Federated queries in W3C SPARQL 1.1 currently evaluated over different SPARQL endpoints. In this paper, we present a federated path querying language as an extension of regular path querying language for supporting RDF graph integration with relational database. The federated path querying language is absolutely more expressive than nested regular expressions and negation-free property paths. Its additional expressivity can be used for capturing the conjunction and federation of nested regular path queries. Despite the increase in expressivity, we also show that federated path queries are still enjoy a low computational complexity and can be evaluated efficiently."
"Machine Learning has been a big success story during the AI resurgence. One particular stand out success relates to unsupervised learning from a massive amount of data, albeit much of it relates to one modality/type of data at a time. In spite of early assertions of the unreasonable effectiveness of data, there is increasing recognition of utilizing knowledge whenever it is available or can be created purposefully. In this paper, we focus on discussing the indispensable role of knowledge for deeper understanding of complex text and multimodal data in situations where (i) large amounts of training data (labeled/unlabeled) are not available or labor intensive to create, (ii) the objects (particularly text) to be recognized are complex (i.e., beyond simple entity-person/location/organization names), such as implicit entities and highly subjective content, and (iii) applications need to use complementary or related data in multiple modalities/media. What brings us to the cusp of rapid progress is our ability to (a) create knowledge, varying from comprehensive or cross domain to domain or application specific, and (b) carefully exploit the knowledge to further empower or extend the applications of ML/NLP techniques. Using the early results in several diverse situations - both in data types and applications - we seek to foretell unprecedented progress in our ability for deeper understanding and exploitation of multimodal data."
"Emoji are a contemporary and extremely popular way to enhance electronic communication. Without rigid semantics attached to them, emoji symbols take on different meanings based on the context of a message. Thus, like the word sense disambiguation task in natural language processing, machines also need to disambiguate the meaning or sense of an emoji. In a first step toward achieving this goal, this paper presents EmojiNet, the first machine readable sense inventory for emoji. EmojiNet is a resource enabling systems to link emoji with their context-specific meaning. It is automatically constructed by integrating multiple emoji resources with BabelNet, which is the most comprehensive multilingual sense inventory available to date. The paper discusses its construction, evaluates the automatic resource creation process, and presents a use case where EmojiNet disambiguates emoji usage in tweets. EmojiNet is available online for use at http://emojinet.knoesis.org."
"The all-relevant problem of feature selection is the identification of all strongly and weakly relevant attributes. This problem is especially hard to solve for time series classification and regression in industrial applications such as predictive maintenance or production line optimization, for which each label or regression target is associated with several time series and meta-information simultaneously. Here, we are proposing an efficient, scalable feature extraction algorithm, which filters the available features in an early stage of the machine learning pipeline with respect to their significance for the classification or regression task, while controlling the expected percentage of selected but irrelevant features.   The proposed algorithm combines established feature extraction methods with a feature importance filter. It has a low computational complexity, allows to start on a problem with only limited domain knowledge available, can be trivially parallelized, is highly scalable and based on well studied non-parametric hypothesis tests. We benchmark our proposed algorithm on all binary classification problems of the UCR time series classification archive as well as time series from a production line optimization project and simulated stochastic processes with underlying qualitative change of dynamics."
"Count-Sketch \cite{CCFC02} and Count-Median \cite{CM04} are two widely used sketching algorithms for processing large-scale distributed and streaming datasets, such as finding frequent elements, computing frequency moments, performing point queries, etc. % Their popularity is largely due to the fact that linear sketches can be naturally composed in the distributed model and be efficiently updated in the streaming model. Moreover, both the sketching phase and the recovery phase can be parallelized. The errors of Count-Sketch and Count-Median are expressed in terms of the sum of coordinates of the input vector excluding those largest ones, or, the mass on the tail of the vector. Thus, the precondition for these algorithms to perform well is that the mass on the tail is small, which is, however, not always the case -- in many real-world datasets the coordinates of the input vector have a non-zero bias, which will generate a large mass on the tail.   In this paper we propose linear sketches that are {\em bias-aware}. They can be used as substitutes to Count-Sketch and Count-Median, and achieve strictly better error guarantees. We also demonstrate their practicality by an extensive experimental evaluation on both real and synthetic datasets."
"In the last two decades, several classes of codes are introduced to protect the copyrighted digital data. They have important applications in the scenarios like digital fingerprinting and broadcast encryption schemes. In this paper we will discuss three important classes of such codes, namely, frameproof codes, parent-identifying codes and traceability codes.   Firstly, suppose $N(t)$ is the minimal integer such that there exists a binary $t$-frameproof code of length $N$ with cardinality larger than $N$, we prove that $N(t)\ge\frac{15+\sqrt{33}}{24} (t-2)^2$, which is a great improvement of the previously known bound $N(t)\ge\binom{t+1}{2}$. Moreover, we find that the determination of $N(t)$ is closely related to a conjecture of Erd\H{o}s, Frankl and F\""uredi posed in the 1980's, which implies the conjectured value $N(t)=t^2+o(t^2)$. Secondly, we derive a new upper bound for parent-identifying codes, which is superior than all previously known bounds. Thirdly, we present an upper bound for 3-traceability codes, which shows that a $q$-ary 3-traceability code of length $N$ can have at most $cq^{\lceil N/9\rceil}$ codewords, where $c$ is a constant only related to the code length $N$. It is the first meaningful upper bound for 3-traceability codes and our result supports a conjecture of Blackburn et al. posed in 2010."
"We propose a new tensor factorization method, called the Sparse Hierarchical-Tucker (Sparse H-Tucker), for sparse and high-order data tensors. Sparse H-Tucker is inspired by its namesake, the classical Hierarchical Tucker method, which aims to compute a tree-structured factorization of an input data set that may be readily interpreted by a domain expert. However, Sparse H-Tucker uses a nested sampling technique to overcome a key scalability problem in Hierarchical Tucker, which is the creation of an unwieldy intermediate dense core tensor; the result of our approach is a faster, more space-efficient, and more accurate method. We extensively test our method on a real healthcare dataset, which is collected from 30K patients and results in an 18th order sparse data tensor. Unlike competing methods, Sparse H-Tucker can analyze the full data set on a single multi-threaded machine. It can also do so more accurately and in less time than the state-of-the-art: on a 12th order subset of the input data, Sparse H-Tucker is 18x more accurate and 7.5x faster than a previously state-of-the-art method. Even for analyzing low order tensors (e.g., 4-order), our method requires close to an order of magnitude less time and over two orders of magnitude less memory, as compared to traditional tensor factorization methods such as CP and Tucker. Moreover, we observe that Sparse H-Tucker scales nearly linearly in the number of non-zero tensor elements. The resulting model also provides an interpretable disease hierarchy, which is confirmed by a clinical expert."
"Over a finite field $\mathbb{F}_{q^m}$, the evaluation of skew polynomials is intimately related to the evaluation of linearized polynomials. This connection allows one to relate the concept of polynomial independence defined for skew polynomials to the familiar concept of linear independence for vector spaces. This relation allows for the definition of a representable matroid called the $\mathbb{F}_{q^m}[x;\sigma]$-matroid, with rank function that makes it a metric space. Specific submatroids of this matroid are individually bijectively isometric to the projective geometry of $\mathbb{F}_{q^m}$ equipped with the subspace metric. This isometry allows one to use the $\mathbb{F}_{q^m}[x;\sigma]$-matroid in a matroidal network coding application."
"Identifying user's identity is a key problem in many data mining applications, such as product recommendation, customized content delivery and criminal identification. Given a set of accounts from the same or different social network platforms, user identification attempts to identify all accounts belonging to the same person. A commonly used solution is to build the relationship among different accounts by exploring their collective patterns, e.g., user profile, writing style, similar comments. However, this kind of method doesn't work well in many practical scenarios, since the information posted explicitly by users may be false due to various reasons. In this paper, we re-inspect the user identification problem from a novel perspective, i.e., identifying user's identity by matching his/her cameras. The underlying assumption is that multiple accounts belonging to the same person contain the same or similar camera fingerprint information. The proposed framework, called User Camera Identification (UCI), is based on camera fingerprints, which takes fully into account the problems of multiple cameras and reposting behaviors."
"Submodularity is one of the most important property of combinatorial optimization, and $k$-submodularity is a generalization of submodularity. Maximization of $k$-submodular function is NP-hard, and approximation algorithms are studied. For monotone $k$-submodular function, [Iwata, Tanigawa, and Yoshida 2016] gave $k/(2k-1)$-approximation algorithm. In this paper, we give a deterministic algorithm by derandomizing that algorithm. Derandomization scheme is from [Buchbinder and Feldman 2016]. Our algorithm is $k/(2k-1)$-approximation and polynomial-time algorithm."
"Modern data integration systems need to process large amounts of data from a variety of data sources and with real-time integration constraints. They are not only employed in enterprises for managing internal data but are also used for a variety of web services that use techniques such as entity resolution or data cleaning in live systems. In this work, we discuss a new generation of data integration systems that operate on (un-)structured data in an online setting, i.e., systems which process continuously modified datasets upon which the integration task is based. We use as an example of such a system an online event integration system called StoryPivot. It observes events extracted from news articles in data sources such as the 'Guardian' or the 'Washington Post' which are integrated to show users the evolution of real-world stories over time. The design decisions for StoryPivot are influenced by the trade-off between maintaining high quality integration results while at the same time building a system that processes and integrates events in near real-time. We evaluate our design decisions with experiments on two real-world datasets and generalize our findings to other data integration tasks that have a similar system setup."
"Cross-validation (CV) is a technique for evaluating the ability of statistical models/learning systems based on a given data set. Despite its wide applicability, the rather heavy computational cost can prevent its use as the system size grows. To resolve this difficulty in the case of Bayesian linear regression, we develop a formula for evaluating the leave-one-out CV error approximately without actually performing CV. The usefulness of the developed formula is tested by statistical mechanical analysis for a synthetic model. This is confirmed by application to a real-world supernova data set as well."
"We describe mts, which is a generic framework for parallelizing certain types of tree search programs, that (a) provides a single common wrapper containing all of the parallelization, and (b) minimizes the changes needed to the existing single processor legacy code. The mts code was derived from ideas used to develop mplrs, a parallelization of the reverse search vertex enumeration code lrs. The tree search properties required for the use of mts are satisfied by any reverse search algorithm as well as other tree search methods such as backtracking and branch and bound. mts is programmed in C, uses the MPI parallel environment, and can be run on a network of computers.   As examples we parallelize two simple existing reverse search codes: generating topological orderings and generating spanning trees of a graph. We give computational results comparing the parallel codes with state of the art sequential codes for the same problems."
"In this paper, we give algorithms and methods of construction of self-dual codes over finite fields using orthogonal matrices. Randomization in the orthogonal group, and code extension are the main tools. Some optimal, almost MDS, and MDS self-dual codes over both small and large prime fields are constructed."
"We introduce a notion of complexity of diagrams (and in particular of objects and morphisms) in an arbitrary category, as well as a notion of complexity of functors between categories equipped with complexity functions. We discuss several examples of this new definition in categories of wide common interest, such as finite sets, Boolean functions, topological spaces, vector spaces, graded algebras, schemes and modules. We show that on one hand categorical complexity recovers in several settings classical notions of non-uniform computational complexity (such as circuit complexity), while on the other hand it has features which make it mathematically more natural. We also postulate that studying functor complexity is the categorical analog of classical questions in complexity theory about separating different complexity classes."
"We consider efficient communications over the multiple-input multiple-output (MIMO) multiway distributed relay channel (MDRC) with full data exchange, where each user, equipped with multiple antennas, broadcasts its message to all the other users via the help of a number of distributive relays. We propose a physical-layer network coding (PNC) based scheme involving linear precoding for channel alignment nested lattice coding for PNC, and lattice-based precoding for interference mitigation, We show that, with the proposed scheme, distributed relaying achieves the same sum-rate as cooperative relaying in the high SNR regime. We also show that the proposed scheme achieve the asymptotic sum capacity of the MIMO MDRC within a constant gap at high SNR. Numerical results demonstrate that the proposed scheme considerably outperforms the existing schemes including decode-and-forward and amplify-and-forward."
"In this era of modern devices and high speed communication the issue of spectral overloading is increasing with time and becoming more serious. With the advancement in LED industry, light fidelity (Li-Fi) based indoor network is an attractive substitute for the existing radio frequency (RF) based communication networks. Because of its capability to perform dual function of lighting as well as high speed communication, Li-Fi is attracting both industrial as well as academic researchers. The Li-Fi networking paradigm offers performance enhancements which can make it an attractive backup option to be used for networking setup in Internet of things (IOT) and its indoor capabilities make it an attractive choice for large scale indoor communication in next generation wireless networking environment. This paper discusses key potentials of Li-Fi based indoor communication system and point out the challenges which indoor based Li-Fi network is facing under the knowledge of existing research work in the field of Li-Fi."
"Even though, many researchers tried to explore the various possibilities on multi objective feature selection, still it is yet to be explored with best of its capabilities in data mining applications rather than going for developing new ones. In this paper, multi-objective evolutionary algorithm ENORA is used to select the features in a multi-class classification problem. The fusion of AnDE (averaged n-dependence estimators) with n=1, a variant of naive Bayes with efficient feature selection by ENORA is performed in order to obtain a fast hybrid classifier which can effectively learn from big data. This method aims at solving the problem of finding optimal feature subset from full data which at present still remains to be a difficult problem. The efficacy of the obtained classifier is extensively evaluated with a range of most popular 21 real world dataset, ranging from small to big. The results obtained are encouraging in terms of time, Root mean square error, zero-one loss and classification accuracy."
"With the fast growth of communication networks, the video data transmission from these networks is extremely vulnerable. Error concealment is a technique to estimate the damaged data by employing the correctly received data at the decoder. In this paper, an efficient boundary matching algorithm for estimating damaged motion vectors (MVs) is proposed. The proposed algorithm performs error concealment for each damaged macro block (MB) according to the list of identified priority of each frame. It then uses a classic boundary matching criterion or the proposed boundary matching criterion adaptively to identify matching distortion in each boundary of candidate MB. Finally, the candidate MV with minimum distortion is selected as an MV of damaged MB and the list of priorities is updated. Experimental results show that the proposed algorithm improves both objective and subjective qualities of reconstructed frames without any significant increase in computational cost. The PSNR for test sequences in some frames is increased about 4.7, 4.5, and 4.4 dB compared to the classic boundary matching, directional boundary matching, and directional temporal boundary matching algorithm, respectively."
"In a social network, even about the same information the excitements between different pairs of users are different. If you want to spread a piece of new information and maximize the expected total amount of excitements, which seed users should you choose? This problem indeed is substantially different from the renowned influence maximization problem and cannot be tackled using the existing approaches. In this paper, motivated by the demand in a few interesting applications, we model the novel problem of activity maximization. We tackle the problem systematically. We first analyze the complexity and the approximability of the problem. We develop an upper bound and a lower bound that are submodular so that the Sandwich framework can be applied. We then devise a polling-based randomized algorithm that guarantees a data dependent approximation factor. Our experiments on three real data sets clearly verify the effectiveness and scalability of our method, as well as the advantage of our method against the other heuristic methods."
"Cluster analysis has become one of the most exercised research areas over the past few decades in computer science. As a consequence, numerous clustering algorithms have already been developed to find appropriate partitions of a set of objects. Given multiple such clustering solutions, it is a challenging task to obtain an ensemble of these solutions. This becomes more challenging when the ground truth about the number of clusters is unavailable. In this paper, we introduce a crowd-powered model to collect solutions of image clustering from the general crowd and pose it as a clustering ensemble problem with variable number of clusters. The varying number of clusters basically reflects the crowd workers' perspective toward a particular set of objects. We allow a set of crowd workers to independently cluster the images as per their perceptions. We address the problem by finding out centroid of the clusters using an appropriate distance measure and prioritize the likelihood of similarity of the individual cluster sets. The effectiveness of the proposed method is demonstrated by applying it on multiple artificial datasets obtained from crowd."
"Strip packing is a classical packing problem, where the goal is to pack a set of rectangular objects into a strip of a given width, while minimizing the total height of the packing. The problem has multiple applications, e.g. in scheduling and stock-cutting, and has been studied extensively.   When the dimensions of objects are allowed to be exponential in the total input size, it is known that the problem cannot be approximated within a factor better than $3/2$, unless $\mathrm{P}=\mathrm{NP}$. However, there was no corresponding lower bound for polynomially bounded input data. In fact, Nadiradze and Wiese [SODA 2016] have recently proposed a $(1.4 + \epsilon)$ approximation algorithm for this variant, thus showing that strip packing with polynomially bounded data can be approximated better than when exponentially large values in the input data are allowed. Their result has subsequently been improved to a $(4/3 + \epsilon)$ approximation by two independent research groups [FSTTCS 2016, arXiv:1610.04430]. This raises a question whether strip packing with polynomially bounded input data admits a quasi-polynomial time approximation scheme, as is the case for related two-dimensional packing problems like maximum independent set of rectangles or two-dimensional knapsack.   In this paper we answer this question in negative by proving that it is NP-hard to approximate strip packing within a factor better than $12/11$, even when admitting only polynomially bounded input data. In particular, this shows that the strip packing problem admits no quasi-polynomial time approximation scheme, unless $\mathrm{NP} \subseteq \mathrm{DTIME}(2^{\mathrm{polylog}(n)})$."
"We study the online submodular maximization problem with free disposal under a matroid constraint. Elements from some ground set arrive one by one in rounds, and the algorithm maintains a feasible set that is independent in the underlying matroid. In each round when a new element arrives, the algorithm may accept the new element into its feasible set and possibly remove elements from it, provided that the resulting set is still independent. The goal is to maximize the value of the final feasible set under some monotone submodular function, to which the algorithm has oracle access.   For $k$-uniform matroids, we give a deterministic algorithm with competitive ratio at least $0.2959$, and the ratio approaches $\frac{1}{\alpha_\infty} \approx 0.3178$ as $k$ approaches infinity, improving the previous best ratio of $0.25$ by Chakrabarti and Kale (IPCO 2014), Buchbinder et al. (SODA 2015) and Chekuri et al. (ICALP 2015). We also show that our algorithm is optimal among a class of deterministic monotone algorithms that accept a new arriving element only if the objective is strictly increased.   Further, we prove that no deterministic monotone algorithm can be strictly better than $0.25$-competitive even for partition matroids, the most modest generalization of $k$-uniform matroids, matching the competitive ratio by Chakrabarti and Kale (IPCO 2014) and Chekuri et al. (ICALP 2015). Interestingly, we show that randomized algorithms are strictly more powerful by giving a (non-monotone) randomized algorithm for partition matroids with ratio $\frac{1}{\alpha_\infty} \approx 0.3178$."
"In this paper, we design space-time block codes (STBCs) to broadcast the common information omnidirectionally in a massive MIMO downlink. To reduce the burden of the downlink channel estimation and achieve partial spatial diversity from base station (BS) transmit antennas, we propose channel-independently precoded low-dimensional STBC. The precoding matrix and the signal constellation in the low-dimensional STBC are jointly designed to guarantee omnidirectional coverage at any instant time and sufficiently utilize the power amplifier capacities of BS transmit antennas, and at the same time, achieve the full diversity of the low-dimensional STBC. Under this framework, several designs are presented. To provide transmit diversity order of two, a precoded Alamouti code is proposed, which has a fast symbol-wise maximum-likelihood (ML) decoding. To provide transmit diversity order of four, three types of STBCs are proposed, being referred to as precoded orthogonal STBC (OSTBC), precoded quasi-orthogonal STBC (QOSTBC), and precoded coordinate interleaved orthogonal design (CIOD), respectively. The last two codes have the same complexity for pair-wise ML decoding, while precoded QOSTBC has a higher coding gain when the bit rate is lower than or equal to 4 bps/Hz, and precoded CIOD has a higher coding gain when the bit rate is higher than 4 bps/Hz. Precoded OSTBC has a higher decoding complexity and a lower coding gain than the other two codes, since in the precoded OSTBC the information symbols need to be jointly designed and decoded. Moreover, a precoded no-zero-entry Toeplitz code and a precoded no-zero-entry overlapped Alamouti code are also proposed. These two codes can achieve a higher diversity order with linear receivers."
"Online Social Networks explode with activity whenever a crisis event takes place. Most content generated as part of this activity is a mixture of text and images, and is particularly useful for first responders to identify popular topics of interest and gauge the pulse and sentiment of citizens. While multiple researchers have used text to identify, analyze and measure themes and public sentiment during such events, little work has explored visual themes floating on networks in the form of images, and the sentiment inspired by them. Given the potential of visual content for influencing users' thoughts and emotions, we perform a large scale analysis to compare popular themes and sentiment across images and textual content posted on Facebook during the terror attacks that took place in Paris in 2015. Using state-of-the-art image summarization techniques, we discovered multiple visual themes which were popular in images, but were not identifiable through text. We uncovered instances of misinformation and false flag (conspiracy) theories among popular image themes, which were not prominent in user generated textual content, and can be of particular inter- est to first responders. Our analysis also revealed that while textual content posted after the attacks reflected negative sentiment, images inspired positive sentiment. To the best of our knowledge, this is the first large scale study of images posted on social networks during a crisis event."
"We show that for a number of parameterized problems for which only $2^{O(k)} n^{O(1)}$ time algorithms are known on general graphs, subexponential parameterized algorithms with running time $2^{O(k^{1-\frac{1}{1+\delta}} \log^2 k)} n^{O(1)}$ are possible for graphs of polynomial growth with growth rate (degree) $\delta$, that is, if we assume that every ball of radius $r$ contains only $O(r^\delta)$ vertices. The algorithms use the technique of low-treewidth pattern covering, introduced by Fomin et al. [FOCS 2016] for planar graphs; here we show how this strategy can be made to work for graphs with polynomial growth.   Formally, we prove that, given a graph $G$ of polynomial growth with growth rate $\delta$ and an integer $k$, one can in randomized polynomial time find a subset $A \subseteq V(G)$ such that on one hand the treewidth of $G[A]$ is $O(k^{1-\frac{1}{1+\delta}} \log k)$, and on the other hand for every set $X \subseteq V(G)$ of size at most $k$, the probability that $X \subseteq A$ is $2^{-O(k^{1-\frac{1}{1+\delta}} \log^2 k)}$. Together with standard dynamic programming techniques on graphs of bounded treewidth, this statement gives subexponential parameterized algorithms for a number of subgraph search problems, such as Long Path or Steiner Tree, in graphs of polynomial growth.   We complement the algorithm with an almost tight lower bound for Long Path: unless the Exponential Time Hypothesis fails, no parameterized algorithm with running time $2^{k^{1-\frac{1}{\delta}-\varepsilon}}n^{O(1)}$ is possible for any $\varepsilon > 0$ and an integer $\delta \geq 3$."
"This paper considers a multiple-input multiple-output (MIMO) system with low-resolution analog-to-digital converters (ADCs). In this system, the paper presents a new MIMO detection approach using coding theory. The principal idea of the proposed approach is to transform a non-linear MIMO channel to a linear MIMO channel by leveraging both a $p$-level quantizer and a lattice code where $p\geq 2$. After transforming to the linear MIMO channel with the sets of finite input and output elements, efficient MIMO detection methods are proposed to attain both diversity and multiplexing gains by using algebraic coding theory. In particular, using the proposed methods, the analytical characterizations of achievable rates are derived for different MIMO configurations. One major observation is that the proposed approach is particularly useful for a large MIMO system with the ADCs that use a few bits."
"We analyze the performance of encoder-decoder neural models and compare them with well-known established methods. The latter represent different classes of traditional approaches that are applied to the monotone sequence-to-sequence tasks OCR post-correction, spelling correction, grapheme-to-phoneme conversion, and lemmatization. Such tasks are of practical relevance for various higher-level research fields including digital humanities, automatic text correction, and speech recognition. We investigate how well generic deep-learning approaches adapt to these tasks, and how they perform in comparison with established and more specialized methods, including our own adaptation of pruned CRFs."
"We extend the Frank-Wolfe (FW) optimization algorithm to solve constrained smooth convex-concave saddle point (SP) problems. Remarkably, the method only requires access to linear minimization oracles. Leveraging recent advances in FW optimization, we provide the first proof of convergence of a FW-type saddle point solver over polytopes, thereby partially answering a 30 year-old conjecture. We also survey other convergence results and highlight gaps in the theoretical underpinnings of FW-style algorithms. Motivating applications without known efficient alternatives are explored through structured prediction with combinatorial penalties as well as games over matching polytopes involving an exponential number of constraints."
"Fast binary descriptors build the core for many vision based applications with real-time demands like object detection, Visual Odometry or SLAM. Commonly it is assumed, that the acquired images and thus the patches extracted around keypoints originate from a perspective projection ignoring image distortion or completely different types of projections such as omnidirectional or fisheye. Usually the deviations from a perfect perspective projection are corrected by undistortion. Latter, however, introduces severe artifacts if the cameras field-of-view gets larger. In this paper, we propose a distorted and masked version of the BRIEF descriptor for calibrated cameras. Instead of correcting the distortion holistically, we distort the binary tests and thus adapt the descriptor to different image regions."
"The SemEval-2010 benchmark dataset has brought renewed attention to the task of automatic keyphrase extraction. This dataset is made up of scientific articles that were automatically converted from PDF format to plain text and thus require careful preprocessing so that irrevelant spans of text do not negatively affect keyphrase extraction performance. In previous work, a wide range of document preprocessing techniques were described but their impact on the overall performance of keyphrase extraction models is still unexplored. Here, we re-assess the performance of several keyphrase extraction models and measure their robustness against increasingly sophisticated levels of document preprocessing."
"Natural-language processing of historical documents is complicated by the abundance of variant spellings and lack of annotated data. A common approach is to normalize the spelling of historical words to modern forms. We explore the suitability of a deep neural network architecture for this task, particularly a deep bi-LSTM network applied on a character level. Our model compares well to previously established normalization algorithms when evaluated on a diverse set of texts from Early New High German. We show that multi-task learning with additional normalization data can improve our model's performance further."
"The hybrid clustering-classification neural network is proposed. This network allows increasing a quality of information processing under the condition of overlapping classes due to the rational choice of a learning rate parameter and introducing a special procedure of fuzzy reasoning in the clustering process, which occurs both with an external learning signal (supervised) and without the one (unsupervised). As similarity measure neighborhood function or membership one, cosine structures are used, which allow to provide a high flexibility due to self-learning-learning process and to provide some new useful properties. Many realized experiments have confirmed the efficiency of proposed hybrid clustering-classification neural network; also, this network was used for solving diagnostics task of reactive arthritis."
"We consider average-energy games, where the goal is to minimize the long-run average of the accumulated energy. Decidability of average-energy games with a lower-bound constraint on the energy level (but no upper bound) is an open problem; in particular, there is no known upper bound on the memory that is required for winning strategies.   By reducing average-energy games with lower-bounded energy to infinite-state mean-payoff games and analyzing the frequency of low-energy configurations, we show an almost tight doubly-exponential upper bound on the necessary memory, and that the winner of average-energy games with lower-bounded energy can be determined in doubly-exponential time. We also prove EXPSPACE-hardness of this problem.   Finally, we consider multi-dimensional extensions of all types of average-energy games: without bounds, with only a lower bound, and with both a lower and an upper bound on the energy. We show that the fully-bounded version is the only case to remain decidable in multiple dimensions."
"The elusive quest for intelligence in artificial intelligence prompts us to consider that instituting human-level intelligence in systems may be (still) in the realm of utopia. In about a quarter century, we have witnessed the winter of AI (1990) being transformed and transported to the zenith of tabloid fodder about AI (2015). The discussion at hand is about the elements that constitute the canonical idea of intelligence. The delivery of intelligence as a pay-per-use-service, popping out of an app or from a shrink-wrapped software defined point solution, is in contrast to the bio-inspired view of intelligence as an outcome, perhaps formed from a tapestry of events, cross-pollinated by instances, each with its own microcosm of experiences and learning, which may not be discrete all-or-none functions but continuous, over space and time. The enterprise world may not require, aspire or desire such an engaged solution to improve its services for enabling digital transformation through the deployment of digital twins, for example. One might ask whether the ""work-flow on steroids"" version of decision support may suffice for intelligence? Are we harking back to the era of rule based expert systems? The image conjured by the publicity machines offers deep solutions with human-level AI and preposterous claims about capturing the ""brain in a box"" by 2020. Even emulating insects may be difficult in terms of real progress. Perhaps we can try to focus on worms (Caenorhabditis elegans) which may be better suited for what business needs to quench its thirst for so-called intelligence in AI."
"Convolutional neural networks (CNN) are widely used in computer vision, especially in image classification. However, the way in which information and invariance properties are encoded through in deep CNN architectures is still an open question. In this paper, we propose to modify the standard convo- lutional block of CNN in order to transfer more information layer after layer while keeping some invariance within the net- work. Our main idea is to exploit both positive and negative high scores obtained in the convolution maps. This behav- ior is obtained by modifying the traditional activation func- tion step before pooling. We are doubling the maps with spe- cific activations functions, called MaxMin strategy, in order to achieve our pipeline. Extensive experiments on two classical datasets, MNIST and CIFAR-10, show that our deep MaxMin convolutional net outperforms standard CNN."
"This paper studies the problem of learning weighted automata from a finite labeled training sample. We consider several general families of weighted automata defined in terms of three different measures: the norm of an automaton's weights, the norm of the function computed by an automaton, or the norm of the corresponding Hankel matrix. We present new data-dependent generalization guarantees for learning weighted automata expressed in terms of the Rademacher complexity of these families. We further present upper bounds on these Rademacher complexities, which reveal key new data-dependent terms related to the complexity of learning weighted automata."
"In this technical report we summarise the spatio-temporal features and present the core operators of FocusST specification framework. We present the general idea of these operators, using a Steam Boiler System example to illustrate how the specification framework can be applied.   FocusST was inspired by Focus, a framework for formal specification and development of interactive systems. In contrast to Focus, FocusST is devoted to specify and to analyse spatial (S) and timing (T) aspects of the systems, which is also reflected in the name of the framework: the extension ST highlights the spatio-temporal nature of the specifications."
"Self-orthogonal $J$-affine variety codes have been successfully used to obtain quantum stabilizer codes with excellent parameters. In a previous paper we gave formulae for the dimension of this family of quantum codes, but no bound for the minimum distance was given. In this work, we show how to derive quantum stabilizer codes with designed minimum distance from $J$-affine variety codes and their subfield-subcodes. Moreover, this allows us to obtain new quantum codes, some of them either, with better parameters, or with larger distances than the previously known codes."
"The modelling and simulation of the interaction among vehicles and pedestrians during cross-walking is an open challenge for both research and practical computational solutions supporting urban/traffic decision makers and managers. The social cost of pedestrians' risky behaviour pushes the development of a new generation of computational models integrating analytical knowledge, data and experience about the complex dynamics occurring in pedestrian/vehicle interactions, which are not completely understood despite recent efforts. This paper presents the results of a significant data gathering campaign realised at an unsignalized zebra crossing. The selected area of the city of Milan (Italy) is characterised by a significant presence of elderly inhabitants and pedestrian-vehicle risky interactions, testified by a high number of accidents involving pedestrians in the past years. The results concern the analysis of: (i) vehicular and pedestrian traffic volumes; (ii) level of service; (iii) pedestrian-vehicle interactions, considering the impact of ageing on crossing behaviour. Results showed that the phenomenon is characterised by three main phases: approaching, appraising (evaluation of the distance and speed of oncoming vehicles) and crossing. The final objective of the research is to support the development of a microscopic agent-based tool for simulating pedestrian behaviour at unsignalized crosswalks, focusing on the specific needs of the elderly pedestrians."
"In these days embedded system have an important role in different Fields and applications like Network embedded system , Real-time embedded systems which supports the mission-critical domains, mostly having the time constraints, Stand-alone systems which includes the network router etc. A great deployment in the processors made for completing the demanding needs of the users. There is also a large-scale deployment occurs in sensor networks for providing the advance facilities, for handled such type of embedded systems a specific operating system must provide. This paper presents some software infrastructures that have the ability of supporting such types of embedded systems."
"Computer-based simulation of pedestrian dynamics reached meaningful results in the last decade, thanks to empirical evidences and acquired knowledge fitting fundamental diagram constraints and space utilization. Moreover, computational models for pedestrian wayfinding often neglect extensive empirical evidences supporting the calibration and validation phase of simulations. The paper presents the results of a set of controlled experiments (with human volunteers) designed and performed to understand pedestrian's route choice. The setting offers alternative paths to final destinations, at different crowding conditions. Results show that the length of paths and level of congestion influence decisions (negative feedback), as well as imitative behaviour of ""emergent leaders"" choosing a new path (positive feedback). A novel here illustrated model for the simulation of pedestrian route choice captures such evidences, encompassing both the tendency to avoid congestion and to follow emerging leaders. The found conflicting tendencies are modelled with the introduction of a utility function allowing a consistent calibration over the achieved results. A demonstration of the simulated dynamics on a larger scenario will be also illustrated in the paper."
"We design and develop a work-efficient multithreaded algorithm for sparse matrix-sparse vector multiplication (SpMSpV) where the matrix, the input vector, and the output vector are all sparse. SpMSpV is an important primitive in the emerging GraphBLAS standard and is the workhorse of many graph algorithms including breadth-first search, bipartite graph matching, and maximal independent set. As thread counts increase, existing multithreaded SpMSpV algorithms can spend more time accessing the sparse matrix data structure than doing arithmetic. Our shared-memory parallel SpMSpV algorithm is work efficient in the sense its total work is proportional to the number of arithmetic operations required. The key insight is to avoid each thread individually scan the list of matrix columns.   Our algorithm is simple to implement and operates on existing column-based sparse matrix formats. It performs well on diverse matrices and vectors with heterogeneous sparsity patterns. A high-performance implementation of the algorithm attains up to 15x speedup on a 24-core Intel Ivy Bridge processor and up to 49x speedup on a 64-core Intel KNL manycore processor. In contrast to implementations of existing algorithms, the performance of our algorithm is sustained on a variety of different input types include matrices representing scale-free and high-diameter graphs."
"We prove a result which strongly hints at the computational weakness of a model of tile assembly that has so far resisted many attempts of formal analysis or positive constructions. Specifically, we prove that, in Winfree's abstract Tile Assembly Model, when restricted to use only noncooperative bindings, any long enough path starting from the seed that can grow in all terminal assemblies is pumpable, meaning that this path can be extended into an infinite, ultimately periodic path. This result can be seen as a geometric generalization of the pumping lemma of finite state automata, and closes the question of what can be computed deterministically in this model. Moreover, this question has motivated the development of a new method called visible glues. We believe that this method can also be used to tackle other long-standing problems in computational geometry, in relation for instance with self-avoiding paths. Tile assembly (including non-cooperative tile assembly) was originally introduced by Winfree and Rothemund in STOC 2000 to understand how to program shapes. The non-cooperative variant, also known as temperature 1 tile assembly, is the model where tiles are allowed to bind as soon as they match on one side, whereas in cooperative tile assembly, some tiles need to match on several sides in order to bind. Previously, exactly one known result (SODA 2014) showed a restriction on the assemblies general non-cooperative self-assembly could achieve, without any implication on its computational expressiveness. With non-square tiles (like polyominos, SODA 2015), other recent works have shown that the model quickly becomes computationally powerful."
"NPATH is a metric introduced by Brian A. Nejmeh in [13] that is aimed at overcoming some important limitations of McCabe's cyclomatic complexity. Despite the fact that the declared NPATH objective is to count the number of acyclic execution paths through a function, the definition given for the C language in [13] fails to do so even for very simple programs. We show that counting the number of acyclic paths in CFG is unfeasible in general. Then we define a new metric for C-like languages, called ACPATH, that allows to quickly compute an very good estimation of the number of acyclic execution paths through the given function. We show that, if the function body does not contain backward gotos and does not contain jumps into a loop from outside the loop, then such estimation is actually exact."
"We describe and analyze a simple and effective algorithm for sequence segmentation applied to speech processing tasks. We propose a neural architecture that is composed of two modules trained jointly: a recurrent neural network (RNN) module and a structured prediction model. The RNN outputs are considered as feature functions to the structured model. The overall model is trained with a structured loss function which can be designed to the given segmentation task. We demonstrate the effectiveness of our method by applying it to two simple tasks commonly used in phonetic studies: word segmentation and voice onset time segmentation. Results sug- gest the proposed model is superior to previous methods, ob- taining state-of-the-art results on the tested datasets."
"The sizes of Markov equivalence classes of directed acyclic graphs play important roles in measuring the uncertainty and complexity in causal learning. A Markov equivalence class can be represented by an essential graph and its undirected subgraphs determine the size of the class. In this paper, we develop a method to derive the formulas for counting the sizes of Markov equivalence classes. We first introduce a new concept of core graph. The size of a Markov equivalence class of interest is a polynomial of the number of vertices given its core graph. Then, we discuss the recursive and explicit formula of the polynomial, and provide an algorithm to derive the size formula via symbolic computation for any given core graph. The proposed size formula derivation sheds light on the relationships between the size of a Markov equivalence class and its representation graph, and makes size counting efficient, even when the essential graphs contain non-sparse undirected subgraphs."
"In this paper, automated user verification techniques for smartphones are investigated. A unique non-commercial dataset, the University of Maryland Active Authentication Dataset 02 (UMDAA-02) for multi-modal user authentication research is introduced. This paper focuses on three sensors - front camera, touch sensor and location service while providing a general description for other modalities. Benchmark results for face detection, face verification, touch-based user identification and location-based next-place prediction are presented, which indicate that more robust methods fine-tuned to the mobile platform are needed to achieve satisfactory verification accuracy. The dataset will be made available to the research community for promoting additional research."
"Functional endoscopic sinus surgery (FESS) is a surgical procedure used to treat acute cases of sinusitis and other sinus diseases. FESS is fast becoming the preferred choice of treatment due to its minimally invasive nature. However, due to the limited field of view of the endoscope, surgeons rely on navigation systems to guide them within the nasal cavity. State of the art navigation systems report registration accuracy of over 1mm, which is large compared to the size of the nasal airways. We present an anatomically constrained video-CT registration algorithm that incorporates multiple video features. Our algorithm is robust in the presence of outliers. We also test our algorithm on simulated and in-vivo data, and test its accuracy against degrading initializations."
"In this paper, a solution to the problem of Active Authentication using trace histories is addressed. Specifically, the task is to perform user verification on mobile devices using historical location traces of the user as a function of time. Considering the movement of a human as a Markovian motion, a modified Hidden Markov Model (HMM)-based solution is proposed. The proposed method, namely the Marginally Smoothed HMM (MSHMM), utilizes the marginal probabilities of location and timing information of the observations to smooth-out the emission probabilities while training. Hence, it can efficiently handle unforeseen observations during the test phase. The verification performance of this method is compared to a sequence matching (SM) method , a Markov Chain-based method (MC) and an HMM with basic Laplace Smoothing (HMM-lap). Experimental results using the location information of the UMD Active Authentication Dataset-02 (UMDAA02) and the GeoLife dataset are presented. The proposed MSHMM method outperforms the compared methods in terms of equal error rate (EER). Additionally, the effects of different parameters on the proposed method are discussed."
"While deep learning has become a key ingredient in the top performing methods for many computer vision tasks, it has failed so far to bring similar improvements to instance-level image retrieval. In this article, we argue that reasons for the underwhelming results of deep methods on image retrieval are threefold: i) noisy training data, ii) inappropriate deep architecture, and iii) suboptimal training procedure. We address all three issues.   First, we leverage a large-scale but noisy landmark dataset and develop an automatic cleaning method that produces a suitable training set for deep retrieval. Second, we build on the recent R-MAC descriptor, show that it can be interpreted as a deep and differentiable architecture, and present improvements to enhance it. Last, we train this network with a siamese architecture that combines three streams with a triplet loss. At the end of the training process, the proposed architecture produces a global image representation in a single forward pass that is well suited for image retrieval. Extensive experiments show that our approach significantly outperforms previous retrieval approaches, including state-of-the-art methods based on costly local descriptor indexing and spatial verification. On Oxford 5k, Paris 6k and Holidays, we respectively report 94.7, 96.6, and 94.8 mean average precision. Our representations can also be heavily compressed using product quantization with little loss in accuracy. For additional material, please see www.xrce.xerox.com/Deep-Image-Retrieval."
"The development of fluid-structure interaction (FSI) software involves trade-offs between ease of use, generality, performance, and cost. Typically there are large learning curves when using low-level software to model the interaction of an elastic structure immersed in a uniform density fluid. Many existing codes are not publicly available, and the commercial software that exists usually requires expensive licenses and may not be as robust or allow the necessary flexibility that in house codes can provide. We present an open source immersed boundary software package, IB2d, with full implementations in both MATLAB and Python, that is capable of running a vast range of biomechanics models and is accessible to scientists who have experience in high-level programming environments. IB2d contains multiple options for constructing material properties of the fiber structure, as well as the advection-diffusion of a chemical gradient, muscle mechanics models, and artificial forcing to drive boundaries with a preferred motion."
"In identity-based encryption (IBE) systems, an efficient key delegation method to manage a large number of users and an efficient key revocation method to handle the dynamic credentials of users are needed. Revocable hierarchical IBE (RHIBE) can provide these two methods by organizing the identities of users as a hierarchy and broadcasting an update key for non-revoked users per each time period. To provide the key revocation functionality, previous RHIBE schemes use a tree-based revocation scheme. However, this approach has an inherent limitation such that the number of update key elements depends on the number of revoked users.   In this paper, we propose two new RHIBE schemes in multilinear maps that use the public-key broadcast encryption scheme instead of using the tree-based revocation scheme to overcome the mentioned limitation. In our first RHIBE scheme, the number of private key elements and update key elements is reduced to $O(\ell)$ and $O(\ell)$ respectively where $\ell$ is the depth of a hierarchical identity. In our second RHIBE scheme, we can further reduce the number of private key elements from $O(\ell)$ to $O(1)$."
"Linearisability is a central notion for verifying concurrent libraries: a given library is proven safe if its operational history can be rearranged into a new sequential one which, in addition, satisfies a given specification. Linearisability has been examined for libraries in which method arguments and method results are of ground type, including libraries parameterised with such methods. In this paper we extend linearisability to the general higher-order setting: methods can be passed as arguments and returned as values. A library may also depend on abstract methods of any order. We use this generalised notion to show correctness of several higher-order example libraries."
"We establish quantitative stability results for the entropy power inequality (EPI). Specifically, we show that if uniformly log-concave densities nearly saturate the EPI, then they must be close to Gaussian densities in the quadratic Wasserstein distance. Further, if one of the densities is log-concave and the other is Gaussian, then the deficit in the EPI can be controlled in terms of the $L^1$-Wasserstein distance. As a counterpoint, an example shows that the EPI can be unstable with respect to the quadratic Wasserstein distance when densities are uniformly log-concave on sets of measure arbitrarily close to one. Our stability results can be extended to non-log-concave densities, provided certain regularity conditions are met. The proofs are based on optimal transportation."
"Haskell, as implemented in the Glasgow Haskell Compiler (GHC), has been adding new type-level programming features for some time. Many of these features---chiefly: generalized algebraic datatypes (GADTs), type families, kind polymorphism, and promoted datatypes---have brought Haskell to the doorstep of dependent types. Many dependently typed programs can even currently be encoded, but often the constructions are painful.   In this dissertation, I describe Dependent Haskell, which supports full dependent types via a backward-compatible extension to today's Haskell. An important contribution of this work is an implementation, in GHC, of a portion of Dependent Haskell, with the rest to follow. The features I have implemented are already released, in GHC 8.0. This dissertation contains several practical examples of Dependent Haskell code, a full description of the differences between Dependent Haskell and today's Haskell, a novel type-safe dependently typed lambda-calculus (called Pico) suitable for use as an intermediate language for compiling Dependent Haskell, and a type inference and elaboration algorithm, Bake, that translates Dependent Haskell to type-correct Pico."
"Cyber has changed the scope of the national security mission and is placing new strains on our diplomatic, warfighting, legal, and economic/budgetary processes. Cybersecurity processes and techniques are increasingly critical to our warfighting missions, but they can also inhibit the pace and potential for high impact, game-changing innovation. Throughout its history, the Navy has shown the ability of innovation (in policy, process, and technology) to change the game when our security is on the line. We believe the Navy is capable of dramatically impacting not only the U.S. capabilities in cyber conflict and information operations, but also in cyber defense and information assurance, as well as cybersecurity for our society. While cyber risk management is challenging, the transition from DoD Information Assurance Certification and Accreditation Process (DIACAP) to the Risk Management Framework (RMF) has the potential to harmonize our cybersecurity efforts with our need (and demonstrated ability to provide) for game-changing strategies, tactics, and technologies. We offer a foundation for the foregoing assertions and recommendations on ways to encourage innovation in the context of effective cyber risk management."
"We present and validate a novel mathematical model of the blockchain mining process and use it to conduct an economic evaluation of the double-spend attack, which is fundamental to all blockchain systems. Our analysis focuses on the value of transactions that can be secured under a conventional double-spend attack, both with and without a concurrent eclipse attack. Our model quantifies the importance of several factors that determine the attack's success, including confirmation depth, attacker mining power, and any confirmation deadline set by the merchant. In general, the security of a transaction against a double-spend attack increases roughly logarithmically with the depth of the block, made easier by the increasing sum of coin turned-over (between individuals) in the blocks, but more difficult by the increasing proof of work required. In recent blockchain data, we observed a median block turnover value of 6 BTC. Based on this value, a merchant requiring a single confirmation is protected against only attackers that can increase the current mining power by 1% or less. However, similar analysis shows that a merchant that requires a much longer 72 confirmations (~12 hours) will eliminate all potential profit for any double-spend attacker adding mining power less than 40% of the current mining power."
"This report presents a submission to the Process Discovery Contest. The contest is dedicated to the assessment of tools and techniques that discover business process models from event logs. The objective is to compare the efficiency of techniques to discover process models that provide a proper balance between ""overfitting"" and ""underfitting"". In the context of the Process Discovery Contest, process discovery is turned into a classification task with a training set and a test set; where a process model needs to decide whether traces are fitting or not. In this report, we first show how we use two discovery techniques, namely: Inductive Miner and Decomposition, to discover process models from the training set using ProM tool. Second, we show how we use replay results to 1) check the rediscoverability of models, and to 2) classify unseen traces (in test logs) as fitting or not. Then, we discuss the classification results of validation logs, the complexity of discovered models, and their impact on the selection of models for submission. The report ends with the pictures of the submitted process models."
"In this paper, the task of recognizing signs made by hearing impaired people at sentence level has been addressed. A novel method of extracting spatial features to capture hand movements of a signer has been proposed. Frames of a given video of a sign are preprocessed to extract face and hand components of a signer. The local centroids of the extracted components along with the global centroid are exploited to extract spatial features. The concept of interval valued type symbolic data has been explored to capture variations in the same sign made by the different signers at different instances of time. A suitable symbolic similarity measure is studied to establish matching between test and reference signs and a simple nearest neighbour classifier is used to recognize an unknown sign as one among the known signs by specifying a desired level of threshold. An extensive experimentation is conducted on a considerably large database of signs created by us during the course of research work in order to evaluate the performance of the proposed system"
"In this work, we present and analyze reported failures of artificially intelligent systems and extrapolate our analysis to future AIs. We suggest that both the frequency and the seriousness of future AI failures will steadily increase. AI Safety can be improved based on ideas developed by cybersecurity experts. For narrow AIs safety failures are at the same, moderate, level of criticality as in cybersecurity, however for general AI, failures have a fundamentally different impact. A single failure of a superintelligent system may cause a catastrophic event without a chance for recovery. The goal of cybersecurity is to reduce the number of successful attacks on the system; the goal of AI Safety is to make sure zero attacks succeed in bypassing the safety mechanisms. Unfortunately, such a level of performance is unachievable. Every security system will eventually fail; there is no such thing as a 100% secure system."
"We prove that the the mixing time of the Glauber dynamics for sampling independent sets on $n$-vertex $k$-uniform hypergraphs is $O(n\log n)$ when the maximum degree $\Delta$ satisfies $\Delta \leq c 2^{k/2}$, improving on the previous bound [BDK06] of $\Delta \leq k-2$. This result brings the algorithmic bound to within a constant factor of the hardness bound of [BGG+16] which showed that it is NP-hard to approximately count independent sets on hypergraphs when $\Delta \geq 5 \cdot 2^{k/2}$."
"This paper presents Centre for Development of Advanced Computing Mumbai's (CDACM) submission to NLP Tools Contest on Statistical Machine Translation in Indian Languages (ILSMT) 2015 (collocated with ICON 2015). The aim of the contest was to collectively explore the effectiveness of Statistical Machine Translation (SMT) while translating within Indian languages and between English and Indian languages. In this paper, we report our work on all five language pairs, namely Bengali-Hindi (\bnhi), Marathi-Hindi (\mrhi), Tamil-Hindi (\tahi), Telugu-Hindi (\tehi), and English-Hindi (\enhi) for Health, Tourism, and General domains. We have used suffix separation, compound splitting and preordering prior to SMT training and testing."
"It is known that there exists a multiple-unicast network which has a rate $1$ linear network coding solution if and only if the characteristic of the finite field belongs to a given finite or co-finite set of primes. In this paper, we show that for any non-zero positive rational number $\frac{k}{n}$, there exists a multiple-unicast network which has a rate $\frac{k}{n}$ fractional linear network coding solution if and only if the characteristic of the finite field belongs to a given finite or co-finite set of primes."
"Diamond Light Source (DLS), the UK synchrotron facility, attracts scientists from across the world to perform ground-breaking x-ray experiments. With over 3000 scientific users per year, vast amounts of data are collected across the experimental beamlines, with the highest volume of data collected during tomographic imaging experiments. A growing interest in tomography as an imaging technique, has led to an expansion in the range of experiments performed, in addition to a growth in the size of the data per experiment.   Savu is a portable, flexible, scientific processing pipeline capable of processing multiple, n-dimensional datasets in serial on a PC, or in parallel across a cluster. Developed at DLS, and successfully deployed across the beamlines, it uses a modular plugin format to enable experiment-specific processing and utilises parallel HDF5 to remove RAM restrictions. The Savu design, described throughout this paper, focuses on easy integration of existing and new functionality, flexibility and ease of use for users and developers alike."
"In this paper we introduce a novel way to speed up the discovery of counterexamples in bounded model checking, based on parallel runs over versions of a system in which features have been randomly disabled. As shown in previous work, adding constraints to a bounded model checking problem can reduce the size of the verification problem and dramatically decrease the time required to find counterexample. Adapting a technique developed in software testing to this problem provides a simple way to produce useful partial verification problems, with a resulting decrease in average time until a counterexample is produced. If no counterexample is found, partial verification results can also be useful in practice."
"In this paper, we revisit the problem of finding the longest systematic-length $k$ in a linear minimum storage regenerating (MSR) code, for a given storage capacity of each node $l$ and an arbitrary number of parity nodes $r$. We study it by following the geometric analysis of linear subspaces and operators. First, a simple quadratic bound is given, which implies that $k=r+2$ is the largest number of systematic nodes in the \emph{scalar} linear MSR scenario. Second, an $r$-based-log bound is derived, which is superior to the upper bound on log-base $2$ in the prior work. Finally, an explicit bound depending on the value of $\frac{r^2}{l}$ is introduced, which further extends the corresponding result in the literature."
"A multiset consists of elements, but the notion of a multiset is distinguished from that of a set by carrying information of how many times each element occurs in a given multiset. In this work we will investigate the notion of iterative multisets, where multisets are iteratively built up from other multisets, in the context Martin-L\""of Type Theory, in the presence of Voevodsky's Univalence Axiom.   Aczel 1978 introduced a model of constructive set theory in type theory, using a W-type quantifying over a universe, and an inductively defined equivalence relation on it. Our investigation takes this W-type and instead considers the identity type on it, which can be computed from the Univalence Axiom. Our thesis is that this gives a model of multisets. In order to demonstrate this, we adapt axioms of constructive set theory to multisets, and show that they hold for our model."
"An efficient algorithm for the construction of polar codes for higher-order modulation is presented based on information-theoretic principles. The bit reliabilities after successive demapping are estimated using the LM-rate, an achievable rate for mismatched decoding. The successive demapper bit channels are then replaced by binary input Additive White Gaussian Noise (biAWGN) surrogate channels and polar codes are constructed using the Gaussian approximation (GA). This LM-rate Demapper GA (LM-DGA) construction is used to construct polar codes for several demapping strategies proposed in literature. For all considered demappers, the LM-DGA constructed polar codes have the same performance as polar codes constructed by Monte Carlo (MC) simulation. The proposed LM-DGA construction is much faster than the MC construction. For 64-QAM, spectral efficiency 3 bits/s/Hz, and block length 1536 bits, simulation results show that LM-DGA constructed polar codes with cyclic redundancy check and successive cancellation list decoding are 1 dB more power efficient than state-of-the-art AR4JA low-density parity-check codes."
"We prove that low-rank matrices can be recovered efficiently from a small number of measurements that are sampled from orbits of a certain matrix group. As a special case, our theory makes statements about the phase retrieval problem. Here, the task is to recover a vector given only the amplitudes of its inner product with a small number of vectors from an orbit. Variants of the group in question have appeared under different names in many areas of mathematics. In coding theory and quantum information, it is the complex Clifford group; in time-frequency analysis the oscillator group; and in mathematical physics the metaplectic group. It affords one particularly small and highly structured orbit that includes and generalizes the discrete Fourier basis: While the Fourier vectors have coefficients of constant modulus and phases that depend linearly on their index, the vectors in said orbit have phases with a quadratic dependence. In quantum information, the orbit is used extensively and is known as the set of stabilizer states. We argue that due to their rich geometric structure and their near-optimal recovery properties, stabilizer states form an ideal model for structured measurements for phase retrieval. Our results hold for $m\geq C \kappa_r r d \log(d)$ measurements, where the oversampling factor k varies between $\kappa_r=1$ and $\kappa_r = r^2$ depending on the orbit. The reconstruction is stable towards both additive noise and deviations from the assumption of low rank. If the matrices of interest are in addition positive semidefinite, reconstruction may be performed by a simple constrained least squares regression. Our proof methods could be adapted to cover orbits of other groups."
"The performance of a multiuser communication system with single-antenna transmitting terminals and a multi-antenna base-station receiver is analytically investigated. The system operates under independent and non-identically distributed rank-$1$ Rician fading channels with imperfect channel estimation and residual hardware impairments (compensation algorithms are assumed, which mitigate the main impairments) at the transceiver. The spatial multiplexing mode of operation is considered where all the users are simultaneously transmitting their streams to the receiver. Zero-forcing is applied along with successive interference cancellation (SIC) as a means for efficient detection of the received streams. New analytical closed-form expressions are derived for some important performance metrics, namely, the outage probability and ergodic capacity of the entire system. Both the analytical expressions and simulation results show the impact of imperfect channel estimation and hardware impairments to the overall system performance in the usage scenarios of massive MIMO and mmWave communication systems."
"In this paper, the performance of an underlay multiple-input multiple-output (MIMO) cognitive radio system is analytically studied. In particular, the secondary transmitter operates in a spatial multiplexing transmission mode, while a zero-forcing (ZF) detector is employed at the secondary receiver. Additionally, the secondary system is interfered by multiple randomly distributed single-antenna primary users (PUs). To enhance the performance of secondary transmission, optimal power allocation is performed at the secondary transmitter with a constraint on the interference temperature (IT) specified by the PUs. The outage probability of the secondary receiver is explicitly derived in an exact closed-form expression. Also, some special cases of practical interest, including co-located PUs and massive MIMO, are discussed. Further, to mitigate instantaneous excessive interference onto PUs caused by the time-average IT, an iterative antenna reduction algorithm is developed for the secondary transmitter and, accordingly, the average number of transmit antennas is analytically computed. Extensive numerical and simulation results corroborate the effectiveness of our analysis."
"Predictive modeling is increasingly being employed to assist human decision-makers. One purported advantage of replacing human judgment with computer models in high stakes settings-- such as sentencing, hiring, policing, college admissions, and parole decisions-- is the perceived ""neutrality"" of computers. It is argued that because computer models do not hold personal prejudice, the predictions they produce will be equally free from prejudice. There is growing recognition that employing algorithms does not remove the potential for bias, and can even amplify it, since training data were inevitably generated by a process that is itself biased. In this paper, we provide a probabilistic definition of algorithmic bias. We propose a method to remove bias from predictive models by removing all information regarding protected variables from the permitted training data. Unlike previous work in this area, our framework is general enough to accommodate arbitrary data types, e.g. binary, continuous, etc. Motivated by models currently in use in the criminal justice system that inform decisions on pre-trial release and paroling, we apply our proposed method to a dataset on the criminal histories of individuals at the time of sentencing to produce ""race-neutral"" predictions of re-arrest. In the process, we demonstrate that the most common approach to creating ""race-neutral"" models-- omitting race as a covariate-- still results in racially disparate predictions. We then demonstrate that the application of our proposed method to these data removes racial disparities from predictions with minimal impact on predictive accuracy."
"Vector representation of sentences is important for many text processing tasks that involve clustering, classifying, or ranking sentences. Recently, distributed representation of sentences learned by neural models from unlabeled data has been shown to outperform the traditional bag-of-words representation. However, most of these learning methods consider only the content of a sentence and disregard the relations among sentences in a discourse by and large.   In this paper, we propose a series of novel models for learning latent representations of sentences (Sen2Vec) that consider the content of a sentence as well as inter-sentence relations. We first represent the inter-sentence relations with a language network and then use the network to induce contextual information into the content-based Sen2Vec models. Two different approaches are introduced to exploit the information in the network. Our first approach retrofits (already trained) Sen2Vec vectors with respect to the network in two different ways: (1) using the adjacency relations of a node, and (2) using a stochastic sampling method which is more flexible in sampling neighbors of a node. The second approach uses a regularizer to encode the information in the network into the existing Sen2Vec model. Experimental results show that our proposed models outperform existing methods in three fundamental information system tasks demonstrating the effectiveness of our approach. The models leverage the computational power of multi-core CPUs to achieve fine-grained computational efficiency. We make our code publicly available upon acceptance."
"This work presents a parametrized family of divergences, namely Alpha-Beta Log- Determinant (Log-Det) divergences, between positive definite unitized trace class operators on a Hilbert space. This is a generalization of the Alpha-Beta Log-Determinant divergences between symmetric, positive definite matrices to the infinite-dimensional setting. The family of Alpha-Beta Log-Det divergences is highly general and contains many divergences as special cases, including the recently formulated infinite dimensional affine-invariant Riemannian distance and the infinite-dimensional Alpha Log-Det divergences between positive definite unitized trace class operators. In particular, it includes a parametrized family of metrics between positive definite trace class operators, with the affine-invariant Riemannian distance and the square root of the symmetric Stein divergence being special cases. For the Alpha-Beta Log-Det divergences between covariance operators on a Reproducing Kernel Hilbert Space (RKHS), we obtain closed form formulas via the corresponding Gram matrices."
"Product review websites provide an incredible lens into the wide variety of opinions and experiences of different people, and play a critical role in helping users discover products that match their personal needs and preferences. To help address questions that can't easily be answered by reading others' reviews, some review websites also allow users to pose questions to the community via a question-answering (QA) system. As one would expect, just as opinions diverge among different reviewers, answers to such questions may also be subjective, opinionated, and divergent. This means that answering such questions automatically is quite different from traditional QA tasks, where it is assumed that a single `correct' answer is available. While recent work introduced the idea of question-answering using product reviews, it did not account for two aspects that we consider in this paper: (1) Questions have multiple, often divergent, answers, and this full spectrum of answers should somehow be used to train the system; and (2) What makes a `good' answer depends on the asker and the answerer, and these factors should be incorporated in order for the system to be more personalized. Here we build a new QA dataset with 800 thousand questions---and over 3.1 million answers---and show that explicitly accounting for personalization and ambiguity leads both to quantitatively better answers, but also a more nuanced view of the range of supporting, but subjective, opinions."
"Maximum coverage and minimum set cover problems --collectively called coverage problems-- have been studied extensively in streaming models. However, previous research not only achieve sub-optimal approximation factors and space complexities, but also study a restricted set arrival model which makes an explicit or implicit assumption on oracle access to the sets, ignoring the complexity of reading and storing the whole set at once. In this paper, we address the above shortcomings, and present algorithms with improved approximation factor and improved space complexity, and prove that our results are almost tight. Moreover, unlike most of previous work, our results hold on a more general edge arrival model. More specifically, we present (almost) optimal approximation algorithms for maximum coverage and minimum set cover problems in the streaming model with an (almost) optimal space complexity of $\tilde{O}(n)$, i.e., the space is {\em independent of the size of the sets or the size of the ground set of elements}. These results not only improve over the best known algorithms for the set arrival model, but also are the first such algorithms for the more powerful {\em edge arrival} model. In order to achieve the above results, we introduce a new general sketching technique for coverage functions: This sketching scheme can be applied to convert an $\alpha$-approximation algorithm for a coverage problem to a $(1-\eps)\alpha$-approximation algorithm for the same problem in streaming, or RAM models. We show the significance of our sketching technique by ruling out the possibility of solving coverage problems via accessing (as a black box) a $(1 \pm \eps)$-approximate oracle (e.g., a sketch function) that estimates the coverage function on any subfamily of the sets."
"Pok\'emon Go has received unprecedented media coverage for a location-based game that uses augmented reality techniques. The game has been commonly associated with greater access to public spaces, increasing the number of people out on the streets, and generally improving health, social, and security indices. However, the true impact of Pok\'emon Go on people's mobility patterns in a city is still largely unknown. In this paper we perform a natural experiment using data from mobile networks to evaluate the effect of Pok\'emon Go on the pulse of a big city: Santiago of Chile. We found a significant effect of Pok\'emon Go on the floating population of Santiago: up to 13.8\% more people being outside at certain times, even if they do not seem to go out of their usual way. These effects at specific times were found by performing several regressions using count models over snapshots of the cell phone network. The effect is significant after controlling for land use, daily patterns, and points of interest in the city. Particularly, we found that, in business days, there is more people on the street at commuting times, meaning that people did not change their daily routines but slightly adapted them to play the game. Conversely, on Saturday and Sunday night, people indeed went out to play to places nearby their homes. Even if the statistical effects of the game do not reflect the massive reach portrayed by the media, it still allowed the streets to become a new place for people to spend time in. This is important, because results like these are expected to inform long-term infrastructure investments by city officials, jointly with public policies aimed at, for example, stimulating pedestrian traffic or suggesting alternative routes. Our work supports the notion that location-based games like Pok\'emon Go have benefits for the life in the city."
"In this paper, we extend the notion of gapped strings to elastic-degenerate strings. An elastic-degenerate string can been seen as an ordered collection of k > 1 seeds (substrings/subpatterns) interleaved by elastic-degenerate symbols such that each elastic-degenerate symbol corresponds to a set of two or more variable length strings. Here, we present an algorithm for solving the pattern matching problem with (solid) pattern and elastic-degenerate text, running in O(N+{\alpha}{\gamma}nm) time; where m is the length of the given pattern; n and N are the length and total size of the given elastic-degenerate text, respectively; {\alpha} and {\gamma} are small constants, respectively representing the maximum number of strings in any elastic-degenerate symbol of the text and the largest number of elastic-degenerate symbols spanned by any occurrence of the pattern in the text. The space used by the algorithm is linear in the size of the input for a constant number of elastic-degenerate symbols in the text; {\alpha} and {\gamma} are so small in real applications that the algorithm is expected to work very efficiently in practice."
"This paper addresses a Gaussian interference channel with two transmitter-receiver~(Tx-Rx) pairs under stochastic data arrival~(GIC-SDA). Information bits arrive at the transmitters according to independent and asynchronous Bernoulli processes~(Tx-Tx~asynchrony). Each information source turns off after generating a given total number of bits. The transmissions are \textit{asynchronous} (Tx-Rx~asynchrony) in the sense that each Tx sends a codeword to its Rx immediately after there are enough bits available in its buffer. Such asynchronous style of transmission is shown to significantly reduce the transmission delay in comparison with the existing Tx-Rx synchronous transmission schemes. The receivers learn the activity frames of both transmitters by employing sequential joint-typicality detection. As a consequence, the GIC-SDA under Tx-Rx asynchrony is represented by a standard GIC with state known at the receivers. The cardinality of the state space is $\binom{2N_1+2N_2}{2N_2}$ in which $N_1, N_2$ are the numbers of transmitted codewords by the two transmitters. Each realization of the state imposes two sets of constraints on $N_1, N_2$ referred to as the geometric and reliability constraints. In a scenario where the transmitters are only aware of the statistics of Tx-Tx~asynchrony, it is shown how one designs $N_1,N_2$ to achieve target transmission rates for both users and minimize the probability of unsuccessful decoding."
"Management of chronic diseases such as heart failure, diabetes, and chronic obstructive pulmonary disease (COPD) is a major problem in health care. A standard approach that the medical community has devised to manage widely prevalent chronic diseases such as chronic heart failure (CHF) is to have a committee of experts develop guidelines that all physicians should follow. These guidelines typically consist of a series of complex rules that make recommendations based on a patient's information. Due to their complexity, often the guidelines are either ignored or not complied with at all, which can result in poor medical practices. It is not even clear whether it is humanly possible to follow these guidelines due to their length and complexity. In the case of CHF management, the guidelines run nearly 80 pages. In this paper we describe a physician-advisory system for CHF management that codes the entire set of clinical practice guidelines for CHF using answer set programming. Our approach is based on developing reasoning templates (that we call knowledge patterns) and using these patterns to systemically code the clinical guidelines for CHF as ASP rules. Use of the knowledge patterns greatly facilitates the development of our system. Given a patient's medical information, our system generates a recommendation for treatment just as a human physician would, using the guidelines. Our system will work even in the presence of incomplete information. Our work makes two contributions: (i) it shows that highly complex guidelines can be successfully coded as ASP rules, and (ii) it develops a series of knowledge patterns that facilitate the coding of knowledge expressed in a natural language and that can be used for other application domains. This paper is under consideration for acceptance in TPLP."
"The complexity of large-scale distributed systems, particularly when deployed in physical space, calls for new mechanisms to address composability and reusability of collective adaptive behaviour. Computational fields have been proposed as an effective abstraction to fill the gap between the macro-level of such systems (specifying a system's collective behaviour) and the micro-level (individual devices' actions of computation and interaction to implement that collective specification), thereby providing a basis to better facilitate the engineering of collective APIs and complex systems at higher levels of abstraction. This paper proposes a full formal foundation for field computations, in terms of a core (higher-order) calculus of computational fields containing a few key syntactic constructs, and equipped with typing, denotational and operational semantics. Critically, this allows formal establishment of a link between the micro- and macro-levels of collective adaptive systems, by a result of full abstraction and adequacy for the (aggregate) denotational semantics with respect to the (per-device) operational semantics."
"We present a novel, non-standard recommender system for large-scale security policy management(SPM). Our system Helios discovers and recommends unknown and unseen anomalies in large-scale access logs with minimal supervision and no starting information on users and items. Typical recommender systems assume availability of user- and item-related information, but such information is not usually available in access logs. To resolve this problem, we first use discrete categorical labels to construct categorical combinations from access logs in a bootstrapping manner. Then, we utilize rank statistics of entity rank and order categorical combinations for recommendation. From a double-sided cold start, with minimal supervision, Helios learns to recommend most salient anomalies at large-scale, and provides visualizations to security experts to explain rationale behind the recommendations. Our experiments show Helios to be suitable for large-scale applications: from cold starts, in less than 60 minutes, Helios can analyze roughly 4.6 billion records in logs of 400GB with about 300 million potential categorical combinations, then generate ranked categorical combinations as recommended discoveries. We also show that, even with limited computing resources, Helios accelerates unknown and unseen anomaly discovery process for SPM by 1 to 3 orders of magnitude, depending on use cases. In addition, Helios' design is flexible with metrics and measurement fields used for discoveries and recommendations. Overall, our system leads to more efficient and customizable SPM processes with faster discoveries of unseen and unknown anomalies."
"Describable visual facial attributes are now commonplace in human biometrics and affective computing, with existing algorithms even reaching a sufficient point of maturity for placement into commercial products. These algorithms model objective facets of facial appearance, such as hair and eye color, expression, and aspects of the geometry of the face. A natural extension, which has not been studied to any great extent thus far, is the ability to model subjective attributes that are assigned to a face based purely on visual judgements. For instance, with just a glance, our first impression of a face may lead us to believe that a person is smart, worthy of our trust, and perhaps even our admiration - regardless of the underlying truth behind such attributes. Psychologists believe that these judgements are based on a variety of factors such as emotional states, personality traits, and other physiognomic cues. But work in this direction leads to an interesting question: how do we create models for problems where there is no ground truth, only measurable behavior? In this paper, we introduce a new convolutional neural network-based regression framework that allows us to train predictive models of crowd behavior for social attribute assignment. Over images from the AFLW face database, these models demonstrate strong correlations with human crowd ratings."
"Ground vehicles equipped with monocular vision systems are a valuable source of high resolution image data for precision agriculture applications in orchards. This paper presents an image processing framework for fruit detection and counting using orchard image data. A general purpose image segmentation approach is used, including two feature learning algorithms; multi-scale Multi-Layered Perceptrons (MLP) and Convolutional Neural Networks (CNN). These networks were extended by including contextual information about how the image data was captured (metadata), which correlates with some of the appearance variations and/or class distributions observed in the data. The pixel-wise fruit segmentation output is processed using the Watershed Segmentation (WS) and Circular Hough Transform (CHT) algorithms to detect and count individual fruits. Experiments were conducted in a commercial apple orchard near Melbourne, Australia. The results show an improvement in fruit segmentation performance with the inclusion of metadata on the previously benchmarked MLP network. We extend this work with CNNs, bringing agrovision closer to the state-of-the-art in computer vision, where although metadata had negligible influence, the best pixel-wise F1-score of $0.791$ was achieved. The WS algorithm produced the best apple detection and counting results, with a detection F1-score of $0.858$. As a final step, image fruit counts were accumulated over multiple rows at the orchard and compared against the post-harvest fruit counts that were obtained from a grading and counting machine. The count estimates using CNN and WS resulted in the best performance for this dataset, with a squared correlation coefficient of $r^2=0.826$."
"We present a fast variational Bayesian algorithm for performing non-negative matrix factorisation and tri-factorisation. We show that our approach achieves faster convergence per iteration and timestep (wall-clock) than Gibbs sampling and non-probabilistic approaches, and do not require additional samples to estimate the posterior. We show that in particular for matrix tri-factorisation convergence is difficult, but our variational Bayesian approach offers a fast solution, allowing the tri-factorisation approach to be used more effectively."
"Ordering vertices of a graph is key to minimize fill-in and data structure size in sparse direct solvers, maximize locality in iterative solvers, and improve performance in graph algorithms. Except for naturally parallelizable ordering methods such as nested dissection, many important ordering methods have not been efficiently mapped to distributed-memory architectures. In this paper, we present the first-ever distributed-memory implementation of the reverse Cuthill-McKee (RCM) algorithm for reducing the profile of a sparse matrix. Our parallelization uses a two-dimensional sparse matrix decomposition. We achieve high performance by decomposing the problem into a small number of primitives and utilizing optimized implementations of these primitives. Our implementation shows strong scaling up to 1024 cores for smaller matrices and up to 4096 cores for larger matrices."
"Web application performance is heavily reliant on the hit rate of memory-based caches. Current DRAM-based web caches statically partition their memory across multiple applications sharing the cache. This causes under utilization of memory which negatively impacts cache hit rates. We present Memshare, a novel web memory cache that dynamically manages memory across applications. Memshare provides a resource sharing model that guarantees private memory to different applications while dynamically allocating the remaining shared memory to optimize overall hit rate. Today's high cost of DRAM storage and the availability of high performance CPU and memory bandwidth, make web caches memory capacity bound. Memshare's log-structured design allows it to provide significantly higher hit rates and dynamically partition memory among applications at the expense of increased CPU and memory bandwidth consumption. In addition, Memshare allows applications to use their own eviction policy for their objects, independent of other applications. We implemented Memshare and ran it on a week-long trace from a commercial memcached provider. We demonstrate that Memshare increases the combined hit rate of the applications in the trace by an 6.1% (from 84.7% hit rate to 90.8% hit rate) and reduces the total number of misses by 39.7% without affecting system throughput or latency. Even for single-tenant applications, Memshare increases the average hit rate of the current state-of-the-art memory cache by an additional 2.7% on our real-world trace."
"In this paper, a new online method based on nonparametric weighted feature extraction (NWFE) is proposed. NWFE was introduced to enjoy optimum characteristics of linear discriminant analysis (LDA) and nonparametric discriminant analysis (NDA) while rectifying their drawbacks. It emphasizes the points near decision boundary by putting greater weights on them and deemphasizes other points. Incremental nonparametric weighted feature extraction (INWFE) is the online version of NWFE. INWFE has advantages of NWFE method such as extracting more than L-1 features in contrast to LDA. It is independent of the class distribution and performs well in complex distributed data. The effects of outliers are reduced due to the nature of its nonparametric scatter matrix. Furthermore, it is possible to add new samples asynchronously, i.e. whenever a new sample becomes available at any given time, it can be added to the algorithm. This is useful for many real world applications since all data cannot be available in advance. This method is implemented on Gaussian and non-Gaussian multidimensional data, a number of UCI datasets and Indian Pine dataset. Results are compared with NWFE in terms of classification accuracy and execution time. For nearest neighbour classifier it shows that this technique converges to NWFE at the end of learning process. In addition, the computational complexity is reduced in comparison with NWFE in terms of execution time."
"Models such as latent semantic analysis and those based on neural embeddings learn distributed representations of text, and match the query against the document in the latent semantic space. In traditional information retrieval models, on the other hand, terms have discrete or local representations, and the relevance of a document is determined by the exact matches of query terms in the body text. We hypothesize that matching with distributed representations complements matching with traditional local representations, and that a combination of the two is favorable. We propose a novel document ranking model composed of two separate deep neural networks, one that matches the query and the document using a local representation, and another that matches the query and the document using learned distributed representations. The two networks are jointly trained as part of a single neural network. We show that this combination or `duet' performs significantly better than either neural network individually on a Web page ranking task, and also significantly outperforms traditional baselines and other recently proposed models based on neural networks."
"Explosive demand for wireless internet services has posed critical challenges for wireless network due to its limited capacity. To tackle this hurdle, wireless Internet service providers (WISPs) take the smart data pricing to manage data traffic loads. Meanwhile, from the users' perspective, it is also reasonable and desired to employ mobile data traffic management under the pricing policies of WISPs to improve the economic efficiency of data consumption. In this paper we introduce a concept of cost efficiency for user's mobile data management, defined as the ratio of user's mobile data consumption benefits and its expense. We propose an integrated cost-efficiency-based data traffic management scheme including long-term data demand planning, short-term data traffic pre-scheduling and real-time data traffic management. The real-time data traffic management algorithm is proposed to coordinate user's data consumption to tailor to the pre-scheduled data traffic profile. Numerical results demonstrate the effectiveness of cost efficiency framework in indicating and motivating mobile user's data consumption behavior. The proposed management scheme can effectively motivate the user to adjust its data consumption profile to obtain the optimal data consumption cost efficiency."
"We consider fundamental scheduling problems motivated by energy issues. In this framework, we are given a set of jobs, each with a release time, deadline and required processing length. The jobs need to be scheduled on a machine so that at most g jobs are active at any given time. The duration for which a machine is active (i.e., ""on"") is referred to as its active time. The goal is to find a feasible schedule for all jobs, minimizing the total active time. When preemption is allowed at integer time points, we show that a minimal feasible schedule already yields a 3-approximation (and this bound is tight) and we further improve this to a 2-approximation via LP rounding techniques. Our second contribution is for the non-preemptive version of this problem. However, since even asking if a feasible schedule on one machine exists is NP-hard, we allow for an unbounded number of virtual machines, each having capacity of g. This problem is known as the busy time problem in the literature and a 4-approximation is known for this problem. We develop a new combinatorial algorithm that gives a 3-approximation. Furthermore, we consider the preemptive busy time problem, giving a simple and exact greedy algorithm when unbounded parallelism is allowed, i.e., g is unbounded. For arbitrary g, this yields an algorithm that is 2-approximate."
"A key barrier to making phonetic studies scalable and replicable is the need to rely on subjective, manual annotation. To help meet this challenge, a machine learning algorithm was developed for automatic measurement of a widely used phonetic measure: vowel duration. Manually-annotated data were used to train a model that takes as input an arbitrary length segment of the acoustic signal containing a single vowel that is preceded and followed by consonants and outputs the duration of the vowel. The model is based on the structured prediction framework. The input signal and a hypothesized set of a vowel's onset and offset are mapped to an abstract vector space by a set of acoustic feature functions. The learning algorithm is trained in this space to minimize the difference in expectations between predicted and manually-measured vowel durations. The trained model can then automatically estimate vowel durations without phonetic or orthographic transcription. Results comparing the model to three sets of manually annotated data suggest it out-performed the current gold standard for duration measurement, an HMM-based forced aligner (which requires orthographic or phonetic transcription as an input)."
"We present an improved method for a sound probabilistic estimation of the model count of a boolean formula under projection. The problem solved can be used to encode a variety of quantitative program analyses, such as concerning security of resource consumption. We implement the technique and discuss its application to quantifying information flow in programs."
"In this paper we focus on spatial Markov population models, describing the stochastic evolution of populations of agents, explicitly modelling their spatial distribution, representing space as a discrete, finite graph. More specifically, we present a heuristic approach to aggregating spatial locations, which is designed to preserve the dynamical behaviour of the model whilst reducing the computational cost of analysis. Our approach combines stochastic approximation ideas (moment closure, linear noise), with computational statistics (spectral clustering) to obtain an efficient aggregation, which is experimentally shown to be reasonably accurate on two case studies: an instance of epidemic spreading and a London bike sharing scenario."
"Bisimulation metrics provide a robust and accurate approach to study the behavior of nondeterministic probabilistic processes. In this paper, we propose a logical characterization of bisimulation metrics based on a simple probabilistic variant of the Hennessy-Milner logic. Our approach is based on the novel notions of mimicking formulae and distance between formulae. The former are a weak version of the well known characteristic formulae and allow us to characterize also (ready) probabilistic simulation and probabilistic bisimilarity. The latter is a 1-bounded pseudometric on formulae that mirrors the Hausdorff and Kantorovich lifting the defining bisimilarity pseudometric. We show that the distance between two processes equals the distance between their own mimicking formulae."
"Dataflow networks have application in various forms of stream processing, for example for parallel processing of multimedia data. The description of dataflow graphs, including their firing behavior, is typically non-compositional and not amenable to separate compilation. This article considers a dataflow language with a type and effect system that captures the firing behavior of actors. This system allows definitions to abstract over actor firing rates, supporting the definition and safe composition of actor definitions where firing rates are not instantiated until a dataflow graph is launched."
"Ecology studies the interactions between individuals, species and the environment. The ability to predict the dynamics of ecological systems would support the design and monitoring of control strategies and would help to address pressing global environmental issues. It is also important to plan for efficient use of natural resources and maintenance of critical ecosystem services. The mathematical modelling of ecological systems often includes nontrivial specifications of processes that influence the birth, death, development and movement of individuals in the environment, that take into account both biotic and abiotic interactions. To assist in the specification of such models, we introduce MELA, a process algebra for Modelling in Ecology with Location Attributes. Process algebras allow the modeller to describe concurrent systems in a high-level language. A key feature of concurrent systems is that they are composed of agents that can progress simultaneously but also interact - a good match to ecological systems. MELA aims to provide ecologists with a straightforward yet flexible tool for modelling ecological systems, with particular emphasis on the description of space and the environment. Here we present four example MELA models, illustrating the different spatial arrangements which can be accommodated and demonstrating the use of MELA in epidemiological and predator-prey scenarios."
"Nowadays, more and more increasingly hard computations are performed in challenging fields like weather forecasting, oil and gas exploration, and cryptanalysis. Many of such computations can be implemented using a computer cluster with a large number of servers. Incoming computation requests are then, via a so-called load balancing policy, distributed over the servers to ensure optimal performance. Additionally, being able to switch-off some servers during low period of workload, gives potential to reduced energy consumption. Therefore, load balancing forms, albeit indirectly, a trade-off between performance and energy consumption. In this paper, we introduce a syntax for load-balancing policies to dynamically select a server for each request based on relevant criteria, including the number of jobs queued in servers, power states of servers, and transition delays between power states of servers. To evaluate many policies, we implement two load balancers in: (i) iDSL, a language and tool-chain for evaluating service-oriented systems, and (ii) a simulation framework in AnyLogic. Both implementations are successfully validated by comparison of the results."
"Transfer entropy has been used to quantify the directed flow of information between source and target variables in many complex systems. Originally formulated in discrete time, we provide a framework for considering transfer entropy in continuous time systems. By appealing to a measure theoretic formulation we generalise transfer entropy, describing it in terms of Radon-Nikodym derivatives between measures of complete path realisations. The resulting formalism introduces and emphasises the idea that transfer entropy is an expectation of an individually fluctuating quantity along a path, in the same way we consider the expectation of physical quantities such as work and heat. We recognise that transfer entropy is a quantity accumulated over a finite time interval, whilst permitting an associated instantaneous transfer entropy rate. We use this approach to produce an explicit form for the transfer entropy for pure jump processes, and highlight the simplified form in the specific case of point processes (frequently used in neuroscience to model neural spike trains). We contrast our approach with previous attempts to formulate information flow between continuous time point processes within a discrete time framework, which incur issues that our continuous time approach naturally avoids. Finally, we present two synthetic spiking neuron model examples to exhibit the pertinent features of our formalism, namely that the information flow for point processes consists of discontinuous jump contributions (at spikes in the target) interrupting a continuously varying contribution (relating to waiting times between target spikes)."
"With the recent emergence of 5G era, heterogeneous cellular networks (HCNs) have invoked a popular research interest. In this paper, we provide a comprehensive analysis for multiantenna transmissions in a multi-tier downlink HCN. We first propose a reliability-oriented threshold-based mobile association policy, where each user connects to the strongest base station from which this user can obtain the largest truncated long-term received power. Under our mobile association policy, we derive analytical expressions for the exact outage probability of an arbitrary randomly located user, along with computationally convenient lower and upper bounds. Asymptotic analysis on the outage probability shows that introducing a large access threshold into mobile association significantly decreases the outage probability. We further investigate the spectrum efficiency and the energy efficiency of the HCN. Our theoretic analysis and numerical validations show that both the spectrum and energy efficiencies can be improved by properly choosing the access threshold."
"In this paper we describe our experience of using Microsoft Azure cloud computing platform for static analysis. We start by extending Static Driver Verifier to operate in the Microsoft Azure cloud with significant improvements in performance and scalability. We present our results of using SDV on single drivers and driver suites using various configurations of the cloud relative to a local machine. Finally, we describe the Static Module Verifier platform, a highly extensible and configurable platform for static analysis of generic modules, where we have integrated support for verification using a cloud services provider (Microsoft Azure in this case)."
"The use of cloud technology can offer significant savings for the deployment of services, provided that the service is able to make efficient use of the available virtual resources to meet service-level requirements. To avoid software designs that scale poorly, it is important to make deployment decisions for the service at design time, early in the development of the service itself. ABS offers a formal, model-based approach which integrates the design of services with the modeling of deployment decisions. In this paper, we illustrate the main concepts of this approach by modeling a scalable pool of workers with an auto-scaling strategy and by using the model to compare deployment decisions with respect to client traffic with peak loads."
"Technologies for composition of loosely-coupled web services in a modular and flexible way are in high demand today. On the one hand, the services must be flexible enough to be reused in a variety of contexts. On the other hand, they must be specific enough so that their composition may be provably consistent. The existing technologies (WS-CDL, WSCI and session types) require a behavioural contract associated with each service, which is impossible to derive automatically. Furthermore, neither technology supports flow inheritance: a mechanism that automatically and transparently propagates data through service pipelines. This paper presents a novel mechanism for automatic interface configuration of such services. Instead of checking consistency of the behavioural contracts, our approach focuses solely on that of data formats in the presence of subtyping, polymorphism and flow inheritance. The paper presents a toolchain that automatically derives service interfaces from the code and performs interface configuration taking non-local constraints into account. Although the configuration mechanism is global, the services are compiled separately. As a result, the mechanism does not raise source security issues despite global service availability in adaptable form."
"Applications developed over the cloud coordinate several, often anonymous, computational resources, distributed over different execution nodes, within flexible architectures. Coordination models able to represent quantitative data provide a powerful basis for their analysis and validation. This paper extends IMCreo, a semantic model for Stochastic reo based on interactive Markov chains, to enhance its scalability, by regarding each channel and node, as well as interface components, as independent stochastic processes that may (or may not) synchronise with the rest of the coordination circuit."
"In this paper, by investigating the factor of the $x^n+1$, we deduce that the structure of the reversible negacyclic code over the finite field $\mathbb{F}_{q}$, where $q$ is an odd prime power. Though studying $q-$cyclotomic cosets modulo $2n$, we obtain the parameters of negacyclic BCH code of length $n=\frac{q^\ell+1}{2}$ , $n=\frac{q^m-1}{2(q-1)}$ and $n=\frac{q^{t\cdot2^\tau}-1}{2(q^t+1)}$. Some optimal linear codes from negacyclic codes are given. Finally, we discuss a class of MDS LCD negacyclic codes."
"Based on the observation that the transparency of an algorithm comes with a cost for the algorithm designer when the users (data providers) are strategic, this paper studies the impact of strategic intent of the users on the design and performance of transparent ML algorithms. We quantitatively study the {\bf price of transparency} in the context of strategic classification algorithms, by modeling the problem as a nonzero-sum game between the users and the algorithm designer. The cost of having a transparent algorithm is measured by a quantity, named here as price of transparency which is the ratio of the designer cost at the Stackelberg equilibrium, when the algorithm is transparent (which allows users to be strategic) to that of the setting where the algorithm is not transparent."
"Distributed, parallel crowd workers can accomplish simple tasks through workflows, but teams of collaborating crowd workers are necessary for complex goals. Unfortunately, a fundamental condition for effective teams - familiarity with other members - stands in contrast to crowd work's flexible, on-demand nature. We enable effective crowd teams with Huddler, a system for workers to assemble familiar teams even under unpredictable availability and strict time constraints. Huddler utilizes a dynamic programming algorithm to optimize for highly familiar teammates when individual availability is unknown. We first present a field experiment that demonstrates the value of familiarity for crowd teams: familiar crowd teams doubled the performance of ad-hoc (unfamiliar) teams on a collaborative task. We then report a two-week field deployment wherein Huddler enabled crowd workers to convene highly familiar teams in 18 minutes on average. This research advances the goal of supporting long-term, team-based collaborations without sacrificing the flexibility of crowd work."
"Percolation threshold of a network is the critical value such that when nodes or edges are randomly selected with probability below the value, the network is fragmented but when the probability is above the value, a giant component connecting large portion of the network would emerge. Assessing the percolation threshold of networks has wide applications in network reliability, information spread, epidemic control, etc. The theoretical approach so far to assess the percolation threshold is mainly based on spectral radius of adjacency matrix or non-backtracking matrix, which is limited to dense graphs or locally treelike graphs, and is less effective for sparse networks with non-negligible amount of triangles and loops. In this paper, we study high-order non-backtracking matrices and their application to assessing percolation threshold. We first define high-order non-backtracking matrices and study the properties of their spectral radii. Then we focus on 2nd-order non-backtracking matrix and demonstrate analytically that the reciprocal of its spectral radius gives a tighter lower bound than those of adjacency and standard non-backtracking matrices. We further build a smaller size matrix with the same largest eigenvalue as the 2nd-order non-backtracking matrix to improve computation efficiency. Finally, we use both synthetic networks and 42 real networks to illustrate that the use of 2nd-order non-backtracking matrix does give better lower bound for assessing percolation threshold than adjacency and standard non-backtracking matrices."
"Ant colony system (ACS) is a promising approach which has been widely used in problems such as Travelling Salesman Problems (TSP), Job shop scheduling problems (JSP) and Quadratic Assignment problems (QAP). In its original implementation, parameters of the algorithm were selected by trial and error approach. Over the last few years, novel approaches have been proposed on adapting the parameters of ACS in improving its performance. The aim of this paper is to use a framework introduced for self-tuning optimization algorithms combined with the firefly algorithm (FA) to tune the parameters of the ACS solving symmetric TSP problems. The FA optimizes the problem specific parameters of ACS while the parameters of the FA are tuned by the selected framework itself. With this approach, the user neither has to work with the parameters of ACS nor the parameters of FA. Using common symmetric TSP problems we demonstrate that the framework fits well for the ACS. A detailed statistical analysis further verifies the goodness of the new ACS over the existing ACS and also of the other techniques used to tune the parameters of ACS."
"Distributed storage codes have important applications in the design of modern storage systems. In a distributed storage system, every storage node has a probability to fail and once an individual storage node fails, it must be reconstructed using data stored in the surviving nodes. Computation load and network bandwidth are two important issues we need to concern when repairing a failed node. The traditional maximal distance separable (MDS) storage codes have low repair complexity but high repair bandwidth. On the contrary, minimal storage regenerating (MSR) codes have low repair bandwidth but high repair complexity. Fortunately, the newly introduced piggyback codes combine the advantages of both ones.   In this paper, by introducing a novel piggybacking design framework for systematic MDS codes, we construct a storage code whose average repair bandwidth rate, i.e., the ratio of average repair bandwidth and the amount of the original data, can be as low as $\frac{\sqrt{2r-1}}{r}$, which significantly improves the ratio $\frac{r-1}{2r-1}$ of the previous result. In the meanwhile, every failed systematic node of the new code can be reconstructed quickly using the decoding algorithm of an MDS code, only with some additional additions over the underlying finite field. This is very fast compared with the complex matrix multiplications needed in the repair of a failed node of an MSR code."
"This paper have two parts. In the first part we discuss word embeddings. We discuss the need for them, some of the methods to create them, and some of their interesting properties. We also compare them to image embeddings and see how word embedding and image embedding can be combined to perform different tasks. In the second part we implement a convolutional neural network trained on top of pre-trained word vectors. The network is used for several sentence-level classification tasks, and achieves state-of-art (or comparable) results, demonstrating the great power of pre-trainted word embeddings over random ones."
"Online identification of post-contingency transient stability is essential in power system control, as it facilitates the grid operator to decide and coordinate system failure correction control actions. Utilizing machine learning methods with synchrophasor measurements for transient stability assessment has received much attention recently with the gradual deployment of wide-area protection and control systems. In this paper, we develop a transient stability assessment system based on the long short-term memory network. By proposing a temporal self-adaptive scheme, our proposed system aims to balance the trade-off between assessment accuracy and response time, which may both be crucial in real-world analysis scenarios. Compared with previous work, a most significant enhancement is that our system learns from the temporal data dependencies of the input data, which contributes to a better assessment accuracy. In addition, the model structure of our system is relatively less complex, speeding up the model training process. Case studies on three power systems demonstrate the efficacy of the proposed transient stability assessment system."
"The problem of forecasting conditional probabilities of the next event given the past is considered in a general probabilistic setting. Given an arbitrary (large, uncountable) set C of predictors, we would like to construct a single predictor that performs asymptotically as well as the best predictor in C, on any data. Here we show that there are sets C for which such predictors exist, but none of them is a Bayesian predictor with a prior concentrated on C. In other words, there is a predictor with sublinear regret, but every Bayesian predictor must have a linear regret. This negative finding is in sharp contrast with previous results that establish the opposite for the case when one of the predictors in $C$ achieves asymptotically vanishing error. In such a case, if there is a predictor that achieves asymptotically vanishing error for any measure in C, then there is a Bayesian predictor that also has this property, and whose prior is concentrated on (a countable subset of) C."
"We study Hopf monoids in entropic semi-additive varieties (equivalently, entropic J\'onsson-Tarski varieties and categories of semimodules over a commutative semiring, respectively) with an emphasis on adjunctions related to the enveloping monoid functor and the primitive element functor. These investigations are based on the concept of the abelian core of a semi-additive variety variety and its monoidal structure in case the variety is entropic. We also complement the study of generalized finite duals."
"Parkinson's disease (PD) is one of the major public health problems in the world. It is a well-known fact that around one million people suffer from Parkinson's disease in the United States whereas the number of people suffering from Parkinson's disease worldwide is around 5 million. Thus, it is important to predict Parkinson's disease in early stages so that early plan for the necessary treatment can be made. People are mostly familiar with the motor symptoms of Parkinson's disease, however, an increasing amount of research is being done to predict the Parkinson's disease from non-motor symptoms that precede the motor ones. If an early and reliable prediction is possible then a patient can get a proper treatment at the right time. Nonmotor symptoms considered are Rapid Eye Movement (REM) sleep Behaviour Disorder (RBD) and olfactory loss. Developing machine learning models that can help us in predicting the disease can play a vital role in early prediction. In this paper, we extend a work which used the non-motor features such as RBD and olfactory loss. Along with this the extended work also uses important biomarkers. In this paper, we try to model this classifier using different machine learning models that have not been used before. We developed automated diagnostic models using Multilayer Perceptron, BayesNet, Random Forest and Boosted Logistic Regression. It has been observed that Boosted Logistic Regression provides the best performance with an impressive accuracy of 97.159 % and the area under the ROC curve was 98.9%. Thus, it is concluded that these models can be used for early prediction of Parkinson's disease."
"The emerging field of quantum machine learning has the potential to substantially aid in the problems and scope of artificial intelligence. This is only enhanced by recent successes in the field of classical machine learning. In this work we propose an approach for the systematic treatment of machine learning, from the perspective of quantum information. Our approach is general and covers all three main branches of machine learning: supervised, unsupervised and reinforcement learning. While quantum improvements in supervised and unsupervised learning have been reported, reinforcement learning has received much less attention. Within our approach, we tackle the problem of quantum enhancements in reinforcement learning as well, and propose a systematic scheme for providing improvements. As an example, we show that quadratic improvements in learning efficiency, and exponential improvements in performance over limited time periods, can be obtained for a broad class of learning problems."
"IP networks with a hybrid control plane deploy a distributed routing protocol like OSPF and the centralized paradigm of SDN in parallel. The advantages and disadvantages of a hybrid control plane have already been extensively discussed in the literature. This paper examines to what extent hybrid SDN/OSPF can solve the IP traffic matrix and related monitoring problems, inherent to the IP layer. The problem persists in hybrid networks, as the required SDN byte counters for a complete traffic matrix may not be sufficiently implemented (or even not at all), or the deployed SDN nodes may be too few, or not adequately located in the network. For such cases, we propose to augment the SDN traffic statistics with SNMP-based measurements on IP backup links. We address implementation and network function virtualization aspects of the required hybrid monitoring infrastructure and discuss the timing issues of the measurements based on hands-on experiences in our lab. We furthermore provide a placement algorithm for SDN nodes and backup links that can guarantee a complete IP traffic matrix."
"This paper introduces a new quantization scheme for real and complex Grassmannian sources. The proposed approach relies on a structured codebook based on a geometric construction of a collection of bent grids defined from an initial mesh on the unit-norm sphere. The associated encoding and decoding algorithms have very low complexity (equivalent to a scalar quantizer), while their efficiency (in terms of the achieved distortion) is on par with the best known structured approaches, and compares well with the theoretical bounds. These properties make this codebook suitable for high-resolutions, real-time applications such as channel state feedback in massive multiple-input multiple-output (MIMO) wireless communication systems."
"The notion of g-frames for Hilbert spaces was introduced and studied by Wenchang Sun [16] as a generalization of the notion of frames. In this paper, we define computable g-frames in computable Hilbert spaces and obtain computable versions of some of their characterizations and related results."
"Opinion polls mediated through a social network can give us, in addition to usual demographics data like age, gender and geographic location, a friendship structure between voters and the temporal dynamics of their activity during the voting process. Using a Facebook application we collected friendship relationships, demographics and votes of over ten thousand users on the referendum on the definition of marriage in Croatia held on 1st of December 2013. We also collected data on online news articles mentioning our application. Publication of these articles align closely with large peaks of voting activity, indicating that these external events have a crucial influence in engaging the voters. Also, existence of strongly connected friendship communities where majority of users vote during short time period, and the fact that majority of users in general tend to friend users that voted the same suggest that peer influence also has its role in engaging the voters. As we are not able to track activity of our users at all times, and we do not know their motivations for expressing their votes through our application, the question is whether we can infer peer and external influence using friendship network of users and the times of their voting. We propose a new method for estimation of magnitude of peer and external influence in friendship network and demonstrate its validity on both simulated and actual data."
"Network Function Virtualization (NFV) is a new paradigm, enabling service innovation through virtualization of traditional network functions located flexibly in the network in form of Virtual Network Functions (VNFs). Since VNFs can only be placed onto servers located in networked data centers, which is the NFV's salient feature, the traffic directed to these data center areas has significant impact on network load balancing. Network load balancing can be even more critical for an ordered sequence of VNFs, also known as Service Function Chains (SFCs), a common cloud and network service approach today. To balance the network load, VNF's can be placed in a smaller cluster of servers in the network thus minimizing the distance to the data center. The optimization of the placement of these clusters is a challenge as also other factors need to be considered, such as the resource utilization. To address this issue, we study the problem of VNF placement with replications, and especially the potential of VNFs replications to help load balance the network. We design and compare three optimization methods, including Linear Programing (LP) model, Genetic Algorithm (GA) and Random Fit Placement Algorithm (RFPA) for the allocation and replication of VNFs. Our results show that the optimum placement and replication can significantly improve load balancing, for which we also propose a GA heuristics applicable to larger networks."
The statistical behavior of the eigenvalues of the sample covariance matrix (SCM) plays a key role in determining the performance of adaptive beamformers (ABF) in presence of noise. This paper presents a method to compute the approximate eigenvalue density function (EDF) for the SCM of a \cin{} field when only a finite number of shapshots are available. The EDF of the ensemble covariance matrix (ECM) is modeled as an atomic density with many fewer atoms than the SCM size. The model results in substantial computational savings over more direct methods of computing the EDF. The approximate EDF obtained from this method agrees closely with histograms of eigenvalues obtained from simulation.
"In this paper, we consider a time-switching (TS) co-located simultaneous wireless information and power transfer (SWIPT) system consisting of multiple multi-antenna access points which serve multiple single antenna users. In this scenario, we propose a multi-objective optimization (MOO) framework to design jointly the Pareto optimal beamforming vector and the TS ratio for each receiver. The objective is to maximize the utility vector including the achieved data rates and the harvested energies of all users simultaneously. This problem is a non-convex rank-constrained MOO problem which is relaxed and transformed into a non-convex semidefinite program (SDP) based on the weighted Chebycheff method. The majorization-minimization algorithm is utilized to solve the nonconvex SDP and the optimal solution is proved to satisfy the rank constraint. We also study the problem of optimizing the beamforming vectors in a fixed TS ratio scenario with the same approach. Numerical results are provided for two coordinated access points with MISO configuration. The results illustrate the trade-off between harvested energy and information data rate objectives and show the effect of optimizing the precoding strategy and TS ratio on this trade-off."
"This dissertation studies different aspects of D2D communications and its impact on the key performance indicators of the network. We design an architecture for the collaboration of cellular users by means of timely exploited D2D opportunities. We begin by presenting the analytical study on opportunistic outband D2D communications. The study reveals the great potential of opportunistic outband D2D communications for enhancing energy efficiency, fairness, and capacity of cellular networks when groups of D2D users can be form and managed in the cellular network. Then we introduce a protocol that is compatible with the latest release of IEEE and 3GPP standards and allows for implementation of our proposal in a today's cellular network. To validate our analytical findings, we use our experimental Software Defined Radio (SDR)-based testbed to further study our proposal in a real world scenario. The experimental results confirm the outstanding potential of opportunistic outband D2D communications. Finally, we investigate the performance merits and disadvantages of different D2D modes. Our investigation reveals, despite the common belief, that all D2D modes are complementary and their merits are scenario based."
"Suffix array is a fundamental data structure for many applications that involve string searching and data compression. Designing time/space-efficient suffix array construction algorithms has attracted significant attentions and considerable advances have been made in the last 20 years. We obtain the suffix array construction algorithms that are optimal both in time and space for both integer alphabets and general alphabets. Concretely, we make the following contributions:   1. For integer alphabets, we obtain the first algorithm which takes linear time and uses only $O(1)$ workspace (the workspace is the space needed beyond the input string and the output suffix array). The input string may be modified during the execution of the algorithm, but should be restored upon termination of the algorithm. Our algorithm is easy to implement. Our C implementation of the algorithm requires only 8 Bytes of workspace.   2. We strengthen the first result by providing the first linear time in-place algorithm for read-only integer alphabets (i.e., we cannot modify the input string $T$). This settles the open problem posed by Franceschini and Muthukrishnan in ICALP 2007 \cite{franceschini2007place}.   3. For read-only general alphabets (i.e., only comparisons are allowed), we present an in-place $O(n\log n)$ time algorithm, recovering the result obtained by Franceschini and Muthukrishnan \cite{franceschini2007place}."
"A positional numeration system is given by a base and by a set of digits. The base is a real or complex number $\beta$ such that $|\beta|>1$, and the digit set $\A$ is a finite set of digits including $0$. Thus a number can be seen as a finite or infinite string of digits. An on-line algorithm processes the input piece-by-piece in a serial fashion. On-line arithmetic, introduced by Trivedi and Ercegovac, is a mode of computation where operands and results flow through arithmetic units in a digit serial manner, starting with the most significant digit.   In this paper, we first formulate a generalized version of the on-line algorithms for multiplication and division of Trivedi and Ercegovac for the cases that $\beta$ is any real or complex number, and digits are real or complex. We then define the so-called OL Property, and show that if $(\beta, \A)$ has the OL Property, then on-line multiplication and division are feasible by the Trivedi-Ercegovac algorithms. For a real base $\beta$ and a digit set $\A$ of contiguous integers, the system $(\beta, \A)$ has the OL Property if $# \A > |\beta|$. For a complex base $\beta$ and symmetric digit set $\A$ of contiguous integers, the system $(\beta, \A)$ has the OL Property if $# \A > \beta\overline{\beta} + |\beta + \overline{\beta}|$. Provided that addition and subtraction are realizable in parallel in the system $(\beta, \A)$ and that preprocessing of the denominator is possible, our on-line algorithms for multiplication and division have linear time complexity. Three examples are presented in detail: base $\beta=\frac{3+\sqrt{5}}{2}$ with digits $\A=\{-1,0,1\}$; base $\beta=2i$ with digits $\A = \{-2,-1, 0,1,2\}$; and base $\beta = -\frac{3}{2} + i \frac{\sqrt{3}}{2} = -1 + \omega$, where $\omega = \exp{\frac{2i\pi}{3}}$, with digits $\A = \{0, \pm 1, \pm \omega, \pm \omega^2 \}$."
"It is well known that the software process impacts the quality of the resulting product. There are also anecdotal claims that agile processes result in higher level of quality than traditional methodologies. However, still solid evidence of this is missing. This work reports in an empirical analysis of the correlation between software process and software quality with specific reference to agile and traditional processes. More than 100 software developers and engineers from 21 countries have been surveyed with an online questionnaire. We have used the percentage of satisfied customers estimated by the software developers and engineers as the main dependent variable. The results evidence some interesting patterns: architectural styles may not have a significant influence on quality, agile methodologies might result in happier customers, larger companies and shorter projects seems to produce better products."
"The calibration and validation of pedestrian simulations require the acquisition of empirical evidences of human behaviour. The current work presents the results of an experiment focused on the potentially combined effect of counter flow and grouping on pedestrian dynamics. In particular, we focused on: (i) four different configurations of flow ratio (the rate between the minor flow and the total flow in bidirectional scenarios); (ii) dyads, as the most frequently observed and basic social groups of crowds. Results showed that the increase of flow ratio negatively impacted the speed of pedestrians. Dyads walked significantly slower than singletons, due to the difficulty in movement coordination among group members (proxemics) in case of counter flow. The collected results represent an useful contribution towards the validation of pedestrian simulations."
"A Distortion Contribution Analysis (DCA) obtains the distortion at the output of an analog electronic circuit as a sum of distortion contributions of its sub-circuits. Similar to a noise analysis, a DCA helps a designer to pinpoint the actual source of the distortion. Classically, the DCA uses the Volterra theory to model the circuit and its sub-circuits. This DCA has been proven useful for small circuits or heavily simplified examples. In more complex circuits however, the amount of contributions increases quickly, making the interpretation of the results difficult. In this paper, the Best Linear Approximation (BLA) is used to perform the DCA instead. The BLA represents the behaviour of a sub-circuit as a linear circuit with the unmodelled distortion represented by a noise source. Combining the BLA with a classic noise analysis yields a DCA that is simple to understand, yet capable to handle complex excitation signals and complex strongly non-linear circuits."
"Tourism has been an increasingly important factor in global economy, society and environment, accounting for a significant share of GDP and labor force. Policy and research on tourism traditionally rely on surveys and economic datasets, which are based on small samples and depict tourism dynamics at low spatial and temporal granularity. Anonymous call detail records (CDRs) are a novel source of data, showing enormous potential in areas of high societal value: such as epidemics, poverty, and urban development. This study demonstrates the added value of using CDRs for the formulation, analysis and evaluation of tourism strategies, at the national and local levels. In the context of the European country of Andorra, we use CDRs to evaluate marketing strategies in tourism, understand tourists' experiences, and evaluate revenues and externalities generated by touristic events. We do this by extracting novel indicators in high spatial and temporal resolutions, such as tourist flows per country of origin, flows of new tourists, tourist revisits, tourist externalities on transportation congestion, spatial distribution, economic impact, and profiling of tourist interests. We exemplify the use of these indicators for the planning and evaluation of high impact touristic events, such as cultural festivals and sports competitions."
"We investigate the value of parallel repetition of one-round games with any number of players $k\ge 2$. It has been an open question whether an analogue of Raz's Parallel Repetition Theorem holds for games with more than two players, i.e., whether the value of the repeated game decays exponentially with the number of repetitions. Verbitsky has shown, via a reduction to the density Hales-Jewett theorem, that the value of the repeated game must approach zero, as the number of repetitions increases. However, the rate of decay obtained in this way is extremely slow, and it is an open question whether the true rate is exponential as is the case for all two-player games.   Exponential decay bounds are known for several special cases of multi-player games, e.g., free games and anchored games. In this work, we identify a certain expansion property of the base game and show all games with this property satisfy an exponential decay parallel repetition bound. Free games and anchored games satisfy this expansion property, and thus our parallel repetition theorem reproduces all earlier exponential-decay bounds for multiplayer games. More generally, our parallel repetition bound applies to all multiplayer games that are connected in a certain sense.   We also describe a very simple game, called the GHZ game, that does not satisfy this connectivity property, and for which we do not know an exponential decay bound. We suspect that progress on bounding the value of this the parallel repetition of the GHZ game will lead to further progress on the general question."
"This is the first in a series of papers on rank decompositions of the matrix multiplication tensor. In this paper we: establish general facts about rank decompositions of tensors, describe potential ways to search for new matrix multiplication decompositions, give a geometric proof of the theorem of Burichenko's theorem establishing the symmetry group of Strassen's algorithm, and present two particularly nice subfamilies in the Strassen family of decompositions."
"The explosion in the e-commerce industry which has been necessitated by the growth and advance expansion of Information technology and its related facilities in recent years have been met with adverse security issues consequently affecting the industry and the entire online activities. This paper exams the prevailing security threats e-commerce is facing which is predominantly known as cyber-crime and how computer related technology and facilities such as digital forensics tools can be adopted extensively to ensure security in online related business activities. This paper investigated the risk, damage and the cost cyber-crime poses to individuals and organizations when they become victims. As it is obvious transacting business online as well as all related online activities has become inherent in our everyday life. The paper also comprehensively elucidate on some of the cyber-crime activities that are posing serious threat to the security of E-commerce. Amazon and eBay were used as the case of study in relation to respondents who patronizes these renowned e-commerce sites for various transactions.   Keywords: E-commerce Security,Cyber-Crime,digital forensics, Network forensics, Network security, Online transactions, Identity theft, hacking."
"On-line social networks are complex ensembles of inter-linked communities that interact on different topics. Some communities are characterized by what are usually referred to as deviant behaviors, conducts that are commonly considered inappropriate with respect to the society's norms or moral standards. Eating disorders, drug use, and adult content consumption are just a few examples. We refer to such communities as deviant networks. It is commonly believed that such deviant networks are niche, isolated social groups, whose activity is well separated from the mainstream social-media life. According to this assumption, research studies have mostly considered them in isolation. In this work we focused on adult content consumption networks, which are present in many on-line social media and in the Web in general. We found that few small and densely connected communities are responsible for most of the content production. Differently from previous work, we studied how such communities interact with the whole social network. We found that the produced content flows to the rest of the network mostly directly or through bridge-communities, reaching at least 450 times more users. We also show that a large fraction of the users can be inadvertently exposed to such content through indirect content resharing. We also discuss a demographic analysis of the producers and consumers networks. Finally, we show that it is easily possible to identify a few core users to radically uproot the diffusion process. We aim at setting the basis to study deviant communities in context."
"Emulating atomic read/write shared objects in a message-passing system is a fundamental problem in distributed computing. Considering that network communication is the most expensive resource, efficiency is measured first of all in terms of the communication needed to implement read and write operations. It is well known that 2 communication round-trip phases involving in total 4 message exchanges are sufficient to implemented atomic operations. It is also known that under certain constraints on the number of readers with respect to the numbers of replica servers and failures it is possible to implement single-writer atomic objects such that each operation involves one round-trip phase. We present algorithms that allow operations to complete in 3 communication exchanges without imposing any constraints on the number of readers and writers. Specifically, we present an atomic memory implementation for the SWMR setting, where reads complete in 3 communication exchanges and writes complete in 2 exchanges. We pose the question of whether it is possible to implement MWMR memory where operations complete in at most 3 communication exchanges. We answer this question in the negative by showing that an atomic memory implementation is impossible if both read and write operations take 3 communication exchanges, even when assuming two writers, two readers, and a single replica server failure. Motivated by this impossibility result, we provide a MWMR atomic memory implementation where reads involve 3 and writes 4 communication exchanges. In light of our impossibility result these algorithms are optimal in terms of the number of communication exchanges. We rigorously reason about the correctness of the algorithms."
"Data-to-text systems are powerful in generating reports from data automatically and thus they simplify the presentation of complex data. Rather than presenting data using visualisation techniques, data-to-text systems use natural (human) language, which is the most common way for human-human communication. In addition, data-to-text systems can adapt their output content to users' preferences, background or interests and therefore they can be pleasant for users to interact with. Content selection is an important part of every data-to-text system, because it is the module that determines which from the available information should be conveyed to the user. This survey initially introduces the field of data-to-text generation, describes the general data-to-text system architecture and then it reviews the state-of-the-art content selection methods. Finally, it provides recommendations for choosing an approach and discusses opportunities for future research."
"The aim of this work is to introduce an efficient procedure for discrete multi-agent planning under local complex temporal logic behavior specifications. While the first part of an agent's behavior specification constraints the agent's trace and is independent, the second part of the specification expresses the agent's tasks in terms of the services to be provided along the trace and may impose requests for the other agents' collaborations. To fight the extreme computational complexity of centralized multi-agent planning, we propose a two-phase automata-based solution, where we systematically decouple the planning procedure for the two types of specifications. At first, we only consider the former specifications in a fully decentralized way and we compactly represent each agents' admissible traces by abstracting away the states that are insignificant for the satisfaction of their latter specifications. Second, the synchronized planning procedure uses only the compact representations. The satisfaction of the overall specification is guaranteed by construction for each agent. An illustrative example demonstrating the practical benefits of the solution is included."
"We study the problem of plan synthesis for multi-agent systems, to achieve complex, high-level, long-term goals that are assigned to each agent individually. As the agents might not be capable of satisfying their respective goals by themselves, requests for other agents' collaborations are a part of the task descriptions. We consider that each agent is modeled as a discrete state-transition system and its task specification takes a form of a linear temporal logic formula, which may contain requirements and constraints on the other agent's behavior. A traditional automata-based approach to multi-agent plan synthesis from such specifications builds on centralized team planning and full team synchronization after each agents' discrete step, and thus suffers from extreme computational demands. We aim at reducing the computational complexity by decomposing the plan synthesis problem into finite horizon planning problems that are solved iteratively, upon the run of the agents. As opposed to full synchronization, we introduce an event-based synchronization that allows our approach to efficiently adapt to different time durations of different agents' discrete steps. We discuss the correctness of the solution and find assumptions, under which the proposed iterative algorithm leads to provable eventual satisfaction of the desired specifications."
"Because falls are funny, YouTube and other video sharing sites contain a large repository of real-life falls. We propose extracting gait and balance information from these videos to help us better understand some of the factors that contribute to falls. Proof-of-concept is explored in a single video containing multiple (n=14) falls/non-falls in the presence of an unexpected obstacle. The analysis explores: computing spatiotemporal parameters of gait in a video captured from an arbitrary viewpoint; the relationship between parameters of gait from the last few steps before the obstacle and falling vs. not falling; and the predictive capacity of a multivariate model in predicting a fall in the presence of an unexpected obstacle. Homography transformations correct the perspective projection distortion and allow for the consistent tracking of gait parameters as an individual walks in an arbitrary direction in the scene. A synthetic top view allows for computing the average stride length and a synthetic side view allows for measuring up and down motions of the head. In leave-one-out cross-validation, we were able to correctly predict whether a person would fall or not in 11 out of the 14 cases (78.6%), just by looking at the average stride length and the range of vertical head motion during the 1-4 most recent steps prior to reaching the obstacle."
"Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images."
"We consider election scenarios with incomplete information, a situation that arises often in practice. There are several models of incomplete information and accordingly, different notions of outcomes of such elections. In one well-studied model of incompleteness, the votes are given by partial orders over the candidates. In this context we can frame the problem of finding a possible winner, which involves determining whether a given candidate wins in at least one completion of a given set of partial votes for a specific voting rule.   The possible winner problem is well-known to be NP-complete in general, and it is in fact known to be NP-complete for several voting rules where the number of undetermined pairs in every vote is bounded only by some constant. In this paper, we address the question of determining precisely the smallest number of undetermined pairs for which the possible winner problem remains NP-complete. In particular, we find the exact values of $t$ for which the possible winner problem transitions to being NP-complete from being in P, where $t$ is the maximum number of undetermined pairs in every vote. We demonstrate tight results for a broad subclass of scoring rules which includes all the commonly used scoring rules (such as plurality, veto, Borda, $k$-approval, and so on), Copeland$^\alpha$ for every $\alpha\in[0,1]$, maximin, and Bucklin voting rules. A somewhat surprising aspect of our results is that for many of these rules, the possible winner problem turns out to be hard even if every vote has at most one undetermined pair of candidates."
"Sum-networks are networks where all the terminals demand the sum of the symbols generated at the sources. It has been shown that for any finite set/co-finite set of prime numbers, there exists a sum-network which has a vector linear solution if and only if the characteristic of the finite field belongs to the given set. It has also been shown that for any positive rational number $k/n$, there exists a sum-network which has capacity equal to $k/n$. It is a natural question whether, for any positive rational number $k/n$, and for any finite set/co-finite set of primes $\{p_1,p_2,\ldots,p_l\}$, there exists a sum-network which has a capacity achieving rate $k/n$ fractional linear network coding solution if and only if the characteristic of the finite field belongs to the given set. We show that indeed there exists such a sum-network by constructing such a sum-network."
"For decades, the crowdsourcing has gained much attention from both academia and industry, which outsources a number of tasks to human workers. Existing works considered improving the task accuracy through voting or learning methods, they usually did not fully take into account reducing the latency of the task completion. When a task requester posts a group of tasks (e.g., sentiment analysis), and one can only obtain answers of all tasks after the last task is accomplished. As a consequence, the time delay of even one task in this group could delay the next step of the task requester's work from minutes to days, which is quite undesirable for the task requester.   Inspired by the importance of the task accuracy and latency, in this paper, we will propose a novel crowdsourcing framework, namely Fast and Reliable crOwdsourcinG framework (FROG), which intelligently assigns tasks to workers, such that the latencies of tasks are reduced and the expected accuracies of tasks are met. Specifically, our FROG framework consists of two important components, task scheduler and notification modules. For the task scheduler module, we formalize a FROG task scheduling (FROG-TS) problem, in which the server actively assigns workers with tasks with high reliability and low latency. We prove that the FROG-TS problem is NP-hard. Thus, we design two heuristic approaches, request-based and batch-based scheduling. For the notification module, we define an efficient worker notifying (EWN) problem, which only sends task invitations to those workers with high probabilities of accepting the tasks. To tackle the EWN problem, we propose a smooth kernel density estimation approach to estimate the probability that a worker accepts the task invitation. Through extensive experiments, we demonstrate the effectiveness and efficiency of our proposed FROG platform on both real and synthetic data sets."
"We present a derivation and theoretical investigation of the Adams-Bashforth and Adams-Moulton family of linear multistep methods for solving ordinary differential equations, starting from a Gaussian process (GP) framework. In the limit, this formulation coincides with the classical deterministic methods, which have been used as higher-order initial value problem solvers for over a century. Furthermore, the natural probabilistic framework provided by the GP formulation allows us to derive probabilistic versions of these methods, in the spirit of a number of other probabilistic ODE solvers presented in the recent literature. In contrast to higher-order Runge-Kutta methods, which require multiple intermediate function evaluations per step, Adams family methods make use of previous function evaluations, so that increased accuracy arising from a higher-order multistep approach comes at very little additional computational cost. We show that through a careful choice of covariance function for the GP, the posterior mean and standard deviation over the numerical solution can be made to exactly coincide with the value given by the deterministic method and its local truncation error respectively. We provide a rigorous proof of the convergence of these new methods, as well as an empirical investigation (up to fifth order) demonstrating their convergence rates in practice."
"Many modern robotics applications require robots to function autonomously in dynamic environments including other decision making agents, such as people or other robots. This calls for fast and scalable interactive motion planning. This requires models that take into consideration the other agent's intended actions in one's own planning. We present a real-time motion planning framework that brings together a few key components including intention inference by reasoning counterfactually about potential motion of the other agents as they work towards different goals. By using a light-weight motion model, we achieve efficient iterative planning for fluid motion when avoiding pedestrians, in parallel with goal inference for longer range movement prediction. This inference framework is coupled with a novel distributed visual tracking method that provides reliable and robust models for the current belief-state of the monitored environment. This combined approach represents a computationally efficient alternative to previously studied policy learning methods that often require significant offline training or calibration and do not yet scale to densely populated environments. We validate this framework with experiments involving multi-robot and human-robot navigation. We further validate the tracker component separately on much larger scale unconstrained pedestrian data sets."
"Progress in text understanding has been driven by the availability of large datasets that test particular capabilities, like recent datasets for assessing reading comprehension. We focus here on the LAMBADA dataset, a word prediction task requiring broader context than the immediate sentence. We view the LAMBADA task as a reading comprehension problem and apply off-the-shelf comprehension models based on neural networks. Though these models are constrained to choose a word from the context, they improve the state of the art on LAMBADA from 7.3% to 45.4%. We analyze 100 instances, finding that neural network readers perform well in cases that involve selecting a name from the context based on dialogue or discourse cues but struggle when coreference resolution or external knowledge is needed."
"This study consolidates Multi-Level Starlet Segmentation (MLSS) and Multi-Level Starlet Optimal Segmentation (MLSOS), techniques for photomicrograph segmentation that use starlet wavelet detail levels to separate areas of interest in an input image. Several segmentation levels can be obtained using Multi-Level Starlet Segmentation; after that, Matthews correlation coefficient (MCC) is used to choose an optimal segmentation level, giving rise to Multi-Level Starlet Optimal Segmentation. In this paper, MLSOS is employed to estimate the concentration of gold nanoparticles with diameter around 47 nm, reducted on natural rubber membranes. These samples were used on the construction of SERS/SERRS substrates and in the study of natural rubber membranes with incorporated gold nanoparticles influence on Leishmania braziliensis physiology. Precision, recall and accuracy are used to evaluate the segmentation performance, and MLSOS presents accuracy greater than 88% for this application."
"Internet data has surfaced as a primary source for investigation of different aspects of human behavior. A crucial step in such studies is finding a suitable cohort (i.e., a set of users) that shares a common trait of interest to researchers. However, direct identification of users sharing this trait is often impossible, as the data available to researchers is usually anonymized to preserve user privacy. To facilitate research on specific topics of interest, especially in medicine, we introduce an algorithm for identifying a trait of interest in anonymous users. We illustrate how a small set of labeled examples, together with statistical information about the entire population, can be aggregated to obtain labels on unseen examples. We validate our approach using labeled data from the political domain.   We provide two applications of the proposed algorithm to the medical domain. In the first, we demonstrate how to identify users whose search patterns indicate they might be suffering from certain types of cancer. In the second, we detail an algorithm to predict the distribution of diseases given their incidence in a subset of the population at study, making it possible to predict disease spread from partial epidemiological data."
"Statistical relational models provide compact encodings of probabilistic dependencies in relational domains, but result in highly intractable graphical models. The goal of lifted inference is to carry out probabilistic inference without needing to reason about each individual separately, by instead treating exchangeable, undistinguished objects as a whole. In this paper, we study the domain recursion inference rule, which, despite its central role in early theoretical results on domain-lifted inference, has later been believed redundant. We show that this rule is more powerful than expected, and in fact significantly extends the range of models for which lifted inference runs in time polynomial in the number of individuals in the domain. This includes an open problem called S4, the symmetric transitivity model, and a first-order logic encoding of the birthday paradox. We further identify new classes S2FO2 and S2RU of domain-liftable theories, which respectively subsume FO2 and recursively unary theories, the largest classes of domain-liftable theories known so far, and show that using domain recursion can achieve exponential speedup even in theories that cannot fully be lifted with the existing set of inference rules."
"Interfacing a kinetic action of a person to an action of a machine system is an important research topic in many application areas. One of the key factors for intimate human-machine interaction is the ability of the control algorithm to detect and classify different user commands with shortest possible latency, thus making a highly correlated link between cause and effect. In our research, we focused on the task of mapping user kinematic actions into sound samples. The presented methodology relies on the wireless sensor nodes equipped with inertial measurement units and the real-time algorithm dedicated for early detection and classification of a variety of movements/gestures performed by a user. The core algorithm is based on the approximate Bayesian inference of Vector Autoregressive Hierarchical Hidden Markov Models (VAR-HHMM), where models database is derived from the set of motion gestures. The performance of the algorithm was compared with an online version of the K-nearest neighbours (KNN) algorithm, where we used offline expert based classification as the benchmark. In almost all of the evaluation metrics (e.g. confusion matrix, recall and precision scores) the VAR-HHMM algorithm outperformed KNN. Furthermore, the VAR-HHMM algorithm, in some cases, achieved faster movement onset detection compared with the offline standard. The proposed concept, although envisioned for movement-to-sound application, could be implemented in other human-machine interfaces."
"As the use of automated decision making systems becomes wide-spread, there is a growing concern about their potential unfairness towards people with certain traits. Anti-discrimination laws in various countries prohibit unfair treatment of individuals based on specific traits, also called sensitive attributes (e.g., gender, race). In many learning scenarios, the trained algorithms (classifiers) make decisions with certain inaccuracy (misclassification rate). As learning mechanisms target minimizing the error rate for all decisions, it is quite possible that the optimally trained algorithm makes decisions for users belonging to different sensitive attribute groups with different error rates (e.g., decision errors for females are higher than for males). To account for and avoid such unfairness when learning, in this paper, we introduce a new notion of unfairness, disparate mistreatment, which is defined in terms of misclassification rates. We then propose an intuitive measure of disparate mistreatment for decision boundary-based classifiers, which can be easily incorporated into their formulation as a convex-concave constraint. Experiments on synthetic as well as real world datasets show that our methodology is effective at avoiding disparate mistreatment, often at a small cost in terms of accuracy."
"The paper presents an algorithm for minimum vertex cover problem, which is an NP-Complete problem. The algorithm computes a minimum vertex cover of each input simple graph. Tested by the attached MATLAB programs, Stage 1 of the algorithm is applicable to, i.e., yields a proved minimum vertex cover for, about 99.99% of the tested 610,000 graphs of order 16 and 99.67% of the tested 1,200 graphs of order 32, and Stage 2 of the algorithm is applicable to all of the above tested graphs. All of the tested graphs are randomly generated graphs of random ""edge density"" or in other words, random probability of each edge. It is proved that Stage 1 and Stage 2 of the algorithm run in $O(n^{5+logn})$ and $O(n^{3(5+logn)/2})$ time respectively, where $n$ is the order of input graph. Because there is no theoretical proof yet that Stage 2 is applicable to all graphs, further stages of the algorithm are proposed, which are in a general form that is consistent with Stages 1 and 2."
"Distributed representation learned with neural networks has recently shown to be effective in modeling natural languages at fine granularities such as words, phrases, and even sentences. Whether and how such an approach can be extended to help model larger spans of text, e.g., documents, is intriguing, and further investigation would still be desirable. This paper aims to enhance neural network models for such a purpose. A typical problem of document-level modeling is automatic summarization, which aims to model documents in order to generate summaries. In this paper, we propose neural models to train computers not just to pay attention to specific regions and content of input documents with attention models, but also distract them to traverse between different content of a document so as to better grasp the overall meaning for summarization. Without engineering any features, we train the models on two large datasets. The models achieve the state-of-the-art performance, and they significantly benefit from the distraction modeling, particularly when input documents are long."
"As food becomes an important part of modern life, recipes shared on the web are a great indicator of civilizations and culinary attitudes in different countries. Similarly, ingredients, flavors, and nutrition information are strong signals of the taste preferences of individuals from various parts of the world. Yet, we do not have a thorough understanding of these palate varieties.   In this paper, we present a large-scale study of recipes published on the Web and their content, aiming to understand cuisines and culinary habits around the world. Using a database of more than 157K recipes from over 200 different cuisines, we analyze ingredients, flavors, and nutritional values which distinguish dishes from different regions, and use this knowledge to assess the predictability of recipes from different cuisines. We then use country health statistics to understand the relation between these factors and health indicators of different nations, such as obesity, diabetes, migration, and health expenditure. Our results confirm the strong effects of geographical and cultural similarities on recipes, health indicators, and culinary preferences between countries around the world."
"Most real-world networks are too large to be measured or studied directly and there is substantial interest in estimating global network properties from smaller sub-samples. One of the most important global properties is the number of vertices/nodes in the network. Estimating the number of vertices in a large network is a major challenge in computer science, epidemiology, demography, and intelligence analysis. In this paper we consider a population random graph G = (V;E) from the stochastic block model (SBM) with K communities/blocks. A sample is obtained by randomly choosing a subset W and letting G(W) be the induced subgraph in G of the vertices in W. In addition to G(W), we observe the total degree of each sampled vertex and its block membership. Given this partial information, we propose an efficient PopULation Size Estimation algorithm, called PULSE, that correctly estimates the size of the whole population as well as the size of each community. To support our theoretical analysis, we perform an exhaustive set of experiments to study the effects of sample size, K, and SBM model parameters on the accuracy of the estimates. The experimental results also demonstrate that PULSE significantly outperforms a widely-used method called the network scale-up estimator in a wide variety of scenarios. We conclude with extensions and directions for future work."
"The application of synchronization theory to build up new cryptosystems has been a hot topic during the last two decades. In this paper we analyze a recent proposal in this field. We pinpoint the main limitations of the software implementation of chaos-based systems designed on the grounds of synchronization theory. In addition, we show that the cryptosystem under evaluation possesses serious security problems that imply a clear reduction of the key space."
"Gradual typing combines static and dynamic typing in the same language, offering the benefits of both to programmers. Static typing provides error detection and strong guarantees while dynamic typing enables rapid prototyping and flexible programming idioms. For programmers to fully take advantage of a gradual type system, however, they must be able to trust their type annotations, and so runtime checks must be performed at the boundaries of static and dynamic code to ensure that static types are respected. Higher order and mutable values cannot be completely checked at these boundaries, and so additional checks must be performed at their use sites. Traditionally, this has been achieved by installing wrappers or proxies on such values that moderate the flow of data between static and dynamic, but these can cause problems if the language supports comparison of object identity or has a foreign function interface.   Reticulated Python is a gradually typed variant of Python implemented via a source-to-source translator for Python 3. It implements a proxy-free alternative design named transient casts. This paper presents a formal semantics for transient casts and shows that not only are they sound, but they work in an open-world setting in which the Reticulated translator has only been applied to some of the program; the rest is untranslated Python. We formalize this open world soundness property and use Coq to prove that it holds for Anthill Python, a calculus that models Reticulated Python."
"A head-mounted display (HMD) could be an important component of augmented reality system. However, as the upper face region is seriously occluded by the device, the user experience could be affected in applications such as telecommunication and multi-player video games. In this paper, we first present a novel experimental setup that consists of two near-infrared (NIR) cameras to point to the eye regions and one visible-light RGB camera to capture the visible face region. The main purpose of this paper is to synthesize realistic face images without occlusions based on the images captured by these cameras. To this end, we propose a novel synthesis framework that contains four modules: 3D head reconstruction, face alignment and tracking, face synthesis, and eye synthesis. In face synthesis, we propose a novel algorithm that can robustly align and track a personalized 3D head model given a face that is severely occluded by the HMD. In eye synthesis, in order to generate accurate eye movements and dynamic wrinkle variations around eye regions, we propose another novel algorithm to colorize the NIR eye images and further remove the ""red eye"" effects caused by the colorization. Results show that both hardware setup and system framework are robust to synthesize realistic face images in video sequences."
"We consider a system in which two nodes take correlated measurements of a random source with time-varying and unknown statistics. The observations of the source at the first node are to be losslessly replicated with a given probability of outage at the second node, which receives data from the first node over a constant-rate errorless channel. We develop a system and associated strategies for joint distributed source coding (encoding and decoding) and transmission control in order to achieve low end-to-end delay. Slepian-Wolf coding in its traditional form cannot be applied in our scenario, since the encoder requires the joint statistics of the observations and the associated decoding delay is very high. We analytically evaluate the performance of our strategies and show that the delay achieved by them are order optimal, as the conditional entropy of the source approaches to the channel rate. We also evaluate the performance of our algorithms based on real-world experiments using two cameras recording videos of a scene at different angles. Having realized our schemes, we demonstrated that, even with a very low-complexity quantizer, a compression ratio of approximately 50% is achievable for lossless replication at the decoder, at an average delay of a few seconds."
"Coordinated Multipoint (CoMP) promised substantial throughput gain for next-generation cellular systems. However, realizing this gain is costly in terms of pilots and backhaul bandwidth, and may require substantial modifications in physicallayer hardware. Targeting efficient throughput gain, we develop a novel coordinated power control scheme for uplink cellular networks called Checks and Balances (C&B), which checks the received signal strength of one user and its generated interference to neighboring base stations, and balances the two. C&B has some highly attractive advantages: C&B (i) can be implemented easily in software, (ii) does not require to upgrade non-CoMP physicallayer hardware, (iii) allows for fully distributed implementation for each user equipment (UE), and (iv) does not need extra pilots or backhaul communications. We evaluate the throughput performance of C&B on an uplink LTE system-level simulation platform, which is carefully calibrated with Huawei. Our simulation results show that C&B achieves much better throughput performance, compared to several widely-used power control schemes."
"Supporting error resilience in future exascale-class supercomputing systems is a critical challenge. Due to transistor scaling trends and increasing memory density, scientific simulations are expected to experience more interruptions caused by transient errors in the system memory. Existing hardware-based detection and recovery techniques will be inadequate to manage the presence of high memory fault rates.   In this paper we propose a partial memory protection scheme based on region-based memory management. We define the concept of regions called havens that provide fault protection for program objects. We provide reliability for the regions through a software-based parity protection mechanism. Our approach enables critical program objects to be placed in these havens. The fault coverage provided by our approach is application agnostic, unlike algorithm-based fault tolerance techniques."
"Spike and Slab priors have been of much recent interest in signal processing as a means of inducing sparsity in Bayesian inference. Applications domains that benefit from the use of these priors include sparse recovery, regression and classification. It is well-known that solving for the sparse coefficient vector to maximize these priors results in a hard non-convex and mixed integer programming problem. Most existing solutions to this optimization problem either involve simplifying assumptions/relaxations or are computationally expensive. We propose a new greedy and adaptive matching pursuit (AMP) algorithm to directly solve this hard problem. Essentially, in each step of the algorithm, the set of active elements would be updated by either adding or removing one index, whichever results in better improvement. In addition, the intermediate steps of the algorithm are calculated via an inexpensive Cholesky decomposition which makes the algorithm much faster. Results on simulated data sets as well as real-world image recovery challenges confirm the benefits of the proposed AMP, particularly in providing a superior cost-quality trade-off over existing alternatives."
"We formalize synthesis of shared control protocols with correctness guarantees for temporal logic specifications. More specifically, we introduce a modeling formalism in which both a human and an autonomy protocol can issue commands to a robot towards performing a certain task. These commands are blended into a joint input to the robot. The autonomy protocol is synthesized using an abstraction of possible human commands accounting for randomness in decisions caused by factors such as fatigue or incomprehensibility of the problem at hand. The synthesis is designed to ensure that the resulting robot behavior satisfies given safety and performance specifications, e.g., in temporal logic. Our solution is based on nonlinear programming and we address the inherent scalability issue by presenting alternative methods. We assess the feasibility and the scalability of the approach by an experimental evaluation."
"In system identification, it is often difficult to find a physical intuition to choose a noise model structure. The importance of this choice is that, for the prediction error method (PEM) to provide asymptotically efficient estimates, the model orders must be chosen according to the true system. However, if only the plant estimates are of interest and the experiment is performed in open loop, the noise model may be over-parameterized without affecting the asymptotic properties of the plant. The limitation is that, as PEM suffers in general from non-convexity, estimating an unnecessarily large number of parameters will increase the chances of getting trapped in local minima. To avoid this, a high order ARX model can first be estimated by least squares, providing non-parametric estimates of the plant and noise model. Then, model order reduction can be used to obtain a parametric model of the plant only. We review existing methods to perform this, pointing out limitations and connections between them. Then, we propose a method that connects favorable properties from the previously reviewed approaches. We show that the proposed method provides asymptotically efficient estimates of the plant with open loop data. Finally, we perform a simulation study, which suggests that the proposed method is competitive with PEM and other similar methods."
"The paper presents a comprehensive closed-form performance analysis framework of multihop cooperative communications as promising schemes in the next generation mmwave systems. As fading channel model, we adopt the advocated Weibull channel for its flexible ability to cover different channel conditions, and supported by many recent measurement campaigns in various emerging 5G scenarios. The analyzed scheme consists of multiple ""Detect-and-Forward"" relays with generalized high-order M-QAM transmissions. The end-to-end performance of the multihop communication is evaluated in terms of outage probability, ergodic capacity, bit error rate, and symbol error rate. For all these metrics, we present exact closed-form expressions and their asymptotic behavior, some in terms of generalized hypergeometric functions. The exactness of our study is set by numerical analysis, and Monte-Carlo simulations assess the accuracy of our obtained results for different system and channel parameters. Moreover, while Fox H and bivariate Fox H functions are not yet implemented in Matlab, we provide new codes for computing these functions in general contexts."
"Biomedical word sense disambiguation (WSD) is an important intermediate task in many natural language processing applications such as named entity recognition, syntactic parsing, and relation extraction. In this paper, we employ knowledge-based approaches that also exploit recent advances in neural word/concept embeddings to improve over the state-of-the-art in biomedical WSD using the MSH WSD dataset as the test set. Our methods involve distant supervision - we do not use any hand-labeled examples for WSD to build our prediction models; however, we employ an existing well known named entity recognition and concept mapping program, MetaMap, to obtain our concept vectors. Over the MSH WSD dataset, our linear time (in terms of numbers of senses and words in the test instance) method achieves an accuracy of 92.24% which is an absolute 3% improvement over the best known results obtained via unsupervised or knowledge-based means. A more expensive approach that we developed relies on a nearest neighbor framework and achieves an accuracy of 94.34%. Employing dense vector representations learned from unlabeled free text has been shown to benefit many language processing tasks recently and our efforts show that biomedical WSD is no exception to this trend. For a complex and rapidly evolving domain such as biomedicine, building labeled datasets for larger sets of ambiguous terms may be impractical. Here we demonstrate that distant supervision that leverages recent advances in representation learning can rival supervised approaches in biomedical WSD."
"Ranking and scoring are ubiquitous. We consider the setting in which an institution, called a ranker, evaluates a set of individuals based on demographic, behavioral or other characteristics. The final output is a ranking that represents the relative quality of the individuals. While automatic and therefore seemingly objective, rankers can, and often do, discriminate against individuals and systematically disadvantage members of protected groups. This warrants a careful study of the fairness of a ranking scheme.   In this paper we propose fairness measures for ranked outputs. We develop a data generation procedure that allows us to systematically control the degree of unfairness in the output, and study the behavior of our measures on these datasets. We then apply our proposed measures to several real datasets, and demonstrate cases of unfairness. Finally, we show preliminary results of incorporating our ranked fairness measures into an optimization framework, and show potential for improving fairness of ranked outputs while maintaining accuracy."
"We describe TrustBase, an architecture that provides certificate-based authentication as an operating system service. TrustBase enforces best practices for certificate validation for all applications and transparently enables existing applications to be strengthened against failures of the CA system. The TrustBase system allows simple deployment of authentication systems that harden the CA system. This enables system administrators, for example, to require certificate revocation checks on all TLS connections, or require STARTTLS for email servers that support it. TrustBase is the first system that is able to secure all TLS traffic, using an approach compatible with all operating systems. We design and evaluate a prototype implementation of TrustBase on Linux, evaluate its security, and demonstrate that it has negligible overhead and universal compatibility with applications. To demonstrate the utility of TrustBase, we have developed six authentication services that strengthen certificate validation for all applications."
"Starting with Aharoni and Linial in 1986, the deficiency delta(F) = c(F) - n(F) >= 1 for minimally unsatisfiable clause-sets F, the difference of the number of clauses and the number of variables, is playing an important role in investigations into the structure of minimal unsatisfiability. The talk belonging to this extended abstract, available at http://cs.swan.ac.uk/~csoliver/papers.html#BORDEAUX2016 , gives a high-level overview on recent developments."
"Purpose: The inverse of a deformation vector field (DVF) is often needed in deformable registration, 4D image reconstruction, and adaptive radiation therapy. This study aims at improving both the accuracy with respect to inverse consistency and efficiency of the numerical DVF inversion by developing a fixed-point iteration method with feedback control.   Method: We introduce an iterative method with active feedback control for DVF inversion. The method is built upon a previous fixed-point iteration method, which is represented as a particular passive instance in the new method. At each iteration step, we measure the inconsistency, namely the residual, between the iterative inverse estimate and the input DVF. The residual is modulated by a feedback control mechanism before being incorporated into the next iterate. The feedback control design is based on analysis of error propagation in the iteration process. The design goal is to make the convergence region as large as possible, and to make estimate errors vanish faster. The feedback control design is assessed with two data sets: an analytic DVF pair, and a DVF between two phases of a 4D XCAT phantom.   Results: The single-parameter feedback control improved both the convergence region and convergence rate of the iterative algorithm, for both datasets. With the analytic data, the iteration becomes convergent over the entire image domain, whereas the the precursor method diverges as the deformation becomes larger. With the XCAT DVF data, feedback control reduced the 95th percentile of residuals from 1 mm to 1E-6 mm. Additionally, convergence rate was accelerated by at least a factor of 2 for both datasets.   Conclusion: The introduced method shows the previously unexplored possibility in exercising active feedback control in DVF inversion, and the unexploited potential in improving both numerical accuracy and computational efficiency."
"Gang affiliates have joined the masses who use social media to share thoughts and actions publicly. Interestingly, they use this public medium to express recent illegal actions, to intimidate others, and to share outrageous images and statements. Agencies able to unearth these profiles may thus be able to anticipate, stop, or hasten the investigation of gang-related crimes. This paper investigates the use of word embeddings to help identify gang members on Twitter. Building on our previous work, we generate word embeddings that translate what Twitter users post in their profile descriptions, tweets, profile images, and linked YouTube content to a real vector format amenable for machine learning classification. Our experimental results show that pre-trained word embeddings can boost the accuracy of supervised learning algorithms trained over gang members social media posts."
"In this paper we present a broad overview of the last 40 years of research on cognitive architectures. Although the number of existing architectures is nearing several hundred, most of the existing surveys do not reflect this growth and focus on a handful of well-established architectures. While their contributions are undeniable, they represent only a part of the research in the field. Thus, in this survey we wanted to shift the focus towards a more inclusive and high-level overview of the research in cognitive architectures. Our final set of 86 architectures includes 55 that are still actively developed, and borrow from a diverse set of disciplines, spanning areas from psychoanalysis to neuroscience. To keep the length of this paper within reasonable limits we discuss only the core cognitive abilities, such as perception, attention mechanisms, learning and memory structure. To assess the breadth of practical applications of cognitive architectures we gathered information on over 700 practical projects implemented using the cognitive architectures in our list. We use various visualization techniques to highlight overall trends in the development of the field. Our analysis of practical applications shows that most architectures are very narrowly focused on a particular application domain. Furthermore, there is an apparent gap between general research in robotics and computer vision and research in these areas within the cognitive architectures field. It is very clear that biologically inspired models do not have the same range and efficiency compared to the systems based on engineering principles and heuristics. Another observation is related to a general lack of collaboration. Several factors hinder communication, such as the closed nature of the individual projects (only one-third of the reviewed here architectures are open-source) and terminological differences."
"Despite the fact that different objects possess distinct class-specific features, they also usually share common patterns. This observation has been exploited partially in a recently proposed dictionary learning framework by separating the particularity and the commonality (COPAR). Inspired by this, we propose a novel method to explicitly and simultaneously learn a set of common patterns as well as class-specific features for classification with more intuitive constraints. Our dictionary learning framework is hence characterized by both a shared dictionary and particular (class-specific) dictionaries. For the shared dictionary, we enforce a low-rank constraint, i.e. claim that its spanning subspace should have low dimension and the coefficients corresponding to this dictionary should be similar. For the particular dictionaries, we impose on them the well-known constraints stated in the Fisher discrimination dictionary learning (FDDL). Further, we develop new fast and accurate algorithms to solve the subproblems in the learning step, accelerating its convergence. The said algorithms could also be applied to FDDL and its extensions. The efficiencies of these algorithms are theoretically and experimentally verified by comparing their complexities and running time with those of other well-known dictionary learning methods. Experimental results on widely used image datasets establish the advantages of our method over state-of-the-art dictionary learning methods."
"Debugging is difficult. Recent studies show that automatic bug localization techniques have limited usefulness. One of the reasons is that programmers typically have to understand why the program fails before fixing it. In this work, we aim to help programmers understand a bug by automatically generating likely invariants which are violated in the failed tests. Given a program with an initial assertion and at least one test case failing the assertion, we first generate random test cases, identify potential bug locations through bug localization, and then generate program state mutation based on active learning techniques to identify a predicate ""explaining"" the cause of the bug. The predicate is a classifier for the passed test cases and failed test cases. Our main contribution is the application of invariant learning for bug explanation, as well as a novel approach to overcome the problem of lack of test cases in practice. We apply our method to real-world bugs and show the generated invariants are often correlated to the actual bug fixes."
"In this paper, we discuss structure learning of causal networks from multiple data sets obtained by external intervention experiments where we do not know what variables are manipulated. For example, the conditions in these experiments are changed by changing temperature or using drugs, but we do not know what target variables are manipulated by the external interventions. From such data sets, the structure learning becomes more difficult. For this case, we first discuss the identifiability of causal structures. Next we present a graph-merging method for learning causal networks for the case that the sample sizes are large for these interventions. Then for the case that the sample sizes of these interventions are relatively small, we propose a data-pooling method for learning causal networks in which we pool all data sets of these interventions together for the learning. Further we propose a re-sampling approach to evaluate the edges of the causal network learned by the data-pooling method. Finally we illustrate the proposed learning methods by simulations."
"Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation.   Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling.   So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice."
"Different from traditional point target tracking systems assuming that a target generates at most one single measurement per scan, there exists a class of multipath target tracking systems where each measurement may originate from the interested target via one of multiple propagation paths or from clutter, while the correspondence among targets, measurements, and propagation paths is unknown. The performance of multipath target tracking systems can be improved if multiple measurements from the same target are effectively utilized, but suffers from two major challenges. The first is multipath detection that detects appearing and disappearing targets automatically, while one target may produce $s$ tracks for $s$ propagation paths. The second is multipath tracking that calculates the target-to-measurement-to-path assignment matrices to estimate target states, which is computationally intractable due to the combinatorial explosion. Based on variational Bayesian framework, this paper introduces a novel probabilistic joint detection and tracking algorithm (JDT-VB) that incorporates data association, path association, state estimation and automatic track management. The posterior probabilities of these latent variables are derived in a closed-form iterative manner, which is effective for dealing with the coupling issue of multipath data association identification risk and state estimation error. Loopy belief propagation (LBP) is exploited to approximate the multipath data association, which significantly reduces the computational cost. The proposed JDT-VB algorithm can simultaneously deal with the track initiation, maintenance, and termination for multiple multipath target tracking with time-varying number of targets, and its performance is verified by a numerical simulation of over-the-horizon radar."
"The past few years have witnessed increasing research interest on covariance-based feature representation. A variety of methods have been proposed to boost its efficacy, with some recent ones resorting to nonlinear kernel technique. Noting that the essence of this feature representation is to characterise the underlying structure of visual features, this paper argues that an equally, if not more, important approach to boosting its efficacy shall be to improve the quality of this characterisation. Following this idea, we propose to exploit the structure sparsity of visual features in skeletal human action recognition, and compute sparse inverse covariance estimate (SICE) as feature representation. We discuss the advantage of this new representation on dealing with small sample, high dimensionality, and modelling capability. Furthermore, utilising the monotonicity property of SICE, we efficiently generate a hierarchy of SICE matrices to characterise the structure of visual features at different sparsity levels, and two discriminative learning algorithms are then developed to adaptively integrate them to perform recognition. As demonstrated by extensive experiments, the proposed representation leads to significantly improved recognition performance over the state-of-the-art comparable methods. In particular, as a method fully based on linear technique, it is comparable or even better than those employing nonlinear kernel technique. This result well demonstrates the value of exploiting structure sparsity for covariance-based feature representation."
"In this paper, we take a new look at the possibilistic c-means (PCM) and adaptive PCM (APCM) clustering algorithms from the perspective of uncertainty. This new perspective offers us insights into the clustering process, and also provides us greater degree of flexibility. We analyze the clustering behavior of PCM-based algorithms and introduce parameters $\sigma_v$ and $\alpha$ to characterize uncertainty of estimated bandwidth and noise level of the dataset respectively. Then uncertainty (fuzziness) of membership values caused by uncertainty of the estimated bandwidth parameter is modeled by a conditional fuzzy set, which is a new formulation of the type-2 fuzzy set. Experiments show that parameters $\sigma_v$ and $\alpha$ make the clustering process more easy to control, and main features of PCM and APCM are unified in this new clustering framework (UPCM). More specifically, UPCM reduces to PCM when we set a small $\alpha$ or a large $\sigma_v$, and UPCM reduces to APCM when clusters are confined in their physical clusters and possible cluster elimination are ensured. Finally we present further researches of this paper."
"The estimation of grayscale images using their single-bit zero mean Gaussian noise-affected pixels is presented in this paper. The images are assumed to be bandlimited in the Fourier Cosine transform (FCT) domain. The images are oversampled over their Nyquist rate in the FCT domain. We propose a non-recursive approach based on first order approximation of Cumulative Distribution Function (CDF) to estimate the image from single bit pixels which itself is based on Banach's contraction theorem. The decay rate for mean squared error of estimating such images is found to be independent of the precision of the quantizer and it varies as $O(1/N)$ where $N$ is the ""effective"" oversampling ratio with respect to the Nyquist rate in the FCT domain."
"We consider the problem of transfer learning in an online setting. Different tasks are presented sequentially and processed by a within-task algorithm. We propose a lifelong learning strategy which refines the underlying data representation used by the within-task algorithm, thereby transferring information from one task to the next. We show that when the within-task algorithm comes with some regret bound, our strategy inherits this good property. Our bounds are in expectation for a general loss function, and uniform for a convex loss. We discuss applications to dictionary learning and finite set of predictors. In the latter case, we improve previous $O(1/\sqrt{m})$ bounds to $O(1/m)$ where $m$ is the per task sample size."
"Big data holds critical importance in the current generation of information technology, with applications ranging from financial, industrial, academic to defense sectors. With the exponential rise of open source data from social media and increasing government monitoring, big data is now also linked with national security, and subsequently to the intelligence community. In this study I review the scope of big data sciences in the functioning of intelligence community. The major part of my study focuses on the inherent limitations of big data, which affects the intelligence agencies from gathering of information to anticipating surprises. The limiting factors range from technical to ethical issues connected with big data. My study concludes the need of experts with domain knowledge from intelligence community to efficiently guide big data analysis for timely filling the knowledge gaps. As a case study on limitations of using big data, I narrate some of the ongoing work in nuclear intelligence using simple analytics and argue on why big data analysis in that case would lead to unnecessary complications. For further investigation, I highlight cases of crowdsource forecasting tournaments and predicting unrest from social media."
This paper presents the Voronoi diagram-based evolutionary algorithm (VorEAl). VorEAl partitions input space in abnormal/normal subsets using Voronoi diagrams. Diagrams are evolved using a multi-objective bio-inspired approach in order to conjointly optimize classification metrics while also being able to represent areas of the data space that are not present in the training dataset. As part of the paper VorEAl is experimentally validated and contrasted with similar approaches.
"Principal component analysis (PCA) is a widespread technique for data analysis that relies on the covariance-correlation matrix of the analyzed data. However to properly work with high-dimensional data, PCA poses severe mathematical constraints on the minimum number of different replicates or samples that must be included in the analysis. Here we show that a modified algorithm works not only on well dimensioned datasets, but also on degenerated ones."
"Recent research on deep learning, a set of machine learning techniques able to learn deep architectures, has shown how robotic perception and action greatly benefits from these techniques. In terms of spacecraft navigation and control system, this suggests that deep architectures may be considered now to drive all or part of the on-board decision making system. In this paper this claim is investigated in more detail training deep artificial neural networks to represent the optimal control action during a pinpoint landing, assuming perfect state information. It is found to be possible to train deep networks for this purpose and that the resulting landings, driven by the trained networks, are close to simulated optimal ones. These results allow for the design of an on-board real time optimal control system able to cope with large sets of possible initial states while still producing an optimal response."
"We propose a randomized polynomial time algorithm for computing nontrivial zeros of quadratic forms in 4 or more variables over $\mathbb{F}_q(t)$, where $\mathbb{F}_q$ is a finite field of odd characteristic. The algorithm is based on a suitable splitting of the form into two forms and finding a common value they both represent. We make use of an effective formula on the number of fixed degree irreducible polynomials in a given residue class. We apply our algorithms for computing a Witt decomposition of a quadratic form, for computing an explicit isometry between quadratic forms and finding zero divisors in quaternion algebras over quadratic extensions of $\mathbb{F}_q(t)$."
"We propose a general framework to build certified proofs of distributed self-stabilizing algorithms with the proof assistant Coq. We first define in Coq the locally shared memory model with composite atomicity, the most commonly used model in the self-stabilizing area. We then validate our framework by certifying a non trivial part of an existing silent self-stabilizing algorithm which builds a k-clustering of the network. We also certified a quantitative property related to the output of this algorithm. Precisely, we show that the computed k-clustering contains at most floor( (n-1)/(k+1) )+ 1 clusterheads, where n is the number of nodes in the network. To obtain these results, we also developed a library which contains general tools related to potential functions and cardinality of sets."
"Digital traces of conversations in micro-blogging platforms and OSNs provide information about user opinion with a high degree of resolution. These information sources can be exploited to under- stand and monitor collective behaviors. In this work, we focus on polarization classes, i.e., those topics that require the user to side exclusively with one position. The proposed method provides an iterative classification of users and keywords: first, polarized users are identified, then polarized keywords are discovered by monitoring the activities of previously classified users. This method thus allows tracking users and topics over time. We report several experiments conducted on two Twitter datasets during political election time-frames. We measure the user classification accuracy on a golden set of users, and analyze the relevance of the extracted keywords for the ongoing political discussion."
"In this paper, we investigate the two-hop full-duplex (FD) relay channel with self-interference and fading, which is comprised of a source, an FD relay, and a destination, where a direct source-destination link does not exist and the FD relay is impaired by self-interference. For this channel, we propose three buffer-aided relaying schemes with adaptive reception-transmission at the FD relay for the cases when the source and the relay both perform adaptive-power allocation, fixed-power allocation, and fixed-rate transmission, respectively. The proposed buffer-aided relaying schemes significantly improve the achievable rate and the throughput of the considered relay channel by enabling the FD relay to adaptively select to either receive, transmit, or simultaneously receive and transmit in a given time slot based on the qualities of the receiving, transmitting, and self-interference channels. Our numerical results show that significant performance gains are achieved using the proposed buffer-aided relaying schemes compared to conventional FD relaying, where the FD relay is forced to always simultaneously receive and transmit, and to buffer-aided half-duplex relaying, where the half-duplex relay cannot simultaneously receive and transmit."
"This paper presents a new condition, the fully physical consistency for a set of inertial parameters to determine if they can be generated by a physical rigid body. The proposed condition ensure both the positive definiteness and the triangular inequality of 3D inertia matrices as opposed to existing techniques in which the triangular inequality constraint is ignored. This paper presents also a new parametrization that naturally ensures that the inertial parameters are fully physical consistency. The proposed parametrization is exploited to reformulate the inertial identification problem as a manifold optimization problem, that ensures that the identified parameters can always be generated by a physical body. The proposed optimization problem has been validated with a set of experiments on the iCub humanoid robot."
"We present a new probabilistic model checker Storm. Using state-of-the-art libraries, we aim for both high performance and versatility. This extended abstract gives a brief overview of the features of Storm."
"Virtual switches have become popular among cloud operating systems to interconnect virtual machines in a more flexible manner. However, this paper demonstrates that virtual switches introduce new attack surfaces in cloud setups, whose effects can be disastrous. Our analysis shows that these vulnerabilities are caused by: (1) inappropriate security assumptions (privileged virtual switch execution in kernel and user space), (2) the logical centralization of such networks (e.g., OpenStack or SDN), (3) the presence of bi-directional communication channels between data plane systems and the centralized controller, and (4) non-standard protocol parsers.   Our work highlights the need to accommodate the data plane(s) in our threat models. In particular, it forces us to revisit today's assumption that the data plane can only be compromised by a sophisticated attacker: we show that compromising the data plane of modern computer networks can actually be performed by a very simple attacker with limited resources only and at low cost (i.e., at the cost of renting a virtual machine in the Cloud). As a case study, we fuzzed only 2\% of the code-base of a production quality virtual switch's packet processor (namely OvS), identifying serious vulnerabilities leading to unauthenticated remote code execution. In particular, we present the ""reign worm"" which allows us to fully compromise test-setups in less than 100 seconds. We also evaluate the performance overhead of existing mitigations such as ASLR, PIEs, and unconditional stack canaries on OvS. We find that while applying these countermeasures in kernel-space incurs a significant overhead, in user-space the performance overhead is negligible."
"The 21st century belongs to the world of computing, specially as a result of the so-called cloud computing. This technology enables ubiquitous information management and thus people can access all their data from any place and at any time. In this landscape, the emergence of cloud storage has had an important role in the last five years. Nowadays, several free-access public cloud storage services make it possible for users to have a free backup of their assets and to manage and share them, representing a low-cost opportunity for Small and Medium Companies (SME). However, the adoption of cloud storage involves data outsourcing, so a user does not have the guarantee about the way her data will be processed and protected. Therefore, it seems necessary to endow public cloud storage with a set of means to protect users' confidentiality and privacy, to assess data integrity and to guarantee a proper backup of information assets. Along this paper we discuss the main challenges to achieve such a goal, underlining the set of functionalities already implemented in the most popular public cloud storage services."
"The Lloyd-Max algorithm is a classical approach to perform K-means clustering. Unfortunately, its cost becomes prohibitive as the training dataset grows large. We propose a compressive version of K-means (CKM), that estimates cluster centers from a sketch, i.e. from a drastically compressed representation of the training dataset. We demonstrate empirically that CKM performs similarly to Lloyd-Max, for a sketch size proportional to the number of cen-troids times the ambient dimension, and independent of the size of the original dataset. Given the sketch, the computational complexity of CKM is also independent of the size of the dataset. Unlike Lloyd-Max which requires several replicates, we further demonstrate that CKM is almost insensitive to initialization. For a large dataset of 10^7 data points, we show that CKM can run two orders of magnitude faster than five replicates of Lloyd-Max, with similar clustering performance on artificial data. Finally, CKM achieves lower classification errors on handwritten digits classification."
"Finding the common structural features of two molecules is a fundamental task in cheminformatics. Most drugs are small molecules, which can naturally be interpreted as graphs. Hence, the task is formalized as maximum common subgraph problem. Albeit the vast majority of molecules yields outerplanar graphs this problem remains NP-hard.   We consider a variation of the problem of high practical relevance, where the rings of molecules must not be broken, i.e., the block and bridge structure of the input graphs must be retained by the common subgraph. We present an algorithm for finding a maximum common connected induced subgraph of two given outerplanar graphs subject to this constraint. Our approach runs in time $\mathcal{O}(\Delta n^2)$ in outerplanar graphs on $n$ vertices with maximum degree $\Delta$. This leads to a quadratic time complexity in molecular graphs, which have bounded degree. The experimental comparison on synthetic and real-world datasets shows that our approach is highly efficient in practice and outperforms comparable state-of-the-art algorithms."
"As collecting huge amounts of personal data from individuals has been established as a standard nowadays, it is really important to use these data in a conscientious way. For example, when performing inference using these data, one has to make sure individuals' identities or the privacy of the data are not compromised. Differential privacy is a powerful framework that introduces stochasticity into the computation to guarantee that it is difficult to breach the privacy using the output of the computation. Differentially private versions of many important machine learning methods have been proposed, but still there is a long way to pave towards an efficient unified approach applicable to handle many models. In this paper, we propose a differentially private variational inference method with a very wide applicability. The variational inference is based on stochastic gradient ascent and can handle non-conjugate models as well as conjugate ones. Differential privacy is achieved by perturbing the gradients. We explore ways to make the algorithm more efficient through privacy amplification from subsampling and through clipping the gradients to limit the amount of information they leak. We explore the effect of different parameter combinations in logistic regression problems where the method can reach an accuracy close to non-private level under reasonably strong privacy guarantees."
"We report a light-field based method that allows the optical encryption of three-dimensional (3D) volumetric information at the microscopic scale in a single 2D light-field image. The system consists of a microlens array and an array of random phase/amplitude masks. The method utilizes a wave optics model to account for the dominant diffraction effect at this new scale, and the system point-spread function (PSF) serves as the key for encryption and decryption. We successfully developed and demonstrated a deconvolution algorithm to retrieve spatially multiplexed discrete and continuous volumetric data from 2D light-field images. Showing that the method is practical for data transmission and storage, we obtained a faithful reconstruction of the 3D volumetric information from a digital copy of the encrypted light-field image. The method represents a new level of optical encryption, paving the way for broad industrial and biomedical applications in processing and securing 3D data at the microscopic scale."
"Extracting entities and relations for types of interest from text is important for understanding massive text corpora. Traditionally, systems of entity relation extraction have relied on human-annotated corpora for training and adopted an incremental pipeline. Such systems require additional human expertise to be ported to a new domain, and are vulnerable to errors cascading down the pipeline. In this paper, we investigate joint extraction of typed entities and relations with labeled data heuristically obtained from knowledge bases (i.e., distant supervision). As our algorithm for type labeling via distant supervision is context-agnostic, noisy training data poses unique challenges for the task. We propose a novel domain-independent framework, called CoType, that runs a data-driven text segmentation algorithm to extract entity mentions, and jointly embeds entity mentions, relation mentions, text features and type labels into two low-dimensional spaces (for entity and relation mentions respectively), where, in each space, objects whose types are close will also have similar representations. CoType, then using these learned embeddings, estimates the types of test (unlinkable) mentions. We formulate a joint optimization problem to learn embeddings from text corpora and knowledge bases, adopting a novel partial-label loss function for noisy labeled data and introducing an object ""translation"" function to capture the cross-constraints of entities and relations on each other. Experiments on three public datasets demonstrate the effectiveness of CoType across different domains (e.g., news, biomedical), with an average of 25% improvement in F1 score compared to the next best method."
"The perspective we take on a system determines the features and properties of this system that we are focusing on. It determines where we search for causes to explain the effects on the system that we observe. It determines the terms in which we expect the information about the system to be expressed. And it can also influence the choice of formalism that will be used to convey the information. using Boolean Automata Networks as prototypes of interaction systems, this paper means to start making these considerations concrete in order to draw a practical benefit out of them."
"Causality is omnipresent in scientists' verbalisations of their understanding, even though we have no formal consensual scientific definition for it. In Automata Networks, it suffices to say that automata ""influence"" one another to introduce a notion of causality. One might argue that this merely is an incidental side effect of preferring statements expressed in natural languages to mathematical formulae. The discussion of this paper shows that if this is the case, then it is worth considering the effects of those preferences on the contents of the statements we make and the formulae we derive. And if it is not the case, that causality is a mere incidental side effect of our preferences of formulation, then causality must be worth some scientific attention per se. In any case, the paper illustrates how the innate sense of causality we have may be made deliberate and formal use of without having to pin down the elusive notion of causality to anything fixed and formal that wouldn't do justice to the wide range of ways it is involved in science-making."
"Linear rules have played an increasing role in structural proof theory in recent years. It has been observed that the set of all sound linear inference rules in Boolean logic is already coNP-complete, i.e. that every Boolean tautology can be written as a (left- and right-)linear rewrite rule. In this paper we study properties of systems consisting only of linear inferences. Our main result is that the length of any 'nontrivial' derivation in such a system is bound by a polynomial. As a consequence there is no polynomial-time decidable sound and complete system of linear inferences, unless coNP=NP. We draw tools and concepts from term rewriting, Boolean function theory and graph theory in order to access some required intermediate results. At the same time we make several connections between these areas that, to our knowledge, have not yet been presented and constitute a rich theoretical framework for reasoning about linear TRSs for Boolean logic."
"Nowadays the composition and formation of effective teams is highly important for both companies to assure their competitiveness and for a wide range of emerging applications exploiting multiagent collaboration (e.g. crowdsourcing, human-agent collaborations). The aim of this article is to provide an integrative perspective on team composition, team formation and their relationship with team performance. Thus, we review the contributions in both the computer science literature and the organisational psychology literature dealing with these topics. Our purpose is twofold. First, we aim at identifying the strengths and weaknesses of the contributions made by these two diverse bodies of research. Second, we pursue to identify cross-fertilisation opportunities that help both disciplines benefit from one another. Given the volume of existing literature, our review is not intended to be exhaustive. Instead, we have preferred to focus on the most significant contributions in both fields together with recent contributions that break new ground to spur innovative research."
"Potholes though seem inconsequential, may cause accidents resulting in loss of human life. In this paper, we present an automated system to efficiently manage the potholes in a ward by deploying geotagging and image processing techniques that overcomes the drawbacks associated with the existing survey-oriented systems. Image processing is used for identification of target pothole regions in the 2D images using edge detection and morphological image processing operations. A method is developed to accurately estimate the dimensions of the potholes from their images, analyze their area and depth, estimate the quantity of filling material required and therefore enabling pothole attendance on a priority basis. This will further enable the government official to have a fully automated system for effectively managing pothole related disasters."
"Frameshift translation is an important phenomenon that contributes to the appearance of novel Coding DNA Sequences (CDS) and functions in gene evolution, by allowing alternative amino acid translations of genes coding regions. Frameshift translations can be identified by aligning two CDS, from a same gene or from homologous genes, while accounting for their codon structure. Two main classes of algorithms have been proposed to solve the problem of aligning CDS, either by amino acid sequence alignment back-translation, or by simultaneously accounting for the nucleotide and amino acid levels. The former does not allow to account for frameshift translations and up to now, the latter exclusively accounts for frameshift translation initiation, not accounting for the length of the translation disruption caused by a frameshift.   Here, we introduce a new scoring scheme with an algorithm for the pairwise alignment of CDS accounting for frameshift translation initiation and length, while simultaneously accounting for nucleotide and amino acid sequences. We compare the method to other CDS alignment methods based on an application to the comparison of pairs of CDS from homologous \emph{human}, \emph{mouse} and \emph{cow} genes of ten mammalian gene families from the Ensembl-Compara database. The results show that our method is particularly robust to parameter changes as compared to existing methods. It also appears to be a good compromise, performing well both in the presence and absence of frameshift translations between the CDS. An implementation of the method is available at https://github.com/UdeS-CoBIUS/FsePSA."
"Since the Bin Packing Problem (BPP) is one of the main NP-hard problems, a lot of approximation algorithms have been suggested for it. It has been proven that the best algorithm for BPP has the approximation ratio of 3/2 and the time order of O(n), unless P=NP. In the current paper, a linear 3/2-approximation algorithm is presented. The suggested algorithm not only has the best possible theoretical factors, approximation ratio, space order, and time order, but also outperforms the other approximation algorithms according to the experimental results, therefore, we are able to draw the conclusion that the algorithms is the best approximation algorithm which has been presented for the problem until now. Key words: Approximation Algorithm, Bin Packing Problem, Approximation Ratio, NP-hard."
"In many research works, there has been an orientation to studying and developing many of the applications of public-key cryptography to secure the data while transmitting in the systems, In this paper we present an approach to encrypt and decrypt the message text according to the ASCII(American Standard Code for Information Interchange) and RSA algorithm by converting the message text into binary representation and dividing this representation to bytes(8s of 0s and 1s) and applying a bijective function between the group of those bytes and the group of characters of ASCII and then using this mechanism to be compatible with using RSA algorithm, finally, Java application was built to apply this approach directly."
"Over the last decade block-structured adaptive mesh refinement (SAMR) has found increasing use in large, publicly available codes and frameworks. SAMR frameworks have evolved along different paths. Some have stayed focused on specific domain areas, others have pursued a more general functionality, providing the building blocks for a larger variety of applications. In this survey paper we examine a representative set of SAMR packages and SAMR-based codes that have been in existence for half a decade or more, have a reasonably sized and active user base outside of their home institutions, and are publicly available. The set consists of a mix of SAMR packages and application codes that cover a broad range of scientific domains. We look at their high-level frameworks, and their approach to dealing with the advent of radical changes in hardware architecture. The codes included in this survey are BoxLib, Cactus, Chombo, Enzo, FLASH, and Uintah."
"Supervised dimensionality reduction has emerged as an important theme in the last decade. Despite the plethora of models and formulations, there is a lack of a simple model which aims to project the set of patterns into a space defined by the classes (or categories). To this end, we set up a model in which each class is represented as a 1D subspace of the vector space formed by the features. Assuming the set of classes does not exceed the cardinality of the features, the model results in multi-class supervised learning in which the features of each class are projected into the class subspace. Class discrimination is automatically guaranteed via the imposition of orthogonality of the 1D class sub-spaces. The resulting optimization problem - formulated as the minimization of a sum of quadratic functions on a Stiefel manifold - while being non-convex (due to the constraints), nevertheless has a structure for which we can identify when we have reached a global minimum. After formulating a version with standard inner products, we extend the formulation to reproducing kernel Hilbert spaces in a straightforward manner. The optimization approach also extends in a similar fashion to the kernel version. Results and comparisons with the multi-class Fisher linear (and kernel) discriminants and principal component analysis (linear and kernel) showcase the relative merits of this approach to dimensionality reduction."
"Go is a production-level statically typed programming language whose design features explicit message-passing primitives and lightweight threads, enabling (and encouraging) programmers to develop concurrent systems where components interact through communication more so than by lock-based shared memory concurrency. Go can only detect global deadlocks at runtime, but provides no compile-time protection against all too common communication mis-matches or partial deadlocks. This work develops a static verification framework for liveness and safety in Go programs, able to detect communication errors and partial deadlocks in a general class of realistic concurrent programs, including those with dynamic channel creation, unbounded thread creation and recursion. Our approach infers from a Go program a faithful representation of its communication patterns as a behavioural type. By checking a syntactic restriction on channel usage, dubbed fencing, we ensure that programs are made up of finitely many different communication patterns that may be repeated infinitely many times. This restriction allows us to implement a decision procedure for liveness and safety in types which in turn statically ensures liveness and safety in Go programs. We have implemented a type inference and decision procedures in a tool-chain and tested it against publicly available Go programs."
"We define a game semantics for second order classical arithmetic PA2 (with quantifiers over predicates on integers and full comprehension axiom). Our semantics is effective: moves are described by a finite amount of information and whenever there is some winning strategy for the player defending the truth of the formula, then there is some primitive recursive winning strategy. Then we show that our game semantics is sound and complete for the truth assignment for formulas of PA2. In our game model, the value of a predicate variable is some family of ""generic"" games. This value is ""unknown"" during the play, but at the end of the play it is used by a ""judge of the play"" to decide who is the winner."
"The tool presence detection challenge at M2CAI 2016 consists of identifying the presence/absence of seven surgical tools in the images of cholecystectomy videos. Here, we propose to use deep architectures that are based on our previous work where we presented several architectures to perform multiple recognition tasks on laparoscopic videos. In this technical report, we present the tool presence detection results using two architectures: (1) a single-task architecture designed to perform solely the tool presence detection task and (2) a multi-task architecture designed to perform jointly phase recognition and tool presence detection. The results show that the multi-task network only slightly improves the tool presence detection results. In constrast, a significant improvement is obtained when there are more data available to train the networks. This significant improvement can be regarded as a call for action for other institutions to start working toward publishing more datasets into the community, so that better models could be generated to perform the task."
"Objective: In this paper, we develop a personalized real-time risk scoring algorithm that provides timely and granular assessments for the clinical acuity of ward patients based on their (temporal) lab tests and vital signs; the proposed risk scoring system ensures timely intensive care unit (ICU) admissions for clinically deteriorating patients. Methods: The risk scoring system learns a set of latent patient subtypes from the offline electronic health record data, and trains a mixture of Gaussian Process (GP) experts, where each expert models the physiological data streams associated with a specific patient subtype. Transfer learning techniques are used to learn the relationship between a patient's latent subtype and her static admission information (e.g. age, gender, transfer status, ICD-9 codes, etc). Results: Experiments conducted on data from a heterogeneous cohort of 6,321 patients admitted to Ronald Reagan UCLA medical center show that our risk score significantly and consistently outperforms the currently deployed risk scores, such as the Rothman index, MEWS, APACHE and SOFA scores, in terms of timeliness, true positive rate (TPR), and positive predictive value (PPV). Conclusion: Our results reflect the importance of adopting the concepts of personalized medicine in critical care settings; significant accuracy and timeliness gains can be achieved by accounting for the patients' heterogeneity. Significance: The proposed risk scoring methodology can confer huge clinical and social benefits on more than 200,000 critically ill inpatient who exhibit cardiac arrests in the US every year."
"A transfer learning method for generating features suitable for surgical tools and phase recognition from the ImageNet classification features [1] is proposed here. In addition, methods are developed for generating contextual features and combining them with time series analysis for final classification using multi-class random forest. The proposed pipeline is tested over the training and testing datasets of M2CAI16 challenges: tool and phase detection. Encouraging results are obtained by leave-one-out cross validation evaluation on the training dataset."
"Positive-definite kernel functions are fundamental elements of kernel methods and Gaussian processes. A well-known construction of such functions comes from Bochner's characterization, which connects a positive-definite function with a probability distribution. Another construction, which appears to have attracted less attention, is Polya's criterion that characterizes a subset of these functions. In this paper, we study the latter characterization and derive a number of novel kernels little known previously.   In the context of large-scale kernel machines, Rahimi and Recht (2007) proposed a random feature map (random Fourier) that approximates a kernel function, through independent sampling of the probability distribution in Bochner's characterization. The authors also suggested another feature map (random binning), which, although not explicitly stated, comes from Polya's characterization. We show that with the same number of random samples, the random binning map results in an Euclidean inner product closer to the kernel than does the random Fourier map. The superiority of the random binning map is confirmed empirically through regressions and classifications in the reproducing kernel Hilbert space."
"We propose the Hit-and-Run algorithm for planning and sampling problems in non-convex spaces. For sampling, we show the first analysis of the Hit-and-Run algorithm in non-convex spaces and show that it mixes fast as long as certain smoothness conditions are satisfied. In particular, our analysis reveals an intriguing connection between fast mixing and the existence of smooth measure-preserving mappings from a convex space to the non-convex space. For planning, we show advantages of Hit-and-Run compared to state-of-the-art planning methods such as Rapidly-Exploring Random Trees."
"We obtain continuity bounds for basic information characteristics of quantum channels depending on their input dimension (when it is finite) and on the maximal level of input energy (when the input dimension is infinite).   First we prove continuity bounds for the output conditional mutual information for a single channel and for $n$ copies of a channel.   Then we obtain estimates for variation of the output Holevo quantity with respect to simultaneous variations of a channel and of an input ensemble.   As a result tight and close-to-tight continuity bounds for basic capacities of quantum channels depending on the input dimension are obtained. They complement the Leung-Smith continuity bounds depending on the output dimension.   Finally, continuity bounds for basic capacities of infinite-dimensional channels with the input energy constraints are proved. They show uniform continuity of these capacities on the set of all channels with respect to the norm of complete boundedness."
"CNNs have massively improved performance in object detection in photographs. However research into object detection in artwork remains limited. We show state-of-the-art performance on a challenging dataset, People-Art, which contains people from photos, cartoons and 41 different artwork movements. We achieve this high performance by fine-tuning a CNN for this task, thus also demonstrating that training CNNs on photos results in overfitting for photos: only the first three or four layers transfer from photos to artwork. Although the CNN's performance is the highest yet, it remains less than 60\% AP, suggesting further work is needed for the cross-depiction problem. The final publication is available at Springer via http://dx.doi.org/10.1007/978-3-319-46604-0_57"
"In this paper, we revisit the generalized block power methods for approximating the eigenvector associated with $\lambda_1 = 1$ of a Markov chain transition matrix. Our analysis of the block power method shows that when $s$ linearly independent probability vectors are used as the initial block, the convergence of the block power method to the stationary distribution depends on the magnitude of the $(s+1)$th dominant eigenvalue $\lambda_{s+1}$ of $P$ instead of that of $\lambda_2$ in the power method. Therefore, the block power method with block size $s$ is particularly effective for transition matrices where $|\lambda_{s+1}|$ is well separated from $\lambda_1 = 1$ but $|\lambda_2|$ is not. This approach is particularly useful when visiting the elements of a large transition matrix is the main computational bottleneck over matrix--vector multiplications, where the block power method can effectively reduce the total number of times to pass over the matrix. To further reduce the overall computational cost, we combine the block power method with a sliding window scheme, taking advantage of the subsequent vectors of the latest $s$ iterations to assemble the block matrix. The sliding window scheme correlates vectors in the sliding window to quickly remove the influences from the eigenvalues whose magnitudes are smaller than $|\lambda_{s}|$ to reduce the overall number of matrix--vector multiplications to reach convergence. Finally, we compare the effectiveness of these methods in a Markov chain model representing a stochastic luminal calcium release site."
"We prove that triangulated IC-planar and NIC-planar graphs can be recognized in cubic time.   A graph is 1-planar if it can be drawn in the plane with at most one crossing per edge. A drawing is IC-planar if, in addition, each vertex is incident to at most one crossing edge and NIC-planar if two pairs of crossing edges share at most one vertex. In a triangulated drawing each face is a triangle. In consequence, planar-maximal and maximal IC-planar and NIC-planar graphs can be recognized in O(n^5) time and maximum and optimal ones in O(n^3) time. In contrast, recognizing 3-connected IC-planar and NIC-planar graphs is NP-complete, even if the graphs are given with a rotation system which describes the cyclic ordering of the edges at each vertex. Our results complement similar ones for 1-planar graphs."
"We consider the actuator placement problem for linear systems. Specifically, we aim to identify an actuator which requires the least amount of control energy to drive the system from an arbitrary initial condition to the origin in the worst case. Said otherwise, we investigate the minimax problem of minimizing the control energy over the worst possible initial conditions. Recall that the least amount of control energy needed to drive a linear controllable system from any initial condition on the unit sphere to the origin is upper-bounded by the inverse of the smallest eigenvalue of the associated controllability Gramian, and moreover, the upper-bound is sharp. The minimax problem can be thus viewed as the optimization problem of minimizing the upper-bound via the placement of an actuator. In spite of its simple and natural formulation, this problem is difficult to solve. In fact, properties such as the stability of the system matrix, which are not related to controllability, now play important roles. We focus in this paper on the special case where the system matrix is positive definite. Under this assumption, we are able to provide a complete solution to the optimal actuator placement problem and highlight the difficulty in solving the general problem."
"This paper studies social interactions in a game theoretic model with players in a large social network. We consider observations from one single equilibrium of a large network game with asymmetric information, in which each player chooses an action from a finite set and is subject to interactions with her friends. Simple assumptions about the structure are made to establish the existence and uniqueness of equilibrium. In particular, we show that the equilibrium strategies satisfy a network decaying dependence (NDD) condition requiring that dependence between any two players' decisions decays with their network distance. The formulation of such an NDD property is novel and serves as the basis for statistical inference. Further, we establish the identification of the structural model and introduce a computationally feasible and efficient estimation method. We illustrate the estimation method with an actual application to college attendance, as well as in Monte Carlo experiments."
"Existing deep embedding methods in vision tasks are capable of learning a compact Euclidean space from images, where Euclidean distances correspond to a similarity metric. To make learning more effective and efficient, hard sample mining is usually employed, with samples identified through computing the Euclidean feature distance. However, the global Euclidean distance cannot faithfully characterize the true feature similarity in a complex visual feature space, where the intraclass distance in a high-density region may be larger than the interclass distance in low-density regions. In this paper, we introduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of learning a similarity metric adaptive to local feature structure. The metric can be used to select genuinely hard samples in a local neighborhood to guide the deep embedding learning in an online and robust manner. The new layer is appealing in that it is pluggable to any convolutional networks and is trained end-to-end. Our local similarity-aware feature embedding not only demonstrates faster convergence and boosted performance on two complex image retrieval datasets, its large margin nature also leads to superior generalization results under the large and open set scenarios of transfer learning and zero-shot learning on ImageNet 2010 and ImageNet-10K datasets."
"We investigate the problem of equilibrium computation for ""large"" $n$-player games. Large games have a Lipschitz-type property that no single player's utility is greatly affected by any other individual player's actions. In this paper, we mostly focus on the case where any change of strategy by a player causes other players' payoffs to change by at most $\frac{1}{n}$. We study algorithms having query access to the game's payoff function, aiming to find $\epsilon$-Nash equilibria. We seek algorithms that obtain $\epsilon$ as small as possible, in time polynomial in $n$.   Our main result is a randomised algorithm that achieves $\epsilon$ approaching $\frac{1}{8}$ for 2-strategy games in a {\em completely uncoupled} setting, where each player observes her own payoff to a query, and adjusts her behaviour independently of other players' payoffs/actions. $O(\log n)$ rounds/queries are required. We also show how to obtain a slight improvement over $\frac{1}{8}$, by introducing a small amount of communication between the players.   Finally, we give extension of our results to large games with more than two strategies per player, and alternative largeness parameters."
"The Stochastic Context Tree (SCOT) is a useful tool for studying infinite random sequences generated by an m-Markov Chain (m-MC). It captures the phenomenon that the probability distribution of the next state sometimes depends on less than m of the preceding states. This allows compressing the information needed to describe an m-MC. The SCOT construction has been earlier used under various names: VLMC, VOMC, PST, CTW. In this paper we study the possibility of reducing the m-MC to a 1-MC on the leaves of the SCOT. Such context trees are called perfect-memory. We give various combinatorial characterizations of perfect-memory context trees and an efficient algorithm to find the minimal perfect-memory extension of a SCOT."
"In this paper, we define visual log of a software system as data capturing the interactions between its users and its graphic user interface (GUI), such as screen-shots and screen recordings. We vision that mining such visual log could be useful for bug reproducing and debugging, automated GUI testing, user interface designing, question answering of common usages in software support, etc. Toward that vision, we propose a core framework for mining visual log of software. This framework focuses on detecting GUI elements and changes in visual log, removing users' private data, recognizing user interactions with GUI elements, and learning GUI usage patterns. We also performed a small study on the characteristics of GUI elements in mobile apps. The findings from this study suggested several heuristics to design techniques for recognizing GUI elements and interactions."
"The damage personal attacks make to online discourse motivates many platforms to try to curb the phenomenon. However, understanding the prevalence and impact of personal attacks in online platforms at scale remains surprisingly difficult. The contribution of this paper is to develop and illustrate a method that combines crowdsourcing and machine learning to analyze personal attacks at scale. We show an evaluation method for a classifier in terms of the aggregated number of crowd-workers it can approximate. We apply our methodology to English Wikipedia, generating a corpus of over 100k high quality human-labeled comments and 63M machine-labeled ones from a classifier that is as good as the aggregate of 3 crowd-workers. Using the corpus of machine-labeled scores, our methodology allows us to explore some of the open questions about the nature of online personal attacks. This reveals that the majority of personal attacks on Wikipedia are not the result of a few malicious users, nor primarily the consequence of allowing anonymous contributions."
"Design matrices are sparse matrices in which the supports of different columns intersect in a few positions. Such matrices come up naturally when studying problems involving point sets with many collinear triples. In this work we consider design matrices with block (or matrix) entries. Our main result is a lower bound on the rank of such matrices, extending the bounds proved in {BDWY12,DSW12} for the scalar case. As a result we obtain several applications in combinatorial geometry. The first application involves extending the notion of structural rigidity (or graph rigidity) to the setting where we wish to bound the number of `degrees of freedom' in perturbing a set of points under collinearity constraints (keeping some family of triples collinear). Other applications are an asymptotically tight Sylvester-Gallai type result for arrangements of subspaces (improving {DH16}) and a new incidence bound for high dimensional line/curve arrangements.   The main technical tool in the proof of the rank bound is an extension of the technique of matrix scaling to the setting of block matrices. We generalize the definition of doubly stochastic matrices to matrices with block entries and derive sufficient conditions for a doubly stochastic scaling to exist."
"The human auditory system is able to distinguish the vocal source of thousands of speakers, yet not much is known about what features the auditory system uses to do this. Fourier Transforms are capable of capturing the pitch and harmonic structure of the speaker but this alone proves insufficient at identifying speakers uniquely. The remaining structure, often referred to as timbre, is critical to identifying speakers but we understood little about it. In this paper we use recent advances in neural networks in order to manipulate the voice of one speaker into another by transforming not only the pitch of the speaker, but the timbre. We review generative models built with neural networks as well as architectures for creating neural networks that learn analogies. Our preliminary results converting voices from one speaker to another are encouraging."
"Many applications in speech, robotics, finance, and biology deal with sequential data, where ordering matters and recurrent structures are common. However, this structure cannot be easily captured by standard kernel functions. To model such structure, we propose expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes. We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic procedure and exploit the structure of these kernels for fast and scalable training and prediction. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application, where the predictive uncertainties provided by GP-LSTM are uniquely valuable."
"In the note two errors in Low and Lapsley's article ""Optimization Flow Control, I: Basic Algorithm and Convergence"", ""IEEE/ACM Transactions on Networking"", 7(6), pp. 861-874, 1999, are shown. Because of these errors the proofs of both theorems presented in the article are incomplete and some assessments are wrong."
"In the note an error in Low and Lapsley's article (""Optimization Flow Control, I: Basic Algorithm and Convergence"", IEEE/ACM Transactions on Networking, 7(6), pp. 861-874, 1999) is pointed out. Because of this error the proof of the Theorem 2 presented in the article is incomplete and some assessments are wrong. In the second part of the note the author proposes a correction to this proof."
"Quantum computations usually take place under the control of the classical world. We introduce a Classically-controlled Quantum Turing Machine (CQTM) which is a Turing Machine (TM) with a quantum tape for acting on quantum data, and a classical transition function for a formalized classical control. In CQTM, unitary transformations and measurements are allowed. We show that any classical TM is simulated by a CQTM without loss of efficiency. The gap between classical and quantum computations, already pointed out in the framework of measurement-based quantum computation is confirmed. To appreciate the similarity of programming classical TM and CQTM, examples are given."
"Given a classical error-correcting block code, the task of quantum list decoding is to produce from any quantumly corrupted codeword a short list containing all messages whose codewords exhibit high ""presence"" in the quantumly corrupted codeword. Efficient quantum list decoders have been used to prove a quantum hardcore property of classical codes. However, the code rates of all known families of efficiently quantum list-decodable codes are, unfortunately, too small for other practical applications. To improve those known code rates, we prove that a specific code family of polynomially small code rate over a fixed code alphabet, obtained by concatenating generalized Reed-Solomon codes as outer codes with Hadamard codes as inner codes, has an efficient quantum list-decoding algorithm if its codewords have relatively high codeword presence in a given quantumly corrupted codeword. As an immediate application, we use the quantum list decodability of this code family to solve a certain form of quantum search problems in polynomial time. When the codeword presence becomes smaller, in contrast, we show that the quantum list decodability of generalized Reed-Solomon codes with high confidence is closely related to the efficient solvability of the following two problems: the noisy polynomial interpolation problem and the bounded distance vector problem. Moreover, assuming that NP is not included in BQP, we also prove that no efficient quantum list decoder exists for the generalized Reed-Solomon codes."
